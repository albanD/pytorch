{"d21ee2de66": {"title": "[wip] Upgrade msvc to 14.13 (#40109)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40109\n\nghstack-source-id: 106426627\n\nTest Plan: oss CI\n\nDifferential Revision: D22072830\n\nfbshipit-source-id: 6fa03725f3fe272795553c9c4acf46130b8c6039", "pr_number": "40109", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/vs_install.ps1", ".circleci/verbatim-sources/build-parameters/pytorch-build-params.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "README.md"], "labels": ["merged"]}, "0e074074f3": {"title": "Disable inlining an opaque tensor into a constant (#40367)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40367\n\nIf the tensor has no storage then do not inline as a constant. This\nsituation when Mkldnn tensors are used.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D22158240\n\nPulled By: bzinodev\n\nfbshipit-source-id: 8d2879044f2429004983a1242d837367b75a9f2a", "pr_number": "40367", "files_changed": ["torch/csrc/jit/ir/constants.cpp"], "labels": ["merged", "oncall: jit"]}, "f6b9848c25": {"title": "Use chain.from_iterable in optimizer.py (#40156)", "body": "Summary:\nThis is a faster and more idiomatic way of using `itertools.chain`. Instead of computing all the items in the iterable and storing them in memory, they are computed one-by-one and never stored as a huge list. This can save on both runtime and memory space.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40156\n\nReviewed By: ezyang\n\nDifferential Revision: D22189038\n\nPulled By: vincentqb\n\nfbshipit-source-id: 160b2c27f442686821a6ea541e1f48f4a846c186", "pr_number": "40156", "files_changed": ["torch/optim/optimizer.py"], "labels": ["merged", "open source"]}, "b623bdeabb": {"title": "Move TensorOptions ops to c10 (#39492)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39492\n\nThis PR adds use_c10_dispatcher: full to ops taking TensorOptions. To allow this, since the c10 operator library doesn't know about TensorOptions, we need to register the operator kernels as optional<ScalarType>, optional<Device>, optional<Layout>, optional<bool> instead, and also call them this way.\n\nChanges:\n\nAdd use_c10_dispatcher: full to those ops\nWrite hacky_wrapper_for_legacy_signatures which takes an old-style kernel (i.e. one written to take TensorOptions) an creates a wrapper kernel for it that takes the scattered optional<ScalarType>, optional<Device>, optional<Layout>, optional<bool> instead.\nChange codegen so that all op registrations are wrapped into hacky_wrapper_for_legacy_signatures. This is added to all ops but is a no-op if the op doesn't take TensorOptions. This allows us in the future to just change a kernel signature from TensorOptions to the scattered version and have it work without having to touch codegen.\nChange codegen so that the frontend calls those operators with expanded arguments instead of with a TensorOptions object. This is required because now the kernels are written in this way.\nThis PR does not remove TensorOptions special cases from codegen, but instead it separates kernels from the codegen/frontend issues. After this, kernels can be worked on separately without having to touch codegen and codegen can be worked on without having to touch kernels.\n\nCodegen diff: P133121032\n\nghstack-source-id: 106426630\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D21581908\n\nfbshipit-source-id: 6d4a9f526fd70fae40581bf26f3ccf794ce6a89e", "pr_number": "39492", "files_changed": ["aten/src/ATen/common_with_cwrap.py", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h", "aten/src/ATen/cwrap_parser.py", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/gen_backend_select_register.py", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "aten/src/ATen/nn_parse.py", "aten/src/ATen/templates/BackendSelectRegister.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/PerOpRegistration.cpp", "aten/src/ATen/templates/SparseTypeDerived.cpp", "aten/src/ATen/templates/TypeDefault.cpp", "aten/src/ATen/templates/TypeDerived.cpp", "aten/src/ATen/test/extension_backend_test.cpp", "c10/core/ScalarType.h", "c10/test/util/TypeList_test.cpp", "c10/util/TypeList.h", "test/cpp_extensions/msnpu_extension.cpp", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/ProfiledType.cpp", "tools/autograd/templates/VariableType.cpp"], "labels": ["merged"]}, "5ad885b823": {"title": "[Caffe2][Pruning] Make the caffe2 Sum operator support long types (#40379)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40379\n\nThe current sum operator doesn't support Long .. hence modify the code\n\nTest Plan: Write a test case\n\nReviewed By: jspark1105, yinghai\n\nDifferential Revision: D21917365\n\nfbshipit-source-id: b37d2c100c70d17d2f89c309e40360ddfab584ee", "pr_number": "40379", "files_changed": ["caffe2/operators/utility_ops.cu", "caffe2/operators/utility_ops.h", "caffe2/python/operator_test/utility_ops_test.py", "caffe2/python/serialized_test/SerializedTestCoverage.md", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_elementwise_max.zip", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_elementwise_max_grad.zip", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_elementwise_min.zip", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_elementwise_min_grad.zip", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_lengths_gather.zip", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_lengths_to_ranges.zip", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_size_op.zip", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_slice.zip", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_sum.zip", "caffe2/python/serialized_test/data/operator_test/utility_ops_test.test_transpose.zip", "caffe2/quantization/server/elementwise_sum_relu_op.cc"], "labels": ["fb-exported", "merged"]}, "09285070a7": {"title": "Doc fix for complex views (#40450)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40450\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22190911\n\nPulled By: anjali411\n\nfbshipit-source-id: eb13559c7a2f62d63344601c750b5715686e95c3", "pr_number": "40450", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "581ad48806": {"title": "Revert D21581908: Move TensorOptions ops to c10", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD21581908\n\nOriginal commit changeset: 6d4a9f526fd7\n\nfbshipit-source-id: fe1e6368a09120ea40dea405e8409983541e3cb5", "pr_number": null, "files_changed": ["aten/src/ATen/common_with_cwrap.py", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h", "aten/src/ATen/cwrap_parser.py", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/gen_backend_select_register.py", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "aten/src/ATen/nn_parse.py", "aten/src/ATen/templates/BackendSelectRegister.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/PerOpRegistration.cpp", "aten/src/ATen/templates/SparseTypeDerived.cpp", "aten/src/ATen/templates/TypeDefault.cpp", "aten/src/ATen/templates/TypeDerived.cpp", "aten/src/ATen/test/extension_backend_test.cpp", "c10/core/ScalarType.h", "c10/test/util/TypeList_test.cpp", "c10/util/TypeList.h", "test/cpp_extensions/msnpu_extension.cpp", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/ProfiledType.cpp", "tools/autograd/templates/VariableType.cpp"], "labels": []}, "d8ec19bc03": {"title": "Revert D22072830: [wip] Upgrade msvc to 14.13", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22072830\n\nOriginal commit changeset: 6fa03725f3fe\n\nfbshipit-source-id: 901de185e607810cb3871c2e4d23816848c97f4b", "pr_number": null, "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/vs_install.ps1", ".circleci/verbatim-sources/build-parameters/pytorch-build-params.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "README.md"], "labels": []}, "cc9075c5d4": {"title": "Add some syntax sugar for when backends use the same function. (#40182)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40182\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22187354\n\nPulled By: ezyang\n\nfbshipit-source-id: 875a6a7837981b60830bd7b1c35d2a3802ed7dd7", "pr_number": "40182", "files_changed": ["aten/src/ATen/native/README.md", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "aten/src/ATen/preprocess_declarations.py"], "labels": ["merged"]}, "111b399c91": {"title": "Delete requires_tensor (#40184)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40184\n\nWhenever requires_tensor is True, it is also the case that abstract\nis true.  Thus, it is not necessary to specify requires_tensor.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22187353\n\nPulled By: ezyang\n\nfbshipit-source-id: d665bb69cffe491bd989495020e1ae32340aa9da", "pr_number": "40184", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "tools/autograd/gen_variable_type.py"], "labels": ["merged"]}, "c7d79f35e3": {"title": "Header rename complex_type.h -> complex.h (#39885)", "body": "Summary:\nThis file should have been renamed as `complex.h`, but unfortunately, it was named as `complex_type.h` due to a name clash with FBCode. Is this still the case and is it easy to resolve the name clash? Maybe related to the comment at https://github.com/pytorch/pytorch/pull/39834#issuecomment-642950012\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39885\n\nDifferential Revision: D22018575\n\nPulled By: ezyang\n\nfbshipit-source-id: e237ccedbe2b30c31aca028a5b4c8c063087a30f", "pr_number": "39885", "files_changed": ["aten/src/ATen/Dispatch.h", "aten/src/ATen/cpu/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_complex_float.h", "aten/src/ATen/cpu/vml.h", "aten/src/ATen/cuda/DeviceUtils.cuh", "aten/src/ATen/native/cpu/zmath.h", "aten/src/THC/THCAtomics.cuh", "c10/core/ScalarType.h", "c10/test/util/complex_math_test_common.h", "c10/test/util/complex_test_common.h", "c10/util/Half.h", "c10/util/complex.h", "c10/util/complex_math.h", "c10/util/complex_type.h", "c10/util/complex_utils.h"], "labels": ["merged", "module: complex", "open source", "triaged"]}, "2e6da36298": {"title": "[android][ci] Fix CI packaging headers to aar (#40442)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40442\n\nProblem:\nNightly builds do not include libtorch headers as local build.\nThe reason is that on docker images path is different than local path when building with `scripts/build_pytorch_android.sh`\n\nSolution:\nIntroducing gradle property to be able to specify it and add its specification to gradle build job and snapshots publishing job which run on the same docker image.\n\nTest:\nci-all jobs check https://github.com/pytorch/pytorch/pull/40443\nchecking that gradle build will result with headers inside aar\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22190955\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 9379458d8ab024ee991ca205a573c21d649e5f8a", "pr_number": "40442", "files_changed": ["android/pytorch_android/build.gradle"], "labels": ["merged"]}, "a6a2dd14ea": {"title": "Fix typo in warning message (#39854)", "body": "Summary:\nFix typo\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39854\n\nReviewed By: ezyang\n\nDifferential Revision: D22193544\n\nPulled By: zou3519\n\nfbshipit-source-id: 04b9f59da7b6ba0649fc6d315adcf20685e10930", "pr_number": "39854", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged", "open source"]}, "883e4c44b2": {"title": "Raise exception when trying to build PyTorch on 32-bit Windows system (#40321)", "body": "Summary:\nMakes errors in cases described in https://github.com/pytorch/pytorch/issues/27815 more obvious\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40321\n\nDifferential Revision: D22198352\n\nPulled By: malfet\n\nfbshipit-source-id: 327d81103c066048dcf5f900fd9083b09942af0e", "pr_number": "40321", "files_changed": ["setup.py"], "labels": ["merged"]}, "7e32e6048d": {"title": "Fix linspace step computation for large integral types (#40132)", "body": "Summary:\nConvert start and end to `step_t` before computing the difference\nShould fix `torch.linspace(-2147483647, 2147483647, 10, dtype=torch.int32)`\n\nCloses https://github.com/pytorch/pytorch/issues/40118\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40132\n\nDifferential Revision: D22190095\n\nPulled By: malfet\n\nfbshipit-source-id: 01cb158a30c505191df663d021804d411b697871", "pr_number": "40132", "files_changed": ["aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp", "aten/src/ATen/native/cuda/RangeFactories.cu", "test/test_torch.py"], "labels": ["merged"]}, "ddb8565b25": {"title": "Revert D22162469: [pytorch][PR] Migrate `var` & `std` to ATen", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22162469 (https://github.com/pytorch/pytorch/commit/7a3c223bbb711c7a93910ce406a0126b8000b43b)\n\nOriginal commit changeset: 8d901c779767\n\nfbshipit-source-id: 9e0fa439732478349c0ac6c7baafba063edfac5d", "pr_number": null, "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/core/NamedRegistrations.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/generic/THCTensorMathReduce.cu", "aten/src/THC/generic/THCTensorMathReduce.h", "test/backward_compatibility/check_backward_compatibility.py"], "labels": []}, "92d3182c11": {"title": "Revert D21232894: Unify PyTorch mobile's threadpool usage.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD21232894 (https://github.com/pytorch/pytorch/commit/b9d3869df357038f798eef579fe1c69cf246887d)\n\nOriginal commit changeset: 8b3de86247fb\n\nfbshipit-source-id: e6517cfec08f7dd0f4f8877dab62acf1d65afacd", "pr_number": null, "files_changed": [".circleci/scripts/binary_ios_upload.sh", "BUILD.bazel", "CMakeLists.txt", "android/pytorch_android/CMakeLists.txt", "android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "aten/src/ATen/ParallelNative.cpp", "aten/src/ATen/native/NNPACK.cpp", "aten/src/ATen/native/quantized/cpu/q_avgpool.cpp", "aten/src/ATen/native/quantized/cpu/q_avgpool3d.cpp", "aten/src/ATen/native/quantized/cpu/qadd.cpp", "aten/src/ATen/native/quantized/cpu/qchannel_shuffle.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp", "aten/src/ATen/native/quantized/cpu/qhardswish.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qpool.cpp", "aten/src/ATen/native/quantized/cpu/qreduction.cpp", "aten/src/ATen/native/quantized/cpu/qrelu.cpp", "aten/src/ATen/native/quantized/cpu/qsigmoid.cpp", "aten/src/ATen/native/quantized/cpu/qtanh.cpp", "aten/src/ATen/native/xnnpack/Common.h", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Linear.cpp", "aten/src/ATen/native/xnnpack/MaxPooling.cpp", "caffe2/CMakeLists.txt", "caffe2/utils/CMakeLists.txt", "caffe2/utils/threadpool/ThreadPoolMobile.cc", "caffe2/utils/threadpool/ThreadPoolMobile.h", "caffe2/utils/threadpool/ThreadPoolXNNPACK.cc", "caffe2/utils/threadpool/ThreadPoolXNNPACK.h", "caffe2/utils/threadpool/pthreadpool-cpp.cc", "caffe2/utils/threadpool/pthreadpool-cpp.h", "caffe2/utils/threadpool/pthreadpool.cc", "caffe2/utils/threadpool/pthreadpool.h", "caffe2/utils/threadpool/pthreadpool_impl.cc", "caffe2/utils/threadpool/pthreadpool_new_if_impl.c", "caffe2/utils/threadpool/pthreadpool_utils_new_if.h", "cmake/Dependencies.cmake", "cmake/External/nnpack.cmake", "cmake/TorchConfig.cmake.in", "ios/TestApp/benchmark/setup.rb", "scripts/xcode_build.rb"], "labels": []}, "c314e0deb5": {"title": "[quant] Quantized adaptive_avg_pool3d (#40271)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40271\n\nCloses #40244\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22134318\n\nPulled By: z-a-f\n\nfbshipit-source-id: 0489b6c083a3cbc21a1d81d8bfcc499372308088", "pr_number": "40271", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/quantization/test_quantized_op.py", "torch/nn/quantized/functional.py"], "labels": ["merged"]}, "a2d4d9eca6": {"title": "Improve Dynamic Library for Windows (#40365)", "body": "Summary:\n1. Use LoadLibraryEx if available\n2. Print more info on error\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40365\n\nDifferential Revision: D22194974\n\nPulled By: malfet\n\nfbshipit-source-id: e8309f39d78fd4681de5aa032288882910dff928", "pr_number": "40365", "files_changed": ["aten/src/ATen/DynamicLibrary.cpp"], "labels": ["merged", "open source", "triaged"]}, "7b0f867c48": {"title": "Perf improvement of Conv2d and Conv3d (#40324)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40324\n\n1) avoid the use of item 2) bypass the im2col for 1x1 conv\n\nTest Plan:\nunit test and perf benchmark to show improvement\n```\nnum = 50\n\nN = 1\nC = 512\nH = 4\nW = 4\n\nM = 512\nkernel_h = 1\nkernel_w = 1\nstride_h = 1\nstride_w = 1\npadding_h = 0\npadding_w = 0\n\nX_np = np.random.randn(N, C, H, W).astype(np.float32)\nW_np = np.random.randn(M, C, kernel_h, kernel_w).astype(np.float32)\nX = torch.from_numpy(X_np)\n\nconv2d_pt = torch.nn.Conv2d(\n    C, M, (kernel_h, kernel_w), stride=(stride_h, stride_w),\n    padding=(padding_h, padding_w), groups=1, bias=True)\n\nclass ConvNet(torch.nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv2d = conv2d_pt\n\n    def forward(self, x):\n        return self.conv2d(x)\n\nmodel = ConvNet()\n\ndef pt_forward():\n    # with torch.autograd.profiler.profile(record_shapes=True) as prof:\n    model(X)\n    # print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n\ntorch._C._set_mkldnn_enabled(False)\n\nt = Timer(\"pt_forward()\", \"from __main__ import pt_forward, X\")\n```\nBefore the optimization:\npt time = 5.841153813526034\nAfter the optimization:\npt time = 4.513134760782123\n\nDifferential Revision: D22149067\n\nfbshipit-source-id: 538d9eea5b729e6c3da79444bde1784bde828876", "pr_number": "40324", "files_changed": ["aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp"], "labels": ["fb-exported", "merged"]}, "46b9e519aa": {"title": "Remove print (#40475)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40475\n\nAs title\nghstack-source-id: 106474870\n\nTest Plan: CI\n\nDifferential Revision: D22200640\n\nfbshipit-source-id: 1f4c7bbf54be8c4187c9338fefdf14b501597d98", "pr_number": "40475", "files_changed": ["torch/quantization/_numeric_suite.py"], "labels": ["merged"]}, "f035f73d53": {"title": "Fix the issue that run clang-tidy on the aten folder (#39713)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39713\n\nDifferential Revision: D22203850\n\nPulled By: mruberry\n\nfbshipit-source-id: 43f690e748b7a3c123ad20f6d640d6dae25c641c", "pr_number": "39713", "files_changed": [".github/workflows/lint.yml", "tools/git-pre-commit"], "labels": ["merged", "open source", "triaged"]}, "0ecea2d64d": {"title": "[JIT x RPC] Consolidate Future type class and Future impl class (#40406)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40406\n\nSame motivation for https://github.com/pytorch/pytorch/issues/35110.\n\n`Future` and `RRef` are two important types for `rpc` module, should make users feel easy to use.\n\nReference, https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#directive-autoclass\n\nFollow https://github.com/pytorch/pytorch/pull/35694.\nghstack-source-id: 106484664\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork && \\\nbuck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par \\\n-r test_rref_local_value\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/tensorpipe:rpc_fork_tensorpipe\n```\n\npyre -l caffe2/torch/fb/training_toolkit\npyre -l caffe2/torch/fb/distributed\npyre -l aiplatform\n\nDifferential Revision: D7722176\n\nfbshipit-source-id: f3b9ccd7bccb233b2b33ad59dd65e178ba34d67f", "pr_number": "40406", "files_changed": ["docs/source/futures.rst", "torch/_jit_internal.py", "torch/distributed/rpc/functions.py", "torch/futures/__init__.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["merged", "oncall: jit"]}, "b4eb82cd29": {"title": "Temporary commit at 6/17/2020, 6:49:44 PM", "body": "Summary: [WIP] Logit Fake16 Op\n\nTest Plan: [WIP] Tests will be enabled in test_op_nnpi_fp16.py file.\n\nReviewed By: hyuen\n\nDifferential Revision: D22109329\n\nfbshipit-source-id: fd73850c3ec61375ff5bbf0ef5460868a874fbf3", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/unary_fp16_fake_op.cc", "caffe2/contrib/fakelowp/unary_fp16_fake_op.h"], "labels": []}, "72e8690b78": {"title": "Fix typo. in error message (#39958)", "body": "Summary:\nChanged sould to should\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39958\n\nReviewed By: ezyang\n\nDifferential Revision: D22193674\n\nPulled By: zou3519\n\nfbshipit-source-id: ad7bc0aa3ee1f31f5e7965ae36c1903b28509095", "pr_number": "39958", "files_changed": ["torch/onnx/utils.py"], "labels": ["merged", "open source"]}, "e439cf738a": {"title": "Fix examples Adaptive avg pooling typo (#40217)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40217\n\nReviewed By: ezyang\n\nDifferential Revision: D22193711\n\nPulled By: zou3519\n\nfbshipit-source-id: f96f71e025aa1c81b232e78b1d5b3a3bbd8f331f", "pr_number": "40217", "files_changed": ["torch/nn/modules/pooling.py"], "labels": ["merged", "open source"]}, "a4dec0674c": {"title": "[doc] fix typo in formula of MarginRankingLoss (#40285)", "body": "Summary:\nThis is just a minor doc fix:\n\nthe `MarginRankingLoss` takes 2 input samples `x1` and `x2`, not just `x`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40285\n\nReviewed By: ezyang\n\nDifferential Revision: D22195069\n\nPulled By: zou3519\n\nfbshipit-source-id: 909f491c94dca329a37216524f4088e9096e0bc6", "pr_number": "40285", "files_changed": ["torch/nn/modules/loss.py"], "labels": ["merged", "open source"]}, "ecd9a64712": {"title": "fix `torch.jit.trace_module` documentation (#40248)", "body": "Summary:\nThis should fix https://github.com/pytorch/pytorch/issues/39328\n\nBefore:\n\n![image](https://user-images.githubusercontent.com/24580222/85076992-4720e800-b18f-11ea-9c6e-19bcf3f1cb7d.png)\n\nAfter:\n\n![image](https://user-images.githubusercontent.com/24580222/85077064-6ddf1e80-b18f-11ea-9274-e8cee6909baa.png)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40248\n\nReviewed By: ezyang\n\nDifferential Revision: D22195038\n\nPulled By: zou3519\n\nfbshipit-source-id: c4bff6579a422a56ed28b644f5558b20d901c94e", "pr_number": "40248", "files_changed": ["torch/jit/__init__.py"], "labels": ["merged", "oncall: jit", "open source"]}, "4975be80f8": {"title": "fix typo \"normal\" -> \"Cauchy\" (#40334)", "body": "Summary:\njust looks like a real simple typo\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40334\n\nReviewed By: ezyang\n\nDifferential Revision: D22195107\n\nPulled By: zou3519\n\nfbshipit-source-id: 6c43842d22cbc15db2307976381f6dc1536b5047", "pr_number": "40334", "files_changed": ["torch/distributions/half_cauchy.py"], "labels": ["merged", "open source"]}, "e490352dc4": {"title": "Simplify complex case for tanh backward (#39997)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39997\n\nDifferential Revision: D22195797\n\nPulled By: anjali411\n\nfbshipit-source-id: 21eb91bcbd3bfc67acd322a1579fe737b0c02e6e", "pr_number": "39997", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu"], "labels": ["merged", "open source", "triaged"]}, "43ab9c677b": {"title": "Add invariants check to BatchedTensorImpl (#40171)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40171\n\nIt checks that all of the bdims in BatchedTensorImpl are sorted in\norder of ascending `level`.\n\nTest Plan: - Check that nothing breaks in `./build/bin/vmap_test`\n\nDifferential Revision: D22102077\n\nPulled By: zou3519\n\nfbshipit-source-id: 094b7abc6c65208437f0f51a0d0083091912decc", "pr_number": "40171", "files_changed": ["aten/src/ATen/BatchedTensorImpl.cpp", "aten/src/ATen/BatchedTensorImpl.h"], "labels": ["merged"]}, "727463a727": {"title": "Initial vmap frontend API (#40172)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40172\n\nThis PR introduces the initial vmap frontend API. It has the following\nlimitations that we can resolve in the future:\n- the inputs must be a flat list of tensors\n- the outputs must be a flat list of tensors\n- in_dims = 0 (so we always vmap over dim 0 of input tensors)\n- out_dims = 0 (so the returned tensors have their vmap dim appear at\ndim 0)\n- Coverage limited to operations that have batching rules implemented\n(torch.mul, torch.sum, torch.expand).\n\nThere are some other semantic limitations (like not being able to handle\nmutation, aside from pytorch operations that perform mutation) that will\nbe documented in the future.\n\nI wanted to introduce the API before adding a slow fallback for the\ncoverage so that we can test future batching rules (and coverage) via\nthe python API to avoid verbosity in C++-land.\n\nThe way vmap works is that `vmap(func)(inputs)` wraps all Tensor inputs\nto be batched in BatchedTensors, sends those into func, and then unwraps\nthe output BatchedTensors. Operations on BatchedTensors perform the batched\noperations that the user is asking for. When performing nested vmaps,\neach nested vmap adds a batch dimension upon entry and removes a batch\ndimension on exit.\n\nComing up in the near future:\n- Support for non-zero in_dims and out_dims\n- docstring for vmap\n- slow fallback for operators that do not have a batching rule\nimplemented.\n\nTest Plan: - `pytest test/test_vmap.py -v`\n\nDifferential Revision: D22102076\n\nPulled By: zou3519\n\nfbshipit-source-id: b119f0a8a3a3b1717c92dbbd180dfb1618295563", "pr_number": "40172", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "aten/src/ATen/native/Batching.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_vmap.py", "torch/__init__.py", "torch/_vmap_internals.py"], "labels": ["merged"]}, "b4ccdef090": {"title": "Allow torch.cuda.amp.GradScaler to support sparse gradients (#36786)", "body": "Summary:\nShould close https://github.com/pytorch/pytorch/issues/35810.\n\nI decided to keep sparse handling on the Python side for clarity, although it could be moved to the C++ side (into `_amp_non_finite_check_and_unscale_`) without much trouble.\n\nFor non-fp16 sparse grads the logic is simple (call `_amp_non_finite_check_and_unscale_` on `grad._values()`) instead of `grad` itself.  At least I hope it's that easy.\n\nFor fp16 sparse grads, it's tricker.  Sparse tensors can be uncoalesced.  From the [Note](https://pytorch.org/docs/master/sparse.html#torch.sparse.FloatTensor):\n> Our sparse tensor format permits uncoalesced sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries.\n\nAn uncoalesced scaled fp16 grad may have values at duplicate coordinates that are all finite but large, such that adding them to make the coalesced version WOULD cause overflows.**  If I checked `_values()` on the uncoalesced version, it might not report overflows, but I think it should.\n\nSo, if the grad is sparse, fp16, and uncoalesced, I still call `_amp_non_finite_check_and_unscale_` to unscale `grad._values()` in-place, but I also double-check the coalesced version by calling a second `_amp_non_finite_check_and_unscale_` on `grad.coalesce()._values()`.  `coalesce()` is out-of-place, so this call doesn't redundantly affect `grad._values()`, but it does have the power to populate the same `found_inf` tensor.  The `is_coalesced()` check and `coalesce()` probably aren't great for performance, but if someone needs a giant embedding table in FP16, they're better than nothing and memorywise, they'll only create a copy of nnz gradient values+indices, which is still way better than changing the whole table to FP32.\n\nAn `unscale` variant with liberty to create unscaled grads out-of-place, and replace `param.grad` instead of writing through it, could get away with just one `_amp_non_finite_check_and_unscale_`.  It could say `coalesced = grad.coalesced()`, do only the stronger `_amp_non_finite_check_and_unscale_` on `coalesced._values()`, and set `param.grad = coalesced`.  I could even avoid replacing `param.grad` itself by going one level deeper and setting `param.grad`'s indices and values to `coalesced`'s, but that seems brittle and still isn't truly \"in place\".\n\n** you could whiteboard an uncoalesced fp32 grad with the same property, but fp32's range is big enough that I don't think it's realistic.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36786\n\nReviewed By: ezyang\n\nDifferential Revision: D22202832\n\nPulled By: ngimel\n\nfbshipit-source-id: b70961a4b6fc3a4c1882f65e7f34874066435735", "pr_number": "36786", "files_changed": ["test/test_cuda.py", "torch/cuda/amp/grad_scaler.py"], "labels": ["merged", "module: amp (automated mixed precision)", "module: cuda", "open source", "triaged"]}, "3ed96e465c": {"title": "Report error when ATEN_THEADING is OMP and USE_OPENMP is turned off. (#40146)", "body": "Summary:\nCurrently, even if USE_OPENMP is turned off, ATEN_THEADING can still use OpenMP. This commit fixes it.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40146\n\nReviewed By: ezyang\n\nDifferential Revision: D22208758\n\nPulled By: pbelevich\n\nfbshipit-source-id: 0866c9bb9b3b5b99d586aed176eb0fbe177efa4a", "pr_number": "40146", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["merge-this-please", "merged", "open source"]}, "597cb04b2f": {"title": "Use Int8QuantParamsBlob to pass the scale and zeropoint params (#40494)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40494\n\nResubmit the diff because D22124313 (https://github.com/pytorch/pytorch/commit/1ec4337b7d58fd8afbfc181f975d3940c5e2e8d6) was reverted due to CI test failures\nAdded the int8_gen_quant_params.cc to CMakeList.txt to fix the CI failures\n\nTest Plan: buck test caffe2/caffe2/quantization/server:\n\nReviewed By: hx89\n\nDifferential Revision: D22204244\n\nfbshipit-source-id: a2c8b668f199cc5b0c5894086f554f7c459b1ad7", "pr_number": "40494", "files_changed": ["caffe2/operators/quantized/int8_fc_op.cc", "caffe2/operators/quantized/int8_quantize_op.cc", "caffe2/quantization/server/CMakeLists.txt", "caffe2/quantization/server/dnnlowp_test_utils.py", "caffe2/quantization/server/fully_connected_dnnlowp_op.cc", "caffe2/quantization/server/fully_connected_dnnlowp_op_test.py", "caffe2/quantization/server/pybind.cc", "caffe2/quantization/server/quantize_dnnlowp_op.cc", "caffe2/quantization/server/quantize_dnnlowp_op_test.py"], "labels": ["fb-exported", "merged"]}, "a2e1a948a4": {"title": "Increase number of iterations in DDP SPMD tests (#40506)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40506\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22208965\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7d27b60e2c09e641b4eeb1c89d9f9917c4e72e52", "pr_number": "40506", "files_changed": ["test/distributed/test_c10d_spawn.py"], "labels": ["merged"]}, "de7ac60cf4": {"title": "Add out= variants for cuda.comm.broadcast/gather/scatter (#39681)", "body": "Summary:\nPartially fixes https://github.com/pytorch/pytorch/issues/38911\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39681\n\nDifferential Revision: D22161342\n\nPulled By: mrshenli\n\nfbshipit-source-id: 60295077159b02087823e93bb6ebac9d70adea0a", "pr_number": "39681", "files_changed": ["test/test_cuda.py", "torch/csrc/cuda/comm.cpp", "torch/csrc/cuda/comm.h", "torch/csrc/cuda/python_comm.cpp", "torch/cuda/_utils.py", "torch/cuda/comm.py"], "labels": ["merged", "module: cuda", "oncall: distributed", "open source", "triaged"]}, "fc4824aa4a": {"title": "enable mkldnn dilation conv (#40483)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40483\n\nReviewed By: ezyang\n\nDifferential Revision: D22213696\n\nPulled By: ngimel\n\nfbshipit-source-id: 0321eee8fcaf144b20a5182aa76f98d505c65400", "pr_number": "40483", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "test/test_mkldnn.py"], "labels": ["merged", "open source"]}, "fe18dcd692": {"title": "Use GLOG logging prefixes (#40491)", "body": "Summary:\nPyTorch should stop polluting global namespace with symbols such as `ERROR` `WARNING` and `INFO`.\nSince `logging_is_not_google_glog.h` is a C++ header, define severity levels in namespace and add `GLOG_` prefix to match an unshortened glog severity levels.\nChange `LOG` and `LOG_IF` macros to use prefix + namespaced severity levels.\n\nCloses https://github.com/pytorch/pytorch/issues/40083\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40491\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D22210925\n\nPulled By: malfet\n\nfbshipit-source-id: 0ec1181a53baa8bca2f526f245e398582304aeab", "pr_number": "40491", "files_changed": ["c10/util/Logging.cpp", "c10/util/logging_is_not_google_glog.h"], "labels": ["better-engineering", "merged", "oncall: jit", "triaged"]}, "527ab13436": {"title": "[NCCL] Explicitly Abort NCCL Communicators on Process Group Destruction (#40241)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40241\n\nWe abort incomplete NCCL Communicators in the ProcessGroupNCCL\ndestructor, otherwise pending NCCL communciators may block other CUDA ops.\n\nCloses: https://github.com/pytorch/pytorch/issues/32231\nghstack-source-id: 106469423\n\nTest Plan: CI/Sandcastle\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22103662\n\nfbshipit-source-id: 1f6f88b56bd7a5e9ca5a41698995a76e60e8ad9f", "pr_number": "40241", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged"]}, "3e2d2fc856": {"title": "[NCCL Docs] Adding Comments for Work-level Finish in ProcessGroup (#40404)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40404\n\nAdds docs to the finish function in ProcessGroup::Work. It's better to have some documentation around these functions since we have some PR's with API-changes/optimizations for these work-level functions here and in the subclasses.\nghstack-source-id: 106381736\n\nTest Plan: CI (Docs change only)\n\nDifferential Revision: D22174891\n\nfbshipit-source-id: 7901ea3b35caf6f69f37178ca574104d3412de28", "pr_number": "40404", "files_changed": ["torch/lib/c10d/ProcessGroup.hpp"], "labels": ["merged"]}, "0c923eea0a": {"title": "Add finishAndThrow function to ProcessGroup::Work, and use with Gloo (#40405)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40405\n\nThis adds a finishAndThrow function that completes the work object,\nsets an exception if one is provided by the user, and throws an exception (if\nit is already set or passed by the caller). This is now done by grabbing the\nlock just once and simplifies the wait functions in ProcessGroupGloo.\nghstack-source-id: 106516114\n\nTest Plan: CI\n\nDifferential Revision: D22174890\n\nfbshipit-source-id: ea74702216c4328187c8d193bf39e1fea43847f6", "pr_number": "40405", "files_changed": ["torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["merged"]}, "72f2c479e3": {"title": "Migrate equal from the TH to Aten (CPU) (#33286)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/24697\nVitalyFedyunin\nglaringlee\n\nTest script:\n```Python\nimport timeit\n\nsetup_ones = \"\"\"\nimport torch\na = torch.ones(({n}, {n}), dtype={dtype})\nb = torch.ones(({n}, {n}), dtype={dtype})\n\"\"\"\n\nfor n, t in [(1000, 10000), (2000, 10000)]:\n  for dtype in ('torch.bool', 'torch.int', 'torch.long', 'torch.bfloat16', 'torch.float', 'torch.double'):\n  #for dtype in ('torch.bool', 'torch.int', 'torch.long', 'torch.float', 'torch.double'):\n    print('torch.ones(({n}, {n})) equal for {t} times {dtype}'.format(n=n, t=t, dtype=dtype))\n    print(timeit.timeit(stmt='torch.equal(a, b)', setup=setup_ones.format(n=n, dtype=dtype), number=t))\n\nsetup_rand = \"\"\"\nimport torch\na = torch.rand(({n}, {n}), dtype={dtype})\nb = a.clone()\n\"\"\"\nfor n, t in [(1000, 10000), (2000, 10000)]:\n  for dtype in ('torch.float', 'torch.double'):\n    print('torch.rand(({n}, {n})) for {t} times {dtype}'.format(n=n, t=t, dtype=dtype))\n    print(timeit.timeit(stmt='torch.equal(a, b)', setup=setup_rand.format(n=n, dtype=dtype), number=t))\n\nsetup_non_contiguous = \"\"\"\nimport torch\na = torch.rand(({n}, {n}), dtype={dtype})\na2 = a[:, 500:]\na3 = a2.clone()\ntorch.equal(a2, a3)\n\"\"\"\nfor n, t in [(1000, 10000), (2000, 10000)]:\n  for dtype in ('torch.float', 'torch.double'):\n    print('non_contiguous torch.rand(({n}, {n})) for {t} times {dtype}'.format(n=n, t=t, dtype=dtype))\n    print(timeit.timeit(stmt='torch.equal(a2, a3)', setup=setup_non_contiguous.format(n=n, dtype=dtype), number=t))\n\nsetup_not_equal = \"\"\"\nimport torch\na = torch.rand(({n}, {n}), dtype={dtype})\nb = torch.rand(({n}, {n}), dtype={dtype})\ntorch.equal(a, b)\n\"\"\"\nfor n, t in [(1000, 10000), (2000, 10000)]:\n  for dtype in ('torch.float', 'torch.double'):\n    print('not equal torch.rand(({n}, {n})) for {t} times {dtype}'.format(n=n, t=t, dtype=dtype))\n    print(timeit.timeit(stmt='torch.equal(a, b)', setup=setup_not_equal.format(n=n, dtype=dtype), number=t))\n```\n\nTH\n```\ntorch.ones((1000, 1000)) equal for 10000 times torch.bool\n1.8391206220258027\ntorch.ones((1000, 1000)) equal for 10000 times torch.int\n1.8877864250680432\ntorch.ones((1000, 1000)) equal for 10000 times torch.long\n1.938108820002526\ntorch.ones((1000, 1000)) equal for 10000 times torch.bfloat16\n3.184849138953723\ntorch.ones((1000, 1000)) equal for 10000 times torch.float\n1.8825413499725983\ntorch.ones((1000, 1000)) equal for 10000 times torch.double\n2.7266416549682617\ntorch.ones((2000, 2000)) equal for 10000 times torch.bool\n7.227149627986364\ntorch.ones((2000, 2000)) equal for 10000 times torch.int\n7.76215292501729\ntorch.ones((2000, 2000)) equal for 10000 times torch.long\n9.631909006042406\ntorch.ones((2000, 2000)) equal for 10000 times torch.bfloat16\n8.097328286035918\ntorch.ones((2000, 2000)) equal for 10000 times torch.float\n5.5739822529722005\ntorch.ones((2000, 2000)) equal for 10000 times torch.double\n8.444009944912978\ntorch.rand((1000, 1000)) for 10000 times torch.float\n1.168096570065245\ntorch.rand((1000, 1000)) for 10000 times torch.double\n1.6577326939441264\ntorch.rand((2000, 2000)) for 10000 times torch.float\n5.49395391496364\ntorch.rand((2000, 2000)) for 10000 times torch.double\n8.507486199960113\nnon_contiguous torch.rand((1000, 1000)) for 10000 times torch.float\n6.074504268006422\nnon_contiguous torch.rand((1000, 1000)) for 10000 times torch.double\n6.1426916810451075\nnon_contiguous torch.rand((2000, 2000)) for 10000 times torch.float\n37.501055537955835\nnon_contiguous torch.rand((2000, 2000)) for 10000 times torch.double\n44.6880351039581\nnot equal torch.rand((1000, 1000)) for 10000 times torch.float\n0.029356416082009673\nnot equal torch.rand((1000, 1000)) for 10000 times torch.double\n0.025421109050512314\nnot equal torch.rand((2000, 2000)) for 10000 times torch.float\n0.026333761983551085\nnot equal torch.rand((2000, 2000)) for 10000 times torch.double\n0.02748022007290274\n```\n\nATen\n```\ntorch.ones((1000, 1000)) equal for 10000 times torch.bool\n0.7961567062884569\ntorch.ones((1000, 1000)) equal for 10000 times torch.int\n0.49172434909269214\ntorch.ones((1000, 1000)) equal for 10000 times torch.long\n0.9459248608909547\ntorch.ones((1000, 1000)) equal for 10000 times torch.bfloat16\n2.0877483217045665\ntorch.ones((1000, 1000)) equal for 10000 times torch.float\n0.606857153121382\ntorch.ones((1000, 1000)) equal for 10000 times torch.double\n1.1388208279386163\ntorch.ones((2000, 2000)) equal for 10000 times torch.bool\n2.0329296849668026\ntorch.ones((2000, 2000)) equal for 10000 times torch.int\n3.534358019940555\ntorch.ones((2000, 2000)) equal for 10000 times torch.long\n8.19841272290796\ntorch.ones((2000, 2000)) equal for 10000 times torch.bfloat16\n6.595649406313896\ntorch.ones((2000, 2000)) equal for 10000 times torch.float\n4.193911510054022\ntorch.ones((2000, 2000)) equal for 10000 times torch.double\n7.931309659034014\ntorch.rand((1000, 1000)) for 10000 times torch.float\n0.8877940969541669\ntorch.rand((1000, 1000)) for 10000 times torch.double\n1.4142901846207678\ntorch.rand((2000, 2000)) for 10000 times torch.float\n4.010025603231043\ntorch.rand((2000, 2000)) for 10000 times torch.double\n8.126411964651197\nnon_contiguous torch.rand((1000, 1000)) for 10000 times torch.float\n0.602473056409508\nnon_contiguous torch.rand((1000, 1000)) for 10000 times torch.double\n0.6784545010887086\nnon_contiguous torch.rand((2000, 2000)) for 10000 times torch.float\n3.0991827426478267\nnon_contiguous torch.rand((2000, 2000)) for 10000 times torch.double\n5.719010795000941\nnot equal torch.rand((1000, 1000)) for 10000 times torch.float\n0.046060710679739714\nnot equal torch.rand((1000, 1000)) for 10000 times torch.double\n0.036034489050507545\nnot equal torch.rand((2000, 2000)) for 10000 times torch.float\n0.03686975734308362\nnot equal torch.rand((2000, 2000)) for 10000 times torch.double\n0.04189508780837059\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33286\n\nDifferential Revision: D22211962\n\nPulled By: glaringlee\n\nfbshipit-source-id: a5c48f328432c1996f28e19bc75cb495fb689f6b", "pr_number": "33286", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMoreMath.cpp"], "labels": ["merged", "open source", "topic: porting", "triaged"]}, "c120fdc05b": {"title": "Unify `torch/csrc/cuda/shared/cudnn.cpp` include path (#40525)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40525\n\nMove `USE_CUDNN` define under `USE_CUDA` guard, add `cuda/shared/cudnn.cpp` to filelist if either USE_ROCM or USE_CUDNN is set.\nThis is a prep change for PyTorch CUDA src filelist unification change.\n\nTest Plan: CI\n\nDifferential Revision: D22214899\n\nfbshipit-source-id: b71b32fc603783b41cdef0e7fab2cc9cbe750a4e", "pr_number": "40525", "files_changed": ["torch/CMakeLists.txt"], "labels": ["fb-exported", "merged"]}, "16f276cef9": {"title": "Add C++-only `int dim` overloads to `std`-related operations (#40451)", "body": "Summary:\nFixes gh-40287\n\nThe `int -> bool` conversion takes higher precedence than `int -> IntArrayRef`. So, calling `std(0)` in C++ would select the `std(unbiased=False)` overload instead.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40451\n\nDifferential Revision: D22217926\n\nPulled By: ezyang\n\nfbshipit-source-id: 7520792fab5ab6665bddd03b6f57444c6c729af4", "pr_number": "40451", "files_changed": ["aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/TensorBody.h", "test/cpp/api/tensor.cpp"], "labels": ["merged", "open source", "triaged"]}, "3dcc329746": {"title": "Use tree-based sum for floats to avoid numerical instability (#39516)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/38716, fixes https://github.com/pytorch/pytorch/issues/37234\n\nThis algorithm does the summation along a single axis with multiple \"levels\" of accumulator, each of which is designed to hold the sum of an order of magnitude more values than the previous.\n\ne.g. if there are 2^16 elements, the first level will hold the sum of 2^4 elements, and so on in increasing powers of 2: 2^4, 2^8, 2^12 and finally 2^16.\n\nThis limits the differences in magnitude of the partial results being added together, and so we don't lose accuracy as the axis length increases.\n\nWIP to write a vectorized version.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39516\n\nReviewed By: ezyang\n\nDifferential Revision: D22106251\n\nPulled By: ngimel\n\nfbshipit-source-id: b56de4773292439dbda62b91f44ff37715850ae9", "pr_number": "39516", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cpu/SumKernel.cpp", "benchmarks/operator_benchmark/pt/sum_test.py", "test/cpp/api/nn_utils.cpp"], "labels": ["open source", "triaged"]}, "e12f73ee12": {"title": "Add missing file to BUILD.bazel (#40536)", "body": "Summary:\nAdd `int8_gen_quant_params.cc` added by\nhttps://github.com/pytorch/pytorch/pull/40494/ to bazel build rules\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40536\n\nReviewed By: mruberry\n\nDifferential Revision: D22219595\n\nPulled By: malfet\n\nfbshipit-source-id: 2875a0b9c55bad2b052a898661b96eab490f6451", "pr_number": "40536", "files_changed": ["BUILD.bazel"], "labels": ["merged"]}, "adcd755e69": {"title": "Fix backup solution (#40515)", "body": "Summary:\nThese were changes that had to be made in the `release/1.6` branch in order to get backups to work.\n\nThey should be brought to the master branch.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40515\n\nDifferential Revision: D22221308\n\nPulled By: seemethere\n\nfbshipit-source-id: 24e2a0196a8e775fe324a383c8f0c681118b741b", "pr_number": "40515", "files_changed": [".circleci/scripts/binary_linux_upload.sh", ".circleci/scripts/binary_macos_upload.sh", ".circleci/scripts/binary_windows_upload.sh"], "labels": ["merged", "module: ci", "releng"]}, "06debf6373": {"title": "move __range_length and __derive_index to lite interpreter (#40533)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40533\n\nThese ops are required by the demucs denoiser model\n\nTest Plan: build\n\nReviewed By: kaustubh-kp, linbinyu\n\nDifferential Revision: D22216217\n\nfbshipit-source-id: f300ac246fe3a7a6566a70bb89858770af68a90c", "pr_number": "40533", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "85b87df5ba": {"title": "Revert D22208758: [pytorch][PR] Report error when ATEN_THEADING is OMP and USE_OPENMP is turned off.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22208758 (https://github.com/pytorch/pytorch/commit/3ed96e465cbbb8c7f9ea4fea2abe3cb84dac535d)\n\nOriginal commit changeset: 0866c9bb9b3b\n\nfbshipit-source-id: 9e2b469469e274292b2559c02aa0256425fd355e", "pr_number": null, "files_changed": ["caffe2/CMakeLists.txt"], "labels": []}, "82e9318a16": {"title": "Adjust CUDA memory leak test (#40504)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40504\n\nMake CUDA mem liek test not flaky\n\nTest Plan: python test/test_profiler.py\n\nDifferential Revision: D22215527\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 5f1051896342ac50cd3a21ea86ce7487b5f82a19", "pr_number": "40504", "files_changed": ["test/test_profiler.py"], "labels": ["merged"]}, "cf8a9b50ca": {"title": "Allow ReflectionPad to accept 0-dim batch sizes. (#39231)", "body": "Summary:\nAllows ReflectionPad 1D and 2D to accept 0-dim batch sizes.\n\nRelated to issues:\n\n* https://github.com/pytorch/pytorch/issues/38115\n* https://github.com/pytorch/pytorch/issues/12013\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39231\n\nReviewed By: ezyang\n\nDifferential Revision: D22205717\n\nPulled By: mruberry\n\nfbshipit-source-id: 6744661002fcbeb4aaafd8693fb550ed53f3e00f", "pr_number": "39231", "files_changed": ["aten/src/ATen/native/ReflectionPad.cpp", "aten/src/ATen/native/cuda/ReflectionPad.cu", "test/test_nn.py"], "labels": ["merged", "open source", "triaged"]}, "eae1ed99a3": {"title": "caffe2 | Fix building with `-Wrange-loop-analysis` on", "body": "Summary: `-Wrange-loop-analysis` is turned on by default for clang 10 (see https://reviews.llvm.org/D73834). This fixes a warning that's found with that.\n\nTest Plan: Build with clang 10 and check there are no `range-loop-analysis` warnings.\n\nReviewed By: yinghai\n\nDifferential Revision: D22207072\n\nfbshipit-source-id: 858ba8a36c653071eab961cb891ce945faf0fa87", "pr_number": null, "files_changed": ["caffe2/opt/mobile.cc"], "labels": []}, "88ea51c061": {"title": "doc string fix for torch.cuda.set_rng_state_all (#40544)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/40239\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40544\n\nDifferential Revision: D22233989\n\nPulled By: ezyang\n\nfbshipit-source-id: b5098357a3e0c50037f95ba0d701523d5dce2628", "pr_number": "40544", "files_changed": ["torch/cuda/random.py"], "labels": ["merged", "open source"]}, "7038579c03": {"title": "Add batching rule for unsqueeze, squeeze, and transpose (#40455)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40455\n\nThese don't need to be implemented right now but are useful later down\nthe line. I thought I would use these in implementing vmap's `out_dims`\nfunctionality, but it turns out they weren't necessary. Since the code\nexists and is useful anyways, I am leaving this PR here.\n\nTest Plan:\n- `./build/bin/vmap_test`. We could test this using the vmap frontend API,\nbut there is the catch that vmap cannot directly take integers right\nnow (all inputs passed to vmap must be Tensors at the moment). It's\npossible to hack around that by declaring lambdas that take in a single\ntensor argument, but those don't look nice.\n\nDifferential Revision: D22216167\n\nPulled By: zou3519\n\nfbshipit-source-id: 1a010f5d7784845cca19339d37d6467f5b987c32", "pr_number": "40455", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "aten/src/ATen/test/vmap_test.cpp"], "labels": ["merged"]}, "43757ea913": {"title": "Add batching rule for Tensor.permute (#40517)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40517\n\nThis is necessary for implementing the vmap frontend API's out_dims\nfunctionality.\n\nTest Plan:\n- `./build/bin/vmap_test`. The vmap python API can't accept inputs that\naren't integers right now. There are workarounds around that (use a\nlambda) but that doesn't look too nice. In the future we'll test all\nbatching rules in Python.\n\nDifferential Revision: D22216168\n\nPulled By: zou3519\n\nfbshipit-source-id: b6ef552f116fddc433e242c1594059b9d2fe1ce4", "pr_number": "40517", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "aten/src/ATen/test/vmap_test.cpp"], "labels": ["merged"]}, "c362138f43": {"title": "Disallow passing functions that don't return Tensors to vmap (#40518)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40518\n\nI overlooked this in the initial vmap frontend api PR. Right now we\nwant to restrict vmap to taking in functions that only return Tensors.\nA function that only return tensors can look like one of the following:\n```\ndef fn1(x):\n    ...\n    return y\n\ndef fn2(x):\n    ...\n    return y, z\n```\nfn1 returns a Tensor, while fn2 returns a tuple of Tensors. So we add a\ncheck that the output of the function passed to vmap returns either a\nsingle tensor or a tuple of tensors.\n\nNB: These checks allow passing a function that returns a tuple with a\nsingle-element tensor from vmap. That seems OK to me.\n\nTest Plan: - `python test/test_vmap.py -v`\n\nDifferential Revision: D22216166\n\nPulled By: zou3519\n\nfbshipit-source-id: a92215e9c26f6138db6b10ba81ab0c2c2c030929", "pr_number": "40518", "files_changed": ["test/test_vmap.py", "torch/_vmap_internals.py"], "labels": ["merged"]}, "7369dc8d1f": {"title": "Use CPU Allocator for reading from zip container", "body": "Summary:\nThis code path is used to read tensor bodies, so we need it to respect\nalignment and padding requirements.\n\nTest Plan: Ran an internal test that was failing.\n\nReviewed By: zdevito\n\nDifferential Revision: D22225622\n\nfbshipit-source-id: f2126727f96616366850642045ab9704f3885824", "pr_number": null, "files_changed": ["caffe2/serialize/inline_container.cc"], "labels": []}, "461014d54b": {"title": "Unify libtorch_python_cuda_core_sources filelists between CMakeList, fbcode and bazel (#40554)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40554\n\nGet a sublist of `libtorch_python_cuda_sources` named `libtorch_python_cuda_core_sources`. Use it to replace the list which has the same content in `CMakeList.txt`.\nThis is a change to make consistency between CMakeList and bazel.\n\nTest Plan: CI\n\nReviewed By: malfet\n\nDifferential Revision: D22223207\n\nfbshipit-source-id: 2bde3c42a0b2d60d689581561075df4ef52ab694", "pr_number": "40554", "files_changed": ["tools/build_variables.bzl", "torch/CMakeLists.txt"], "labels": ["fb-exported", "merged"]}, "f41173b975": {"title": "[PyPer][quant] Add quantized embedding operators to OSS. (#40076)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40076\n\nPull Request resolved: https://github.com/pytorch/glow/pull/4606\n\n[PyPer][quant] Add quantized embedding operators to OSS.\n\nThis is the first step in supporting Graph Mode Quantization for EmbeddingBag.\n\nAt a high level, the next steps would be\na) Implementation of Embedding prepack/unpack operators,\nb) Implementation of torch.nn.quantized.dynamic.EmbeddingBag Module,\nc) Implementation of torch.nn.quantized.EmbeddingBag Module,\nd) Implementation (modification) of IR passes to support graph quantization of EmbeddingBag module.\n\nMore in-depth details regarding each step will be in the follow up diffs. Consider this as an initial diff that moves operators to respective places that's required for us to proceed.\n\nTest Plan: ```buck test mode/no-gpu caffe2/test:quantization -- --stress-runs 100  test_embedding_bag```\n\nReviewed By: supriyar\n\nDifferential Revision: D21949828\n\nfbshipit-source-id: cad5ed0a855db7583bddb1d93e2da398c128024a", "pr_number": "40076", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py", "test/test_quantization.py"], "labels": ["fb-exported", "merged"]}, "4d40ec1480": {"title": "[PyTorch Error Logging][1/N] Adding Error Logging for Run_Method (#40535)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40535\n\nAdding error logging for run_method.\nAdding CANCEL(the method cannot be found) and FAIL status(error occurred when running the method)\nghstack-source-id: 106604786\n\nTest Plan: {F240891059}\n\nReviewed By: xcheng16\n\nDifferential Revision: D22097857\n\nfbshipit-source-id: 4bdc8e3993e40cb1ba51e4706be6637e3afd40b4", "pr_number": "40535", "files_changed": ["torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/observer.h"], "labels": ["merged", "oncall: jit"]}, "dfbf0164c9": {"title": "Revert D22103662: [NCCL] Explicitly Abort NCCL Communicators on Process Group Destruction", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22103662 (https://github.com/pytorch/pytorch/commit/527ab13436c059631b18630d58b7fdbaf67686f3)\n\nOriginal commit changeset: 1f6f88b56bd7\n\nfbshipit-source-id: d0944462c021ec73c7f883f98609fc4a3408efd9", "pr_number": null, "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": []}, "e231405ef6": {"title": "[jit] Fix type annotations in select assignments (#40528)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40528\n\nPreviously, an assignment like `self.foo : List[int] = []` would ignore\nthe type hint.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D22222927\n\nPulled By: suo\n\nfbshipit-source-id: b0af19b87c6fbe0670d06b55f2002a783d00549d", "pr_number": "40528", "files_changed": ["test/jit/test_class_type.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["merged", "oncall: jit"]}, "c6e0c67449": {"title": "[PyTorch Error Logging][2/N] Adding Error Logging for Loading Model (#40537)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40537\n\nAdding Error Logging when loading model, adding event \"MOBILE_MODULE_LOAD\"\nghstack-source-id: 106615128\n\nTest Plan: {F241028136}\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22098818\n\nfbshipit-source-id: 4de7df4432c7c6c297a9dc173e5cafa13fe2833c", "pr_number": "40537", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/observer.h"], "labels": ["merged", "oncall: jit"]}, "e180ca652f": {"title": "Add __all__ to torch/_C/_VariableFunctions.pyi (#40499)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/40397\n\nInspired by ezyang's comment at https://github.com/pytorch/pytorch/issues/40397#issuecomment-648233001, this PR attempts to leverage using `__all__` to explicitly export private functions from `_VariableFunctions.pyi` in order to make `mypy` aware of them after:\n\n```\nif False:\n    from torch._C._VariableFunctions import *\n```\n\nThe generation of the `__all__` template variable excludes some items from `unsorted_function_hints`, as it seems that those without hints end up not being explicitly included in the `.pyi` file: I leaned on the side of caution and opted for having `__all__` consistent with the definitions inside the file. Additionally, added some pretty-printing to avoid having an extremely long line.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40499\n\nDifferential Revision: D22240716\n\nPulled By: ezyang\n\nfbshipit-source-id: 77718752577a82b1e8715e666a8a2118a9d3a1cf", "pr_number": "40499", "files_changed": ["tools/pyi/gen_pyi.py", "torch/_C/_VariableFunctions.pyi.in"], "labels": ["merged", "open source", "triaged"]}, "b05c34259b": {"title": "relax size check in flatten_for_scatter_gather (#40573)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40573\n\nPer title, to workaround apex sbn bug.\n\nTest Plan: Covered by existing tests\n\nReviewed By: blefaudeux\n\nDifferential Revision: D22236942\n\nfbshipit-source-id: ddb164ee347a7d472a206087e4dbd16aa9d72387", "pr_number": "40573", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["fb-exported", "merged"]}, "c790476384": {"title": "Back out \"Revert D22072830: [wip] Upgrade msvc to 14.13\" (#40594)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40594\n\nOriginal commit changeset: 901de185e607\nghstack-source-id: 106642590\n\nTest Plan: oss ci\n\nDifferential Revision: D22247269\n\nfbshipit-source-id: be0c64d1a579f8aa3999cb84a9d20488095a81bd", "pr_number": "40594", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/vs_install.ps1", ".circleci/verbatim-sources/build-parameters/pytorch-build-params.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "README.md", "aten/src/ATen/core/stack.h"], "labels": ["merged"]}, "ac79c874ce": {"title": "[PyTorch Operator] [2/n] Adding python test", "body": "Summary: Adding python test file with image files wit the input image being p.jpg. Test for the quality difference between the raw image and the decoded image\n\nTest Plan:\nParsing buck files: finished in 1.5 sec\nBuilding: finished in 6.4 sec (100%) 10241/10241 jobs, 2 updated\n  Total time: 8.0 sec\nMore details at https://www.internalfb.com/intern/buck/build/387cb1c1-2902-4f90-ae9f-83fb6d473487\nTpx test run coordinator for Facebook. See https://fburl.com/tpx for details.\nRunning with tpx session id: 93e6ef88-ec68-41cb-9de7-7868a14e6d65\nTrace available for this run at /tmp/tpx-20200623-055836.283269/trace.log\nStarted reporting to test run: https://our.intern.facebook.com/intern/testinfra/testrun/4222124679431330\n    \u2713 ListingSuccess: caffe2/test:test_bundled_images - main (18.865)\n    \u2713 Pass: caffe2/test:test_bundled_images - test_single_tensors (test_bundled_images.TestBundledInputs) (18.060)\n    \u2713 Pass: caffe2/test:test_bundled_images - main (18.060)\nSummary\n  Pass: 2\n  ListingSuccess: 1\nFinished test run: https://our.intern.facebook.com/intern/testinfra/testrun/4222124679431330\n\nReviewed By: dreiss\n\nDifferential Revision: D22046611\n\nfbshipit-source-id: fabc604269a5a4d8a37135ce776200da2794a252", "pr_number": null, "files_changed": ["test/test_bundled_images.py", "test/test_img/p1.jpg"], "labels": []}, "5466231187": {"title": "Fixes lint (#40606)", "body": "Summary:\n'= ' => '='\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40606\n\nDifferential Revision: D22252511\n\nPulled By: mruberry\n\nfbshipit-source-id: 5f90233891be58a742371e4416166a267aee4669", "pr_number": "40606", "files_changed": ["test/test_bundled_images.py"], "labels": ["merged"]}, "547ea787ff": {"title": "[ONNX] Add eliminate_unused_items pass (#38812)", "body": "Summary:\nThis PR:\n\n- Adds eliminate_unused_items pass that removes unused inputs and initializers.\n- Fixes run_embed_params function so it doesn't export unnecessary parameters.\n- Removes  test_modifying_params in test_verify since it's no longer needed.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38812\n\nReviewed By: ezyang\n\nDifferential Revision: D22236416\n\nPulled By: houseroad\n\nfbshipit-source-id: 30e1a6e8823a7e36b51ae1823cc90476a53cd5bb", "pr_number": "38812", "files_changed": ["test/onnx/debug_embed_params.py", "test/onnx/test_utility_funs.py", "test/onnx/test_verify.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/onnx/constant_fold.cpp", "torch/csrc/jit/passes/onnx/eliminate_unused_items.cpp", "torch/csrc/jit/passes/onnx/eliminate_unused_items.h", "torch/csrc/jit/passes/onnx/helper.cpp", "torch/csrc/jit/passes/onnx/helper.h", "torch/csrc/jit/python/init.cpp", "torch/onnx/utils.py"], "labels": ["merged", "oncall: jit", "open source", "triaged"]}, "fab412a8f3": {"title": "Bump nightlies to 1.7.0 (#40519)", "body": "Summary:\nedit: apparently we hardcode a lot more versions that I would've anticipated.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40519\n\nDifferential Revision: D22221280\n\nPulled By: seemethere\n\nfbshipit-source-id: ba15a910a6755ec08c10f7783ed72b1e06e6b570", "pr_number": "40519", "files_changed": [".circleci/scripts/binary_populate_env.sh", "android/README.md", "android/gradle.properties", "android/test_app/app/build.gradle", "test/onnx/expect/TestOperators.test_softmaxcrossentropy.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_3d.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_3d_none.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_4d.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_ignore_index.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_weights.expect", "torch/csrc/onnx/onnx.h", "version.txt"], "labels": ["merged", "module: ci", "releng"]}, "3ab60ff696": {"title": "Remove cpu vec256 for std::complex (#39830)", "body": "Summary:\nstd::complex is gone. We are now using c10::complex\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39830\n\nDifferential Revision: D22252066\n\nPulled By: malfet\n\nfbshipit-source-id: cdd5bb03ec66825d82177d609cbcf0738922dba0", "pr_number": "39830", "files_changed": ["aten/src/ATen/cpu/vec256/vec256.h", "aten/src/ATen/cpu/vec256/vec256_std_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_std_complex_float.h"], "labels": ["merged", "module: complex", "open source"]}, "f1406c43fc": {"title": "[papaya][aten] Fix compiler error: loop variable 'tensor' is always a copy because the range of type 'c10::List<at::Tensor>' does not return a reference. (#40599)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40599\n\n.\n\nTest Plan: CI\n\nReviewed By: smessmer\n\nDifferential Revision: D22246106\n\nfbshipit-source-id: a5d0535e627b9f493fca7234dcfc15c521b0ed7f", "pr_number": "40599", "files_changed": ["aten/src/ATen/core/dispatch/DispatchKeyExtractor.h"], "labels": ["fb-exported", "merged"]}, "dfc7e71d13": {"title": "[Selective Build] Apply query-based on instrumentation_tests", "body": "Summary:\n1. Modularize some bzl files to break circular buck load\n2. Use query-based on instrumentation_tests\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: kwanmacher\n\nDifferential Revision: D22188728\n\nfbshipit-source-id: affbabd333c51c8b1549af6602c6bb79fabb7236", "pr_number": null, "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/LiteNativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/NativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java", "android/pytorch_android/src/main/java/org/pytorch/PyTorchCodegenLoader.java"], "labels": []}, "44bf822084": {"title": "Add C++ standard version check to top level headers (#40510)", "body": "Summary:\nRemove `-std=c++14` flag from `utils.cmake`, since PyTorch C++ API can be invoked by any compiler compliant with C++14 standard or later\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40510\n\nDifferential Revision: D22253313\n\nPulled By: malfet\n\nfbshipit-source-id: ff731525868b251c27928fc98b0724080ead9be2", "pr_number": "40510", "files_changed": ["aten/src/ATen/ATen.h", "cmake/public/utils.cmake", "torch/csrc/api/include/torch/all.h"], "labels": ["merged"]}, "a4cabd1a3c": {"title": "Generalize Python dispatcher testing API; disallow overwriting fallback (#40469)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40469\n\n- The old testing interface C._dispatch_import was based off the old\n  c10::import variation, which meant the API lined up in a strange\n  way with the actual torch/library.h.  This diff reduces the\n  differences by letting you program the Library constructor directly.\n\n- Using this newfound flexibility, we add a test for backend fallbacks\n  from Python; specifically testing that we disallow registering a\n  backend fallback twice.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22236351\n\nPulled By: ezyang\n\nfbshipit-source-id: f8365e3033e9410c7e6eaf9f78aa32e1f7d55833", "pr_number": "40469", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/library.cpp", "test/test_dispatch.py", "torch/csrc/utils/python_dispatch.cpp"], "labels": ["merged"]}, "a0ba7fb43e": {"title": "Precompute entries in dispatch tables (#40512)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40512\n\nFixes https://github.com/pytorch/pytorch/issues/32454\n\nThe heart of this diff is changing this:\n\n```\ninline const KernelFunction& Dispatcher::dispatch_(const DispatchTable& dispatchTable, DispatchKey dispatchKey) c\nnst {\n  const KernelFunction* backendKernel = dispatchTable.lookup(dispatchKey);\n\n  if (nullptr != backendKernel) {\n    return *backendKernel;\n  }\n\n  const auto& backendFallbackKernel = backendFallbackKernels_[dispatchKey];\n  if (backendFallbackKernel.isValid()) {\n    return backendFallbackKernel;\n  }\n\n  const KernelFunction* catchallKernel = dispatchTable.lookupCatchallKernel();\n  if (C10_LIKELY(nullptr != catchallKernel)) {\n    return *catchallKernel;\n  }\n\n  reportError(dispatchTable, dispatchKey);\n}\n```\n\nto this:\n\n```\nconst KernelFunction& OperatorEntry::lookup(DispatchKey k) const {\n  const auto& kernel = dispatchTable_[static_cast<uint8_t>(k)];\n  if (C10_UNLIKELY(!kernel.isValid())) {\n    reportError(k);\n  }\n  return kernel;\n}\n```\n\nThe difference is that instead of checking a bunch of places to find the\nright kernel to use for an operator, all of the operators are\nprecomputed into dispatchTable_ itself (so you don't have to consult\nanything else at runtime.)  OperatorEntry::computeDispatchTableEntry\ncontains that computation (which is exactly the same as it was before.)\nBy doing this, we are able to substantially simplify many runtime\ncomponents of dispatch.\n\nThe diff is fairly large, as there are also some refactors interspersed\nwith the substantive change:\n\n- I deleted the DispatchTable abstraction, folding it directly into\n  OperatorEntry.  It might make sense to have some sort of DispatchTable\n  abstraction (if only to let you do operator[] on DispatchKey without\n  having to cast it to integers first), but I killed DispatchTable to\n  avoid having to design a new abstraction; the old abstraction wasn't\n  appropriate for the new algorithm.\n\n- I renamed OperatorEntry::KernelEntry to AnnotatedKernel, and use it\n  to store backend fallbacks as well as regular kernel registrations\n  (this improves error messages when you incorrectly register a backend\n  fallback twice).\n\n- I moved schema_ and debug_ into an AnnotatedSchema type, to make the\n  invariant clearer that these are set together, or not at all.\n\n- I moved catch-all kernels out of kernels_ into its own property\n  (undoing a refactor I did before).  The main reason I did this was\n  because our intended future state is to not have a single catch-all,\n  but rather possibly multiple catch-alls which fill-in different\n  portions of the dispatch table.  This may change some more in\n  the future: if we allow registrations for multiple types of\n  catch alls, we will need a NEW data type (representing bundles\n  of dispatch keys) which can represent this case, or perhaps\n  overload DispatchKey to also record these types.\n\nThe key changes for precomputation:\n\n- OperatorEntry::updateDispatchTable_ is now updated to fill in the\n  entry at a DispatchKey, considering both kernels (what it did\n  before) as well as catch-all and backend fallback.  There is also\n  OperatorEntry::updateDispatchTableFull_ which will update the\n  entire dispatch table (which is necessary when someone sets a\n  catch-all kernel).  OperatorEntry::computeDispatchTableEntry\n  holds the canonical algorithm specifying how we decide what\n  function will handle a dispatch key for the operator.\n\n- Because dispatch table entry computation requires knowledge of\n  what backend fallbacks are (which is recorded in Dispatcher,\n  not OperatorEntry), several functions on OperatorEntry now\n  take Dispatcher as an argument so they can query this information.\n\n- I modified the manual boxing wrapper invariant: previously, kernels\n  stored in kernels_ did NOT have manual boxing wrappers and this\n  was maintained by DispatchTable.  Now, we just ALWAYS maintain\n  manual boxing wrappers for all KernelFunctions we store.\n\n- DispatchKeyExtractor is greatly simplified: we only need to maintain\n  a single per-operator bitmask of what entries are fallthrough\n  (we don't need the global bitmask anymore).\n\n- Introduced a new debugging 'dumpComputedTable' method, which prints\n  out the computed dispatch table, and how we computed it to be some way.\n  This was helpful for debugging cases when the dispatch table and\n  the canonical metadata were not in sync.\n\nThings that I didn't do but would be worth doing at some point:\n\n- I really wanted to get rid of the C10_UNLIKELY branch for\n  whether or not the KernelFunction is valid, but it looks like\n  I cannot easily do this while maintaining good error messages.\n  In principle, I could always populate a KernelFunction which\n  errors, but the KernelFunction needs to know what the dispatch\n  key that is missing is (this is not passed in from the\n  calling convention).  Actually, it might be possible to do\n  something with functors, but I didn't do it here.\n\n- If we are going to get serious about catchalls for subsets of\n  operators, we will need to design a new API for them.  This diff\n  is agnostic to this question; we don't change public API at all.\n\n- Precomputation opens up the possibility of subsuming DispatchStub\n  by querying CPU capability when filling in the dispatch table.\n  This is not implemented yet. (There is also a mild blocker here,\n  which is that DispatchStub is also used to share TensorIterator\n  configuration, and this cannot be directly supported by the\n  regular Dispatcher.)\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22236352\n\nPulled By: ezyang\n\nfbshipit-source-id: d6d90f267078451816b1899afc3f79737b4e128c", "pr_number": "40512", "files_changed": ["aten/src/ATen/core/dispatch/DispatchKeyExtractor.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/DispatchTable.cpp", "aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "test/test_dispatch.py"], "labels": ["merged"]}, "67c79bb045": {"title": "update schema to reflect aliasing behavior (#39794)", "body": "Summary:\nFixes: https://github.com/pytorch/pytorch/issues/38555\n\nI did an audit of `native_functions.yaml` and found several functions in addition to `reshape` which were not reporting that they could alias:\n\n```\ntorch.jit.script\ndef foo(t: torch.Tensor):\n    new_value = torch.tensor(1, dtype=t.dtype, device=t.device)\n\n    t.flatten()[0] = new_value\n    t.reshape(-1)[1] = new_value\n    t.view_as(t)[2] = new_value\n    t.expand_as(t)[3] = new_value\n    t.reshape_as(t)[4] = new_value\n    t.contiguous()[5] = new_value\n    t.detach()[6] = new_value\n\n    return t\n```\n\nCurrently none of the values are assigned after dead code elimination, after this PR all are. (And the JIT output matches that of eager.)\n\nI don't think this needs to be unit tested; presumably the generic machinery already is and this just brings these ops under the same umbrella.\n\n**BC-breaking note**: This updates the native operator schema and the aliasing rules for autograd. JIT passes will no longer incorrectly optimize mutations on graphs containing these ops, and inplace ops on the result of `flatten` will now properly be tracked in Autograd and the proper backward graph will be created.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39794\n\nDifferential Revision: D22008358\n\nPulled By: robieta\n\nfbshipit-source-id: 9d3ff536e58543211e08254a75c6110f2a3b4992", "pr_number": "39794", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "tools/autograd/gen_autograd.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/jit/passes/requires_grad_analysis.cpp", "torch/csrc/jit/passes/shape_analysis.cpp"], "labels": ["merged", "oncall: jit", "topic: bc-breaking"]}, "fc8bca094c": {"title": "skip_if_rocm test_rnn in test_c10d_spawn.py (#40577)", "body": "Summary:\nTest was added a few months back in https://github.com/pytorch/pytorch/issues/36503 but recently became flaky for ROCm.\n\nCC ezyang xw285cornell sunway513\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40577\n\nDifferential Revision: D22258196\n\nPulled By: ezyang\n\nfbshipit-source-id: 8a22b0c17b536b3d42d0382f7737df0f8823ba08", "pr_number": "40577", "files_changed": ["test/distributed/test_c10d_spawn.py"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "10822116c5": {"title": "build docker image for CUDA11 (#40534)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40534\n\nDifferential Revision: D22258874\n\nPulled By: seemethere\n\nfbshipit-source-id: 1954a22ed52e1a65caf89725ab1db9f40ff917b8", "pr_number": "40534", "files_changed": [".circleci/cimodel/data/simple/util/docker_constants.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".circleci/verbatim-sources/workflows/workflows-ecr-gc.yml"], "labels": ["merged", "open source", "triaged"]}, "fb5d784fb4": {"title": "Further reduce windows build/test matrix (#40592)", "body": "Summary:\nSwitch windows CPU testers from `windows.xlarge` to `windows.medium` class.\nRemove VS 14.16 CUDA build\nOnly do smoke force-on-cpu tests using VS2019+CUDA10.1 config.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40592\n\nDifferential Revision: D22259351\n\nPulled By: malfet\n\nfbshipit-source-id: f934ff774dfc7d47f12c3da836ca314c12d92208", "pr_number": "40592", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/build-parameters/binary-build-params.yml", ".circleci/verbatim-sources/build-parameters/pytorch-build-params.yml", ".circleci/verbatim-sources/header-section.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "7676682584": {"title": "Fix illegal opcode bug in caffe2 (#40584)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40584\n\nAlso patch [this github issue](https://github.com/pytorch/pytorch/issues/33124)\ninvolving an illegal assembly instruction in 8x8-dq-aarch64-neon.S.\n\nTest Plan:\nBuild binaries, copy to shaker, run executables. Also run all\nexisting caffe tests.\n\nReviewed By: kimishpatel\n\nDifferential Revision: D22240670\n\nfbshipit-source-id: 51960266ce58699fe6830bcf75632b92a122f638", "pr_number": "40584", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm/8x8-dq-aarch64-neon.S"], "labels": ["fb-exported", "merged"]}, "5036c94a6e": {"title": "properly skip legacy tests regardless of the default executor (#40381)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40381\n\nDifferential Revision: D22173938\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 305fc4484977e828cc4cee6e053a1e1ab9f0d6c7", "pr_number": "40381", "files_changed": ["test/test_jit_fuser_te.py"], "labels": ["merged"]}, "b8f4f6868d": {"title": "[JIT] Remove dead store in exit_transforms.cpp (#40611)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40611\n\nThis commit removes a dead store in `transformWith` of exit_transforms.cpp.\n\nTest Plan: Continuous integration.\n\nReviewed By: suo\n\nDifferential Revision: D22254136\n\nfbshipit-source-id: f68c4625f7be8ae29b3500303211b2299ce5d6f6", "pr_number": "40611", "files_changed": ["torch/csrc/jit/frontend/exit_transforms.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "0494e0ad70": {"title": "Back out \"Revert D21581908: Move TensorOptions ops to c10\" (#40595)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40595\n\nghstack-source-id: 106691774\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D22247729\n\nfbshipit-source-id: 14745588cae267c1e0cc51cd9541a9b8abb830e5", "pr_number": "40595", "files_changed": ["aten/src/ATen/common_with_cwrap.py", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h", "aten/src/ATen/cwrap_parser.py", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/gen_backend_select_register.py", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "aten/src/ATen/nn_parse.py", "aten/src/ATen/templates/BackendSelectRegister.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/PerOpRegistration.cpp", "aten/src/ATen/templates/SparseTypeDerived.cpp", "aten/src/ATen/templates/TypeDefault.cpp", "aten/src/ATen/templates/TypeDerived.cpp", "aten/src/ATen/test/extension_backend_test.cpp", "c10/core/ScalarType.h", "c10/test/util/TypeList_test.cpp", "c10/util/TypeList.h", "test/cpp_extensions/msnpu_extension.cpp", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/ProfiledType.cpp", "tools/autograd/templates/VariableType.cpp"], "labels": ["merged"]}, "edac323378": {"title": "Add special rules to launch docker image with RocM (#40632)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40632\n\nDifferential Revision: D22262316\n\nPulled By: malfet\n\nfbshipit-source-id: 3d525767bfbfc8e2497541849d85cabf0379a43b", "pr_number": "40632", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "41ea7f2d86": {"title": "Add channels-last support to bundled_inputs (#36764)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36764\n\nThis allows bundling inputs that are large uniform buffers in\nchannels-last memory format.\n\nTest Plan: Unit test.\n\nDifferential Revision: D21142660\n\nPulled By: dreiss\n\nfbshipit-source-id: 31bbea6586d07c1fd0bcad4cb36ed2b8bb88a7e4", "pr_number": "36764", "files_changed": ["test/test_bundled_inputs.py", "torch/utils/bundled_inputs.py"], "labels": ["merged"]}, "375cd852fa": {"title": "Add a utility function for bundling large input tensors (#37055)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37055\n\nSometimes it's okay to bundle a large example input tensor with a model.\nAdd a utility function to make it easy for users to do that *on purpose*.\n\nTest Plan: Unit test.\n\nDifferential Revision: D22264239\n\nPulled By: dreiss\n\nfbshipit-source-id: 05c6422be1aa926cca850f994ff1ae83c0399119", "pr_number": "37055", "files_changed": ["test/test_bundled_inputs.py", "torch/utils/bundled_inputs.py"], "labels": ["merged"]}, "6debc28964": {"title": "Ignore error code from `apt-get purge` (#40631)", "body": "Summary:\nThis replicates the pattern of other \"do for luck\" commands.\nPrep change to add RocM to CircleCI.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40631\n\nDifferential Revision: D22261707\n\nPulled By: malfet\n\nfbshipit-source-id: 3dadfa434deab866a8800715f3197e84169cf43e", "pr_number": "40631", "files_changed": [".circleci/scripts/setup_linux_system_environment.sh"], "labels": ["merged"]}, "24a8614cac": {"title": "[Reland][doc] Add overflow notice for cuFFT on half precision (#40551)", "body": "Summary:\nReland of https://github.com/pytorch/pytorch/issues/35594\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40551\n\nReviewed By: ezyang\n\nDifferential Revision: D22249831\n\nPulled By: ngimel\n\nfbshipit-source-id: b221b3c0a490ccaaabba50aa698a2490536e0917", "pr_number": "40551", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source", "triaged"]}, "47c72be3d7": {"title": "Port /test/cpp_extensions/rng_extension.cpp to new operator registration API (#39459)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39459\n\nUpdate to this PR: this code isn't going to fully solve https://github.com/pytorch/pytorch/issues/37010. The changes required for 37010 is more than this PR initially planned. Instead, this PR switches op registration of rng related tests to use the new API (similar to what was done in #36925)\n\nTest Plan:\n1) unit tests\n\nImported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22264889\n\nfbshipit-source-id: 82488ac6e3b762a756818434e22c2a0f9cb9dd47", "pr_number": "39459", "files_changed": ["aten/src/ATen/test/extension_backend_test.cpp", "test/cpp_extensions/rng_extension.cpp", "test/test_cpp_extensions_aot.py"], "labels": ["merged"]}, "897e610c82": {"title": "FP16 rounding-to-nearest for row-wise SparseAdagrad fusion (#40466)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40466\n\nExtend row wise sparse Adagrad fusion op to FP16 (rounding-to-nearest) for PyTorch.\n\nReviewed By: jianyuh\n\nDifferential Revision: D22003571\n\nfbshipit-source-id: e97e01745679a9f6e7b0f81ce5a6ebf4d4a1df41", "pr_number": "40466", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cuh"], "labels": ["fb-exported", "merged"]}, "4102fbdf08": {"title": "[1/n] Allow dense NaN value in dper raw input processor output", "body": "Summary:\n## TLDR\nSupport using NaN default value for missing dense features in RawInputProcessor for *DPER2*. In preparation for subsequent support for null flag features in *compute meta*. For train_eval this is already supported in DPER3 and we do not plan to support this in DPER2 train eval.\n## Overview\nIntern project plan to support adding dense flags for missing feature values instead of replacing with zero.\n\nProject plan :\nhttps://docs.google.com/document/d/1OsPUTjpJycwxWLCue3Tnb1mx0uDC_2KKWvC1Rwpo2NI/edit?usp=sharing\n\n## Code paths:\nSee https://fb.quip.com/eFXUA0tbDmNw for the call stack for all affected code paths.\n\nTest Plan:\n# A. DPER3 blob value inspection\n## 1. Build local bento kernel in fbcode folder\n`buck build mode/dev-nosan //bento/kernels:bento_kernel_ads_ranking`\n\n## 2. Use kernel `ads_ranking (local)` to print dense feature blob values\nn280239\n\n## 2.1 Try `default_dense_value = \"0.0\"` (default)\n```\npreproc_6/feature_preproc_6/dper_feature_processor_7/raw_input_proc_7/float_feature_sparse_to_dense_7/float_features [[0.       ]\n [0.       ]\n [0.       ]\n [0.       ]\n [0.       ]\n [0.       ]\n [0.       ]\n [1.       ]\n [1.7857143]\n [1.7777778]\n [1.       ]\n [0.       ]\n [0.5625   ]\n [0.       ]\n [0.       ]\n [0.8      ]\n [0.       ]\n [1.       ]\n [0.56     ]\n [0.       ]]\n```\n## 2.2 Try `default_dense_value = \"123\"`\n```\npreproc_2/feature_preproc_2/dper_feature_processor_3/raw_input_proc_3/float_feature_sparse_to_dense_3/float_features [[123.       ]\n [123.       ]\n [123.       ]\n [123.       ]\n [123.       ]\n [123.       ]\n [123.       ]\n [  1.       ]\n [  1.7857143]\n [  1.7777778]\n [  1.       ]\n [123.       ]\n [  0.5625   ]\n [123.       ]\n [123.       ]\n [  0.8      ]\n [123.       ]\n [  1.       ]\n [  0.56     ]\n [123.       ]]\n```\n## 2.3 Try `default_dense_value = float(\"nan\")`\n```\nRuntimeError: [enforce fail at enforce_finite_op.h:40] std::isfinite(input_data[i]). Index 0 is not finite (e.g., NaN, Inf): -nan (Error from operator:\ninput: \"unary_4/logistic_regression_loss_4/average_loss_4/average_loss\" name: \"\" type: \"EnforceFinite\" device_option { random_seed: 54 })\n```\nwhich is expected due to nan input.\n\n# B. Unit test\n`buck test  fblearner/flow/projects/dper/tests/preprocs:raw_feature_extractor_test`\n\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/5348024586274923/\n\n{F241336814}\n\nDifferential Revision: D21961595\n\nfbshipit-source-id: 3dcb153b3c7f42f391584f5e7f52f3d9c76de31f", "pr_number": null, "files_changed": ["caffe2/python/layers/feature_sparse_to_dense.py"], "labels": []}, "15864d1703": {"title": "Skip allreducing `local_used_maps_dev_` when `find_unused_param=False`", "body": "Summary:\n1. In reducer.cpp, we have a new boolean `find_unused_param_` and its value is set in `Reducer::prepare_for_backward`.\nIf `!find_unused_param_`, then it avoids `allreduce(local_used_maps_dev_)`.\n2. Solves issue [38942](https://github.com/pytorch/pytorch/issues/38942).\n3. Fixes incorrect `find_unused_parameters_` passing like checking `outputs.empty()` or `unused_parameters_.empty()`.\n\nghstack-source-id: 106693089\n\nTest Plan:\n1. Run `test/distributed/test_c10d.py` and make sure all tests pass.\n2. A new test case `test_find_unused_parameters_when_unused_parameters_empty` is included. Old `reducer.cpp` was failing in that unit test because it was checking `find_unused_parameters_` by `unused_parameters_.empty()`. Current `reducer.cpp` passes this unit test.\n3. Two test cases were failing `test_forward_backward_unused_parameters` and `test_forward_backward_optimizer` , because `find_unused_parameter_` of their `reducer` object was not set properly. I fixed that as well.\n\nImported from OSS\n\n**Output of version 14:**\n```\n................s.....s...............................................test/distributed/test_c10d.py:1531: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)\n  tensor = torch.full([100, 100], self.rank)\ntest/distributed/test_c10d.py:1531: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)\n  tensor = torch.full([100, 100], self.rank)\ntest/distributed/test_c10d.py:1531: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)\n  tensor = torch.full([100, 100], self.rank)\ntest/distributed/test_c10d.py:1531: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)\n  tensor = torch.full([100, 100], self.rank)\n.test/distributed/test_c10d.py:1554: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)\n  self.assertEqual(torch.full([10, 10], self.world_size), tensor)\ntest/distributed/test_c10d.py:1554: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)\n  self.assertEqual(torch.full([10, 10], self.world_size), tensor)\ntest/distributed/test_c10d.py:1554: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)\n  self.assertEqual(torch.full([10, 10], self.world_size), tensor)\ntest/distributed/test_c10d.py:1554: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)\n  self.assertEqual(torch.full([10, 10], self.world_size), tensor)\n.....s...............................\n----------------------------------------------------------------------\nRan 108 tests in 214.210s\n\nOK (skipped=3)\n```\n\nDifferential Revision: D22176231\n\nfbshipit-source-id: b5d15f034e13a0915a474737779cc5aa8e068836", "pr_number": null, "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/nn/parallel/distributed.py"], "labels": []}, "e368b11226": {"title": "[JIT] Remove dead stores in loopnest.cpp (#40626)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40626\n\nTest Plan: Continuous integration.\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22259586\n\nfbshipit-source-id: 447accb5b94392f0b5e4c27956a34403bb0d1ea8", "pr_number": "40626", "files_changed": ["torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "0a19534dd2": {"title": "[JIT] Remove dead store in quantization_patterns.h (#40623)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40623\n\nTest Plan: Continuous integration.\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22259209\n\nfbshipit-source-id: 90c9e79e039100f2961195504bb81230bba5c5fe", "pr_number": "40623", "files_changed": ["torch/csrc/jit/passes/quantization/quantization_patterns.h"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "0309f6a4bb": {"title": "[quant][graphmode][fix] cloning schema in insert_observers (#40624)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40624\n\nPreviously we didn't clone schema, so the default schema is used, this is\ncausing issue for some models\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22259519\n\nfbshipit-source-id: e2a393a54cb18f55da0c7152a74ddc22079ac350", "pr_number": "40624", "files_changed": ["torch/csrc/jit/passes/quantization/insert_observers.cpp"], "labels": ["merged", "oncall: jit"]}, "61a8de77cf": {"title": "[quant] aten::repeat work for quantized tensor (#40644)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40644\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22268558\n\nfbshipit-source-id: 3bc9a129bece1b547c519772ecc6b980780fb904", "pr_number": "40644", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/TensorFactories.cpp", "test/quantization/test_quantized_tensor.py", "torch/_overrides.py"], "labels": ["merged"]}, "411bc2b8d5": {"title": "[quant][graphmode][fix] remove unsupported ops in the list (#40653)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40653\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22271413\n\nfbshipit-source-id: a01611b5d90849ac673fa5a310f910c858e907a3", "pr_number": "40653", "files_changed": ["torch/csrc/jit/passes/quantization/helper.cpp"], "labels": ["merged", "oncall: jit"]}, "ac8c8b028d": {"title": "[ROCm] restore jit tests (#40447)", "body": "Summary:\nRemove `skipIfRocm` from most jit tests and enable `RUN_CUDA_HALF` tests for ROCm.\n\nThese changes passed more than three rounds of CI testing against the ROCm CI.\n\nCC ezyang xw285cornell sunway513\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40447\n\nDifferential Revision: D22190711\n\nPulled By: xw285cornell\n\nfbshipit-source-id: bac44825a2675d247b3abe2ec2f80420a95348a3", "pr_number": "40447", "files_changed": ["test/run_test.py", "test/test_cpp_extensions_jit.py", "test/test_jit.py", "test/test_jit_cuda_fuser.py", "test/test_jit_fuser_te.py", "torch/testing/_internal/jit_utils.py"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "68042c7466": {"title": "Skip mypy on pynightly if numpy-1.20.0-dev0... is used (#40656)", "body": "Summary:\nAlso modernize the test script itself by using `mypy.api.run` rather than `subprocess.call`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40656\n\nDifferential Revision: D22274421\n\nPulled By: malfet\n\nfbshipit-source-id: 59232d4d37ee01cda56375b84ac1476d16686bfe", "pr_number": "40656", "files_changed": ["test/test_type_hints.py"], "labels": ["merged"]}, "15be823455": {"title": "caffe2 | Revert range loop analysis fix", "body": "Summary: This reverts a change that was made to fix range loop analysis warning.\n\nTest Plan: CI\n\nReviewed By: nlutsenko\n\nDifferential Revision: D22274461\n\nfbshipit-source-id: dedc3fcaa6e32259460380163758d6c9c9b73211", "pr_number": null, "files_changed": ["caffe2/opt/mobile.cc"], "labels": []}, "2456e078d3": {"title": "[TB] Support custom run_name in add_hparams (#40660)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40660\n\nSupport custom run_name since using timestamp as run_name can be confusing to people\n\nTest Plan:\nhp = {\"lr\": 0.1, \"bool_var\": True, \"string_var\": \"hi\"}\n  mt = {\"accuracy\": 0.1}\n  writer.add_hparams(hp, mt, run_name=\"run1\")\n  writer.flush()\n\nReviewed By: edward-io\n\nDifferential Revision: D22157749\n\nfbshipit-source-id: 3d4974381e3be3298f3e4c40e3d4bf20e49dfb07", "pr_number": "40660", "files_changed": ["torch/utils/tensorboard/writer.py"], "labels": ["fb-exported", "merged"]}, "4c25428c8c": {"title": "[TB] Add support for hparam domain_discrete", "body": "Summary: Add support for populating domain_discrete field in TensorBoard add_hparams API\n\nTest Plan: Unit test test_hparams_domain_discrete\n\nReviewed By: edward-io\n\nDifferential Revision: D22227939\n\nfbshipit-source-id: d2f0cd8e5632cbcc578466ff3cd587ee74f847af", "pr_number": null, "files_changed": ["test/expect/TestTensorBoard.test_hparams_domain_discrete.expect", "test/test_tensorboard.py", "torch/utils/tensorboard/summary.py", "torch/utils/tensorboard/writer.py"], "labels": []}, "21991b63f5": {"title": "Migrate `dot` from the TH to Aten (CPU) (#40354)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/24692\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40354\n\nReviewed By: ezyang\n\nDifferential Revision: D22214203\n\nPulled By: ngimel\n\nfbshipit-source-id: 500e60d1c02b3b39db19b518f2af43cd69f2e984", "pr_number": "40354", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/BlasKernel.cpp", "aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/THBlasUtils.h", "aten/src/TH/generic/THBlas.cpp", "aten/src/TH/generic/THBlas.h", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h"], "labels": ["merged", "open source", "triaged"]}, "1399655a98": {"title": "[Fix] torch_common target shared by lite-interpreter and full-jit", "body": "Summary:\nPull the shared source files to \"torch_common\" to avoid duplicated symbols and operator registrations.\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\nCI\nbuck install -c fbandroid.force_native_library_merge_map=true -c pt.build_from_deps_query=1 -c pt.selective_build=0 -c pt.static_dispatch=0 -r fb4a\n\nReviewed By: kwanmacher\n\nDifferential Revision: D22275201\n\nfbshipit-source-id: dafd3ad36bb33e3ec33f4accfdc5af1d5f8ab775", "pr_number": null, "files_changed": ["BUILD.bazel", "tools/build_variables.bzl"], "labels": []}, "5377827b3e": {"title": "Revert D22275201: [Fix] torch_common target shared by lite-interpreter and full-jit", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22275201 (https://github.com/pytorch/pytorch/commit/1399655a982489860a349fe79ab8faa0e32175af)\n\nOriginal commit changeset: dafd3ad36bb3\n\nfbshipit-source-id: a89c8b1fbb55eb7c116dd6ca9dad04bb90727c0a", "pr_number": null, "files_changed": ["BUILD.bazel", "tools/build_variables.bzl"], "labels": []}, "502ec8f7f7": {"title": "Revert D22227939: [TB] Add support for hparam domain_discrete", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22227939 (https://github.com/pytorch/pytorch/commit/4c25428c8c7cd1667b9c5d64439339d276801403)\n\nOriginal commit changeset: d2f0cd8e5632\n\nfbshipit-source-id: c4329fcead69cb0f3d368a254d8756fb04be742d", "pr_number": null, "files_changed": ["test/expect/TestTensorBoard.test_hparams_domain_discrete.expect", "test/test_tensorboard.py", "torch/utils/tensorboard/summary.py", "torch/utils/tensorboard/writer.py"], "labels": []}, "b4db529352": {"title": "Fix wrong link in docs/source/notes/ddp.rst (#40484)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40484\n\nDifferential Revision: D22259834\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4ec912c600c81010bdb2778c35cbb0321480199f", "pr_number": "40484", "files_changed": ["docs/source/notes/ddp.rst"], "labels": ["merged", "open source", "triaged"]}, "b35cdc5200": {"title": "[Fix] torch_common target shared by lite-interpreter and full-jit\" and turn on query-based selective build (#40673)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40673\n\nAs title. We planed to have lite-interpreter and full-jit co-exist for short-term. To avoid the duplicated symbol and operator registrations in dynamic lib loading, we put the common files in a separate component.\n\nThe original source file list names are reserved.\nghstack-source-id: 106757184\n\nTest Plan: CI\n\nReviewed By: kwanmacher\n\nDifferential Revision: D22276185\n\nfbshipit-source-id: 328a8ba9c3d88437da0d30c6e6791087d0df5e2e", "pr_number": "40673", "files_changed": ["tools/build_variables.bzl"], "labels": ["merged"]}, "4121d34036": {"title": "Python/C++ API Parity: Add impl and tests for ParameterDict (#40654)", "body": "Summary:\nThis diff contains the implementation of C++ api for ParameterDict from https://github.com/pytorch/pytorch/issues/25883, refer to  https://github.com/pytorch/pytorch/issues/36904 and https://github.com/pytorch/pytorch/issues/28652\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40654\n\nTest Plan: Add unit test in this diff\n\nDifferential Revision: D22273265\n\nPulled By: glaringlee\n\nfbshipit-source-id: 9134a92c95eacdd53d5b24470d5f7edbeb40a488", "pr_number": "40654", "files_changed": ["test/cpp/api/CMakeLists.txt", "test/cpp/api/parameterdict.cpp", "torch/csrc/api/include/torch/nn/module.h", "torch/csrc/api/include/torch/nn/modules.h", "torch/csrc/api/include/torch/nn/modules/container/parameterdict.h"], "labels": ["merged", "open source"]}, "4a174c83ca": {"title": "Add option to preserve certain methods during optimize_for_mobile. (#40629)", "body": "Summary:\nBy default freeze_module pass, invoked from optimize_for_mobile,\npreserves only forward method. There is an option to specify a list of\nmethods that can be preserved during freeze_module. This PR exposes that\nto optimize_for_module pass.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40629\n\nTest Plan: python test/test_mobile_optimizer.py\n\nReviewed By: dreiss\n\nDifferential Revision: D22260972\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 452c653269da8bb865acfb58da2d28c23c66e326", "pr_number": "40629", "files_changed": ["test/test_mobile_optimizer.py", "torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.h", "torch/csrc/jit/python/init.cpp", "torch/utils/mobile_optimizer.py"], "labels": ["merged", "oncall: jit"]}, "ed83b9a4be": {"title": "Change function parameter `self` to `input` in torch.__init__.pyi (#40235)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/40223: Incorrect \"self\" keyword arguments in `torch.__init__.pyi` type hints\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40235\n\nDifferential Revision: D22285816\n\nPulled By: ezyang\n\nfbshipit-source-id: ebc35290c0c625916289f1a46abc6ff2197f4bcf", "pr_number": "40235", "files_changed": ["tools/pyi/gen_pyi.py"], "labels": ["merged", "open source", "triaged"]}, "63e5a53b8c": {"title": "DNNL: fix build error when DNNL using TBB threading pool (#40699)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40699\n\nDifferential Revision: D22286334\n\nPulled By: albanD\n\nfbshipit-source-id: 0635a0a5e4bf80d44d90c86945d92e98e26ef480", "pr_number": "40699", "files_changed": ["cmake/Modules/FindMKLDNN.cmake", "third_party/mkl-dnn.BUILD", "third_party/sleef.BUILD"], "labels": ["merged", "module: mkldnn", "open source", "triaged"]}, "fd90e4b309": {"title": "[CircleCI] Add RocM build/test jobs (#39760)", "body": "Summary:\nSet PYTORCH_ROCM_ARCH to `gfx900;gfx906` if `CIRCLECI` environment variable is defined\nAdd RocM build test jobs and schedule them on `xlarge` and `amd-gpu` resource classes respectively.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39760\n\nDifferential Revision: D22290335\n\nPulled By: malfet\n\nfbshipit-source-id: 7462f97b262abcacac3e515086ac6236a45626d2", "pr_number": "39760", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/cimodel/data/simple/util/docker_constants.py", ".circleci/config.yml", ".jenkins/pytorch/build.sh"], "labels": ["merged"]}, "11a74a58c8": {"title": "Setter for real and imag tensor attributes (#39860)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39860\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D22163234\n\nPulled By: anjali411\n\nfbshipit-source-id: 35b4aa16499341edff1a4be4076539ac7c74f5be", "pr_number": "39860", "files_changed": ["aten/src/ATen/native/Copy.cpp", "test/test_autograd.py", "test/test_torch.py", "torch/csrc/autograd/python_variable.cpp"], "labels": ["merged", "module: complex"]}, "9ca4a46bf8": {"title": "Implement parallel scatter reductions for CPU (#36447)", "body": "Summary:\nThis PR implements gh-33389.\n\nAs a result of this PR, users can now specify various reduction modes for scatter operations. Currently, `add`, `subtract`, `multiply` and `divide` have been implemented, and adding new ones is not hard.\n\nWhile we now allow dynamic runtime selection of reduction modes, the performance is the same as as was the case for the `scatter_add_` method in the master branch. Proof can be seen in the graph below, which compares `scatter_add_` in the master branch (blue) and `scatter_(reduce=\"add\")` from this PR (orange).\n![scatter-regression py csv](https://user-images.githubusercontent.com/2629909/82671491-e5e22380-9c79-11ea-95d6-6344760c8578.png)\n\nThe script used for benchmarking is as follows:\n``` python\nimport os\nimport sys\nimport torch\nimport time\nimport numpy\nfrom IPython import get_ipython\n\nMs=256\nNs=512\ndim = 0\ntop_power = 2\nipython = get_ipython()\n\nplot_name = os.path.basename(__file__)\nbranch = sys.argv[1]\nfname = open(plot_name + \".csv\", \"a+\")\n\nfor pM in range(top_power):\n    M = Ms * (2 ** pM)\n    for pN in range(top_power):\n        N = Ns * (2 ** pN)\n        input_one = torch.rand(M, N)\n        index = torch.tensor(numpy.random.randint(0, M, (M, N)))\n        res = torch.randn(M, N)\n\n        test_case = f\"{M}x{N}\"\n        print(test_case)\n        tobj = ipython.magic(\"timeit -o res.scatter_(dim, index, input_one, reduce=\\\"add\\\")\")\n\n        fname.write(f\"{test_case},{branch},{tobj.average},{tobj.stdev}\\n\")\n\nfname.close()\n```\n\nAdditionally, one can see that various reduction modes take almost the same time to execute:\n```\nop: add\n70.6 \u00b5s \u00b1 27.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n26.1 \u00b5s \u00b1 26.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nop: subtract\n71 \u00b5s \u00b1 20.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n26.4 \u00b5s \u00b1 34.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nop: multiply\n70.9 \u00b5s \u00b1 31.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n27.4 \u00b5s \u00b1 29.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nop: divide\n164 \u00b5s \u00b1 48.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n52.3 \u00b5s \u00b1 132 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```\nScript:\n``` python\nimport torch\nimport time\nimport numpy\nfrom IPython import get_ipython\n\nipython = get_ipython()\n\nnrows = 3000\nncols = 10000\ndims = [nrows, ncols]\n\nres = torch.randint(5, 10, dims)\nidx1 = torch.randint(dims[0], (1, dims[1])).long()\nsrc1 = torch.randint(5, 10, (1, dims[1]))\nidx2 = torch.randint(dims[1], (dims[0], 1)).long()\nsrc2 = torch.randint(5, 10, (dims[0], 1))\n\nfor op in [\"add\", \"subtract\", \"multiply\", \"divide\"]:\n    print(f\"op: {op}\")\n    ipython.magic(\"timeit res.scatter_(0, idx1, src1, reduce=op)\")\n    ipython.magic(\"timeit res.scatter_(1, idx2, src2, reduce=op)\")\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36447\n\nDifferential Revision: D22272631\n\nPulled By: ngimel\n\nfbshipit-source-id: 3cdb46510f9bb0e135a5c03d6d4aa5de9402ee90", "pr_number": "36447", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/ScatterGatherKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "6e1cf000b3": {"title": "[jit][oacr] Add some operators for Assistant NLU joint lite model (#40126)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40126\n\nThese are needed for benchmarking / running our model, following Step 7 in the [Lite interpreter wiki](https://www.internalfb.com/intern/wiki/PyTorch/PyTorchDev/Mobile/Lite_Interpreter/#make-your-model-work-wit) and [this thread](https://www.internalfb.com/intern/qa/56293/atenemptymemory_format-missing-on-fb4a).\n\nTest Plan: Sandcastle\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22073611\n\nfbshipit-source-id: daa46a39c386806be8d5d589740663e85451757e", "pr_number": "40126", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "0235676f8a": {"title": "[pytorch][ci] run mobile code analysis on PR (#40247)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40247\n\nThis CI job was bypassed on PR because most part of it has already been\ncovered by mobile-custom-build-dynamic job that runs on every PR.\n\nHowever, it can still fail independently because it builds and analyzes\na small test project, e.g.: if people forget to update the registration API\nused in the test project.\n\nSo this PR changed it to only build and analyze the test project and run\nthe job on every PR.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22126044\n\nPulled By: ljk53\n\nfbshipit-source-id: 6699a200208a65b249bd3a4e43ad72bc07388ce3", "pr_number": "40247", "files_changed": [".circleci/cimodel/data/simple/mobile_definitions.py", ".circleci/config.yml", ".jenkins/pytorch/build-mobile-code-analysis.sh"], "labels": ["merged"]}, "8f5b28674c": {"title": "[JIT] Remove dead store in quantization_patterns.h (#40724)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40724\n\nTest Plan: Continuous integration.\n\nDifferential Revision: D22294600\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 04546579273d8864d91c3c74a654aa75ba34ee45", "pr_number": "40724", "files_changed": ["torch/csrc/jit/passes/quantization/quantization_patterns.h"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "d7cd16858f": {"title": "Add documentation about storage sharing is preserved and serialized f\u2026 (#40412)", "body": "Summary:\n\u2026ile size.\nfixes https://github.com/pytorch/pytorch/issues/40157\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40412\n\nReviewed By: ezyang\n\nDifferential Revision: D22265639\n\nPulled By: ailzhang\n\nfbshipit-source-id: 16b0301f16038bd784e7e92f63253fedc7820adc", "pr_number": "40412", "files_changed": ["docs/source/notes/serialization.rst", "torch/serialization.py"], "labels": ["merged"]}, "9393ac011a": {"title": "[CUDA] addmm for complex (#40431)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40431\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22285916\n\nPulled By: anjali411\n\nfbshipit-source-id: 5863c713bdaa8e5b4f3d2b41fa59108502145a23", "pr_number": "40431", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "test/test_torch.py"], "labels": ["merged"]}, "53af9df557": {"title": "Unify boxed function signature between jit and c10 (#37034)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37034\n\nc10 takes a Stack* in boxed functions while JIT took Stack&.\nc10 doesn't return anything while JIT returns an int which is always zero.\n\nThis changes JIT to follow the c10 behavior.\nghstack-source-id: 106834069\n\nTest Plan: unit tests\n\nDifferential Revision: D20567950\n\nfbshipit-source-id: 1a7aea291023afc52ae706957e9a5ca576fbb53b", "pr_number": "37034", "files_changed": ["aten/src/ATen/core/stack.h", "aten/src/ATen/record_function.h", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_base.cpp", "test/cpp/jit/test_custom_operators.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_schema_matching.cpp", "test/custom_operator/test_custom_ops.cpp", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/jit/OVERVIEW.md", "torch/csrc/jit/codegen/cuda/interface.cpp", "torch/csrc/jit/codegen/fuser/fallback.cpp", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/passes/batch_mm.cpp", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/utils/check_alias_annotation.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_interpreter.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/operator.h", "torch/csrc/jit/runtime/register_c10_ops.cpp", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/csrc/jit/runtime/register_ops_utils.cpp", "torch/csrc/jit/runtime/register_ops_utils.h", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/csrc/jit/runtime/register_string_ops.cpp"], "labels": ["merged", "oncall: jit"]}, "01e2099bb8": {"title": "[TB] Add support for hparam domain_discrete (#40720)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40720\n\nAdd support for populating domain_discrete field in TensorBoard add_hparams API\n\nTest Plan: Unit test test_hparams_domain_discrete\n\nReviewed By: edward-io\n\nDifferential Revision: D22291347\n\nfbshipit-source-id: 78db9f62661c9fe36cd08d563db0e7021c01428d", "pr_number": "40720", "files_changed": ["test/test_tensorboard.py", "torch/utils/tensorboard/summary.py", "torch/utils/tensorboard/writer.py"], "labels": ["fb-exported", "merged"]}, "fabd60ec1a": {"title": "Add comment with UNBOXEDONLY explanation to codegen (#40117)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40117\n\nghstack-source-id: 106804731\n\nTest Plan: just comments\n\nReviewed By: ezyang\n\nDifferential Revision: D22075103\n\nfbshipit-source-id: 76677dc337196b71c50075f2845a1899451a705f", "pr_number": "40117", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen_backend_select_register.py", "tools/autograd/gen_variable_type.py"], "labels": ["merged"]}, "a371652bc8": {"title": "Allow to get string references to strings inside torch::List (#39763)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39763\n\nThis is an ask from fluent. For performance reasons, they need a way to get read access to the std::string inside of a torch::List<std::string> without having to copy that string.\n\nInstead of special casing std::string, we decided to give access to the underlying value. The API now looks like:\n\n```cpp\ntorch::List<std::string> list = ...;\nconst std::string& str = list[2].toIValueRef().toStringRef();\n```\nghstack-source-id: 106806840\n\nTest Plan: unit tests\n\nReviewed By: ezyang\n\nDifferential Revision: D21966183\n\nfbshipit-source-id: 8b80b0244d10215c36b524d1d80844832cf8b69a", "pr_number": "39763", "files_changed": ["aten/src/ATen/core/List.h", "aten/src/ATen/core/List_test.cpp"], "labels": ["merged"]}, "b9cca4b186": {"title": "fix range of results for pairwise operations (#40728)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40728\n\nthere are two reasons the test is failing:\n1) div by 0\n2) result is bigger than fp16 max\n\nfor 1) make the divisor some safe number like 1e-3\n2) when a combination of random numbers results in their resulting value  bigger than 65e3, clip\n\nmultiplication is fine because range of random numbers is 0,100 -> result is 0->10000\n\nTest Plan: ran tes_div test\n\nReviewed By: hl475\n\nDifferential Revision: D22295934\n\nfbshipit-source-id: 173f3f2187137d6c1c4d4a505411a27f1c059f1a", "pr_number": "40728", "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py"], "labels": ["fb-exported", "merged"]}, "31de10a392": {"title": "Int8FC dequantize fix (#40608)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40608\n\nChanges to fix uint8_t to fp16 dequantization error.\nEnabled test_int8_quantize\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: Verified with test_int8_ops_nnpi.py\n\nReviewed By: hyuen\n\nDifferential Revision: D22252860\n\nfbshipit-source-id: bb44673327f0c8f44974cef2ab773aa0d89f4dc7", "pr_number": "40608", "files_changed": ["caffe2/contrib/fakelowp/int8_dequantize_op_nnpi.h", "caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py"], "labels": ["fb-exported", "merged"]}, "4104ab8b18": {"title": "Add `torch.count_nonzero` (#39992)", "body": "Summary:\nReference https://github.com/pytorch/pytorch/issues/38349\n\nTODO:\n\n* [x] Add tests\n* [x] Add docs (pending add to docs.rst)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39992\n\nReviewed By: ezyang\n\nDifferential Revision: D22236738\n\nPulled By: mruberry\n\nfbshipit-source-id: 8520068b086b5ffc4de9e4939e746ff889293987", "pr_number": "39992", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["open source"]}, "4a235b87be": {"title": "pop warning message for cuda module when asan is built in (#35088)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/35088\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20552708\n\nPulled By: glaringlee\n\nfbshipit-source-id: 0b809712378596ccf83211bf8ae39cd71c27dbba", "pr_number": "35088", "files_changed": ["torch/csrc/cuda/Module.cpp"], "labels": ["merged"]}, "2f94b7f95c": {"title": "Initial vmap docstring (#40575)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40575\n\nThis provides some more context for the next ~2 PRs that will implement\nthe `out_dims` and `in_dims` functionality. I will probably add more to\nit later (things I think we should add: examples (maybe in a dedicated\ndocs page), specific examples of things vmap cannot handle).\n\nTest Plan:\n- Code reading for now. When we are ready to add vmap to master documentation,\nI'll build the docs and fix any formatting problems.\n\nDifferential Revision: D22288085\n\nPulled By: zou3519\n\nfbshipit-source-id: 6e28d7bd524242395160c20270159b4b121d6789", "pr_number": "40575", "files_changed": ["torch/_vmap_internals.py"], "labels": ["merged"]}, "a6a31bcd47": {"title": "Enable `out_dims` for vmap frontend API (#40576)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40576\n\n`out_dims` specifies where in the output tensors the vmapped dimension\nshould appear. We implement this by simply creating a view with the\nbatch dimension moved to the desired position.\n\n`out_dims` must either:\n- be int (use the same value for all outputs)\n- be Tuple[int] (so the user specifies one out_dim per output).\n(See the vmap docstring for what we advertise out_dims to do).\n\nI also renamed `TestVmap` to `TestVmapAPI` to make it clearer that we\nare testing the API here and not specific operators (which will go into\ntheir own test class).\n\nTest Plan: - `pytest test/test_vmap.py -v`\n\nDifferential Revision: D22288086\n\nPulled By: zou3519\n\nfbshipit-source-id: c8666cb1a0e22c54473d8045477e14c2089167cf", "pr_number": "40576", "files_changed": ["aten/src/ATen/BatchedTensorImpl.h", "aten/src/ATen/native/Batching.cpp", "test/test_vmap.py", "torch/_vmap_internals.py"], "labels": ["merged"]}, "3cc18d7139": {"title": ".circleci: Remove executor from windows uploads (#40742)", "body": "Summary:\nThis wasn't needed and broke nightly builds\n\nFixes some issues introduced in https://github.com/pytorch/pytorch/pull/40592/files\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40742\n\nDifferential Revision: D22310055\n\nPulled By: seemethere\n\nfbshipit-source-id: 095be3be06a730138d860ca6b73eaf22c24cf08f", "pr_number": "40742", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml"], "labels": ["ci/binaries", "merged", "module: ci"]}, "c56255499a": {"title": "Reverts running clang-tidy on ATen (#40764)", "body": "Summary:\nReverts https://github.com/pytorch/pytorch/pull/39713.\n\nWe are seeing CUDA-related clang-tidy failures on multiple PRs after the above change. The cause of these failures is unclear. Example error message:\n\n```\n2020-06-26T18:45:10.9763273Z + python tools/clang_tidy.py --verbose --paths torch/csrc/ aten/src/ATen/ --diff 5036c94a6e868963e0354fc04c92e204d8d77677 -g-torch/csrc/jit/serialization/export.cpp -g-torch/csrc/jit/serialization/import.cpp -g-torch/csrc/jit/serialization/import_legacy.cpp -g-torch/csrc/onnx/init.cpp '-g-torch/csrc/cuda/nccl.*' -g-torch/csrc/cuda/python_nccl.cpp\n2020-06-26T18:45:11.1990578Z Error while processing /home/runner/work/pytorch/pytorch/aten/src/ATen/native/cuda/UnaryOpsKernel.cu.\n2020-06-26T18:45:11.1992832Z Found compiler error(s).\n2020-06-26T18:45:11.2286995Z Traceback (most recent call last):\n2020-06-26T18:45:11.2288334Z   File \"tools/clang_tidy.py\", line 55, in run_shell_command\n2020-06-26T18:45:11.2288607Z     output = subprocess.check_output(arguments).decode().strip()\n2020-06-26T18:45:11.2289053Z   File \"/opt/hostedtoolcache/Python/3.8.3/x64/lib/python3.8/subprocess.py\", line 411, in check_output\n2020-06-26T18:45:11.2289337Z     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n2020-06-26T18:45:11.2289786Z   File \"/opt/hostedtoolcache/Python/3.8.3/x64/lib/python3.8/subprocess.py\", line 512, in run\n2020-06-26T18:45:11.2290038Z     raise CalledProcessError(retcode, process.args,\n2020-06-26T18:45:11.2292206Z subprocess.CalledProcessError: Command '['clang-tidy', '-p', 'build', '-config', '{\"Checks\": \"-*, bugprone-*, -bugprone-forward-declaration-namespace, -bugprone-macro-parentheses, -bugprone-lambda-function-name, cppcoreguidelines-*, -cppcoreguidelines-interfaces-global-init, -cppcoreguidelines-owning-memory, -cppcoreguidelines-pro-bounds-array-to-pointer-decay, -cppcoreguidelines-pro-bounds-constant-array-index, -cppcoreguidelines-pro-bounds-pointer-arithmetic, -cppcoreguidelines-pro-type-cstyle-cast, -cppcoreguidelines-pro-type-reinterpret-cast, -cppcoreguidelines-pro-type-static-cast-downcast, -cppcoreguidelines-pro-type-union-access, -cppcoreguidelines-pro-type-vararg, -cppcoreguidelines-special-member-functions, hicpp-exception-baseclass, hicpp-avoid-goto, modernize-*, -modernize-return-braced-init-list, -modernize-use-auto, -modernize-use-default-member-init, -modernize-use-using, -modernize-use-trailing-return-type, performance-*, -performance-noexcept-move-constructor, \", \"HeaderFilterRegex\": \"torch/csrc/.*\", \"AnalyzeTemporaryDtors\": false, \"CheckOptions\": null}', '-line-filter', '[{\"name\": \"aten/src/ATen/native/cuda/UnaryOpsKernel.cu\", \"lines\": [[10, 11], [29, 30]]}]', 'aten/src/ATen/native/cuda/UnaryOpsKernel.cu']' returned non-zero exit status 1.\n2020-06-26T18:45:11.2292551Z\n2020-06-26T18:45:11.2292684Z During handling of the above exception, another exception occurred:\n2020-06-26T18:45:11.2292775Z\n2020-06-26T18:45:11.2292894Z Traceback (most recent call last):\n2020-06-26T18:45:11.2293208Z   File \"tools/clang_tidy.py\", line 306, in <module>\n2020-06-26T18:45:11.2293364Z     main()\n2020-06-26T18:45:11.2293817Z   File \"tools/clang_tidy.py\", line 298, in main\n2020-06-26T18:45:11.2293980Z     clang_tidy_output = run_clang_tidy(options, line_filters, files)\n2020-06-26T18:45:11.2294282Z   File \"tools/clang_tidy.py\", line 191, in run_clang_tidy\n2020-06-26T18:45:11.2294439Z     output = run_shell_command(command)\n2020-06-26T18:45:11.2294703Z   File \"tools/clang_tidy.py\", line 59, in run_shell_command\n2020-06-26T18:45:11.2294931Z     raise RuntimeError(\"Error executing {}: {}\".format(\" \".join(arguments), error_output))\n2020-06-26T18:45:11.2296875Z RuntimeError: Error executing clang-tidy -p build -config {\"Checks\": \"-*, bugprone-*, -bugprone-forward-declaration-namespace, -bugprone-macro-parentheses, -bugprone-lambda-function-name, cppcoreguidelines-*, -cppcoreguidelines-interfaces-global-init, -cppcoreguidelines-owning-memory, -cppcoreguidelines-pro-bounds-array-to-pointer-decay, -cppcoreguidelines-pro-bounds-constant-array-index, -cppcoreguidelines-pro-bounds-pointer-arithmetic, -cppcoreguidelines-pro-type-cstyle-cast, -cppcoreguidelines-pro-type-reinterpret-cast, -cppcoreguidelines-pro-type-static-cast-downcast, -cppcoreguidelines-pro-type-union-access, -cppcoreguidelines-pro-type-vararg, -cppcoreguidelines-special-member-functions, hicpp-exception-baseclass, hicpp-avoid-goto, modernize-*, -modernize-return-braced-init-list, -modernize-use-auto, -modernize-use-default-member-init, -modernize-use-using, -modernize-use-trailing-return-type, performance-*, -performance-noexcept-move-constructor, \", \"HeaderFilterRegex\": \"torch/csrc/.*\", \"AnalyzeTemporaryDtors\": false, \"CheckOptions\": null} -line-filter [{\"name\": \"aten/src/ATen/native/cuda/UnaryOpsKernel.cu\", \"lines\": [[10, 11], [29, 30]]}] aten/src/ATen/native/cuda/UnaryOpsKernel.cu: error: cannot find libdevice for sm_20. Provide path to different CUDA installation via --cuda-path, or pass -nocudalib to build without linking with libdevice. [clang-diagnostic-error]\n2020-06-26T18:45:11.2313329Z error: unable to handle compilation, expected exactly one compiler job in ' \"/usr/bin/c++\" \"-cc1\" \"-triple\" \"x86_64-pc-linux-gnu\" \"-aux-triple\" \"nvptx64-nvidia-cuda\" \"-fsyntax-only\" \"-disable-free\" \"-disable-llvm-verifier\" \"-discard-value-names\" \"-main-file-name\" \"UnaryOpsKernel.cu\" \"-mrelocation-model\" \"pic\" \"-pic-level\" \"2\" \"-mthread-model\" \"posix\" \"-fno-trapping-math\" \"-masm-verbose\" \"-mconstructor-aliases\" \"-munwind-tables\" \"-fuse-init-array\" \"-target-cpu\" \"x86-64\" \"-dwarf-column-info\" \"-debugger-tuning=gdb\" \"-momit-leaf-frame-pointer\" \"-resource-dir\" \"/usr/lib/llvm-8/bin/../lib/clang/8.0.1\" \"-internal-isystem\" \"/usr/lib/llvm-8/bin/../lib/clang/8.0.1/include/cuda_wrappers\" \"-internal-isystem\" \"/usr/local/cuda/include\" \"-include\" \"__clang_cuda_runtime_wrapper.h\" \"-isystem\" \"/home/runner/work/pytorch/pytorch/build/third_party/gloo\" \"-isystem\" \"/home/runner/work/pytorch/pytorch/cmake/../third_party/gloo\" \"-isystem\"\n```\n\nMy guess is that our clang-tidy build is improperly configured to handle CUDA code. Until that issue is resolved this stops running clang-tidy on ATen.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40764\n\nDifferential Revision: D22310032\n\nPulled By: mruberry\n\nfbshipit-source-id: 035067e1017f0097026cee9866bba424dd4668b4", "pr_number": "40764", "files_changed": [".github/workflows/lint.yml", "tools/git-pre-commit"], "labels": ["merged", "module: cuda"]}, "5923a802fa": {"title": "Back out \"[pytorch][PR] [ONNX] Add eliminate_unused_items pass\"", "body": "Summary:\nOriginal commit changeset: 30e1a6e8823a\n\ncause issue to fusing BN\n\nTest Plan: revert\n\nReviewed By: houseroad\n\nDifferential Revision: D22296958\n\nfbshipit-source-id: 62664cc77baa8811ad6ecce9d0520a2ab7f89868", "pr_number": null, "files_changed": ["test/onnx/debug_embed_params.py", "test/onnx/test_utility_funs.py", "test/onnx/test_verify.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/onnx/constant_fold.cpp", "torch/csrc/jit/passes/onnx/eliminate_unused_items.cpp", "torch/csrc/jit/passes/onnx/eliminate_unused_items.h", "torch/csrc/jit/passes/onnx/helper.cpp", "torch/csrc/jit/passes/onnx/helper.h", "torch/csrc/jit/python/init.cpp", "torch/onnx/utils.py"], "labels": []}, "ef5a314597": {"title": "[typing] fix register_buffer/parameter (#40669)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40669\n\nDifferential Revision: D22286130\n\nPulled By: ezyang\n\nfbshipit-source-id: c0cc173279678978726895a0830343d5234e474e", "pr_number": "40669", "files_changed": ["torch/nn/modules/module.py"], "labels": ["merged", "open source"]}, "a303fd2ea6": {"title": "Let exp support complex types on CUDA and enable device/dtype in complex tests (#39087)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39087\n\nDifferential Revision: D22169697\n\nPulled By: anjali411\n\nfbshipit-source-id: 4866b7be6742508cc40540ed1ac811f005531d8b", "pr_number": "39087", "files_changed": ["aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "test/test_complex.py", "test/test_torch.py"], "labels": ["module: complex", "open source", "triaged"]}, "c3237c7a87": {"title": "Print hostname of RoCM tester (#40755)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40755\n\nDifferential Revision: D22311699\n\nPulled By: malfet\n\nfbshipit-source-id: 057702800fec84fae787b7837f39348273c80cec", "pr_number": "40755", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "f3949794a3": {"title": "Prototype benchmarking util (#38338)", "body": "Summary:\nThis is the prototype for the modular utils that we've been discussing. It is admittedly a large PR, but a good fraction of that is documentation and examples. I've trimmed a bit on the edges since we last discussed this design (for instance Timer is no longer Fuzzer aware), but it's mostly the same.\n\nIn addition to the library and hermetic examples, I've included `examples.end_to_end` which tests https://github.com/pytorch/pytorch/pull/38061 over a variety of shapes, dtypes, degrees of broadcasting, and layouts. (CC crcrpar)  I only did CPU as I'm not set up on a GPU machine yet. [Results from my devserver](https://gist.github.com/robieta/d1a8e1980556dc3f4f021c9f7c3738e2)\n\nKey takeaways:\n  1) For contiguous Tensors, larger dtypes (fp32 and fp64) and lots of reuse of the mask due to broadcasting, improvements are significant. (Presumably due to better vectorization?)\n  2) There is an extra ~1.5 us overhead, which dominates small kernels.\n  3) Cases with lower write intensity (int8, lower mask fraction, etc) or non-contiguous seem to suffer.\n\nHopefully this demonstrates the proof-of-concept for how this tooling can be used to tune kernels and assess PRs. Looking forward to thoughts and feedback.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38338\n\nDifferential Revision: D21551048\n\nPulled By: robieta\n\nfbshipit-source-id: 6c50e5439a04eac98b8a2355ef731852ba0500db", "pr_number": "38338", "files_changed": ["benchmarks/experimental_components/README.md", "benchmarks/experimental_components/examples/compare.py", "benchmarks/experimental_components/examples/end_to_end.py", "benchmarks/experimental_components/examples/fuzzer.py", "benchmarks/experimental_components/examples/op_benchmark.py", "benchmarks/experimental_components/examples/prepare_e2e.sh", "benchmarks/experimental_components/examples/simple_timeit.py", "benchmarks/experimental_components/op_fuzzers/binary.py", "benchmarks/experimental_components/op_fuzzers/unary.py", "benchmarks/experimental_components/utils/__init__.py", "benchmarks/experimental_components/utils/common.py", "benchmarks/experimental_components/utils/compare.py", "benchmarks/experimental_components/utils/fuzzer.py", "benchmarks/experimental_components/utils/timer.py"], "labels": ["merged"]}, "9ac0febb1f": {"title": "Pin torchvision version for doc_push (#40802)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40802\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22317343\n\nPulled By: ezyang\n\nfbshipit-source-id: 8a982dd93a28d102dfd63163cd44704e899922e0", "pr_number": "40802", "files_changed": [".circleci/scripts/python_doc_push_script.sh"], "labels": ["merged"]}, "1571dd8692": {"title": "Refactor duplicated string literals (#40788)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40788\n\nAvoid repeated the same `:gencode[foo/bar]` over and over again\n\nTest Plan: CI\n\nReviewed By: EscapeZero\n\nDifferential Revision: D22271151\n\nfbshipit-source-id: f8db57db4ee0948bcca0c8945fdf30380ba81cae", "pr_number": "40788", "files_changed": ["tools/build_variables.bzl"], "labels": ["fb-exported", "merged"]}, "e762ce8ecf": {"title": "Avoid initializing `new_group` in test_backward_no_ddp. (#40727)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40727\n\nThis unit test doesn't need to initialize a PG, as a result avoiding\ninitializing a process group.\n\n#Closes: https://github.com/pytorch/pytorch/issues/40292\nghstack-source-id: 106817362\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D22295131\n\nfbshipit-source-id: 5a60e91e4beeb61cc204d24c564106d0215090a6", "pr_number": "40727", "files_changed": ["torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py"], "labels": ["merged"]}, "40e79bb1d3": {"title": "Update the version of ninja and scipy (#40677)", "body": "Summary:\nUpdate scipy to 1.15 and ninja to 1.10.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40677\n\nDifferential Revision: D22311602\n\nPulled By: ezyang\n\nfbshipit-source-id: ddc852b3b8c3091409d1b3bd579dd144b58e5d47", "pr_number": "40677", "files_changed": [".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat"], "labels": ["merged", "open source", "triaged"]}, "9d8dc0318b": {"title": "[pruning] add rowwise counter to sparse adagrad", "body": "Summary: Use the newly added counter op in sparse adagrad\n\nReviewed By: chocjy, ellie-wen\n\nDifferential Revision: D19221100\n\nfbshipit-source-id: d939d83e3b5b3179f57194be2e8864d0fbbee2c1", "pr_number": null, "files_changed": ["caffe2/python/optimizer.py", "caffe2/python/optimizer_test.py"], "labels": []}, "0a75234934": {"title": "Allow np.memmap objects (numpy arrays based on files) to be processed\u2026 (#39847)", "body": "Summary:\nAllow np.memmap objects to be processed by default_collate\n\nnp.memmap objects has the same behavior as numpy arrays, and the only difference is that they are stored in a binary file on the disk. However, the default_collate function used by PyTorch DataLoader only accepts np.array, and rejects np.memmap by type checking. This commit allows np.memmap objects to be processed by default_collate. In this way, users can use in-disk large arrays with PyTorch DataLoader.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39847\n\nReviewed By: ezyang\n\nDifferential Revision: D22284650\n\nPulled By: zou3519\n\nfbshipit-source-id: 003e3208a2afd1afc2e4640df14b3446201e00b4", "pr_number": "39847", "files_changed": ["test/test_dataloader.py", "torch/utils/data/_utils/collate.py"], "labels": ["merged", "open source", "triaged"]}, "29aef8f460": {"title": "Skip some error-producing exp tests that cannot be reliably reproduced (#40824)", "body": "Summary:\nThis is to take care of additional master CI tests for https://github.com/pytorch/pytorch/issues/39087\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40824\n\nDifferential Revision: D22321429\n\nPulled By: ezyang\n\nfbshipit-source-id: 607e284688b3e4ce24d803a030e31991e4e32fd7", "pr_number": "40824", "files_changed": ["test/test_torch.py"], "labels": ["merged"]}, "0ddaaf6a92": {"title": "[codemod][caffe2] Run clang-format - 5/7", "body": "Summary:\nThis directory is opted-in to clang-format but is not format-clean. This blocks continuous formatting from being enabled on fbcode, and causes hassle for other codemods that leave inconsistent formatting. This diff runs clang-format, which is widely used and considered safe.\n\nIf you are unhappy with the formatting of a particular block, please *accept this diff* and then in a stacked commit undo the change and wrap that code in `// clang-format off` and `// clang-format on`, or `/* clang-format off */` and `/* clang-format on */`.\n\ndrop-conflicts\n\nTest Plan: sandcastleit\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22311706\n\nfbshipit-source-id: 1ca59a82e96156a4a5dfad70ba3e64d44c5e762a", "pr_number": null, "files_changed": ["caffe2/mpi/mpi_common.h", "caffe2/mpi/mpi_ops.h", "caffe2/observers/runcnt_observer.h", "caffe2/observers/time_observer.h", "caffe2/onnx/backend.h", "caffe2/onnx/backend_rep.h", "caffe2/onnx/device.h", "caffe2/onnx/onnx_exporter.h", "caffe2/operators/accuracy_op.h", "caffe2/operators/affine_channel_op.h", "caffe2/operators/bbox_transform_op.h", "caffe2/operators/box_with_nms_limit_op.h", "caffe2/operators/cast_op.h", "caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.h", "caffe2/operators/conditional_op.h", "caffe2/operators/conv_transpose_op_mobile.h", "caffe2/operators/create_scope_op.h", "caffe2/operators/deform_conv_op_impl.h", "caffe2/operators/elementwise_linear_op.h", "caffe2/operators/elementwise_ops.h", "caffe2/operators/ensure_cpu_output_op.h", "caffe2/operators/expand_op.h", "caffe2/operators/find_duplicate_elements_op.h", "caffe2/operators/gelu_op.h", "caffe2/operators/generate_proposals_op.h", "caffe2/operators/given_tensor_byte_string_to_uint8_fill_op.h", "caffe2/operators/given_tensor_fill_op.h", "caffe2/operators/h_softmax_op.h", "caffe2/operators/hip/activation_ops_miopen.h", "caffe2/operators/inference_lstm_op.h", "caffe2/operators/leaky_relu_op.h", "caffe2/operators/lengths_reducer_rowwise_8bit_ops.h", "caffe2/operators/map_ops.h", "caffe2/operators/numpy_tile_op.h", "caffe2/operators/onnx_while_op.h", "caffe2/operators/prefetch_op.h", "caffe2/operators/quant_decode_op.h", "caffe2/operators/quantized/int8_add_op.h", "caffe2/operators/quantized/int8_average_pool_op.h", "caffe2/operators/quantized/int8_channel_shuffle_op.h", "caffe2/operators/quantized/int8_given_tensor_fill_op.h", "caffe2/operators/quantized/int8_leaky_relu_op.h", "caffe2/operators/quantized/int8_max_pool_op.h", "caffe2/operators/quantized/int8_relu_op.h", "caffe2/operators/quantized/int8_resize_nearest_op.h", "caffe2/operators/quantized/int8_sigmoid_op.h", "caffe2/operators/quantized/int8_slice_op.h", "caffe2/operators/quantized/int8_softmax_op.h", "caffe2/operators/reduce_front_back_max_ops.h", "caffe2/operators/reduce_front_back_sum_mean_ops.h"], "labels": []}, "179dbd4f25": {"title": "[jit] preserve keys on dictionary input tracing (#40792)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40792\n\nFixes https://github.com/pytorch/pytorch/issues/40529.\n\nOne followup should be to produce a better error message when a new\ndictionary has different keys than the traced input. Right now it\npresents as a fairly opaque `KeyError`.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22311731\n\nPulled By: suo\n\nfbshipit-source-id: c9fbe0b54cf69daed2f11a191d988568521a3932", "pr_number": "40792", "files_changed": ["test/jit/test_tracer.py", "torch/csrc/jit/frontend/tracer.cpp"], "labels": ["merged", "oncall: jit"]}, "8e0714a60d": {"title": "[rfc] Reduce number of coin flips in RecordFunction (#40758)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40758\n\nCurrently we flip a coin for each sampled callback each time\nwe run RecordFunction, this PR is an attempt to skip most of the coin\nflips (for the low-probability observers) and keep the distribution\nclose to the original one\n\nTest Plan:\nCI and record_function_benchmark\n```\n(python_venv) iliacher@devgpu151:~/local/pytorch  (reduce_coin_flops)$ ./build/bin/record_function_benchmark\nWarmup time: 30108 us.\nTime per iteration (1x1): 1496.78 us.\nTime per iteration (16x16): 2142.46 us.\nPure RecordFunction runtime of 10000000 iterations 687929 us, number of callback invocations: 978\n(python_venv) iliacher@devgpu151:~/local/pytorch  (reduce_coin_flops)$ ./build/bin/record_function_benchmark\nWarmup time: 19051 us.\nTime per iteration (1x1): 1581.89 us.\nTime per iteration (16x16): 2195.67 us.\nPure RecordFunction runtime of 10000000 iterations 682402 us, number of callback invocations: 1023\n(python_venv) iliacher@devgpu151:~/local/pytorch  (reduce_coin_flops)$ ./build/bin/record_function_benchmark\nWarmup time: 18715 us.\nTime per iteration (1x1): 1566.11 us.\nTime per iteration (16x16): 2131.17 us.\nPure RecordFunction runtime of 10000000 iterations 693571 us, number of callback invocations: 963\n(python_venv) iliacher@devgpu151:~/local/pytorch  (reduce_coin_flops)$\n\n(python_venv) iliacher@devgpu151:~/local/pytorch  (reduce_coin_flops)$ ./build/bin/record_function_benchmark\nWarmup time: 18814 us.\nTime per iteration (1x1): 1536.2 us.\nTime per iteration (16x16): 1985.82 us.\nPure RecordFunction runtime of 10000000 iterations 944959 us, number of callback invocations: 1015\n(python_venv) iliacher@devgpu151:~/local/pytorch  (reduce_coin_flops)$ ./build/bin/record_function_benchmark\nWarmup time: 18278 us.\nTime per iteration (1x1): 1526.32 us.\nTime per iteration (16x16): 2093.77 us.\nPure RecordFunction runtime of 10000000 iterations 985307 us, number of callback invocations: 1013\n(python_venv) iliacher@devgpu151:~/local/pytorch  (reduce_coin_flops)$ ./build/bin/record_function_benchmark\nWarmup time: 18545 us.\nTime per iteration (1x1): 1524.65 us.\nTime per iteration (16x16): 2080 us.\nPure RecordFunction runtime of 10000000 iterations 952835 us, number of callback invocations: 1048\n```\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D22320879\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 2193f07d2f7625814fe7bc3cc85ba4092fe036bc", "pr_number": "40758", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h", "binaries/record_function_benchmark.cc"], "labels": ["merged"]}, "0203d70c63": {"title": "[nit] fix some typo within documentation (#40692)", "body": "Summary:\nApologize if this seems trivial, but i'd like to fix them on my way of reading some of the source code. Thanks!\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40692\n\nDifferential Revision: D22284651\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4259d1808aa4d15a02cfd486cfb44dd75fdc58f8", "pr_number": "40692", "files_changed": ["torch/csrc/jit/codegen/cuda/ir_printer.h", "torch/nn/modules/linear.py", "torch/nn/modules/loss.py", "torch/nn/modules/module.py", "torch/nn/quantized/modules/functional_modules.py"], "labels": ["merged", "oncall: jit", "open source"]}, "f13653db29": {"title": "[Update transforms.py]use build-in `atanh` in TanhTransform (#40160)", "body": "Summary:\nSince `torch.atanh` is recently implemented in https://github.com/pytorch/pytorch/issues/38388, we should simply use it for `TanhTransform`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40160\n\nDifferential Revision: D22208039\n\nPulled By: ezyang\n\nfbshipit-source-id: 34dfbc91eb9383461e16d3452e3ebe295f39df26", "pr_number": "40160", "files_changed": ["torch/distributions/transforms.py"], "labels": ["merge-this-please", "merged", "open source", "triaged"]}, "2cf9fe2d92": {"title": "Remove more error-exposing tests in exp that cannot be reliably reproduced (#40825)", "body": "Summary:\nContinuing https://github.com/pytorch/pytorch/issues/40824\n\nAll CIs have been enabled (on a branch that starts with `ci-all/`)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40825\n\nDifferential Revision: D22328732\n\nPulled By: ezyang\n\nfbshipit-source-id: 3e517d01a9183d95df0687b328fb268947ea5fb0", "pr_number": "40825", "files_changed": ["test/test_torch.py"], "labels": ["merged", "open source"]}, "5f9e7240f5": {"title": "Fix bug where explicitly providing a namespace never worked. (#40830)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40830\n\nFixes #40725\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22323886\n\nPulled By: ezyang\n\nfbshipit-source-id: b8a61496923d9f086d4c201024748505ba783238", "pr_number": "40830", "files_changed": ["aten/src/ATen/core/library.cpp"], "labels": ["merged"]}, "fcadca1bda": {"title": "serialization: validate sparse tensors after loading (#34059)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33439\n\nThis introduces torch._sparse_coo_tensor_unsafe(...) and\ntorch._validate_sparse_coo_tensor_args(...)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34059\n\nDifferential Revision: D22161254\n\nPulled By: ezyang\n\nfbshipit-source-id: 994efc9b0e30abbc23ddd7b2ec987e6ba08a8ef0", "pr_number": "34059", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensor.cpp", "test/test_serialization.py", "tools/autograd/templates/python_torch_functions.cpp", "torch/_utils.py", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_new.h", "torch/serialization.py"], "labels": ["merged", "open source", "triaged"]}, "a0569ad8f8": {"title": "[android][readme] Aar native linking add fbjni (#40578)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40578\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22239286\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 7a4160b621af8cfcc3b3d9e6da1a75c8afefba27", "pr_number": "40578", "files_changed": ["android/README.md"], "labels": ["merged"]}, "6aebd2c412": {"title": "[quant][graphmode] Add FP16 quant support - Insert Noop Observers (#40708)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40708\n\nInsert NoopObservers for activations and weight tensors for FP16\n\nTest Plan:\npython test/test_quantization.py test_prepare_dynamic\n\nImported from OSS\n\nDifferential Revision: D22335976\n\nfbshipit-source-id: b19e8035c7db3b0b065ec09c9ad6d913eb434f3e", "pr_number": "40708", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/quantization/qconfig.py"], "labels": ["merged"]}, "55b5ab14d3": {"title": "[quant][graphmode] FP16 quant support - Insert cast operators (#40709)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40709\n\nCast to kHalf and back to kFloat before the linear operator to mimic FP16 quant support\n\nTest Plan:\npython test/test_quantization.py test_convert_dynamic_fp16\n\nImported from OSS\n\nDifferential Revision: D22335977\n\nfbshipit-source-id: f964128ec733469672a1ed4cb0d757d0a6c22c3a", "pr_number": "40709", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": ["merged", "oncall: jit"]}, "26543e6caf": {"title": "[quant][graphmode] FP16 quant support - Operator Fusion (#40710)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40710\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D22335975\n\nfbshipit-source-id: 5c176bb6b9c300e1beb83df972149dd5a400b854", "pr_number": "40710", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/finalize.cpp", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/quantization_patterns.h"], "labels": ["merged", "oncall: jit"]}, "c73255801f": {"title": "Fix the autograd codegen for repeat function (#40766)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/40701\n\nA new special case is added to let `dim()` save an int instead of self.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40766\n\nDifferential Revision: D22308354\n\nPulled By: albanD\n\nfbshipit-source-id: 69008230d7398b9e06b8e074a549ae921c2bf603", "pr_number": "40766", "files_changed": ["tools/autograd/load_derivatives.py"], "labels": ["merged", "open source", "triaged"]}, "af34f2f63b": {"title": "Added missing generator argument in type annotation(pytorch#40803) (#40873)", "body": "Summary:\nAdded missing generator argument in type annotation(pytorch#40803)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40873\n\nDifferential Revision: D22344217\n\nPulled By: malfet\n\nfbshipit-source-id: 9871401b97c96fa20c70e3f66334259ead1f8429", "pr_number": "40873", "files_changed": ["tools/pyi/gen_pyi.py"], "labels": ["merged", "module: typing", "open source", "triaged"]}, "49e12d888a": {"title": "[NCCL - reland] Explicitly abort NCCL Communicators on Process Group Destruction (#40585)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40585\n\nThis PR aborts incomplete NCCL Communicators in the ProcessGroupNCCL\ndestructor. This should prevent pending NCCL communicators from blocking other CUDA ops.\nghstack-source-id: 106988073\n\nTest Plan: Sandcastle/ OSS CI\n\nDifferential Revision: D22244873\n\nfbshipit-source-id: 4b4fe65e1bd875a50151870f8120498193d7535e", "pr_number": "40585", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged"]}, "ad30d465d5": {"title": "Move install_torchvision to common.sh so that it can be sourced. (#40828)", "body": "Summary:\nMoving this to a file that can be source by downstream pytorch/xla.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40828\n\nReviewed By: malfet\n\nDifferential Revision: D22339513\n\nPulled By: ailzhang\n\nfbshipit-source-id: c43b18fa2b7e1e8bb6810a6a43bb7dccd4756238", "pr_number": "40828", "files_changed": [".circleci/scripts/python_doc_push_script.sh", ".jenkins/pytorch/common.sh", ".jenkins/pytorch/common_utils.sh", ".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "04b6e4273e": {"title": "clang format reducer.cpp (#40876)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40876\n\nclang format reducer.cpp\nghstack-source-id: 106980050\n\nTest Plan: unit test\n\nDifferential Revision: D22321422\n\nfbshipit-source-id: 54afdff206504c7bbdf2e408928cc32068e15cdc", "pr_number": "40876", "files_changed": ["torch/csrc/distributed/c10d/reducer.cpp"], "labels": ["merged"]}, "2f47e953f7": {"title": "Fixes #40158 (#40617)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/40158\n\nDescription\n- docs update: removed incorrect statements\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40617\n\nReviewed By: ezyang\n\nDifferential Revision: D22308802\n\nPulled By: yns88\n\nfbshipit-source-id: e33084af320f249c0c9ba04bdbe2191d1b954d17", "pr_number": "40617", "files_changed": ["torch/autograd/grad_mode.py"], "labels": ["merged", "open source"]}, "d7c9f96e43": {"title": "Optimize perf for calling ops with custom classes (#38257)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38257\n\nIt seems we're doing a runtime type check for custom classes on each operator call if the operator has custom class arguments.\nThis does not have an effect on operators without custom class arguments, but this is a problem for operators with custom class arguments,\nfor example operators taking a at::native::xnnpack::Conv2dOpContext argument.\n\nThe long term solution would be to move those checks to op registration time instead of doing them at call time,\nbut as an intermediate fix, we can at least make the check fast by\n\n- Using ska::flat_hash_map instead of std::unordered_map\n- Using std::type_index instead of std::string (i.e. avoid calling std::hash on a std::string)\nghstack-source-id: 106805209\n\nTest Plan: waitforsandcastle\n\nReviewed By: ezyang\n\nDifferential Revision: D21507226\n\nfbshipit-source-id: bd120d5574734be843c197673ea4222599fee7cb", "pr_number": "38257", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "torch/custom_class.h"], "labels": ["merged"]}, "8f6e50d013": {"title": "Make some more ops c10-full (#40747)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40747\n\n-\nghstack-source-id: 106833603\n\nTest Plan: waitforsandcastle\n\nReviewed By: ezyang\n\nDifferential Revision: D22299161\n\nfbshipit-source-id: 6e34999b5f8244d9582e4978754039d340720ca8", "pr_number": "40747", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["merged"]}, "ea03f954ad": {"title": "[ONNX] Add warning in ONNX export when constant folding is on in training-amenable mode (#40546)", "body": "Summary:\nThis PR introduces a warning when user tries to export the model to ONNX in training-amenable mode while constant folding is turned on. We want to warn against any unintentional use because constant folding may fold some parameters that may be intended to be trainable in the exported model.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40546\n\nReviewed By: hl475\n\nDifferential Revision: D22310917\n\nPulled By: houseroad\n\nfbshipit-source-id: ba83b8e63af7c458b5ecca8ff2ee1c77e2064f90", "pr_number": "40546", "files_changed": ["torch/onnx/utils.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "59294fbbb9": {"title": "[caffe2] Reimplement RemoveOpsByType with SSA (#40649)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40649\n\nThe original implementation of RemoveOpsByType is pretty buggy and does not remove all instances of the ops that should be removed. It's also quite complicated and hard to modify. I reimplemented it by first converting the graph to its SSA form. The algorithm is quite simple once the graph is in SSA form. It's very similar to constant propagation with a few modifications. The hardest part is to deal with the case of removing an op with the output being an output of the predict net, because that output has to be preserved.\n\n(Note: this ignores all push blocking failures!)\n\nReviewed By: yinghai, dzhulgakov\n\nDifferential Revision: D22220798\n\nfbshipit-source-id: faf6ed5242f1e2f310125d964738c608c6c55c94", "pr_number": "40649", "files_changed": ["caffe2/opt/tvm_transformer.cc", "caffe2/predictor/InferenceGraph.h", "caffe2/predictor/transforms.cc", "caffe2/predictor/transforms.h"], "labels": ["fb-exported", "merged"]}, "9fa1f27968": {"title": "[jit] Fix value association with dictionaries in the tracer (#40885)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40885\n\n`TracingState::setValue` associates a concrete IValue in the traced\nprogram with a `Value*` symbolic. Previously, the logic for how\nGenericDicts worked was special cased to only work for very simple cases\nand silently eat other cases.\n\nThis PR generalizes the logic to reflect the same behavior as using\ndictionaries on input: whenever we encounter a dictionary in the system,\nwe completely \"burn in\" all the keys into the graph, and then\nrecursively call `setValue` on the associated value.\n\nThis has the effect of requiring that any dictionary structure you are\ncreating in a traced program be of fixed structure, similar to how any\ndictionary used as input must be static as well.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22342490\n\nPulled By: suo\n\nfbshipit-source-id: 93e610a4895d61d9b8b19c8d2aa4e6d57777eaf6", "pr_number": "40885", "files_changed": ["test/jit/test_tracer.py", "torch/csrc/jit/frontend/tracer.cpp"], "labels": ["merged", "oncall: jit"]}, "591fffc524": {"title": "Type-annotate serialization.py (#40862)", "body": "Summary:\nMove Storage class from __init__.pyi.in to types.py and make it a protocol, since this is not a real class\nExpose `PyTorchFileReader` and `PyTorchFileWriter` native classes\n\nIgnore function attributes, as there are yet no good way to type annotate those, see https://github.com/python/mypy/issues/2087\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40862\n\nDifferential Revision: D22344743\n\nPulled By: malfet\n\nfbshipit-source-id: 95cdb6f980ee79383960f306223e170c63df3232", "pr_number": "40862", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in", "torch/__init__.py", "torch/serialization.py", "torch/types.py"], "labels": ["merged"]}, "1a74bb84f2": {"title": "Remove Int8FC diff restriction.", "body": "Summary: Remove Int8FC diff restriction.\n\nTest Plan: test_int8_ops_nnpi.py\n\nReviewed By: hyuen\n\nDifferential Revision: D22353200\n\nfbshipit-source-id: c6c80c9dda3245c02da8343ecd5689994baf0143", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py"], "labels": []}, "5db5a0f2bb": {"title": "Re-enable Caffe2 test `RoiAlignTest.CheckCPUGPUEqual` (#40901)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/35547.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40901\n\nDifferential Revision: D22357760\n\nPulled By: malfet\n\nfbshipit-source-id: 43f7dc13a905416288a9a317ae31a4dc78276ce4", "pr_number": "40901", "files_changed": ["caffe2/operators/roi_align_op_gpu_test.cc"], "labels": ["merged", "open source"]}, "6aabd12390": {"title": "fix issue #31759 (allow valid ASCII python identifiers as dimnames) (#40871)", "body": "Summary:\nFixes issue https://github.com/pytorch/pytorch/issues/31759:\n- Changes is_valid_identifier check on named tensor dimensions to allow digits if they are not at the beginning of the name (this allows exactly the ASCII subset of [valid python identifiers](https://docs.python.org/3/reference/lexical_analysis.html#identifiers)).\n- Updates error message for illegal dimension names.\n- Updates and adds relevant tests.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40871\n\nReviewed By: pbelevich\n\nDifferential Revision: D22357314\n\nPulled By: zou3519\n\nfbshipit-source-id: 9550a1136dd0673dd30a5cd5ade28069ba4c9086", "pr_number": "40871", "files_changed": ["aten/src/ATen/core/Dimname.cpp", "aten/src/ATen/test/Dimname_test.cpp", "test/test_namedtensor.py"], "labels": ["merged", "open source"]}, "6ae3cd0d9d": {"title": "Configure RPC metrics handlers and pass them into Thrift RPC Agent (#40602)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40602\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22250592\n\nfbshipit-source-id: d38131f30939fc26af241b40e057a9dc1109e950", "pr_number": "40602", "files_changed": ["tools/build_variables.bzl", "torch/csrc/distributed/rpc/metrics/RpcMetricsHandler.h", "torch/csrc/distributed/rpc/metrics/registry.cpp"], "labels": ["merged"]}, "b678666a04": {"title": "Add `module.training` to docs (#40923)", "body": "Summary:\nA lot of people ask https://discuss.pytorch.org/t/check-if-model-is-eval-or-train/9395/3\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40923\n\nReviewed By: pbelevich\n\nDifferential Revision: D22358799\n\nPulled By: zou3519\n\nfbshipit-source-id: b5465ffedb691fb4811e097c4dbd7bbc405be09c", "pr_number": "40923", "files_changed": ["torch/nn/modules/module.py"], "labels": ["merged", "open source"]}, "db39542509": {"title": "[2/n][Compute Meta] support analysis for null flag features", "body": "Summary:\n## TLDR\nSupport using NaN default value for missing dense features in RawInputProcessor for DPER2. In preparation for subsequent support for null flag features in compute meta. For train_eval this is already supported in DPER3 and we do not plan to support this in DPER2 train eval.\n## Overview\nIntern project plan to support adding dense flags for missing feature values instead of replacing with zero.\n## Project plan :\nhttps://docs.google.com/document/d/1OsPUTjpJycwxWLCue3Tnb1mx0uDC_2KKWvC1Rwpo2NI/edit?usp=sharing\n## Code paths:\nSee https://fb.quip.com/eFXUA0tbDmNw for the call stack for all affected code paths.\n\nTest Plan:\n## fblearner flow test\n\n1. `flow-cli clone f197867430 --run-as-secure-group ads_personalization_systems --force-build` to build a ephemeral package and start a fblearner flow run (may fail)\n2. Clone the new run and change the secure_group to `XXXX` and entitlement to `default` in the UI\n3. Adds explicit_null_min_coverage flag\n4. Optionally reduce `max_examples` since we only test pass/fail instead of quality.\n5. Submit the run to test the change\n\nExample:\nf198538878\n\n## compare output coverages to daiquery runs\n\n1. Randomly select null flag features from compute meta workflow output\n2. Look up the feature id in feature metadata using feature name\n3. Check against a daiquery sample of coverage to see if the coverage falls within guidelines.\nhttps://www.internalfb.com/intern/daiquery/workspace/275342740223489/192619942076136/\n\n## Sampled features:\nGFF_C66_ADS_USER_SUM_84_PAGE_TYPE_RATIO_EVENT_LIKE_IMPRESSION: 15694257\n- original feature compute meta coverage: 0.999992\n- daiquery feature coverage (10k rows): 0.69588\n- null flag compute meta coverage: 0.293409\nGFF_R1303_ADS_USER_SUM_7_PAGE_TYPE_COUNTER_CONVERSION: 16051183\n-  original feature compute meta coverage: 0.949868\n- daiquery feature coverage: 0.82241\n- null flag compute meta coverage: 0.151687\n\n## Unit tests:\n\n`buck test  fblearner/flow/projects/dper/tests/workflows:ads_test`\n\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/6192449504303863/\n\nDifferential Revision: D22026450\n\nfbshipit-source-id: 46c59d849fa89253f14dc2b035c4c677cd6e3a4c", "pr_number": null, "files_changed": ["caffe2/python/layers/feature_sparse_to_dense.py"], "labels": []}, "9f14e48834": {"title": "Override shape hints with real weight shape extracted from workspace (#40872)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40872\n\nShape hints as its name suggests, is hint. We should use real shape from workspace for the weights.\n\nReviewed By: ChunliF\n\nDifferential Revision: D22337680\n\nfbshipit-source-id: e7a6101fb613ccb332c3e34b1c2cb8c6c47ce79b", "pr_number": "40872", "files_changed": ["caffe2/opt/backend_transformer_base.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/onnxifi_transformer.cc"], "labels": ["fb-exported", "merged"]}, "af5bcba217": {"title": ".circleci: Build docker images as part of CI workflow (#40827)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40827\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22360735\n\nPulled By: seemethere\n\nfbshipit-source-id: 4ffbde563fdc3c49fdd14794ed3c2e881030361d", "pr_number": "40827", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/cimodel/data/simple/android_definitions.py", ".circleci/cimodel/data/simple/bazel_definitions.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/cimodel/data/simple/mobile_definitions.py", ".circleci/cimodel/data/simple/nightly_android.py", ".circleci/cimodel/data/simple/util/docker_constants.py", ".circleci/config.yml", ".circleci/docker/build_docker.sh", ".circleci/generate_config_yml.py", ".circleci/validate-docker-version.py", ".circleci/verbatim-sources/commands.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs/docker_jobs.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".github/workflows/lint.yml"], "labels": ["module: ci"]}, "ce63f70981": {"title": "[C2] Fixed a bug in normalization operator (#40925)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40925\n\nnormalization operator does not handle empty tensors correctly. This is a fix.\n\nTest Plan: unit tests\n\nDifferential Revision: D22330340\n\nfbshipit-source-id: 0bccf925bb768ebb997ed0c88130c5556308087f", "pr_number": "40925", "files_changed": ["caffe2/operators/normalize_l1_op.h", "caffe2/operators/normalize_op.h", "caffe2/python/operator_test/normalize_op_test.py"], "labels": ["fb-exported", "merged"]}, "a7e09b8727": {"title": "pytorch | Namespace init_win symbol in qnnpack.", "body": "Summary: Namespacing the symbol, since it clashes with \"the real thing\" otherwise.\n\nTest Plan: Sandcastle + build it on windows\n\nReviewed By: dreiss\n\nDifferential Revision: D22348240\n\nfbshipit-source-id: f9c9a7abc97626ba327605cb4749fc5c38a24d35", "pr_number": null, "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/src/init.c"], "labels": []}, "81aebf380e": {"title": "pytorch | Fix linking of qnnpack params on windows. (#40920)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40920\n\nPytorch depends on this from both C and C++ source files, so unify linking so it's fully fixed.\n\nTest Plan: Build it on Windows\n\nReviewed By: dreiss, supriyar\n\nDifferential Revision: D22348247\n\nfbshipit-source-id: 2933b4804f4725ab1742914656fa367527f8f7e1", "pr_number": "40920", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/src/conv-run.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/params.h"], "labels": ["fb-exported", "merged"]}, "dec3f918a0": {"title": "Migrate 'torch.dot' from TH to Aten (CUDA) (#40646)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40646\n\nSupport double, float, at::Half.\nAvoid creating output result on CPU.\n\nBoth of two tensors must be on GPU.\n\nReviewed By: ngimel\n\nDifferential Revision: D22258840\n\nfbshipit-source-id: 95f4747477f09b40b1d682cd1f76e4c2ba28c452", "pr_number": "40646", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/THCBlas.cu", "aten/src/THC/THCBlas.h", "aten/src/THC/generic/THCTensorMathBlas.cu", "aten/src/THC/generic/THCTensorMathBlas.h"], "labels": ["fb-exported", "merged"]}, "b7517a76ba": {"title": "rshift use default >> operator (#40545)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/40032\nAlso see https://github.com/pytorch/pytorch/pull/35339\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40545\n\nReviewed By: pbelevich\n\nDifferential Revision: D22362816\n\nPulled By: ngimel\n\nfbshipit-source-id: 4bbf9212b21a4158badbfee8146b3b67e94d5a33", "pr_number": "40545", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "9cc73966b3": {"title": "[TVM] Fix build and sync with caffe2/caffe2/python/dlpack.h (#40888)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40888\n\nReviewed By: yinghai\n\nDifferential Revision: D22326379\n\nfbshipit-source-id: 96ffcff5738973312c49368f53f35bf410e4c0c9", "pr_number": "40888", "files_changed": ["caffe2/python/dlpack.h"], "labels": ["fb-exported", "merged"]}, "a1c234e372": {"title": "Revert D22330340: [C2] Fixed a bug in normalization operator", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22330340 (https://github.com/pytorch/pytorch/commit/ce63f70981e94349ae2eba0f2898e7cd0058a351)\n\nOriginal commit changeset: 0bccf925bb76\n\nfbshipit-source-id: e27d70dee0fbe9e708b0cf3be81dbd33c4015026", "pr_number": null, "files_changed": ["caffe2/operators/normalize_l1_op.h", "caffe2/operators/normalize_op.h", "caffe2/python/operator_test/normalize_op_test.py"], "labels": []}, "3c6b8a6496": {"title": "Revert D22360735: .circleci: Build docker images as part of CI workflow", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22360735 (https://github.com/pytorch/pytorch/commit/af5bcba217646c52137b0c1feef036de6827ca85)\n\nOriginal commit changeset: 4ffbde563fdc\n\nfbshipit-source-id: 4ae2288f466703754c9e329d34d344269c70db83", "pr_number": null, "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/cimodel/data/simple/android_definitions.py", ".circleci/cimodel/data/simple/bazel_definitions.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/cimodel/data/simple/mobile_definitions.py", ".circleci/cimodel/data/simple/nightly_android.py", ".circleci/cimodel/data/simple/util/docker_constants.py", ".circleci/config.yml", ".circleci/docker/build_docker.sh", ".circleci/generate_config_yml.py", ".circleci/validate-docker-version.py", ".circleci/verbatim-sources/commands.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs/docker_jobs.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".github/workflows/lint.yml"], "labels": []}, "f8d4878b3c": {"title": "check for unsupported instructions when exporting mobile models (#40791)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40791\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22311469\n\nPulled By: ann-ss\n\nfbshipit-source-id: 7a6abb3f2477e8553f8c71f4aa0442df4f712fb5", "pr_number": "40791", "files_changed": ["test/mobile/test_lite_script_module.py", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["merged", "oncall: jit"]}, "3ca5849f0a": {"title": "Add serializer and deserializer for Int8QuantSchemeBlob and Int8QuantParamsBlob (#40661)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40661\n\nAdd ser-de to support int8 quantization during online training\n\nTest Plan:\n```\nbuck test caffe2/caffe2/fb/fbgemm:int8_serializer_test\n```\n\nReviewed By: hx89\n\nDifferential Revision: D22273292\n\nfbshipit-source-id: 3b1e9c820243acf41044270afce72a262ef92bd4", "pr_number": "40661", "files_changed": ["caffe2/quantization/server/int8_gen_quant_params.h"], "labels": ["fb-exported", "merged"]}, "6095808d22": {"title": "fix pca_lowrank memory consumption (#40853)", "body": "Summary:\nPer title, fixes https://github.com/pytorch/pytorch/issues/40768\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40853\n\nReviewed By: pbelevich\n\nDifferential Revision: D22363906\n\nPulled By: ngimel\n\nfbshipit-source-id: 966a4b230d351f7632c5cfae4a3b7c9a787bc9a5", "pr_number": "40853", "files_changed": ["torch/_lowrank.py"], "labels": ["merged"]}, "28e1d241cd": {"title": "[pytorch] factor out binary size upload command (#40188)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40188\n\nCreate a custom command for this task to avoid copy/paste for new build jobs.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22111114\n\nPulled By: ljk53\n\nfbshipit-source-id: a7d4d6bbd61ba6b6cbaa137ec7f884736957dc39", "pr_number": "40188", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/commands.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["merged"]}, "ff17b83fd8": {"title": "[pytorch][ci] add custom selective build flow for android build (#40199)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40199\n\nMobile custom selective build has already been covered by `test/mobile/custom_build/build.sh`.\nIt builds a CLI binary with host-toolchain and runs on host machine to\ncheck correctness of the result.\n\nBut that custom build test doesn't cover the android/gradle build part.\nAnd we cannot use it to measure and track the in-APK size of custom\nbuild library.\n\nSo this PR adds the selective build test coverage for android NDK build.\nAlso integrate with the CI to upload the custom build size to scuba.\n\nTODO:\nIdeally it should build android/test_app and measure the in-APK size.\nBut the test_app hasn't been covered by any CI yet and is currently\nbroken, so build & measure AAR instead (which can be inaccurate as we\nplan to pack C++ header files into AAR soon).\n\nSample result: https://fburl.com/scuba/pytorch_binary_size/skxwb1gh\n```\n\n+---------------------+-------------+-------------------+-----------+----------+\n|     build_mode      |    arch     |        lib        | Build Num |   Size   |\n+---------------------+-------------+-------------------+-----------+----------+\n| custom-build-single | armeabi-v7a | libpytorch_jni.so |   5901579 | 3.68 MiB |\n| prebuild            | armeabi-v7a | libpytorch_jni.so |   5901014 | 6.23 MiB |\n| prebuild            | x86_64      | libpytorch_jni.so |   5901014 | 7.67 MiB |\n+---------------------+-------------+-------------------+-----------+----------+\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22111115\n\nPulled By: ljk53\n\nfbshipit-source-id: 11d24efbc49a85f851ecd0e481d14123f405b3a9", "pr_number": "40199", "files_changed": [".circleci/cimodel/data/simple/android_definitions.py", ".circleci/cimodel/data/simple/util/branch_filters.py", ".circleci/config.yml", ".circleci/scripts/build_android_gradle.sh", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", "android/build_test_app_custom.sh", "android/test_app/make_assets_custom.py"], "labels": ["merged"]}, "824ab19941": {"title": "[quant][graphmode] Support quantization for `aten::apend` (#40743)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40743\n\n`aten::append` modifies input inplace and the output is ignored, these ops are not\nsupported right now, so we'll need to first make `aten::append` non-inplace\nby change\n```\nignored = aten::append(list, x)\n```\nto\n```\nx_list = aten::ListConstruct(x)\nresult = aten::add(list, x_list)\n```\nand then quantize the aten::add instead.\n\nTest Plan:\nTestQuantizeJitOps.test_general_shape_ops\n\nImported from OSS\n\nDifferential Revision: D22302151\n\nfbshipit-source-id: 931000388e7501e9dd17bec2fad8a96b71a5efc5", "pr_number": "40743", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/finalize.cpp", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/insert_observers.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": ["merged", "oncall: jit"]}, "3890550940": {"title": "[RPC tests] Fix @_skip_if_tensorpipe always skipping for all agents (#40860)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40860\n\nIt turns out that the `@_skip_if_tensorpipe_agent` decorator was written in such a way that it accidentally caused the test to become a no-op (and thus always succeed) for all agents. What this means is that all tests wrapped by that decorator were never ever being run, for any agent.\n\nMy understanding of the root cause is that the following code:\n```\n@_skip_if_tensorpipe_agent\ndef test_foo(self):\n    self.assertEqual(2 + 2, 4)\n```\nended up behaving somewhat like this:\n```\ndef test_foo(self):\n    def original_test_func(self):\n        self.assertEqual(2 + 2, 4)\n    return unittest.skipIf(self.agent == \"TENSORPIPE\")(original_test_func)\n```\nwhich means that the test body of the decorated method was not actually calling the original test method.\n\nThis issue probably came from the `@_skip_if_tensorpipe_agent` being copy-pasted from `requires_process_group_agent` (which, however, is not a decorator but rather a decorator *factory*). An unfortunate naming (calling `decorator` what was in fact the wrapped method) then hindered readability and hid the issue.\n\nNote that a couple of tests had become legitimately broken in the meantime and no one had noticed. The breakages have been introduced in #39909 (a.k.a., D22011868 (https://github.com/pytorch/pytorch/commit/145df306aee0884810aa7660db8e46b1daf90bf7)).\nghstack-source-id: 107045916\n\nTest Plan: Discovered this as part of my refactoring, in D22332611. After fixing the decorator two tests started breaking (for real reasons). After fixing them all is passing.\n\nDifferential Revision: D22332611\n\nfbshipit-source-id: f88ca5574675fdb3cd09a9f6da12bf1e25203a14", "pr_number": "40860", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "d0f2079b5e": {"title": "[RPC tests] Remove world_size and init_method from TensorPipe fixture (#40814)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40814\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nThis prepares the stack by simplifying the TensorPipe fixture. A comment says that the TensorPipe fixture cannot subclass the generic fixture class as that would lead to a diamond class hierarchy which Python doesn't support (whereas in fact it does), and therefore it copies over two properties that are defined on the generic fixture. However, each class that uses the TensorPipe fixture also inherits from the generic fixture, so there's no need to redefine those properties. And, in fact, by not redefining it we save ourselves some trouble when the TensorPipe fixture would end up overriding another override.\nghstack-source-id: 107045914\n\nTest Plan: Sandcastle and CircleCI\n\nDifferential Revision: D22287533\n\nfbshipit-source-id: 254c38b36ba51c9d852562b166027abacbbd60ef", "pr_number": "40814", "files_changed": ["test/distributed/rpc/tensorpipe/test_ddp_under_dist_autograd.py", "torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py"], "labels": ["merged"]}, "f9a71d3de4": {"title": "[RPC tests] Align ddp_under_dist_autograd test with others (#40815)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40815\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nThis prepares the stack by aligning the `ddp_under_dist_autograd` test to the other ones, so that later changes will be more consistent and thus easier to follow. It does so by moving the `skipIf` decorators and the `setUp` methods from the base test suite to the entry point scripts.\nghstack-source-id: 107045911\n\nTest Plan: Sandcastle and CircleCI\n\nDifferential Revision: D22287535\n\nfbshipit-source-id: ab0c9eb774b21d81e0ebd3078df958dbb4bfa0c7", "pr_number": "40815", "files_changed": ["test/distributed/rpc/tensorpipe/test_ddp_under_dist_autograd.py", "test/distributed/test_ddp_under_dist_autograd.py", "torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["merged"]}, "f083cea227": {"title": "[RPC tests] Fix file descriptor leak (#40913)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40913\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nOnce we start merging multiple test suites in a single file (which we'll happen in the next diffs in the stack) the OSX tests on CircleCI start failing due to \"too many open files\". This indicates a file descriptor leak. I then managed to repro it on Linux too by lowering the limit on open file descriptors (`ulimit -n 500`). Each test method that unittest runs is run on a new instance of the Testcase class. With our multiprocessing wrappers, this instance contains a list of child processes. Even after these processes are terminated, it appears they still hold some open file descriptor (for example a pipe to communicate with the subprocess). It also appears unittest is keeping these Testcase instances alive until the entire suite completes, which I suspect is what leads to this \"leak\" of file descriptors. Based on that guess, in this diff I am resetting the list of subprocesses during shutdown, and this seems to fix the problem.\nghstack-source-id: 107045908\n\nTest Plan: Sandcastle and CircleCI\n\nDifferential Revision: D22356784\n\nfbshipit-source-id: c93bb9db60fde72cae0b0c735a50c17e427580a6", "pr_number": "40913", "files_changed": ["torch/testing/_internal/common_distributed.py"], "labels": ["merged"]}, "f3f113f103": {"title": "[quant][graphmode][fix] Print the node in error message (#40889)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40889\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22348266\n\nfbshipit-source-id: eed2ece5c94fcfaf187d6770bed4a7109f0c0b4a", "pr_number": "40889", "files_changed": ["torch/csrc/jit/passes/quantization/insert_observers.cpp"], "labels": ["merged", "oncall: jit"]}, "3b7df2388e": {"title": "[RFC] Profile rpc_async call from JIT (#40652)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40652\n\nResolves https://github.com/pytorch/pytorch/issues/40304, but looking for\nfeedback on whether there is a better approach for this.\n\nIn order to profile `rpc_async` calls made within a torchscript function, we\nadd the profiling logic to `rpcTorchscript` which is the point where the RPC is\ndispatched and is called by the jit `rpc_async` operator. We take a somewhat\nsimilar approach to how this is done in the python API. If profiling is\nenabled, we call `record_function_enter` which creates a `RecordFunction`\nobject and runs its starting callbacks. Then, we schedule end callbacks for\nthis `RecordFunction` to be run when the jit future completes.\n\nOne caveat is that `rpcTorchscript` can also be called by rpc_async from a\nnon-JIT function, in which case the profiling logic lives in Python. We add a\ncheck to ensure that we don't double profile in this case.\nghstack-source-id: 107109485\n\nTest Plan: Added relevant unittests.\n\nDifferential Revision: D22270608\n\nfbshipit-source-id: 9f62d1a2a27f9e05772d0bfba47842229f0c24e1", "pr_number": "40652", "files_changed": ["torch/csrc/autograd/record_function_ops.h", "torch/csrc/distributed/rpc/torchscript_functions.cpp", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["merged", "oncall: jit"]}, "0790d11a18": {"title": "typing for tensor.T/grad_fn torch.Size (#40879)", "body": "Summary:\nfixes  https://github.com/pytorch/pytorch/issues/40658 https://github.com/pytorch/pytorch/issues/40658\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40879\n\nReviewed By: pbelevich\n\nDifferential Revision: D22339146\n\nPulled By: ezyang\n\nfbshipit-source-id: 6b4695e102591e7a2c391eb337c154414bacf67c", "pr_number": "40879", "files_changed": ["torch/_C/__init__.pyi.in"], "labels": ["merged", "open source"]}, "8ecd4f36aa": {"title": "fix __len__, __contains__, getitem inherited from interface class derived from nn container (closes #40603) (#40789)", "body": "Summary:\nDefine static script implementation of __len__ and __contains__ on any subclass derived from a type such as ModuleList, Sequential, or ModuleDict.  Implement getitem for classes derived from ModuleDict.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40789\n\nReviewed By: eellison\n\nDifferential Revision: D22325159\n\nPulled By: wconstab\n\nfbshipit-source-id: fc1562c29640fe800e13b5a1dd48e595c2c7239b", "pr_number": "40789", "files_changed": ["test/jit/test_module_containers.py", "test/test_jit.py", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/jit/_recursive.py"], "labels": ["merged", "oncall: jit"]}, "1e64bf4c40": {"title": "[CircleCI] Delete docker image after testing (#40917)", "body": "Summary:\nNeeded maintenance step to avoid running out of disk space on RocM testers\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40917\n\nDifferential Revision: D22385844\n\nPulled By: malfet\n\nfbshipit-source-id: b6dc9ba888a2e34c311e9bf3c8b7b98fa1ec5435", "pr_number": "40917", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "300a3aaaad": {"title": "[jit] move private implementation out of `jit/__init__.py` (#40807)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40807\n\nWe pack a lot of logic into `jit/__init__.py`, making it unclear to\ndevelopers and users which parts of our API are public vs. internal. This\nis one in a series of PRs intended to pull implementation out into\nseparate files, and leave `__init__.py` as a place to register the\npublic API.\n\nThis PR moves all the tracing-related stuff out, and fixes other spots up\nas necessary. Followups will move other core APIs out.\n\nThe desired end-state is that we conform to the relevant rules in [PEP 8](https://www.python.org/dev/peps/pep-0008/#public-and-internal-interfaces). In particular:\n- Internal implementation goes in modules prefixed by `_`.\n- `__init__.py` exposes a public API from these private modules, and nothing more.\n- We set `__all__` appropriately to declare our public API.\n- All use of JIT-internal functionality outside the JIT are removed (in particular, ONNX is relying on a number internal APIs). Since they will need to be imported explicitly, it will be easier to catch new uses of internal APIs in review.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D22320645\n\nPulled By: suo\n\nfbshipit-source-id: 0720ea9976240e09837d76695207e89afcc58270", "pr_number": "40807", "files_changed": ["mypy.ini", "test/test_jit.py", "torch/jit/__init__.py", "torch/jit/_script.py", "torch/jit/_state.py", "torch/jit/_trace.py", "torch/nn/modules/module.py"], "labels": ["merged", "oncall: jit"]}, "0deb2560b8": {"title": "add eq.str, ne.str, and add.str ops (#40958)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40958\n\nadd 3 str operators to lite interpreter\neq.str\nne.str\nadd.str\n\nTest Plan:\n```\nbuck run //xplat/caffe2/fb/pytorch_predictor:converter /mnt/vol/gfsfblearner-altoona/flow/data/2020-06-29/1ca8a85f-dbd5-4181-b5fc-63d24465c1fc/201084299/2068673333/model.pt1 ~/model_f201084299.bc\n\nbuck run xplat/assistant/model_benchmark_tool/mobile/binary/:lite_predictor -- --model ~/model_f201084299.bc --input_file /tmp/gc_model_input.txt --model_input_args src_tokens,dict_feat,contextual_token_embedding --warmup 1 --iter 2\n\n```\n\nReviewed By: pengtxiafb\n\nDifferential Revision: D22369579\n\nfbshipit-source-id: 7ac9a184d437c875edfb584221edd706bffb16e1", "pr_number": "40958", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "54d7a1e3f4": {"title": "Fix module dict key ordering (#40905)", "body": "Summary:\nfix https://github.com/pytorch/pytorch/issues/40227\nRemoved the sorting operation both in ModuleDict class, updated the docstring.\nAlso remove a sort operation in corresponding unit test, which will lead to unit test fail.\n\nBC Note: Python version after 3.6, the plain dict will preserve the order of keys.\nexample:\nFor a python 3.6+ user, if he is initial a ModuleDict instance using plain python dict:\n{\n\"b\": torch.nn.MaxPool2d(3),\n\"a\": torch.nn.MaxPool2d(3)\n}\n, he will get a ModuleDict which preserve the order:\nModuleDict(\n(b): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n(a): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n)\n\nFor a python 3.5 user, if we maintain the same input, then the output ModuleDict could be:\nModuleDict(\n(a): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n(b): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40905\n\nDifferential Revision: D22357480\n\nPulled By: albanD\n\nfbshipit-source-id: 0e2502769647bb64f404978243ca1ebe5346d573", "pr_number": "40905", "files_changed": ["test/test_nn.py", "torch/nn/modules/container.py"], "labels": ["merged", "open source"]}, "450ba49653": {"title": "Add the missing `resource_class` key in the update_s3_htmls job (#41000)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/40998.\n\nActually I don't know why it is needed. But without it, the build won't start. See my rerun of the update_s3_html3 job: https://app.circleci.com/pipelines/github/pytorch/pytorch/187926/workflows/432dbe98-ca2f-484d-acc7-0482cb3fd01f/jobs/6121551/steps.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41000\n\nDifferential Revision: D22390654\n\nPulled By: malfet\n\nfbshipit-source-id: 0f296c8a82fa92d5382f883bca951e6576f75b15", "pr_number": "41000", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/binary_update_htmls.yml"], "labels": ["merged", "open source"]}, "063d5b0d3f": {"title": "Remove get_fail_msg in test_dataloader.test_proper_exit (#40745)", "body": "Summary:\nClose https://github.com/pytorch/pytorch/issues/40744\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40745\n\nReviewed By: ezyang\n\nDifferential Revision: D22308972\n\nPulled By: colesbury\n\nfbshipit-source-id: 4b4847e6b926b2614c8b14f17a9db3b0376baabe", "pr_number": "40745", "files_changed": ["test/test_dataloader.py"], "labels": ["merged", "open source"]}, "0b9717b86a": {"title": "When linking libtorch_cpu.so, put AVX sources last in the input list (#40449)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/39600\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40449\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22312501\n\nPulled By: colesbury\n\nfbshipit-source-id: 4c09adb0173749046f20b84241d6c940b339ad77", "pr_number": "40449", "files_changed": ["caffe2/CMakeLists.txt", "cmake/Codegen.cmake"], "labels": ["merged", "open source"]}, "e1afa9daff": {"title": "fix cmake bug (#39930)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39930\n\nDifferential Revision: D22391207\n\nPulled By: ezyang\n\nfbshipit-source-id: bde19a112846e124d4e5316ba947f48d4dccf361", "pr_number": "39930", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["merge-this-please", "open source"]}, "46f5cf1e31": {"title": "Improve error reporting of AVX instruction in CI job (#40681)", "body": "Summary:\nClose https://github.com/pytorch/pytorch/issues/40320\n\nLeverage `qemu` and `gdbserver` for printing backtrace and instruction, and help developers to understand the causes of failed tests better.\n\nSigned-off-by: Xiong Wei <xiongw.fnst@cn.fujitsu.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40681\n\nDifferential Revision: D22391512\n\nPulled By: malfet\n\nfbshipit-source-id: 19f125cf6c0e5a51814aff2b1d4d3c81298e3cb6", "pr_number": "40681", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged", "open source"]}, "73c5a78f43": {"title": "Test test_int8_ops_nnpi.py case typo fix. (#41008)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41008\n\nTest test_int8_ops_nnpi.py case typo fix.\n\nTest Plan: test_int8_ops_nnpi.py case typo fix.\n\nReviewed By: hl475\n\nDifferential Revision: D22390331\n\nfbshipit-source-id: 8d257c72114ce890720219eb519b9cb43b2ca49b", "pr_number": "41008", "files_changed": ["caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py"], "labels": ["fb-exported", "merged"]}, "f6f3c0094a": {"title": "Revert D22369579: add eq.str, ne.str, and add.str ops", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22369579 (https://github.com/pytorch/pytorch/commit/0deb2560b8b3c919115ec9302a781395838d97f7)\n\nOriginal commit changeset: 7ac9a184d437\n\nfbshipit-source-id: 9c861b9f6bf32fe51fa0ea516cf09a3d09d78a7c", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": []}, "c935712d58": {"title": "Use unbind for tensor.__iter__ (#40884)", "body": "Summary:\nUnbind, which has a special backward with cat, is arguably better than multiple selects, whose backward is creating & adding a bunch of tensors as big as `self`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40884\n\nReviewed By: pbelevich\n\nDifferential Revision: D22363376\n\nPulled By: zou3519\n\nfbshipit-source-id: 0911cdbb36f9a35d1b95f315d0a2f412424e056d", "pr_number": "40884", "files_changed": ["torch/tensor.py"], "labels": ["merged", "open source"]}, "c38a5cba0d": {"title": "Remove duplicate assignment in collate.py (#40655)", "body": "Summary:\nDuplicated assignment\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40655\n\nReviewed By: ezyang\n\nDifferential Revision: D22308827\n\nPulled By: colesbury\n\nfbshipit-source-id: 48361da8994b3ca00ef29e9afd3ec2672266f00a", "pr_number": "40655", "files_changed": ["torch/utils/data/_utils/collate.py"], "labels": ["merged", "open source"]}, "e173278348": {"title": "Update quantization.rst (#40896)", "body": "Summary:\nAdd documentation for dynamic quantized modules\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40896\n\nDifferential Revision: D22395955\n\nPulled By: z-a-f\n\nfbshipit-source-id: cdc956d1509a0901bc24b73b6ca68a1b65e00cc2", "pr_number": "40896", "files_changed": ["docs/source/quantization.rst"], "labels": ["merged"]}, "7f60642bae": {"title": "[pytorch] add manual registration for trace type (#40903)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40903\n\nThis PR continues the work of #38467 - decoupling Autograd and Trace for manually registered ops.\nghstack-source-id: 107158638\n\nTest Plan: CI\n\nDifferential Revision: D22354804\n\nfbshipit-source-id: f5ea45ade2850296c62707a2a4449d7d67a9f5b5", "pr_number": "40903", "files_changed": ["caffe2/CMakeLists.txt", "test/jit/test_tracer.py", "tools/build_variables.bzl", "torch/csrc/autograd/TraceTypeManual.cpp", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "0fbd42b20f": {"title": "[pytorch] deprecate PYTORCH_DISABLE_TRACING macro (#41004)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41004\n\nTracing has been moved into separate files. Now we can disable it by not compiling the source files for xplat mobile build.\nghstack-source-id: 107158627\n\nTest Plan: CI + build size bot\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22372615\n\nfbshipit-source-id: bf2e2249e401295ff63020a292df119b188fb966", "pr_number": "41004", "files_changed": ["tools/autograd/gen_variable_type.py", "torch/csrc/autograd/TraceTypeManual.cpp"], "labels": ["merged"]}, "d753f1c2e1": {"title": "Fixes formatting of vander, count_nonzero, DistributedSampler documentation (#41025)", "body": "Summary:\nBundle of small edits to fix formatting.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41025\n\nDifferential Revision: D22398364\n\nPulled By: mruberry\n\nfbshipit-source-id: 8d484cb52a1cf4a8eb1f64914574250c9fd5043d", "pr_number": "41025", "files_changed": ["torch/_torch_docs.py", "torch/utils/data/distributed.py"], "labels": ["merged", "module: docs"]}, "e026d91506": {"title": "[JIT] Remove dead store in unpickler.cpp (#40625)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40625\n\nTest Plan: Continuous integration.\n\nReviewed By: suo\n\nDifferential Revision: D22259289\n\nfbshipit-source-id: 76cb097dd06a636004fc780b17cb20f27d3821de", "pr_number": "40625", "files_changed": ["torch/csrc/jit/serialization/unpickler.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "b9b4f05abf": {"title": "[nvFuser] Working towards reductions, codegen improvements (#40864)", "body": "Summary:\nHave basic reduction fusion working, and have improved code generator to approach performance of eager mode reductions. Coming soon will be pointwise-reduction fusions in a way that should prevent the possibility of hitting regressions. Also working on performant softmax kernels in the code generator which may be our next fusion target.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40864\n\nReviewed By: ngimel\n\nDifferential Revision: D22392877\n\nPulled By: soumith\n\nfbshipit-source-id: 457448a807d628b1035f6d90bc0abe8a87bf8447", "pr_number": "40864", "files_changed": [".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-test-helpers/test_python_all_except_nn.bat", "caffe2/CMakeLists.txt", "test/cpp/jit/test_gpu.cpp", "test/cpp/jit/tests.h", "test/run_test.py", "test/test_jit_cuda_fuser.py", "test/test_jit_cuda_fuser_legacy.py", "test/test_jit_cuda_fuser_profiling.py", "tools/build_variables.bzl", "torch/csrc/jit/codegen/cuda/arith.cpp", "torch/csrc/jit/codegen/cuda/arith.h", "torch/csrc/jit/codegen/cuda/compute_at.cpp", "torch/csrc/jit/codegen/cuda/compute_at.h", "torch/csrc/jit/codegen/cuda/dispatch.cpp", "torch/csrc/jit/codegen/cuda/dispatch.h", "torch/csrc/jit/codegen/cuda/expr_evaluator.cpp", "torch/csrc/jit/codegen/cuda/expr_evaluator.h", "torch/csrc/jit/codegen/cuda/fusion.cpp", "torch/csrc/jit/codegen/cuda/fusion.h", "torch/csrc/jit/codegen/cuda/index_compute.cpp", "torch/csrc/jit/codegen/cuda/index_compute.h", "torch/csrc/jit/codegen/cuda/ir_base_nodes.cpp", "torch/csrc/jit/codegen/cuda/ir_base_nodes.h", "torch/csrc/jit/codegen/cuda/ir_cloner.cpp", "torch/csrc/jit/codegen/cuda/ir_cloner.h", "torch/csrc/jit/codegen/cuda/ir_graphviz.cpp", "torch/csrc/jit/codegen/cuda/ir_graphviz.h", "torch/csrc/jit/codegen/cuda/ir_interface_nodes.h", "torch/csrc/jit/codegen/cuda/ir_internal_nodes.h", "torch/csrc/jit/codegen/cuda/ir_iostream.cpp", "torch/csrc/jit/codegen/cuda/ir_iostream.h", "torch/csrc/jit/codegen/cuda/ir_nodes.cpp", "torch/csrc/jit/codegen/cuda/iter_visitor.cpp", "torch/csrc/jit/codegen/cuda/iter_visitor.h", "torch/csrc/jit/codegen/cuda/kernel.cpp", "torch/csrc/jit/codegen/cuda/kernel.h", "torch/csrc/jit/codegen/cuda/kernel_arg.h", "torch/csrc/jit/codegen/cuda/kernel_resource_strings.h", "torch/csrc/jit/codegen/cuda/lower2device.cpp", "torch/csrc/jit/codegen/cuda/lower2device.h", "torch/csrc/jit/codegen/cuda/lower_index.cpp", "torch/csrc/jit/codegen/cuda/lower_index.h", "torch/csrc/jit/codegen/cuda/lower_loops.cpp", "torch/csrc/jit/codegen/cuda/lower_loops.h", "torch/csrc/jit/codegen/cuda/lower_thread_predicate.cpp", "torch/csrc/jit/codegen/cuda/lower_thread_predicate.h", "torch/csrc/jit/codegen/cuda/lower_unroll.cpp", "torch/csrc/jit/codegen/cuda/lower_unroll.h", "torch/csrc/jit/codegen/cuda/lower_utils.cpp", "torch/csrc/jit/codegen/cuda/lower_validation.cpp", "torch/csrc/jit/codegen/cuda/lower_validation.h", "torch/csrc/jit/codegen/cuda/manager.cpp", "torch/csrc/jit/codegen/cuda/manager.h", "torch/csrc/jit/codegen/cuda/mutator.h", "torch/csrc/jit/codegen/cuda/parser.cpp", "torch/csrc/jit/codegen/cuda/parser.h", "torch/csrc/jit/codegen/cuda/partition.cpp", "torch/csrc/jit/codegen/cuda/partition.h", "torch/csrc/jit/codegen/cuda/predicate_compute.h", "torch/csrc/jit/codegen/cuda/register_interface.cpp", "torch/csrc/jit/codegen/cuda/shape_inference.cpp", "torch/csrc/jit/codegen/cuda/tensor_meta.cpp", "torch/csrc/jit/codegen/cuda/tensor_meta.h", "torch/csrc/jit/codegen/cuda/tensor_view.cpp", "torch/csrc/jit/codegen/cuda/transform_iter.cpp", "torch/csrc/jit/codegen/cuda/transform_iter.h", "torch/csrc/jit/codegen/cuda/transform_replay.cpp", "torch/csrc/jit/codegen/cuda/transform_replay.h", "torch/csrc/jit/codegen/cuda/transform_rfactor.cpp", "torch/csrc/jit/codegen/cuda/transform_rfactor.h", "torch/csrc/jit/codegen/cuda/type.cpp", "torch/csrc/jit/codegen/cuda/type.h", "torch/csrc/jit/codegen/cuda/utils.cpp"], "labels": ["merged", "oncall: jit", "open source"]}, "35bd2b3c8b": {"title": "DOC: Clarify that CrossEntropyLoss mean is weighted (#40991)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/40560\n\nThis adds the equation for the weighted mean to `CrossEntropyLoss`'s docs and the `reduction` argument for `CrossEntropyLoss` and `NLLLoss` no longer describes a non-weighted mean of the outputs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40991\n\nDifferential Revision: D22395805\n\nPulled By: ezyang\n\nfbshipit-source-id: a623b6dd2aab17220fe0bf706bd9b62d6ba531fd", "pr_number": "40991", "files_changed": ["torch/nn/modules/loss.py"], "labels": ["merged", "open source"]}, "945ae5bd7b": {"title": "Update the documentation of the scatter_ method with support for reduction methods. (#40962)", "body": "Summary:\nFollow up to https://github.com/pytorch/pytorch/pull/36447 . Update for https://github.com/pytorch/pytorch/issues/33389.\n\nAlso removes unused `unordered_map` include from the CPP file.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40962\n\nDifferential Revision: D22376253\n\nPulled By: ngimel\n\nfbshipit-source-id: 4e7432190e9a847321aec6d6f6634056fa69bdb8", "pr_number": "40962", "files_changed": ["aten/src/ATen/native/cpu/ScatterGatherKernel.cpp", "torch/_tensor_docs.py"], "labels": ["merged", "open source"]}, "87f9b55aa5": {"title": "Use explicit templates in `gpu_kernel_with_scalars` (#40992)", "body": "Summary:\nThis trick should have no effect on performance, but it reduces size of kernels using the template by 10%\nFor example, sizeof(BinaryMulDivKernel.cu.o) compiled by CUDA-10.1 toolchain for sm_75 before the change was 4.2Mb, after 3.8Mb\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40992\n\nDifferential Revision: D22398733\n\nPulled By: malfet\n\nfbshipit-source-id: 6576f4da00dc5fc2575b2313577f52c6571d5e6f", "pr_number": "40992", "files_changed": ["aten/src/ATen/native/cuda/Loops.cuh"], "labels": ["merged"]}, "cbe52d762c": {"title": "Mish Activation Function (#40856)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40856\n\nAdd a new activation function - Mish: A Self Regularized Non-Monotonic Neural Activation Function https://arxiv.org/abs/1908.08681\n\nTest Plan:\nbuck test //caffe2/caffe2/python/operator_test:elementwise_ops_test -- 'test_mish'\n\n{F242275183}\n\nDifferential Revision: D22158035\n\nfbshipit-source-id: 459c1dd0ac5b515913fc09b5f4cd13dcf095af31", "pr_number": "40856", "files_changed": ["caffe2/operators/mish_op.cc", "caffe2/operators/mish_op.h", "caffe2/python/operator_test/elementwise_ops_test.py"], "labels": ["fb-exported", "merged"]}, "c0f9bf9bea": {"title": "s/torch::jit::class_/torch::class_/ (#40795)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40795\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D22314215\n\nPulled By: jamesr66a\n\nfbshipit-source-id: a2fb5c6804d4014f8e437c6858a7be8cd3efb380", "pr_number": "40795", "files_changed": ["aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_unpack.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "torch/csrc/jit/python/python_custom_class.cpp"], "labels": ["merged", "oncall: jit"]}, "a78024476b": {"title": "Port `equal` from THC to ATen (CUDA) (#36483)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/24557\n\nASV benchmark:\n\n```\nimport torch\n\nsizes = [\n    (10**6,),\n    (1000, 1000),\n    (10, 10),\n    (1, 2, 3, 4, 5, 6, 7, 8, 9, 10),\n]\n\nclass EqualTrue:\n    params = range(len(sizes))\n\n    def setup(self, n):\n        dims = sizes[n]\n        self.a = torch.rand(dims, device='cuda')\n        self.b = self.a.clone()\n\n    def time_equal(self, n):\n        torch.equal(self.a, self.b)\n\nclass EqualFalse:\n    params = range(len(sizes))\n\n    def setup(self, n):\n        dims = sizes[n]\n        self.a = torch.rand(dims, device='cuda')\n        self.b = torch.rand(dims, device='cuda')\n\n    def time_equal(self, n):\n        torch.equal(self.a, self.b)\n```\n\nOld results:\n```\n[ 75.00%] \u00b7\u00b7\u00b7 equal.EqualFalse.time_equal\n[ 75.00%] \u00b7\u00b7\u00b7 ======== ============\n               param1\n              -------- ------------\n                 0       67.7\u00b17\u03bcs\n                 1       74.0\u00b12\u03bcs\n                 2      24.4\u00b10.1\u03bcs\n                 3      135\u00b10.2\u03bcs\n              ======== ============\n\n[100.00%] \u00b7\u00b7\u00b7 equal.EqualTrue.time_equal\n[100.00%] \u00b7\u00b7\u00b7 ======== ============\n               param1\n              -------- ------------\n                 0      59.8\u00b10.2\u03bcs\n                 1      59.9\u00b10.3\u03bcs\n                 2      25.0\u00b10.5\u03bcs\n                 3      136\u00b10.2\u03bcs\n              ======== ============\n```\n\nNew results:\n```\n[ 75.00%] \u00b7\u00b7\u00b7 equal.EqualFalse.time_equal\n[ 75.00%] \u00b7\u00b7\u00b7 ======== ============\n               param1\n              -------- ------------\n                 0      44.4\u00b10.2\u03bcs\n                 1      44.5\u00b10.4\u03bcs\n                 2      31.3\u00b10.3\u03bcs\n                 3      96.6\u00b10.5\u03bcs\n              ======== ============\n\n[100.00%] \u00b7\u00b7\u00b7 equal.EqualTrue.time_equal\n[100.00%] \u00b7\u00b7\u00b7 ======== ============\n               param1\n              -------- ------------\n                 0      44.2\u00b10.2\u03bcs\n                 1      44.6\u00b10.2\u03bcs\n                 2      30.8\u00b10.3\u03bcs\n                 3      97.3\u00b10.2\u03bcs\n              ======== ============\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36483\n\nDifferential Revision: D21451829\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 033e8060192c54f139310aeafe8ba784bab94ded", "pr_number": "36483", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/cuda/ReduceLogicKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/generic/THCTensorMathPairwise.cu"], "labels": ["merged", "open source", "triaged"]}, "078669f6c3": {"title": "Back out \"[2/n][Compute Meta] support analysis for null flag features\"", "body": "Summary:\nOriginal commit changeset: 46c59d849fa8\n\nThe original commit is breaking DPER3 release pipeline with the following failures:\nhttps://www.internalfb.com/intern/chronos/jobinstance?jobinstanceid=9007207344413239&smc=chronos_gp_admin_client&offset=0\n```\nChild workflow f 202599639  failed with error: c10::Error: [enforce fail at operator.cc:76] blob != nullptr. op Save: Encountered a non-existing input blob: feature_preproc/feature_sparse_to_dense/default_float_value\n```\nhttps://www.internalfb.com/intern/chronos/jobinstance?jobinstanceid=9007207344855973&smc=chronos_gp_admin_client&offset=0\n```\nChild workflow f 202629391  failed with error: c10::Error: [enforce fail at operator.cc:76] blob != nullptr. op Save: Encountered a non-existing input blob: tum_preproc/inductive/feature_sparse_to_dense/default_float_value\n```\n\nRelated UBN tasks: T69529846, T68986110\n\nTest Plan: Build a DPER3 package on top of this commit, and check that DPER3 release test `model_deliverability_test` is passing.\n\nDifferential Revision: D22396317\n\nfbshipit-source-id: 92d5b30cc146c005d6159a8d5bfe8973e2c546dd", "pr_number": null, "files_changed": ["caffe2/python/layers/feature_sparse_to_dense.py"], "labels": []}, "dac63a13cb": {"title": "run single-threaded gradgradcheck in test_nn (#40999)", "body": "Summary:\nMost time-consuming tests in test_nn (taking about half the time) were gradgradchecks on Conv3d. Reduce their sizes, and, most importantly, run gradgradcheck single-threaded, because that cuts the time of conv3d tests by an order of magnitude, and barely affects other tests.\nThese changes bring test_nn time down from 1200 s to ~550 s on my machine.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40999\n\nDifferential Revision: D22396896\n\nPulled By: ngimel\n\nfbshipit-source-id: 3b247caceb65d64be54499de1a55de377fdf9506", "pr_number": "40999", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_nn.py"], "labels": ["merged"]}, "5d1d8a58b8": {"title": "Enable `in_dims` for vmap frontend api (#40717)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40717\n\n`in_dims` specifies which dimension of the input tensors should be\nvmapped over. One can also specify `None` as an `in_dim` for a particular\ninput to indicate that we do not map over said input.\n\nWe implement `in_dims` by creating a BatchedTensor with BatchDim equal\nto said `in_dim`. Most of this PR is error checking. `in_dims` must\nsatisfy the following:\n- `in_dim` can be either an int or a Tuple[Optional[int]]. If it is an\nint, we use it to mean the `in_dim` for every input.\n- If `in_dims` is not-None at some index `idx`, then the input at index\n`idx` MUST be a tensor (vmap can only map over tensors).\n\njax supports something more generalized: their `in_dims` can match the\nstructure of the `inputs` to the function (i.e., it is a nested python\ndata structure matching the data structure of `inputs` specifying where\nin `inputs` the Tensors to be mapped are and what their map dims should\nbe). We don't have the infrastruture yet so we only support `int` or a\nflat tuple for `in_dims`.\n\nTest Plan: - `pytest test/test_vmap.py -v`\n\nDifferential Revision: D22397914\n\nPulled By: zou3519\n\nfbshipit-source-id: 56d2e14be8b6024e4cde2729eff384da305b4ea3", "pr_number": "40717", "files_changed": ["test/test_vmap.py", "torch/_vmap_internals.py"], "labels": ["merged"]}, "63ef706979": {"title": "[ATen] Add `native_cuda_h` list to CMakeLists.txt (#41038)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/40784\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41038\n\nDifferential Revision: D22404273\n\nPulled By: malfet\n\nfbshipit-source-id: 8df05f948f069ac95591d523222faa1327429e71", "pr_number": "41038", "files_changed": ["aten/src/ATen/CMakeLists.txt"], "labels": ["merged"]}, "6b50874cb7": {"title": "Fix HTTP links in documentation to HTTPS (#40878)", "body": "Summary:\nI ran `make linkcheck` using `sphinx.builders.linkcheck` on the documentation and noticed a few links weren't using HTTPS so I quickly updated them all.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40878\n\nDifferential Revision: D22404647\n\nPulled By: ngimel\n\nfbshipit-source-id: 9c9756db59197304023fddc28f252314f6cf4af3", "pr_number": "40878", "files_changed": ["docs/source/community/contribution_guide.rst", "docs/source/notes/cuda.rst", "torch/_lowrank.py", "torch/cuda/streams.py", "torch/nn/modules/dropout.py", "torch/nn/modules/pooling.py", "torch/optim/asgd.py", "torch/optim/rmsprop.py", "torch/quasirandom.py"], "labels": ["merged", "open source"]}, "6e4f501f1a": {"title": "Improve error message for Pad operator (#39651)", "body": "Summary:\nIn issue https://github.com/pytorch/pytorch/issues/36997 the user encountered a non-meaningful error message when trying to export the model to ONNX. The Pad operator in opset 9 requires the list of paddings to be constant. This PR tries to improve the error message given to the user when this is not the case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39651\n\nReviewed By: hl475\n\nDifferential Revision: D21992262\n\nPulled By: houseroad\n\nfbshipit-source-id: b817111c2a40deba85e4c6cdb874c1713312dba1", "pr_number": "39651", "files_changed": ["test/onnx/test_pytorch_common.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source", "triaged"]}, "b2cc8a2617": {"title": "[ONNX]Fix export of full_like (#40063)", "body": "Summary:\nFix export of full_like when fill_value is of type torch._C.Value.\n\nThis PR fixes a bug when exporting GPT2DoubleHeadsModel https://github.com/huggingface/transformers/issues/4950\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40063\n\nReviewed By: hl475\n\nDifferential Revision: D22398353\n\nPulled By: houseroad\n\nfbshipit-source-id: 6980a61211fe571c2e4a57716970f474851d811e", "pr_number": "40063", "files_changed": ["test/onnx/expect/TestOperators.test_full_like.expect", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source", "triaged"]}, "56396ad024": {"title": "ONNX: support view_as operator (#40496)", "body": "Summary:\nThis PR adds support for the torch `view_as` operator.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40496\n\nReviewed By: hl475\n\nDifferential Revision: D22398318\n\nPulled By: houseroad\n\nfbshipit-source-id: f92057f9067a201b707aa9b8fc4ad34643dd5fa3", "pr_number": "40496", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source", "triaged"]}, "50df097599": {"title": "Fix CUDA jit codegen compilation with gcc-5.4 (#41055)", "body": "Summary:\nIt's a known gcc-5.4 bug that enum class is not hasheable by default, so `std::unordered_map` needs 3rd explicit parameters to compute hash from the type.\n\nShould fix regression caused by https://github.com/pytorch/pytorch/pull/40864\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41055\n\nDifferential Revision: D22405478\n\nPulled By: malfet\n\nfbshipit-source-id: f4bd36bebdc1ad0251ebd1e6cefba866e6605fe6", "pr_number": "41055", "files_changed": ["torch/csrc/jit/codegen/cuda/ir_internal_nodes.h", "torch/csrc/jit/codegen/cuda/ir_nodes.cpp"], "labels": ["merged", "oncall: jit"]}, "4aa543ed2e": {"title": "Fix unordered-map-over-enum for GCC 5.4 (#41063)", "body": "Summary:\nForgot to add this to https://github.com/pytorch/pytorch/pull/41055\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41063\n\nDifferential Revision: D22407451\n\nPulled By: malfet\n\nfbshipit-source-id: 6f06653b165cc4817d134657f87caf643182832a", "pr_number": "41063", "files_changed": ["torch/csrc/jit/codegen/cuda/lower_thread_predicate.cpp"], "labels": ["merged", "oncall: jit"]}, "a6b703cc89": {"title": "Make `torch_cpu` compileable when `USE_TENSORPIPE` is not set. (#40846)", "body": "Summary:\nForward-declare `tensorpipe::Message` class in utils.h\nGuard TensorPipe specific methods in utils.cpp with `#ifdef USE_TENSORPIPE`\nPass `USE_TENSORPIPE` as private flag to `torch_cpu` library\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40846\n\nDifferential Revision: D22338864\n\nPulled By: malfet\n\nfbshipit-source-id: 2ea2aea84527ae7480e353afb55951a068b3b980", "pr_number": "40846", "files_changed": ["caffe2/CMakeLists.txt", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h"], "labels": ["merged"]}, "0e09511af9": {"title": "type annotations for dataloader, dataset, sampler (#39392)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/38913\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39392\n\nReviewed By: anjali411\n\nDifferential Revision: D22102489\n\nPulled By: zou3519\n\nfbshipit-source-id: acb68d9521145f0b047214d62b5bdc5a0d1b9be4", "pr_number": "39392", "files_changed": ["mypy.ini", "test/test_dataloader.py", "torch/utils/data/dataloader.py", "torch/utils/data/dataloader.pyi", "torch/utils/data/dataset.py", "torch/utils/data/dataset.pyi", "torch/utils/data/sampler.py", "torch/utils/data/sampler.pyi"], "labels": ["open source", "triaged"]}, "a04af4dccb": {"title": "Revert D22396896: [pytorch][PR] run single-threaded gradgradcheck in test_nn", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22396896 (https://github.com/pytorch/pytorch/commit/dac63a13cbe105c81db4539147687d593704fe83)\n\nOriginal commit changeset: 3b247caceb65\n\nfbshipit-source-id: 90bbd71ca5128a7f07fe2907c061ee0922d16edf", "pr_number": null, "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "2d98f8170e": {"title": "Add option to warn if elements in a Compare table are suspect (#41011)", "body": "Summary:\nThis PR adds a `.highlight_warnings()` method to `Compare`, which will include a `(! XX%)` next to measurements with high variance to highlight that fact. For example:\n```\n[------------- Record function overhead ------------]\n                      |    lstm_jit   |  resnet50_jit\n1 threads: ------------------------------------------\n      with_rec_fn     |   650         |  8600\n      without_rec_fn  |   660         |  8000\n2 threads: ------------------------------------------\n      with_rec_fn     |   360         |  4200\n      without_rec_fn  |   350         |  4000\n4 threads: ------------------------------------------\n      with_rec_fn     |   250         |  2100\n      without_rec_fn  |   260         |  2000\n8 threads: ------------------------------------------\n      with_rec_fn     |   200 (! 6%)  |  1200\n      without_rec_fn  |   210 (! 6%)  |  1100\n16 threads: -----------------------------------------\n      with_rec_fn     |   220 (! 8%)  |   900 (! 5%)\n      without_rec_fn  |   200 (! 5%)  |  1000 (! 7%)\n32 threads: -----------------------------------------\n      with_rec_fn     |  1000 (! 7%)  |   920\n      without_rec_fn  |  1000 (! 6%)  |   900 (! 6%)\n\nTimes are in milliseconds (ms).\n(! XX%) Measurement has high variance, where XX is the median / IQR * 100.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41011\n\nDifferential Revision: D22412905\n\nPulled By: robieta\n\nfbshipit-source-id: 2c90e719d9a5a1c0267ed113dd1b1b1738fa8269", "pr_number": "41011", "files_changed": ["benchmarks/experimental_components/utils/compare.py"], "labels": ["merged"]}, "733b8c23c4": {"title": "Fix several quantization documentation typos (#40567)", "body": "Summary:\nThis PR fixes several typos I noticed in the docs here: https://pytorch.org/docs/master/quantization.html. In one case there was a misspelled module [torch.nn.instrinsic.qat](https://pytorch.org/docs/master/quantization.html#torch-nn-instrinsic-qat) which I corrected and am including screenshots of below just in case.\n\n<img width=\"1094\" alt=\"before\" src=\"https://user-images.githubusercontent.com/54918401/85766765-5cdd6280-b6e5-11ea-93e6-4944cf820b71.png\">\n\n<img width=\"1093\" alt=\"after\" src=\"https://user-images.githubusercontent.com/54918401/85766769-5d75f900-b6e5-11ea-8850-0d1f5ed67b16.png\">\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40567\n\nDifferential Revision: D22311291\n\nPulled By: ezyang\n\nfbshipit-source-id: 65d1f3dd043357e38a584d9e30f31634a5b0995c", "pr_number": "40567", "files_changed": ["docs/source/quantization.rst", "torch/nn/quantized/functional.py", "torch/quantization/observer.py", "torch/quantization/qconfig.py", "torch/quantization/quantize.py"], "labels": ["merged", "oncall: quantization", "open source", "triaged"]}, "452d5e191b": {"title": "Grammatically updated the tech docs (#41031)", "body": "Summary:\nSmall grammatical update to the torch tech docs\n\n![image](https://user-images.githubusercontent.com/26879385/86633690-e126c400-bfc8-11ea-8892-23cdc037daa9.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41031\n\nDifferential Revision: D22404342\n\nPulled By: ngimel\n\nfbshipit-source-id: 1c723119cfb050c4ef53de7971fe6e0acf3e91a9", "pr_number": "41031", "files_changed": ["torch/__init__.py"], "labels": ["merged", "open source"]}, "00ee54d2a4": {"title": "Fix link to PyTorch organization (from Governance) (#40984)", "body": "Summary:\nPR fixes https://github.com/pytorch/pytorch/issues/40666\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40984\n\nDifferential Revision: D22404543\n\nPulled By: ngimel\n\nfbshipit-source-id: 0d39e8f4d701517cce9c31fddaaad46be3d4844b", "pr_number": "40984", "files_changed": ["docs/source/community/governance.rst"], "labels": ["merged", "open source"]}, "630e7ed9cc": {"title": "Splitting embedding_bag to embedding_bag_forward_only and embedding_bag (#40557)", "body": "Summary:\nCurrently embedding_bag's CPU kernel queries whether weight.requires_grad() is true. This violates layering of AutoGrad and Op Kernels, causing issues in third-party backends like XLA. See this [issue](https://github.com/pytorch/xla/issues/2215) for more details.\n\nThis PR hoists the query of weight.requires_grad() to Python layer, and splits embedding_bag into two separate ops, each corresponding to weight.requires_grad() == true and false.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40557\n\nReviewed By: ailzhang\n\nDifferential Revision: D22327476\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: c815b3690d676a43098e12164517c5debec90fdc", "pr_number": "40557", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["merged"]}, "0e6b750288": {"title": "Insert parentheses around kernel name argument to hipLaunchKernelGGL (#41022)", "body": "Summary:\nThis is to workaround an issue in hipclang wrt templated kernel name arguments to hipLaunchKernelGGL.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41022\n\nDifferential Revision: D22404183\n\nPulled By: ngimel\n\nfbshipit-source-id: 63135ccb9e087f4c8e8663ed383979f7e2c1ba06", "pr_number": "41022", "files_changed": ["torch/utils/hipify/hipify_python.py"], "labels": ["merged", "module: rocm", "open source"]}, "75155df8b4": {"title": "Doc warnings (#41068)", "body": "Summary:\nsolves most of gh-38011 in the framework of solving gh-32703.\n\nThese should only be formatting fixes, I did not try to fix grammer and syntax.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41068\n\nDifferential Revision: D22411919\n\nPulled By: zou3519\n\nfbshipit-source-id: 25780316b6da2cfb4028ea8a6f649bb18b746440", "pr_number": "41068", "files_changed": ["docs/source/jit.rst", "docs/source/nn.functional.rst", "docs/source/packages.rst", "docs/source/quantization.rst", "torch/_torch_docs.py", "torch/autograd/functional.py", "torch/jit/__init__.py", "torch/jit/_trace.py", "torch/jit/supported_ops.py", "torch/nn/modules/conv.py", "torch/nn/utils/prune.py", "torch/onnx/__init__.py", "torch/serialization.py"], "labels": ["merged", "oncall: jit", "open source"]}, "8d570bc708": {"title": "Decouple DataParallel/DistributedDataParallel from CUDA (#38454)", "body": "Summary:\nDecouple DataParallel/DistributedDataParallel from CUDA to support more device types.\n- Move torch/cuda/comm.py to torch/nn/parallel/comm.py with minor changes for common devices support. Torch.cuda.comm is kept as is for backward compatibility\n- Provide common APIs to arbitrary device types without changing existing CUDA APIs in torch.cuda space.\n- Replace the torch.cuda calls in DataParellel/DistributedDataParallel with the new APIs.\n\nRelated RFC: [https://github.com/pytorch/pytorch/issues/36160](https://github.com/pytorch/pytorch/issues/36160)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38454\n\nDifferential Revision: D22051557\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7842dad0e5d3ca0f6fb760bda49182dcf6653af8", "pr_number": "38454", "files_changed": ["mypy.ini", "test/distributed/test_c10d.py", "torch/_utils.py", "torch/cuda/_utils.py", "torch/cuda/comm.py", "torch/nn/parallel/_functions.py", "torch/nn/parallel/comm.py", "torch/nn/parallel/data_parallel.py", "torch/nn/parallel/distributed.py", "torch/nn/parallel/replicate.py"], "labels": ["merged", "module: cuda", "module: data parallel", "oncall: distributed", "open source", "triaged"]}, "93778f3b24": {"title": "Expose certain methods in OpaqueTensorImpl. (#41060)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41060\n\nExposes a const ref opaque_handle and made copy_tensor_metdata a\nprotected method. This helps in reusing code in sub classes of OpaqueTensorImpl\n\nTest Plan: waitforbuildbot\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D22406602\n\nfbshipit-source-id: e3b8338099f257da7f6bbff679f1fdb71e5f335a", "pr_number": "41060", "files_changed": ["aten/src/ATen/OpaqueTensorImpl.h"], "labels": ["merged"]}, "8f0e254790": {"title": "In interpolate, use if instead of elif (#37171)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37171\n\nEvery one of these branches returns or raises, so there's no need for elif.\nThis makes it a little easier to reorder and move conditions.\nghstack-source-id: 106938110\n\nTest Plan: Existing test for interpolate.\n\nDifferential Revision: D21209992\n\nfbshipit-source-id: 5c517e61ced91464b713f7ccf53349b05e27461c", "pr_number": "37171", "files_changed": ["torch/nn/functional.py"], "labels": ["merged"]}, "3c1c74c366": {"title": "In interpolate, move exceptional cases to the bottom (#37172)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37172\n\nThis improves readability by keeping cases with similar behavior close\ntogether.  It should also have a very tiny positive impact on perf.\nghstack-source-id: 106938109\n\nTest Plan: Existing tests for interpolate.\n\nDifferential Revision: D21209996\n\nfbshipit-source-id: c813e56aa6ba7370b89a2784fcb62cc146005258", "pr_number": "37172", "files_changed": ["torch/nn/functional.py"], "labels": ["merged"]}, "4dad829ea3": {"title": "In interpolate, inline the call to _interp_output_size (#37173)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37173\n\nThis function is only called in one place, so inline it.  This eliminates\nboilerplate related to overloads and allows for further simplification\nof shared logic in later diffs.\n\nAll shared local variables have the same names (from closed_over_args),\nand no local variables accidentally collide.\nghstack-source-id: 106938108\n\nTest Plan: Existing tests for interpolate.\n\nDifferential Revision: D21209995\n\nfbshipit-source-id: acfadf31936296b2aac0833f704764669194b06f", "pr_number": "37173", "files_changed": ["torch/nn/functional.py"], "labels": ["merged"]}, "5e03a1e926": {"title": "Add support for int[]? arguments in native_functions.yaml (#37174)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37174\n\nghstack-source-id: 106938112\n\nTest Plan: Upcoming diffs use this for upsampling.\n\nDifferential Revision: D21210002\n\nfbshipit-source-id: d6a55ab6420c05a92873a569221b613149aa0daa", "pr_number": "37174", "files_changed": ["aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/native/TestOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "test/run_test.py", "test/test_native_functions.py", "tools/autograd/gen_autograd_functions.py", "tools/autograd/gen_python_functions.py", "tools/jit/gen_unboxing_wrappers.py", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tracer.h", "torch/csrc/jit/runtime/register_c10_ops.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["merged", "oncall: jit"]}, "38b465db27": {"title": "ROCm 3.5.1 image (#40385)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40385\n\nDifferential Revision: D22421426\n\nPulled By: ezyang\n\nfbshipit-source-id: 1a131cdb1a0d5ad7ccd55dc1db17cae982cc286b", "pr_number": "40385", "files_changed": [".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_cache.sh", ".circleci/docker/common/install_rocm.sh", ".circleci/docker/ubuntu-rocm/Dockerfile", ".circleci/verbatim-sources/workflows/workflows-ecr-gc.yml", ".jenkins/caffe2/build.sh", ".jenkins/pytorch/build.sh", "docker/caffe2/jenkins/common/install_ccache.sh", "tools/amd_build/unwrap_clang.sh"], "labels": ["merged", "module: rocm", "open source"]}, "eea535742f": {"title": "Add bfloat16 support for nccl path (#38515)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/38515\n\nDifferential Revision: D22420896\n\nPulled By: ezyang\n\nfbshipit-source-id: 80d2d0c2052c91c9035e1e025ebb14e210cb0100", "pr_number": "38515", "files_changed": ["test/distributed/test_nccl.py", "torch/csrc/cuda/nccl.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "a4fd4905c8": {"title": "bump docker version to more recent tag (#41105)", "body": "Summary:\nTag was introduced originally as https://github.com/pytorch/pytorch/pull/40385\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41105\n\nReviewed By: malfet\n\nDifferential Revision: D22423910\n\nPulled By: seemethere\n\nfbshipit-source-id: 336fc7ef5243a5863c59762efd182ed7ea6dfc2c", "pr_number": "41105", "files_changed": [".circleci/cimodel/data/simple/util/docker_constants.py", ".circleci/config.yml"], "labels": ["merged", "module: ci"]}, "cc29c192a6": {"title": "add \"aten::add.str\" op and remove two duplicated ops", "body": "Summary: add \"aten::add.str\" op and remove two duplicated ops\n\nTest Plan:\n```\nbuck run //xplat/caffe2/fb/pytorch_predictor:converter /mnt/vol/gfsfblearner-altoona/flow/data/2020-06-29/1ca8a85f-dbd5-4181-b5fc-63d24465c1fc/201084299/2068673333/model.pt1 ~/model_f201084299.bc\n\nbuck run xplat/assistant/model_benchmark_tool/mobile/binary/:lite_predictor -- --model ~/model_f201084299.bc --input_file /tmp/gc_model_input.txt --model_input_args src_tokens,dict_feat,contextual_token_embedding --warmup 1 --iter 2\n```\n\nReviewed By: pengtxiafb\n\nDifferential Revision: D22395604\n\nfbshipit-source-id: 0ce21e8b8ae989d125f2f3739523e3c486590b9f", "pr_number": null, "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "torch/csrc/jit/runtime/register_ops_utils.h", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": []}, "054e5d8943": {"title": ".circleci: Fix job-specs-custom docker tag (#41111)", "body": "Summary:\nShould resolve master breakages\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41111\n\nDifferential Revision: D22426863\n\nPulled By: seemethere\n\nfbshipit-source-id: 561eaaa0d97a6fe13c75c1a73e4324b92d94afed", "pr_number": "41111", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["merged", "module: ci"]}, "a8bc7545d5": {"title": "use PYTORCH_ROCM_ARCH to set GLOO_ROCM_ARCH (#40170)", "body": "Summary:\nPreviously it used the default arch set which may or may not coincide with the user's.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40170\n\nDifferential Revision: D22400866\n\nPulled By: xw285cornell\n\nfbshipit-source-id: 222ba684782024fa68f37bf7d4fdab9a2389bdea", "pr_number": "40170", "files_changed": ["cmake/Dependencies.cmake"], "labels": ["merge-this-please", "merged", "module: rocm", "open source"]}, "bce75a2536": {"title": "add first implementation of swish (#41085)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41085\n\nadd the first LUT implementation of swish\n\nTest Plan:\ncompared against swish lowered as x*sigmoid(x), had to\nincrease the threshold of error but looks generally right\n\nReviewed By: venkatacrc\n\nDifferential Revision: D22418117\n\nfbshipit-source-id: c75fa496aa7a5356ddc87f1d61650f432e389457", "pr_number": "41085", "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/unary_fp16_fake_op.cc"], "labels": ["fb-exported", "merged"]}, "445128d0f2": {"title": "Add PyTorch Glossary (#40639)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40639\n\nDifferential Revision: D22421207\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 7df8bfc85e28bcf1fb08892a3671e7a9cb0dee9c", "pr_number": "40639", "files_changed": ["GLOSSARY.md", "README.md"], "labels": ["merged"]}, "bacca663ff": {"title": "Fix Broken Link in CONTRIBUTING.md (#41066)", "body": "Summary:\nSpotted a broken link, and while I was at it, fixed a few little language and formatting nits.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41066\n\nReviewed By: mruberry\n\nDifferential Revision: D22415371\n\nPulled By: dongreenberg\n\nfbshipit-source-id: 7d11c13235b28a01886063c11a4c5ccb333c0c02", "pr_number": "41066", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged", "module: docs", "open source", "triaged"]}, "3615e344a3": {"title": "Unit test case for the Int8FC to cover quantization scale errors. (#41100)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41100\n\nUnit test case for the Int8FC to cover quantization scale errors.\n\nTest Plan: test_int8_ops_nnpi.py test case test_int8_small_input.\n\nReviewed By: hyuen\n\nDifferential Revision: D22422353\n\nfbshipit-source-id: b1c1baadc32751cd7e98e0beca8f0c314d9e5f10", "pr_number": "41100", "files_changed": ["caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py"], "labels": ["fb-exported", "merged"]}, "c55d8a6f62": {"title": "Remove std::complex from c10::Scalar (#39831)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39831\n\nDifferential Revision: D22018505\n\nPulled By: ezyang\n\nfbshipit-source-id: 4719c0f1673077598c5866dafc7391d9e074f4eb", "pr_number": "39831", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cpu/PowKernel.cpp", "c10/core/Scalar.h", "tools/autograd/derivatives.yaml", "torch/csrc/utils/python_arg_parser.h", "torch/csrc/utils/python_numbers.h", "torch/csrc/utils/python_scalars.h"], "labels": ["merged", "module: complex", "open source"]}, "3e01931e49": {"title": "[JIT] Separate to_backend API into libtorch and libtorch_python (#40839)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40839\n\n**Summary**\nThis commit splits the to_backend API properly into\n`libtorch` and `libtorch_python`. The backend interface and all\nof the code needed to run a graph on a backend is in\nlibtorch, and all of the code related to creating a Python binding\nfor the lowering process is in `libtorch_python`.\n\n**Test Plan**\n`python test/test_jit.py TestBackends`\n\n**Fixes**\nThis commit fixes #40072.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22418664\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: b96e0c34ab84e45dff0df68b8409ded57a55ab25", "pr_number": "40839", "files_changed": ["test/jit/test_backends.py", "tools/build_variables.bzl", "torch/csrc/jit/backends/backend.h", "torch/csrc/jit/backends/backend_detail.cpp", "torch/csrc/jit/backends/backend_detail.h", "torch/csrc/jit/backends/backend_init.cpp"], "labels": ["merged", "oncall: jit"]}, "5a4c45f8d1": {"title": "[JIT] Move TestBackend to test directory (#40840)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40840\n\n**Summary**\nThis commit moves the TestBackend used for the JIT backend\nextension to the tests directory. It was temporarily placed\nin the source directory while figuring out some details of\nthe user experience for this feature.\n\n**Test Plan**\n`python test/test_jit.py TestBackends`\n\n**Fixes**\nThis commit fixes #40067.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22418682\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 9356af1341ec4d552a41c2a8929b327bc8b56057", "pr_number": "40840", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_backend.cpp", "test/jit/test_backends.py", "tools/build_variables.bzl", "torch/csrc/jit/backends/test_backend.cpp", "torch/csrc/jit/backends/test_backend.h"], "labels": ["merged", "oncall: jit"]}, "6777ea19fe": {"title": "[JIT] Add support for backend-lowered submodules (#40841)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40841\n\n**Summary**\nThis commit adds support for using `Modules` that have been lowered as\nsubmodules in `ScriptModules`.\n\n**Test Plan**\nThis commit adds execution and save/load tests to test_backends.py for\nbackend-lowered submodules.\n\n**Fixes**\nThis commit fixes #40069.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22418716\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: d2b2c6d5d2cf3042a620b3bde7d494f1abe28dc1", "pr_number": "40841", "files_changed": ["test/jit/test_backends.py", "torch/csrc/jit/backends/backend_init.cpp", "torch/csrc/jit/python/script_init.cpp"], "labels": ["merged", "oncall: jit"]}, "e2a291b396": {"title": "[JIT] Add out-of-source-tree to_backend tests (#40842)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40842\n\n**Summary**\nThis commit adds out-of-source-tree tests for `to_backend`. These tests check\nthat a Module can be lowered to a backend, exported, loaded (in both\nPython and C++) and executed.\n\n**Fixes**\nThis commit fixes #40067.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22418731\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 621ba4efc1b121fa76c9c7ca377792ac7440d250", "pr_number": "40842", "files_changed": [".jenkins/pytorch/build.sh", ".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-test-helpers/test_custom_backend.bat", ".jenkins/pytorch/win-test.sh", "test/cpp/jit/test_backend.cpp", "test/custom_backend/CMakeLists.txt", "test/custom_backend/backend.py", "test/custom_backend/custom_backend.cpp", "test/custom_backend/custom_backend.h", "test/custom_backend/test_custom_backend.cpp", "test/custom_backend/test_custom_backend.py"], "labels": ["merged", "oncall: jit"]}, "6ef94590fa": {"title": "match int8 quantization of nnpi (#41094)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41094\n\nmimic nnpi's quantization operations\n\nremoved redundant int8 test\n\nTest Plan: ran FC with sizes up to 5, running bigger sizes\n\nReviewed By: venkatacrc\n\nDifferential Revision: D22420537\n\nfbshipit-source-id: 91211c8a6e4d3d3bec2617b758913b44aa44b1b1", "pr_number": "41094", "files_changed": ["caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h"], "labels": ["fb-exported", "merged"]}, "e0e8b98c43": {"title": "Export logic op to pytorch", "body": "Summary: Export logit op to pt for better preproc perf\n\nTest Plan:\nunit test\nAlso tested with model re-generation\n\nReviewed By: houseroad\n\nDifferential Revision: D22324611\n\nfbshipit-source-id: 86accb6b4528e5c818d2c3f8c67926f279d158d6", "pr_number": null, "files_changed": ["caffe2/operators/logit_op.cc", "caffe2/operators/logit_op.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": []}, "3d3fd13e04": {"title": "[quant][graphmode][fix] filter for list append change (#41020)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41020\n\nOnly support quantization of list append for List[Tensor]\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22420698\n\nfbshipit-source-id: 179677892037e136d90d16230a301620c3111063", "pr_number": "41020", "files_changed": ["torch/csrc/jit/passes/quantization/finalize.cpp", "torch/csrc/jit/passes/quantization/insert_observers.cpp"], "labels": ["merged", "oncall: jit"]}, "e4fbcaa2bc": {"title": "[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D22429730\n\nfbshipit-source-id: 585d8df36d7fa18a9c2d3fa54c1d333bf94464d0", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/register_c10_ops.cpp"], "labels": []}, "de4fc23381": {"title": "clean up duplicated op names (#41092)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41092\n\nadded overload name for some full JIT operators and removed some duplicated op registrations\n\nTest Plan:\napply D21032976, then buck run fbsource//xplat/caffe2/fb/pytorch_predictor:predictor\nmake sure there's no runtime error in operator registration\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22419922\n\nfbshipit-source-id: f651898e75b5bdb8dc03fc00b136689536c51707", "pr_number": "41092", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "torch/csrc/jit/runtime/register_ops_utils.h", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "03eec07956": {"title": "Move error messages in-line in `_vmap_internals.py` (#41077)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41077\n\nThis PR is a refactor that moves error messages into their callsites in\n`_vmap_internals.py`. Furthermore, because a little bird told me we've\ndropped python 3.5 support, this PR adopts f-string syntax to clean up\nthe string replace logic. Together these changes make the error messages\nread better IMO.\n\nTest Plan:\n- `python test/test_vmap.py -v`. There exists tests that invoke each of the\nerror messages.\n\nDifferential Revision: D22420473\n\nPulled By: zou3519\n\nfbshipit-source-id: cfd46b2141ac96f0a62864928a95f8eaa3052f4e", "pr_number": "41077", "files_changed": ["torch/_vmap_internals.py"], "labels": ["merged"]}, "dde18041a6": {"title": "[quant][graphmode] Refactor quantization patterns (#40894)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40894\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D22403901\n\nfbshipit-source-id: e0bcf8a628c6a1acfe6fa10a52912360a619bc62", "pr_number": "40894", "files_changed": ["torch/csrc/jit/passes/quantization/finalize.cpp", "torch/csrc/jit/passes/quantization/quantization_patterns.h"], "labels": ["merged", "oncall: jit"]}, "6c9b869930": {"title": "[ROCm] Skip Conv2d, Conv3d transpose fp16 test for ROCm3.5 (#41088)", "body": "Summary:\nThere's a regression in MIOpen in ROCm3.5 that results in failure of autocast tests. Skipping the tests for now and will re-enable once the fixes are in MIOpen.\n\nezyang jeffdaily sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41088\n\nDifferential Revision: D22419823\n\nPulled By: xw285cornell\n\nfbshipit-source-id: 347fb9a03368172fe0b263d14d27ee0c3efbf4f6", "pr_number": "41088", "files_changed": ["torch/testing/_internal/autocast_test_lists.py"], "labels": ["merged", "module: rocm", "open source"]}, "c93e96fbd9": {"title": "[jit] move script-related implementation out of torch/jit/__init__.py (#40902)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40902\n\nSee the bottom of this stack for context.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D22360210\n\nPulled By: suo\n\nfbshipit-source-id: 4275127173a36982ce9ad357aa344435b98e1faf", "pr_number": "40902", "files_changed": ["test/test_jit.py", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/script_init.cpp", "torch/distributed/nn/jit/instantiator.py", "torch/distributed/rpc/api.py", "torch/jit/__init__.py", "torch/jit/_recursive.py", "torch/jit/_script.py", "torch/jit/_state.py", "torch/jit/annotations.py", "torch/jit/frontend.py", "torch/nn/parallel/replicate.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged", "oncall: jit"]}, "58d7d91f88": {"title": "Return atomic (#41028)", "body": "Summary:\nPer title. This is not used currently in the pytorch codebase, but it is a legitimate usecase, and we have extensions that want to do that and are forced to roll their own atomic implementations for non-standard types. Whether atomic op returns old value or not should not affect performance, compiler is able to generate correct code depending on whether return value is used. https://godbolt.org/z/DBU_UW.\nAtomic operations for non-standard integer types (1,2 and 8 byte-width) are left as is, with void return.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41028\n\nDifferential Revision: D22425008\n\nPulled By: ngimel\n\nfbshipit-source-id: ca064edb768a6b290041a599e5b50620bdab7168", "pr_number": "41028", "files_changed": ["aten/src/THC/THCAtomics.cuh"], "labels": ["merged"]}, "131a0ea277": {"title": "Add version number to bytecode. (#36439)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36439\n\nA proposal of versioning in bytecode, as suggested by dzhulgakov in the internal post: https://fb.workplace.com/groups/pytorch.mobile.work/permalink/590192431851054/\n\nkProducedBytecodeVersion is added. If the model version is not the same as the number in the code, an error will be thrown.\n\nThe updated bytecode would look like below. It's a tuple of elements, where the first element is the version number.\n```\n(3,\n ('__torch__.m.forward',\n  (('instructions',\n    (('STOREN', 1, 2),\n     ('DROPR', 1, 0),\n     ('MOVE', 2, 0),\n     ('OP', 0, 0),\n     ('RET', 0, 0))),\n   ('operators', (('aten::Int', 'Tensor'),)),\n   ('constants', ()),\n   ('types', ()),\n   ('register_size', 2))))\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22433532\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 6d62e4abe679cf91a8e18793268ad8c1d94ce746", "pr_number": "36439", "files_changed": ["caffe2/serialize/inline_container.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["merged", "oncall: jit"]}, "2bc9ee97d1": {"title": "Revert D22418731: [JIT] Add out-of-source-tree to_backend tests", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22418731 (https://github.com/pytorch/pytorch/commit/e2a291b396be8db6f9ac1fc7d45e97c6026df590)\n\nOriginal commit changeset: 621ba4efc1b1\n\nfbshipit-source-id: 475ae24c5b612fe285035e5ebb92ffc66780a468", "pr_number": null, "files_changed": [".jenkins/pytorch/build.sh", ".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-test-helpers/test_custom_backend.bat", ".jenkins/pytorch/win-test.sh", "test/cpp/jit/test_backend.cpp", "test/custom_backend/CMakeLists.txt", "test/custom_backend/backend.py", "test/custom_backend/custom_backend.cpp", "test/custom_backend/custom_backend.h", "test/custom_backend/test_custom_backend.cpp", "test/custom_backend/test_custom_backend.py"], "labels": []}, "dfd21ec00d": {"title": "Revert D22418716: [JIT] Add support for backend-lowered submodules", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22418716 (https://github.com/pytorch/pytorch/commit/6777ea19fe5947b88c239b5d02c0f2441a21a4ce)\n\nOriginal commit changeset: d2b2c6d5d2cf\n\nfbshipit-source-id: 5ce177e13cab0be60020f8979f9b6c520cc8654e", "pr_number": null, "files_changed": ["test/jit/test_backends.py", "torch/csrc/jit/backends/backend_init.cpp", "torch/csrc/jit/python/script_init.cpp"], "labels": []}, "ec58d739c6": {"title": ".circleci: Remove pynightly jobs", "body": "These jobs didn't really fulfill the intended purpose that they had once\nhad since the travis python versions were basically locked to 3.7.\n\nGoing to go ahead and remove these along with its docker jobs as well\nsince we don't actively need them anymore.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nghstack-source-id: cdfc4fc2ae15a0c86d322cc706d383d6bc189fbc\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41134", "pr_number": "41134", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh"], "labels": ["merged", "module: ci"]}, "9d1138afec": {"title": "Remove unnecessary atomic ops in DispatchStub (#40930)", "body": "Summary:\nI noticed this very unusual use of atomics in `at::native::DispatchStub`. The comment asserts that `choose_cpu_impl()` will always return the same value on every thread, yet for some reason it uses a CAS loop to exchange the value instead of a simple store? That makes no sense considering it doesn't even read the exchanged value.\n\nThis replaces the CAS loop with a simple store and also improves the non-initializing case to a single atomic load instead of two.\n\nFor reference, the `compare_exchange` was added in https://github.com/pytorch/pytorch/issues/32148 and the while loop added in https://github.com/pytorch/pytorch/issues/35794.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40930\n\nDifferential Revision: D22438224\n\nPulled By: ezyang\n\nfbshipit-source-id: d56028ce18c8c5dbabdf366379a0b6aaa41aa391", "pr_number": "40930", "files_changed": ["aten/src/ATen/native/DispatchStub.h"], "labels": ["merged", "open source", "triaged"]}, "af2680e9ce": {"title": "Update ShipIt sync", "body": "fbshipit-source-id: ceb761e28fe8c53bc53f3b82b304ea8ab0e98183", "pr_number": null, "files_changed": [""], "labels": []}, "97052c5fa8": {"title": "Extend SparseAdagrad fusion with stochastic rounding FP16 (#41107)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41107\n\nExtend row wise sparse Adagrad fusion op to FP16 (stochastic rounding) for PyTorch.\n\nDifferential Revision: D22195408\n\nfbshipit-source-id: e9903ca7ca3b542fb56f36580e69bb2a39b554f6", "pr_number": "41107", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cu", "caffe2/sgd/adagrad_fused_op_gpu.cuh", "caffe2/sgd/rowwise_adagrad_fused.cc"], "labels": ["fb-exported", "merged"]}, "10caf58a52": {"title": "[typing] tensor._version is int (#41125)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41125\n\nDifferential Revision: D22440717\n\nPulled By: ezyang\n\nfbshipit-source-id: f4849c6e13f01cf247b2f64f68a621b055c8bc17", "pr_number": "41125", "files_changed": ["torch/_C/__init__.pyi.in"], "labels": ["merged", "open source"]}, "b8d2ccf009": {"title": "Unify TensorOptions signatures (#39611)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39611\n\nA few ops have been taking non-optional ScalarType, Device and Layout. That isn't supported by the hacky wrapper that makes those\nkernels work with the c10 operator library. This PR unifies the signatures and makes those ops c10-full.\nghstack-source-id: 107330186\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D21915788\n\nfbshipit-source-id: 39f0e114f2766a3b27b80f93f2c1a95fa23c78d4", "pr_number": "39611", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/TensorMethods.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/jit/test_unsupported_ops.py", "tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "3f32332ee6": {"title": "[JIT][Easy]move remove mutation to own file (#41137)", "body": "Summary:\nThis should be in its own file...\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41137\n\nReviewed By: jamesr66a\n\nDifferential Revision: D22437922\n\nPulled By: eellison\n\nfbshipit-source-id: 1b62dde1a4ebac673b5c60aea4f398f734d62501", "pr_number": "41137", "files_changed": ["tools/build_variables.bzl", "torch/csrc/jit/passes/create_functional_graphs.cpp", "torch/csrc/jit/passes/create_functional_graphs.h", "torch/csrc/jit/passes/remove_mutation.cpp", "torch/csrc/jit/passes/remove_mutation.h", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/runtime/graph_executor.cpp"], "labels": ["merged", "oncall: jit"]}, "6725c034b6": {"title": "Migrate addmm, addbmm and THBlas_gemm to ATen (#40927)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/24679, closes https://github.com/pytorch/pytorch/issues/24678\n\n`addbmm` depends on `addmm` so needed to be ported at the same time. I also removed `THTensor_(baddbmm)` which I noticed had already been ported so was just dead code.\n\nAfter having already written this code, I had to fix merge conflicts with https://github.com/pytorch/pytorch/issues/40354 which revealed there was already an established place for cpu blas routines in ATen. However, the version there doesn't make use of ATen's AVX dispatching so thought I'd wait for comment before migrating this into that style.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40927\n\nDifferential Revision: D22418756\n\nPulled By: ezyang\n\nfbshipit-source-id: 44e7bb5964263d73ae8cc6adc5f6d4e966476ae6", "pr_number": "40927", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/native/CPUBlas.cpp", "aten/src/ATen/native/CPUBlas.h", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/NaiveDilatedConvolution.cpp", "aten/src/ATen/native/cpu/BlasKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/THBlasUtils.h", "aten/src/TH/generic/THBlas.cpp", "aten/src/TH/generic/THBlas.h", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "c10/core/ScalarType.h", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "c7768e21b1": {"title": "[JIT] Add GitHub workflow for importing issues to triage project (#41056)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41056\n\n**Summary**\nThis commit adds a new GitHub workflow that automatically adds a card to\nthe \"Need triage\" section of the project board for tracking JIT triage\nfor each new issue that is opened and labelled \"jit\".\n\n**Test Plan**\n???\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22444262\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 4e7d384822bffb978468c303322f3e2c04062644", "pr_number": "41056", "files_changed": [".github/workflows/jit_triage.yml"], "labels": ["merged"]}, "04004bf10c": {"title": "Fix a minor typo \"forget add\" -> \"forget to add\" (#41131)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41131\n\nDifferential Revision: D22441122\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 383ef167b7742e2f211d1cae010b6ebb37c6e7a0", "pr_number": "41131", "files_changed": ["torch/csrc/jit/serialization/python_print.cpp"], "labels": ["merged", "oncall: jit"]}, "f71cccc457": {"title": "test: Add option to continue testing through error (#41136)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41136\n\nRunning this within CI seems impossible since this script exits out\nafter one failed test, so let's just add an option that CI can use to\npower through these errors.\n\nShould not affect current functionality.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22441694\n\nPulled By: seemethere\n\nfbshipit-source-id: 7f152fea15af9d47a964062ad43830818de5a109", "pr_number": "41136", "files_changed": ["test/run_test.py"], "labels": ["merged", "module: tests"]}, "302cf6835e": {"title": "[ROCm][Caffe2] Enable MIOpen 3D Pooling (#38260)", "body": "Summary:\nThis PR contains the following updates:\n1. MIOpen 3D pooling enabled in Caffe2.\n2. Refactored the MIOpen pooling code in caffe2.\n3. Enabled unit test cases for 3D pooling.\n\nCC: ezyang jeffdaily ashishfarmer\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38260\n\nDifferential Revision: D21524754\n\nPulled By: xw285cornell\n\nfbshipit-source-id: ddfe09dc585cd61e42eee22eff8348d326fd0c3b", "pr_number": "38260", "files_changed": ["caffe2/operators/hip/pool_op_miopen.hip", "caffe2/python/operator_test/pooling_test.py", "caffe2/python/workspace.py", "caffe2/video/CMakeLists.txt"], "labels": ["merged", "open source"]}, "33e26656fa": {"title": "list workaround for CREATE_OBJECT failure (#41129)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41129\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22436064\n\nPulled By: ann-ss\n\nfbshipit-source-id: 7cfc38eb953410edfe3d21346c6e377c3b3bfc1f", "pr_number": "41129", "files_changed": ["test/mobile/test_lite_script_module.py", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["merged", "oncall: jit"]}, "8e2841781e": {"title": "[easy] Use torch.typename in JIT error messages (#41024)", "body": "Summary:\nNoticed while trying to script one of the models which happened to have numpy values as constants. Lacking the numpy prefix in the error message was quite confusing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41024\n\nDifferential Revision: D22426399\n\nPulled By: dzhulgakov\n\nfbshipit-source-id: 06158b75355fac6871e4861f82fc637c2420e370", "pr_number": "41024", "files_changed": ["test/test_jit.py", "torch/jit/_recursive.py"], "labels": ["merged", "oncall: jit"]}, "155fb22e77": {"title": "Run single-threaded gradgradcheck in testnn (#41147)", "body": "Summary:\nReland https://github.com/pytorch/pytorch/issues/40999\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41147\n\nReviewed By: mruberry\n\nDifferential Revision: D22450357\n\nPulled By: ngimel\n\nfbshipit-source-id: 02b6e020af5e6ef52542266bd9752b9cfbec4159", "pr_number": "41147", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_nn.py"], "labels": ["merged"]}, "bf9cc5c776": {"title": "Add callback with TLS state API in futures (#40326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40326\n\nAdds a helper function `addCallbackWithTLSState` to both\ntorch/csrc/utils/future.h which is used internally by RPC framework and the JIT\nfuture. Uses this helper function to avoid to pass in TLS state where it is needed for rpc and `record_function_ops.cpp`. For example, the following:\n\n```\nat::ThreadLocalState tls_state;\nfut->addCallback([tls_state = std::move(tls_state)]() {\nat::ThreadLocalStateGuard g(tls_state);\nsome_cb_that_requires_tls_state();\n}\n```\n\nbecomes\n\n```\nfut->addCallbackWithTLSState(some_cb_that_requires_tls_state);\n```\nghstack-source-id: 107383961\n\nTest Plan: RPC Tests and added a test in test_misc.cpp\n\nDifferential Revision: D22147634\n\nfbshipit-source-id: 46c02337b90ee58ca5a0861e932413c40d06ed4c", "pr_number": "40326", "files_changed": ["aten/src/ATen/ThreadLocalState.h", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/torchscript_functions.cpp"], "labels": ["merged", "oncall: jit"]}, "7ff7c9738c": {"title": "Revert D22418756: [pytorch][PR] Migrate addmm, addbmm and THBlas_gemm to ATen", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22418756 (https://github.com/pytorch/pytorch/commit/6725c034b62810c7730fda699e3b8bbbcfcc326a)\n\nOriginal commit changeset: 44e7bb596426\n\nfbshipit-source-id: cbaaf3ad277648901700ef0e47715580e8f8e0dc", "pr_number": null, "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/native/CPUBlas.cpp", "aten/src/ATen/native/CPUBlas.h", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/NaiveDilatedConvolution.cpp", "aten/src/ATen/native/cpu/BlasKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/THBlasUtils.h", "aten/src/TH/generic/THBlas.cpp", "aten/src/TH/generic/THBlas.h", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "c10/core/ScalarType.h", "test/test_torch.py"], "labels": []}, "86f72953dd": {"title": "[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D22452776\n\nfbshipit-source-id: a103da6a5b1db7f1c91ca25490358da268fdfe96", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/passes/remove_mutation.cpp", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/runtime/graph_executor.cpp"], "labels": []}, "690946c49d": {"title": "Generalize constant_table from tensor only to ivalue (#40718)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40718\n\nCurrently only constant except tensor must be inlined during serialization.\nTensor are stored in the contant table. This patch generalizes this capability\nto any IValue. This is particularly useful for non ASCII string literal that\ncannot be inlined.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22298169\n\nPulled By: bzinodev\n\nfbshipit-source-id: 88cc59af9cc45e426ca8002175593b9e431f4bac", "pr_number": "40718", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/test/type_test.cpp", "test/cpp/jit/test_class_import.cpp", "test/cpp/jit/test_interface.cpp", "test/expect/TestJit.test_non_ascii_string.expect", "test/jit/test_freezing.py", "test/test_jit.py", "torch/csrc/jit/jit_log.cpp", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/import_legacy.cpp", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/import_source.h", "torch/csrc/jit/serialization/python_print.cpp", "torch/csrc/jit/serialization/python_print.h"], "labels": ["merged", "oncall: jit"]}, "c038f8afcc": {"title": "Do not install nvidia docker for non-NVIDIA configs (#41144)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41144\n\nDifferential Revision: D22457124\n\nPulled By: malfet\n\nfbshipit-source-id: e615199cb78b315aa700efcc7332ebf4299212bf", "pr_number": "41144", "files_changed": [".circleci/scripts/setup_ci_environment.sh"], "labels": ["merged"]}, "62cee0001e": {"title": "Move async + serialization implementation out of 'jit/__init__.py' (#41018)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41018\n\nSee https://github.com/pytorch/pytorch/pull/40807 for context.\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D22393869\n\nPulled By: suo\n\nfbshipit-source-id: a71cc571a423ccb81cd148444dc2a18d2ee43464", "pr_number": "41018", "files_changed": ["mypy.ini", "torch/jit/__init__.py", "torch/jit/_async.py", "torch/jit/_serialization.py", "torch/jit/mobile/__init__.py"], "labels": ["merged", "oncall: jit"]}, "07fd5f8ff9": {"title": "Create lazy_dyndeps to avoid caffe2 import costs. (#39488)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39488\n\nCurrently caffe2.InitOpLibrary does the dll import uniliaterally. Instead if we make a lazy version and use it, then many pieces of code which do not need the caffe2urrenoperators get a lot faster.\n\nOne a real test, the import time went from 140s to 68s. 8s.\n\nThis also cleans up the algorithm slightly (although it makes a very minimal\ndifference), by parsing the list of operators once, rather than every time a\nnew operator is added, since we defer the RefreshCall until after we've\nimported all the operators.\n\nThe key way we maintain safety, is that as soon as someone does an operation\nwhich requires a operator (or could), we force importing of all available\noperators.\n\nFuture work could include trying to identify which code is needed for which\noperator and only import the needed ones. There may also be wins available by\nplaying with dlmopen (which opens within a namespace), or seeing if the dl\nflags have an impact (I tried this and didn't see an impact, but dlmopen may\nmake it better).\n\nTest Plan:\nI added a new test a lazy_dyndep_test.py (copied from all_compare_test.py).\nI'm a little concerned that I don't see any explicit tests for dyndep, but this\nshould provide decent coverage.\n\nDifferential Revision: D21870844\n\nfbshipit-source-id: 3f65fedb65bb48663670349cee5e1d3e22d560ed", "pr_number": "39488", "files_changed": ["caffe2/python/core.py", "caffe2/python/dyndep.py", "caffe2/python/lazy_dyndep.py", "caffe2/python/lazy_dyndep_test.py"], "labels": ["fb-exported", "merged"]}, "a318234eb0": {"title": "Print raising warnings in Python rather than C++ if other error occurs (#41116)", "body": "Summary:\nWhen we return to Python from C++ in PyTorch and have warnings and and error, we have the problem of what to do when the warnings throw because we can only throw one error.\nPreviously, if we had an error, we punted all warnings to the C++ warning handler which would write them to stderr (i.e. system fid 2) or pass them on to glog.\n\nThis has drawbacks if an error happened:\n- Warnings are not handled through Python even if they don't raise,\n- warnings are always printed with no way to suppress this,\n- the printing bypasses sys.stderr, so Python modules wanting to\n  modify this don't work (with the prominent example being Jupyter).\n\nThis patch does the following instead:\n- Set the warning using standard Python extension mechanisms,\n- if Python decides that this warning is an error and we have a\n  PyTorch error, we print the warning through Python and clear\n  the error state (from the warning).\n\nThis resolves the three drawbacks discussed above, in particular it fixes https://github.com/pytorch/pytorch/issues/37240 .\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41116\n\nDifferential Revision: D22456393\n\nPulled By: albanD\n\nfbshipit-source-id: c3376735723b092efe67319321a8a993402985c7", "pr_number": "41116", "files_changed": ["test/test_autograd.py", "test/test_cpp_extensions_jit.py", "test/test_torch.py", "torch/csrc/Exceptions.cpp"], "labels": ["merged", "open source"]}, "df1f8a48d8": {"title": "add null check for c2 tensor conversion (#41096)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41096\n\nThe spark spot model had some issues in tensor conversion, see P134598596. It happens when we convert an undefined c10 tensor to caffe2 tensor.\nThis diff added a null check.\n\nTest Plan: spark spot model runs without problem\n\nReviewed By: smessmer\n\nDifferential Revision: D22330705\n\nfbshipit-source-id: dfe0f29a48019b6611cad3fd8f2ae49e8db5427e", "pr_number": "41096", "files_changed": ["caffe2/core/operator.h"], "labels": ["fb-exported", "merged"]}, "2252188e85": {"title": "[caffe2] Fix spatial_batch_norm_op dividision-by-zero crash (#40806)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40806\n\nWhen the input is empty, the operator will crash on \"runtime error: division by zero\". This has been causing Inference platform server crashes.\n\nExample crash logs:\n\n{P134526683}\n\nTest Plan:\nUnit test\n\nSee reproducing steps in the Test Plan of D22300135\n\nReviewed By: houseroad\n\nDifferential Revision: D22302089\n\nfbshipit-source-id: aaa5391fddc86483b0f3aba3efa7518e54913635", "pr_number": "40806", "files_changed": ["caffe2/operators/spatial_batch_norm_op.h"], "labels": ["fb-exported", "merged"]}, "7c29a4e66f": {"title": "Don't add NCCL dependency to gloo if system NCCL is used (#41180)", "body": "Summary:\nThis avoids a (currently only) warning of cmake:\n```\nThe dependency target \"nccl_external\" of target \"gloo_cuda\" does not exist.\nCall Stack (most recent call first):\n  CMakeLists.txt:411 (include)\n```\n\nThis will be a real problem once Policy CMP0046 is set which will make this warning be an error\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41180\n\nDifferential Revision: D22460623\n\nPulled By: malfet\n\nfbshipit-source-id: 0222b12b435e5e2fdf2bc85752f95abba1e3d4d5", "pr_number": "41180", "files_changed": ["cmake/Dependencies.cmake"], "labels": ["merged", "open source"]}, "c1fa74b2d7": {"title": "[quant][refactor] test_only_eval_fn (#41078)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41078\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22420699\n\nfbshipit-source-id: cf105cd41d83036df65c6bb3147cc14aaf755897", "pr_number": "41078", "files_changed": ["test/quantization/test_numeric_suite.py", "test/quantization/test_quantize.py", "test/quantization/test_quantize_jit.py", "test/quantization/test_quantized_module.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "e84ef45dd3": {"title": "[JIT] Fix JIT triage workflow (#41170)", "body": "Summary:\n**Summary**\nThis commit fixes the JIT triage workflow based on testing done in my\nown fork.\n\n**Test Plan**\nThis commit has been tested against my own fork. This commit is\ncurrently at the tip of my master branch, and if you open an issue in my\nfork and label it JIT, it will be added to the Triage Review project in\nthat fork under the Needs triage column.\n\n*Old issue that is labelled JIT later*\n\n<img width=\"700\" alt=\"Captura de Pantalla 2020-07-08 a la(s) 6 59 42 p  m\" src=\"https://user-images.githubusercontent.com/4392003/86988551-5b805100-c14d-11ea-9de3-072916211f24.png\">\n\n*New issue that is opened with the JIT label*\n<img width=\"725\" alt=\"Captura de Pantalla 2020-07-08 a la(s) 6 59 17 p  m\" src=\"https://user-images.githubusercontent.com/4392003/86988560-60dd9b80-c14d-11ea-94f0-fac01a0d239b.png\">\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41170\n\nDifferential Revision: D22460584\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 278483cebbaf3b35e5bdde2a541513835b644464", "pr_number": "41170", "files_changed": [".github/workflows/jit_triage.yml"], "labels": ["merged"]}, "2cf31fb577": {"title": "Fix max_pool2d perf regression (#41174)", "body": "Summary:\nThe two pointer variables `ptr_top_diff` and `ptr_top_mask` were introduced in https://github.com/pytorch/pytorch/issues/38953. Some end-to-end testing showed training performance regression due to this change. The performance is restored after removing the two pointer variables, and adding offset directly below in the indexing [ ] calculations.\n\nSee PR change https://github.com/pytorch/pytorch/pull/38953/files#diff-8085d370f4e98295074a51b8a1f829e9R187-R188\n\nhttps://github.com/pytorch/pytorch/blob/e4a3c584d51662d4c14060fc8517464fe3c12142/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu#L186-L195\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41174\n\nDifferential Revision: D22451565\n\nPulled By: ngimel\n\nfbshipit-source-id: 37ed6b9fd785e1be31a027ef5d60794656cc575a", "pr_number": "41174", "files_changed": ["aten/src/ATen/native/cuda/DilatedMaxPool2d.cu"], "labels": ["merged", "open source"]}, "22f940b7bd": {"title": "add clang code coverage compile flags (#41103)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41103\n\nadd a CLANG_CODE_COVERAGE option to CMakeList. If the option is ON, add code coverage needed compile flags.\n\nTest Plan:\nClone pytorch source code to local, modified these changes and builded it with `CLANG_CODE_COVERAGE ON` and `BUILD_TESTS ON`.  Run a manual test and attach code coverage report.\n\n{F243609020}\n\nReviewed By: malfet\n\nDifferential Revision: D22422513\n\nfbshipit-source-id: 27a31395c31b5b5f4b72523954722771d8f61080", "pr_number": "41103", "files_changed": ["CMakeLists.txt", "cmake/Summary.cmake"], "labels": ["fb-exported", "merged"]}, "1f1351488e": {"title": "Revert D21870844: Create lazy_dyndeps to avoid caffe2 import costs.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD21870844 (https://github.com/pytorch/pytorch/commit/07fd5f8ff992dddf4f3b0c286af9f7a3b53b617c)\n\nOriginal commit changeset: 3f65fedb65bb\n\nfbshipit-source-id: 4f661072d72486a9c14711e368247b3d30e28af9", "pr_number": null, "files_changed": ["caffe2/python/core.py", "caffe2/python/dyndep.py", "caffe2/python/lazy_dyndep.py", "caffe2/python/lazy_dyndep_test.py"], "labels": []}, "e374280768": {"title": "Use explicit templates in CUDALoops kernels (#41059)", "body": "Summary:\nFollow up after https://github.com/pytorch/pytorch/pull/40992\nUse explicit templates instead of lambdas to reduce binary size without affecting the perf by 100-200Kb per arch per CU, namely:\nBinaryMulDivKernel.cu 3.8Mb -> 3.5Mb\nCompareEQKernel.cu 1.8Mb -> 1.7Mb\nBinaryAddSubKernel.cu 2.0Mb -> 1.8Mb\nBinaryBitwiseOpsKernels.cu 2.6Mb -> 2.3Mb\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41059\n\nDifferential Revision: D22458928\n\nPulled By: malfet\n\nfbshipit-source-id: cca623bb6e769cfe372977b08463d98b1a02dd14", "pr_number": "41059", "files_changed": ["aten/src/ATen/native/cuda/AbsKernel.cu", "aten/src/ATen/native/cuda/BinaryAddSubKernel.cu", "aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "aten/src/ATen/native/cuda/CompareEQKernel.cu", "aten/src/ATen/native/cuda/CompareGEKernel.cu", "aten/src/ATen/native/cuda/CompareGTKernel.cu", "aten/src/ATen/native/cuda/CompareLEKernel.cu", "aten/src/ATen/native/cuda/CompareLTKernel.cu", "aten/src/ATen/native/cuda/CompareNEKernel.cu", "aten/src/ATen/native/cuda/FillKernel.cu", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu"], "labels": ["merged"]}, "a88099ba3e": {"title": "restore old documentation references (#39086)", "body": "Summary:\nFixes gh-39007\n\nWe replaced actual content with links to generated content in many places to break the documentation into manageable chunks. This caused references like\n```\nhttps://pytorch.org/docs/stable/torch.html#torch.flip\n```\nto become\n```\nhttps://pytorch.org/docs/master/generated/torch.flip.html#torch.flip\n```\nThe textual content that was located at the old reference was replaced with a link to the new reference. This PR adds a `<p id=\"xxx\"/p>` reference next to the link, so that the older references from outside tutorials and forums still work: they will bring the user to the link that they can then follow through to see the actual content.\n\nThe way this is done is to monkeypatch the sphinx writer method that produces the link. It is ugly but practical, and in my mind not worse than adding javascript to do the same thing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39086\n\nDifferential Revision: D22462421\n\nPulled By: jlin27\n\nfbshipit-source-id: b8f913b38c56ebb857c5a07bded6509890900647", "pr_number": "39086", "files_changed": ["docs/source/conf.py"], "labels": ["merged", "open source", "triaged"]}, "bddba1e336": {"title": "Add benchmark for add op. (#40059)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40059\n\nThis benchmark is added specifically for mobile to see if compiler is\nautovectorizing and thus we have no advantage of neon backend for vec256\nfor add op.\n\nTest Plan:\nCI\n\nImported from OSS\n\nDifferential Revision: D22055146\n\nfbshipit-source-id: 43ba6c4ae57c6f05d84887c2750ce21ae1b0f0b5", "pr_number": "40059", "files_changed": ["CMakeLists.txt", "aten/CMakeLists.txt", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/benchmarks/tensor_add.cpp", "caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "scripts/build_android.sh"], "labels": ["merged"]}, "d6feb6141f": {"title": "[Vec256][neon] Add neon backend for vec256 (#39341)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39341\n\nThis PR introduces neon backend for vec256 class for float datatype.\nFor now only aarch64 is enabled due to few issues with enabling in\naarch32 bit.\n\nTest Plan:\nvec256_test\n\nImported from OSS\n\nDifferential Revision: D21822399\n\nfbshipit-source-id: 3851c4336d93d1c359c85b38cf19904f82bc7b8d", "pr_number": "39341", "files_changed": ["CMakeLists.txt", "aten/CMakeLists.txt", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/cpu/vec256/intrinsics.h", "aten/src/ATen/cpu/vec256/vec256.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_float_neon.h", "aten/src/ATen/cpu/vec256/vec256_qint.h", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/vec256_test.cpp", "caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "scripts/build_android.sh"], "labels": ["merged"]}, "82c9f79e0e": {"title": "Add fused add_relu op. (#39342)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39342\n\nMany networks such as resnet have adds followed by relu. This op is the\nfirst step in enabling this fused implementation.\nOnce we have the fused add_relu op, a JIT pass will be written to\nreplace add + relu patterns with add_relu.\n\nTest Plan:\npython test/test_nn.py TestAddRelu\n\nImported from OSS\n\nDifferential Revision: D21822397\n\nfbshipit-source-id: 03df83a3e46ddb48a90c5a6f755227a7e361a0e8", "pr_number": "39342", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_nn.py", "torch/_overrides.py"], "labels": ["merged"]}, "c5dcf056ee": {"title": "JIT pass for add relu fusion. (#39343)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39343\n\nBuilding on top of previous PR that adds fused add_relu op, this PR adds\na JIT pass to transform input graph to find all fusable instancs of add\n+ relu and fuses them.\n\nTest Plan:\npython test/test_jit.py TestJit.test_add_relu_fusion\n\nImported from OSS\n\nDifferential Revision: D21822396\n\nfbshipit-source-id: 12c7e8db54c6d70a2402b32cc06c7e305ffbb1be", "pr_number": "39343", "files_changed": ["test/test_jit.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/fuse_relu.cpp", "torch/csrc/jit/passes/fuse_relu.h", "torch/csrc/jit/python/init.cpp"], "labels": ["merged", "oncall: jit"]}, "7c2c752e6d": {"title": "Revert D22458928: [pytorch][PR] Use explicit templates in CUDALoops kernels", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22458928 (https://github.com/pytorch/pytorch/commit/e374280768bd369f617fa1d5d0066b83297f4608)\n\nOriginal commit changeset: cca623bb6e76\n\nfbshipit-source-id: 6dd24f783ec3b781140f314716ffb02f0892c57a", "pr_number": null, "files_changed": ["aten/src/ATen/native/cuda/AbsKernel.cu", "aten/src/ATen/native/cuda/BinaryAddSubKernel.cu", "aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "aten/src/ATen/native/cuda/CompareEQKernel.cu", "aten/src/ATen/native/cuda/CompareGEKernel.cu", "aten/src/ATen/native/cuda/CompareGTKernel.cu", "aten/src/ATen/native/cuda/CompareLEKernel.cu", "aten/src/ATen/native/cuda/CompareLTKernel.cu", "aten/src/ATen/native/cuda/CompareNEKernel.cu", "aten/src/ATen/native/cuda/FillKernel.cu", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu"], "labels": []}, "a79b416847": {"title": "make Int8 FC bias quantization use round flush to infinity", "body": "Summary:\nthe current quantization rounding function uses fbgemm which\ndefaults to round to nearest. The current implementation of hw uses round\nflush to infinity. Adding such an option to switch the mode of rounding.\n\nTest Plan: ran against test_fc_int8\n\nReviewed By: venkatacrc\n\nDifferential Revision: D22452306\n\nfbshipit-source-id: d2a1fbfc695612fe07caaf84f52669643507cc9c", "pr_number": null, "files_changed": ["caffe2/quantization/server/fbgemm_pack_op.cc", "caffe2/quantization/server/fbgemm_pack_op.h"], "labels": []}, "df252c059c": {"title": "[ROCm] Skip caffe2 unique op test for rocm3.5 (#41219)", "body": "Summary:\nunique op test failure in caffe2 blocks upgrading CI to rocm3.5.1. Skipping the test to unblock will re-enable after root causing and fixing the issue.\njeffdaily sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41219\n\nDifferential Revision: D22471452\n\nPulled By: xw285cornell\n\nfbshipit-source-id: 9e503c8b37c0a4b92632f77b2f8a90281a9889c3", "pr_number": "41219", "files_changed": ["caffe2/python/hypothesis_test.py", "caffe2/python/operator_test/unique_ops_test.py"], "labels": ["merged", "module: rocm", "open source"]}, "f6eb92a354": {"title": "Expose private APIs to enable/disable pickling ScriptModules without RPC (#39631)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39631\n\nBackground:\nCurrently, we cannot send ScriptModule over RPC as an argument.\nOtherwise, it would hit the following error:\n\n> _pickle.PickleError: ScriptModules cannot be deepcopied using\n> copy.deepcopy or saved using torch.save. Mixed serialization of\n> script and non-script modules is not supported. For purely\n> script modules use my_script_module.save(<filename>) instead.\n\nFailed attempt:\ntried to install `torch.jit.ScriptModule` to RPC's\ndispatch table, but it does not work as the dispatch table only\nmatches exact types and using base type `torch.jit.ScriptModule`\ndoes not work for derived typed.\n\nCurrent solution:\nThe current solution exposes `_enable_jit_rref_pickle` and\n`_disable_jit_rref_pickle` APIs to toggle the `allowJitRRefPickle`\nflag. See `test_pickle_script_module_with_rref` as an example.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D21920870\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4d58afce5d0b4b81249b383c173488820b1a47d6", "pr_number": "39631", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/types.cpp", "torch/csrc/distributed/rpc/types.h", "torch/distributed/rpc/__init__.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["merged", "oncall: jit"]}, "75a4862f63": {"title": "Added SiLU activation function (#41034)", "body": "Summary:\nImplemented the SiLU activation function as discussed in https://github.com/pytorch/pytorch/issues/3169.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41034\n\nReviewed By: glaringlee\n\nDifferential Revision: D22465203\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: b27d064529fc99600c586ad49b594b52b718b0d2", "pr_number": "41034", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/nn.functional.rst", "docs/source/nn.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_overrides.py", "torch/nn/functional.py", "torch/nn/modules/__init__.py", "torch/nn/modules/activation.py"], "labels": ["merged", "module: nn"]}, "8a79eec98a": {"title": "Add add_relu fusion pass to optimize_for_mobile. (#40252)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40252\n\nAs title says.\n\nTest Plan:\npython test/test_mobile_optimizer.py\n\nImported from OSS\n\nDifferential Revision: D22126825\n\nfbshipit-source-id: a1880587ba8db9dee0fa450bc463734e4a8693d9", "pr_number": "40252", "files_changed": ["test/test_mobile_optimizer.py", "torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.h", "torch/csrc/jit/python/init.cpp"], "labels": ["merged", "oncall: jit"]}, "08227072e2": {"title": "Benchmark RecordFunction overhead on some models (#40952)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40952\n\nAdding a benchmark to measure RecordFunction overhead,\ncurrently on resnet50 and lstm models\n\nTest Plan:\npython benchmarks/record_function_benchmark/record_function_bench.py\nBenchmarking RecordFunction overhead for lstm_jit\nRunning warmup... finished\nRunning 100 iterations with RecordFunction... finished\nN = 100, avg. time: 251.970 ms, stddev: 39.348 ms\nRunning 100 iterations without RecordFunction... finished\nN = 100, avg. time: 232.828 ms, stddev: 24.556 ms\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D22368357\n\nPulled By: ilia-cher\n\nfbshipit-source-id: bff4f4e0e06fb80fdfcf85966c2468e48ed7bc98", "pr_number": "40952", "files_changed": ["benchmarks/record_function_benchmark/record_function_bench.py", "torch/csrc/autograd/init.cpp"], "labels": ["merged"]}, "62e16934cb": {"title": "[caffe2] Add the dedup implementation of fused RowWiseAdagrad op on GPUs (#40282)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40282\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient \\(caffe2\\.caffe2\\.fb\\.net_transforms\\.tests\\.fuse_sparse_ops_test\\.TestFuseSparseOps\\)' --print-passing-details\n```\n\nhttps://our.intern.facebook.com/intern/testinfra/testrun/4785074632584150\n\nReviewed By: jspark1105\n\nDifferential Revision: D22102737\n\nfbshipit-source-id: fa3fef7cecb1e2cf5c9b6019579dc0f86fd3a3b2", "pr_number": "40282", "files_changed": ["caffe2/sgd/adagrad_fused.cc", "caffe2/sgd/adagrad_fused_op_gpu.cu", "caffe2/sgd/adagrad_fused_op_gpu.cuh", "caffe2/sgd/rowwise_adagrad_fused.cc", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["merged"]}, "e568b3fa2d": {"title": "test nan and inf in TestTorchMathOps (#41225)", "body": "Summary:\nPer title. `lgamma` produces a different result for `-inf` compared to scipy, so there comparison is skipped.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41225\n\nDifferential Revision: D22473346\n\nPulled By: ngimel\n\nfbshipit-source-id: e4ebda1b10e2a061bd4cef38d1d7b5bf0f581790", "pr_number": "41225", "files_changed": ["test/test_torch.py", "torch/testing/__init__.py"], "labels": ["merged"]}, "db38487ece": {"title": "Autograd Doc for Complex Numbers (#41012)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41012\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22476911\n\nPulled By: anjali411\n\nfbshipit-source-id: 7da20cb4312a0465272bebe053520d9911475828", "pr_number": "41012", "files_changed": ["docs/source/notes/autograd.rst"], "labels": ["merged"]}, "33f9fbf8ba": {"title": "Modularize parsing NCCL_BLOCKING_WAIT in ProcessGroupNCCL (#41076)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41076\n\nModularizes Parsing the NCCL_BLOCKING_WAIT environment variable in the ProcessGroupNCCL Constructor.\nghstack-source-id: 107491850\n\nTest Plan: Sandcastle/CI\n\nDifferential Revision: D22401225\n\nfbshipit-source-id: 79866d3f4f1a617cdcbca70e3bea1ce9dcac3316", "pr_number": "41076", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["merged"]}, "4a09501fbe": {"title": "LogitOp LUT based fake FP16 Op. (#41258)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41258\n\nLogitOp LUT based fake FP16 Op.\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: test_op_nnpi_fp16.py covers the test_logit testing.\n\nReviewed By: hyuen\n\nDifferential Revision: D22351963\n\nfbshipit-source-id: e2ed2bd9bfdc58c6f823d7d41557109c08628bd7", "pr_number": "41258", "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/unary_fp16_fake_op.cc", "caffe2/contrib/fakelowp/unary_fp16_fake_op.h"], "labels": ["fb-exported", "merged"]}, "d927aee312": {"title": "Small clarification of torch.cuda.amp multi-model example (#41203)", "body": "Summary:\nsome people have been confused by `retain_graph` in the snippet, they thought it was an additional requirement imposed by amp.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41203\n\nDifferential Revision: D22463700\n\nPulled By: ngimel\n\nfbshipit-source-id: e6fc8871be2bf0ecc1794b1c6f5ea99af922bf7e", "pr_number": "41203", "files_changed": ["docs/source/notes/amp_examples.rst"], "labels": ["merged", "open source"]}, "75b6dd3d49": {"title": "Wrap Caffe2's SparseLengthsSum into a PyTorch op (#39596)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39596\n\nThis diff wraps Caffe2's SparseLengthsSum on GPU as a PT op.\n\nReviewed By: jianyuh\n\nDifferential Revision: D21895309\n\nfbshipit-source-id: 38bb156f9be8d28225d2b44f5b4c93d27779aff9", "pr_number": "39596", "files_changed": ["caffe2/operators/segment_reduction_op_gpu.cu", "caffe2/operators/segment_reduction_op_gpu.cuh", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["fb-exported", "merged"]}, "a548c6b18f": {"title": "add check for duplicated op registration in JIT (#41214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41214\n\nSame as D21032976, add check for duplicated op name in JIT\n\nTest Plan:\nrun full JIT predictor\nalso\nbuck test pytorch-playground\n\nReviewed By: smessmer\n\nDifferential Revision: D22467871\n\nfbshipit-source-id: 9b7a40a217e6c63cca44cad54f9f657b8b207a45", "pr_number": "41214", "files_changed": ["torch/csrc/jit/runtime/operator.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "9b0393fcf1": {"title": "[ONNX]Fix export of flatten (#40418)", "body": "Summary:\nShape is passed to _reshape_to_tensor as a Constant and cannot infer shape of the input when model is exported with dynamic axes set. Instead of a Constant pass output of a subgraph Shape-Slice-Concat to compute the shape for the Reshape node in _reshape_to_tensor function.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40418\n\nReviewed By: hl475\n\nDifferential Revision: D22480127\n\nPulled By: houseroad\n\nfbshipit-source-id: 11853adb6e6914936871db1476916699141de435", "pr_number": "40418", "files_changed": ["test/onnx/expect/TestOperators.test_flatten.expect", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source", "triaged"]}, "16c8146da9": {"title": "Self binning histogram (#40875)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40875\n\nThis op uses the given num_bins and a spacing strategy to automatically bin and compute the histogram of given matrices.\n\nTest Plan: Unit tests.\n\nReviewed By: neha26shah\n\nDifferential Revision: D22329069\n\nfbshipit-source-id: 28406b94e284d52d875f73662fc82f93dbc00064", "pr_number": "40875", "files_changed": ["caffe2/operators/self_binning_histogram_op.cc", "caffe2/operators/self_binning_histogram_op.h", "caffe2/python/operator_test/self_binning_histogram_test.py"], "labels": ["fb-exported", "merged"]}, "cb6c3526c6": {"title": "Migrate addmm, addbmm and THBlas_gemm to ATen (#40927)", "body": "Summary:\nResubmit #40927\nCloses https://github.com/pytorch/pytorch/issues/24679, closes https://github.com/pytorch/pytorch/issues/24678\n\n`addbmm` depends on `addmm` so needed to be ported at the same time. I also removed `THTensor_(baddbmm)` which I noticed had already been ported so was just dead code.\n\nAfter having already written this code, I had to fix merge conflicts with https://github.com/pytorch/pytorch/issues/40354 which revealed there was already an established place for cpu blas routines in ATen. However, the version there doesn't make use of ATen's AVX dispatching so thought I'd wait for comment before migrating this into that style.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40927\n\nReviewed By: ezyang\n\nDifferential Revision: D22468490\n\nPulled By: ngimel\n\nfbshipit-source-id: f8a22be3216f67629420939455e31a88af20201d", "pr_number": "40927", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/native/CPUBlas.cpp", "aten/src/ATen/native/CPUBlas.h", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/NaiveDilatedConvolution.cpp", "aten/src/ATen/native/cpu/BlasKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/THBlasUtils.h", "aten/src/TH/generic/THBlas.cpp", "aten/src/TH/generic/THBlas.h", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "c10/core/ScalarType.h", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "0651887eb4": {"title": "Improve repr for torch.iinfo & torch.finfo (#40488)", "body": "Summary:\n- fix https://github.com/pytorch/pytorch/issues/39991\n- Include directly `min`/`max`/`eps`/`tiny` values in repr of `torch.iinfo` & `torch.finfo` for inspection\n- Use `torch.float16` / `torch.int16` instead of uncorrespond names `Half` / `Short`\n- The improved repr is shown just like:\n```\n>>> torch.iinfo(torch.int8)\niinfo(type=torch.int8, max=127, min=-128)\n>>> torch.iinfo(torch.int16)\niinfo(type=torch.int16, max=32767, min=-32768)\n>>> torch.iinfo(torch.int32)\niinfo(type=torch.int32, max=2.14748e+09, min=-2.14748e+09)\n>>> torch.iinfo(torch.int64)\niinfo(type=torch.int64, max=9.22337e+18, min=-9.22337e+18)\n>>> torch.finfo(torch.float16)\nfinfo(type=torch.float16, eps=0.000976563, max=65504, min=-65504, tiny=6.10352e-05)\n>>> torch.finfo(torch.float32)\nfinfo(type=torch.float32, eps=1.19209e-07, max=3.40282e+38, min=-3.40282e+38, tiny=1.17549e-38)\n>>> torch.finfo(torch.float64)\nfinfo(type=torch.float64, eps=2.22045e-16, max=1.79769e+308, min=-1.79769e+308, tiny=2.22507e-308)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40488\n\nDifferential Revision: D22445301\n\nPulled By: mruberry\n\nfbshipit-source-id: 552af9904c423006084b45d6c4adfb4b5689db54", "pr_number": "40488", "files_changed": ["docs/source/type_info.rst", "test/test_type_info.py", "torch/_C/__init__.pyi.in", "torch/csrc/TypeInfo.cpp", "torch/csrc/utils/tensor_dtypes.cpp", "torch/csrc/utils/tensor_dtypes.h"], "labels": ["merged", "open source", "triaged"]}, "7c143e5d3e": {"title": "Reducing size of docker Linux image (#41207)", "body": "Summary:\n# Description\nThe goal is to reduce the size of the docker image. I checked a few things:\n* Docker layer overlaps\n* Removing .git folder\n* Removing intermediate build artifacts (*.o and *.a)\n\nThe only one that gave satisfying result was the 3rd approach, removing *.o and *.a. The final image went from 10 GB to 9.7 GB.\n\nI used Dive (https://github.com/wagoodman/dive) to inspect the Docker image manually.\n\n# Test:\n* Check the image size was reduced\n* No test failures in CI\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41207\n\nTest Plan:\n* Check the image size was reduced\n* No test failures in CI\n\nDifferential Revision: D22465221\n\nPulled By: ssylvain\n\nfbshipit-source-id: 48754259729401e3c08447b0fa0630ca7217cb98", "pr_number": "41207", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "9daba76ba1": {"title": "Change to.dtype_layout to c10-full (#41169)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41169\n\n-\nghstack-source-id: 107537240\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D22289257\n\nfbshipit-source-id: ed3cc06327951fa886eb3b8f1c8bcc014ae2bc41", "pr_number": "41169", "files_changed": ["aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/basic.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/cpp/api/tensor.cpp", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_factories.py", "tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/variable_factories.h"], "labels": ["merged"]}, "dd0c98d82a": {"title": "[ONNX]Add tests for ConvTranspose 1D and 3D (#40703)", "body": "Summary:\nAdd tests for ConvTranspose 1D and 3D\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40703\n\nReviewed By: hl475\n\nDifferential Revision: D22480087\n\nPulled By: houseroad\n\nfbshipit-source-id: 92846ed7181f543af20669e5ea191bfb5522ea13", "pr_number": "40703", "files_changed": [".jenkins/caffe2/test.sh", "test/onnx/test_pytorch_onnx_onnxruntime.py"], "labels": ["merged", "open source"]}, "7bae5780a2": {"title": "Revert D22329069: Self binning histogram", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22329069 (https://github.com/pytorch/pytorch/commit/16c8146da96ea8338c6ac0601083c3aa66b117b6)\n\nOriginal commit changeset: 28406b94e284\n\nfbshipit-source-id: fc7f3f8c968d1ec7d2a1cf7a4d05900f51055d82", "pr_number": null, "files_changed": ["caffe2/operators/self_binning_histogram_op.cc", "caffe2/operators/self_binning_histogram_op.h", "caffe2/python/operator_test/self_binning_histogram_test.py"], "labels": []}, "1f2e91fa4f": {"title": "Impilcit casting resulting internal build failure. (#41272)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41272\n\nImplicit casting from int to float is resulting in vec256_test build failure\ninternally. This diff fixes that.\n\nTest Plan: Build vec256_test for android and run it on android phone.\n\nReviewed By: ljk53, paulshaoyuqiao\n\nDifferential Revision: D22484635\n\nfbshipit-source-id: ebb9fc2eccb8261ab01d8266150fc3b05166f1e7", "pr_number": "41272", "files_changed": ["aten/src/ATen/test/vec256_test.cpp"], "labels": ["fb-exported", "merged"]}, "ce3ba3b9bc": {"title": "[JIT] Add support for backend-lowered submodules (#41146)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41146\n\n**Summary**\nThis commit adds support for using `Modules` that have been lowered as\nsubmodules in `ScriptModules`.\n\n**Test Plan**\nThis commit adds execution and save/load tests to test_backends.py for\nbackend-lowered submodules.\n\n**Fixes**\nThis commit fixes #40069.\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D22459543\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 02e0c0ccdce26c671ade30a34aca3e99bcdc5ba7", "pr_number": "41146", "files_changed": ["test/jit/test_backends.py", "torch/csrc/jit/backends/backend_init.cpp", "torch/csrc/jit/python/script_init.cpp"], "labels": ["merged", "oncall: jit"]}, "48d6e2adce": {"title": "Disable the mkldnn for conv2d in some special cases (#40610)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40610\n\nWe have benchmarked several models, which shows the native implementation of conv2d is faster mkldnn path. For group conv, the native implementation does not batch all the groups.\n\nTest Plan:\n```\nimport torch\nimport torch.nn.functional as F\n\nimport numpy as np\n\nfrom timeit import Timer\n\nnum = 50\n\nS = [\n#         [1, 1, 100, 40, 16, 3, 3, 1, 1, 1, 1],\n#         [1, 2048, 4, 2, 512, 1, 1, 1, 1, 0, 0],\n#         [1, 512, 4, 2, 512, 3, 3, 1, 1, 1, 1],\n#         [1, 512, 4, 2, 2048, 1, 1, 1, 1, 0, 0],\n#         [1, 2048, 4, 2, 512, 1, 1, 1, 1, 0, 0],\n#         [1, 512, 4, 2, 512, 3, 3, 1, 1, 1, 1],\n#         [1, 512, 4, 2, 2048, 1, 1, 1, 1, 0, 0],\n#         [1, 2048, 4, 2, 512, 1, 1, 1, 1, 0, 0],\n#         [1, 512, 4, 2, 512, 3, 3, 1, 1, 1, 1],\n#         [1, 512, 4, 2, 2048, 1, 1, 1, 1, 0, 0],\n#         [1, 2048, 4, 2, 512, 1, 1, 1, 1, 0, 0],\n#         [1, 512, 4, 2, 512, 3, 3, 1, 1, 1, 1],\n#         [1, 512, 4, 2, 2048, 1, 1, 1, 1, 0, 0],\n#         [1, 2048, 4, 2, 512, 1, 1, 1, 1, 0, 0],\n#         [1, 512, 4, 2, 512, 3, 3, 1, 1, 1, 1],\n#         [1, 512, 4, 2, 2048, 1, 1, 1, 1, 0, 0],\n#         [1, 2048, 4, 2, 512, 1, 1, 1, 1, 0, 0],\n#         [1, 512, 4, 2, 512, 3, 3, 1, 1, 1, 1],\n#         [1, 512, 4, 2, 2048, 1, 1, 1, 1, 0, 0],\n[1, 3, 224, 224, 64, 7, 7, 2, 2, 3, 3, 1],\n[1, 64, 56, 56, 128, 1, 1, 1, 1, 0, 0, 1],\n[1, 128, 56, 56, 128, 3, 3, 1, 1, 1, 1, 32],\n[1, 128, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1],\n[1, 64, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1],\n[1, 256, 56, 56, 128, 1, 1, 1, 1, 0, 0, 1],\n[1, 128, 56, 56, 128, 3, 3, 1, 1, 1, 1, 32],\n[1, 128, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1],\n[1, 256, 56, 56, 128, 1, 1, 1, 1, 0, 0, 1],\n[1, 128, 56, 56, 128, 3, 3, 1, 1, 1, 1, 32],\n[1, 128, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1],\n[1, 256, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1],\n[1, 256, 56, 56, 256, 3, 3, 2, 2, 1, 1, 32],\n[1, 256, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 256, 56, 56, 512, 1, 1, 2, 2, 0, 0, 1],\n[1, 512, 28, 28, 256, 1, 1, 1, 1, 0, 0, 1],\n[1, 256, 28, 28, 256, 3, 3, 1, 1, 1, 1, 32],\n[1, 256, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 28, 28, 256, 1, 1, 1, 1, 0, 0, 1],\n[1, 256, 28, 28, 256, 3, 3, 1, 1, 1, 1, 32],\n[1, 256, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 28, 28, 256, 1, 1, 1, 1, 0, 0, 1],\n[1, 256, 28, 28, 256, 3, 3, 1, 1, 1, 1, 32],\n[1, 256, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 28, 28, 512, 3, 3, 2, 2, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 28, 28, 1024, 1, 1, 2, 2, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1],\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32],\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 1024, 3, 3, 2, 2, 1, 1, 32],\n[1, 1024, 7, 7, 2048, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 14, 14, 2048, 1, 1, 2, 2, 0, 0, 1],\n[1, 2048, 7, 7, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 7, 7, 1024, 3, 3, 1, 1, 1, 1, 32],\n[1, 1024, 7, 7, 2048, 1, 1, 1, 1, 0, 0, 1],\n[1, 2048, 7, 7, 1024, 1, 1, 1, 1, 0, 0, 1],\n[1, 1024, 7, 7, 1024, 3, 3, 1, 1, 1, 1, 32],\n[1, 1024, 7, 7, 2048, 1, 1, 1, 1, 0, 0, 1],\n    ]\nfor x in range(105):\n    P = S[x]\n    print(P)\n    (N, C, H, W) = P[0:4]\n    M = P[4]\n    (kernel_h, kernel_w) = P[5:7]\n    (stride_h, stride_w) = P[7:9]\n    (padding_h, padding_w) = P[9:11]\n\n    X_np = np.random.randn(N, C, H, W).astype(np.float32)\n    W_np = np.random.randn(M, C, kernel_h, kernel_w).astype(np.float32)\n    X = torch.from_numpy(X_np)\n    g = P[11]\n    conv2d_pt = torch.nn.Conv2d(\n        C, M, (kernel_h, kernel_w), stride=(stride_h, stride_w),\n        padding=(padding_h, padding_w), groups=g, bias=True)\n\n    class ConvNet(torch.nn.Module):\n        def __init__(self):\n            super(ConvNet, self).__init__()\n            self.conv2d = conv2d_pt\n\n        def forward(self, x):\n            return self.conv2d(x)\n\n    model = ConvNet()\n\n    def pt_forward():\n        with torch.no_grad():\n            model(X)\n\n    torch._C._set_mkldnn_enabled(True)\n    t = Timer(\"pt_forward()\", \"from __main__ import pt_forward, X\")\n    print(\"MKLDNN pt time = {}\".format(t.timeit(num) / num * 1000.0))\n    torch._C._set_mkldnn_enabled(False)\n    t = Timer(\"pt_forward()\", \"from __main__ import pt_forward, X\")\n    print(\"TH pt time = {}\".format(t.timeit(num) / num * 1000.0))\n\nOMP_NUM_THREADS=1 MKL_NUM_THREADS=1 python bm.py\n```\n\noutput:\n```\n[1, 3, 224, 224, 64, 7, 7, 2, 2, 3, 3, 1]\nMKLDNN pt time = 5.891108009964228\nTH pt time = 7.0624795742332935\n[1, 64, 56, 56, 128, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 1.4464975893497467\nTH pt time = 0.721491202712059\n[1, 128, 56, 56, 128, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 1.4036639966070652\nTH pt time = 3.299683593213558\n[1, 128, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.3908068016171455\nTH pt time = 2.227546200156212\n[1, 64, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.226586602628231\nTH pt time = 1.3865559734404087\n[1, 256, 56, 56, 128, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.31307839602232\nTH pt time = 2.4284918047487736\n[1, 128, 56, 56, 128, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 1.5028003975749016\nTH pt time = 3.824346773326397\n[1, 128, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.4405963867902756\nTH pt time = 2.6227117888629436\n[1, 256, 56, 56, 128, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.405764400959015\nTH pt time = 2.644723802804947\n[1, 128, 56, 56, 128, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 1.5220053866505623\nTH pt time = 3.9365867897868156\n[1, 128, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.606868200004101\nTH pt time = 2.5387956015765667\n[1, 256, 56, 56, 256, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 6.0041105933487415\nTH pt time = 5.305919591337442\n[1, 256, 56, 56, 256, 3, 3, 2, 2, 1, 1, 32]\nMKLDNN pt time = 1.4830979891121387\nTH pt time = 7.532084975391626\n[1, 256, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.025687597692013\nTH pt time = 2.2185291908681393\n[1, 256, 56, 56, 512, 1, 1, 2, 2, 0, 0, 1]\nMKLDNN pt time = 3.5893129743635654\nTH pt time = 2.696530409157276\n[1, 512, 28, 28, 256, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.8203356079757214\nTH pt time = 2.0819314010441303\n[1, 256, 28, 28, 256, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.8583215996623039\nTH pt time = 2.7761065773665905\n[1, 256, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.9077288135886192\nTH pt time = 2.045416794717312\n[1, 512, 28, 28, 256, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.805021796375513\nTH pt time = 2.131381593644619\n[1, 256, 28, 28, 256, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.9023251943290234\nTH pt time = 2.9028950072824955\n[1, 256, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.1174601800739765\nTH pt time = 2.275596000254154\n[1, 512, 28, 28, 256, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.100480604916811\nTH pt time = 2.399571593850851\n[1, 256, 28, 28, 256, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.9321337938308716\nTH pt time = 2.886691205203533\n[1, 256, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.065785188227892\nTH pt time = 2.1640316024422646\n[1, 512, 28, 28, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 5.891813579946756\nTH pt time = 4.2956990003585815\n[1, 512, 28, 28, 512, 3, 3, 2, 2, 1, 1, 32]\nMKLDNN pt time = 0.9399276040494442\nTH pt time = 4.7622935846447945\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.2426914013922215\nTH pt time = 2.3699573799967766\n[1, 512, 28, 28, 1024, 1, 1, 2, 2, 0, 0, 1]\nMKLDNN pt time = 3.0341636016964912\nTH pt time = 2.6606030017137527\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.991385366767645\nTH pt time = 2.6313263922929764\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7330256141722202\nTH pt time = 3.008321188390255\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.880081795156002\nTH pt time = 2.289068605750799\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.9583285935223103\nTH pt time = 2.6302105747163296\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7322711870074272\nTH pt time = 2.8230775892734528\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.8620235808193684\nTH pt time = 2.4078205972909927\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.828651014715433\nTH pt time = 2.616014201194048\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7084695994853973\nTH pt time = 2.8024527989327908\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.7884829975664616\nTH pt time = 2.4237345717847347\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.89030060172081\nTH pt time = 2.5852439925074577\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.724627785384655\nTH pt time = 2.651805803179741\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.249914798885584\nTH pt time = 2.0440668053925037\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.722136974334717\nTH pt time = 2.531316000968218\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7164162024855614\nTH pt time = 2.8521843999624252\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.8891782090067863\nTH pt time = 2.436912599951029\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.0049769952893257\nTH pt time = 2.649025786668062\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7299130037426949\nTH pt time = 2.67714099958539\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.799382768571377\nTH pt time = 2.4427592009305954\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.0201382003724575\nTH pt time = 2.6285660080611706\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.6983320042490959\nTH pt time = 2.9118607938289642\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.8802538104355335\nTH pt time = 2.385452575981617\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.9600497893989086\nTH pt time = 2.594646792858839\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.5688861943781376\nTH pt time = 2.5941073894500732\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.7758505940437317\nTH pt time = 2.336081601679325\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.6135251857340336\nTH pt time = 2.3902921937406063\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.6303061917424202\nTH pt time = 2.6228136010468006\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.8868251852691174\nTH pt time = 2.5620524026453495\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.057632204145193\nTH pt time = 2.691414188593626\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7316274009644985\nTH pt time = 3.14683198928833\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.2674955762922764\nTH pt time = 2.602821197360754\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.0993166007101536\nTH pt time = 2.609328981488943\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7257938012480736\nTH pt time = 2.9255208000540733\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.3086097799241543\nTH pt time = 2.544360812753439\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.0537622049450874\nTH pt time = 2.6343842037022114\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7194169983267784\nTH pt time = 2.9009717889130116\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.6461398042738438\nTH pt time = 2.3600555770099163\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.6328082010149956\nTH pt time = 2.415131386369467\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.6832938082516193\nTH pt time = 2.6299685798585415\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.9594415985047817\nTH pt time = 2.509857602417469\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.956229578703642\nTH pt time = 2.691046390682459\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7222409918904305\nTH pt time = 2.938339803367853\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.9467295855283737\nTH pt time = 2.4219116009771824\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.0215882137417793\nTH pt time = 2.7782391756772995\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.719242412596941\nTH pt time = 2.8529402054846287\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.8062099777162075\nTH pt time = 2.9951974004507065\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.1621821969747543\nTH pt time = 2.5330167822539806\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.690075010061264\nTH pt time = 2.5531245954334736\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.832614816725254\nTH pt time = 2.339891381561756\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.7835668064653873\nTH pt time = 2.513139396905899\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7026367820799351\nTH pt time = 2.796882800757885\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.6479675993323326\nTH pt time = 2.4971639923751354\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.9846629686653614\nTH pt time = 2.4657804146409035\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.5969022028148174\nTH pt time = 2.697007991373539\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.7602720074355602\nTH pt time = 2.4498093873262405\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.963611613959074\nTH pt time = 2.6310251839458942\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7004458084702492\nTH pt time = 2.9164502024650574\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.887732572853565\nTH pt time = 2.4575488083064556\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.8350806050002575\nTH pt time = 2.23197178915143\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.5626789852976799\nTH pt time = 2.704860605299473\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.6168799959123135\nTH pt time = 2.2481359727680683\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.5654693879187107\nTH pt time = 2.2636358067393303\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.6836861930787563\nTH pt time = 2.825192976742983\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.7971909940242767\nTH pt time = 2.471243590116501\n[1, 1024, 14, 14, 512, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.8480279818177223\nTH pt time = 2.553586605936289\n[1, 512, 14, 14, 512, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.7191735878586769\nTH pt time = 2.6465672068297863\n[1, 512, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 2.7811027877032757\nTH pt time = 2.457349617034197\n[1, 1024, 14, 14, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 5.434317365288734\nTH pt time = 4.639615211635828\n[1, 1024, 14, 14, 1024, 3, 3, 2, 2, 1, 1, 32]\nMKLDNN pt time = 0.9400106035172939\nTH pt time = 2.9971951991319656\n[1, 1024, 7, 7, 2048, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 4.494664408266544\nTH pt time = 3.478870000690222\n[1, 1024, 14, 14, 2048, 1, 1, 2, 2, 0, 0, 1]\nMKLDNN pt time = 4.8432330042123795\nTH pt time = 3.6410867795348167\n[1, 2048, 7, 7, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 4.779010973870754\nTH pt time = 3.4093930013477802\n[1, 1024, 7, 7, 1024, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.8385192044079304\nTH pt time = 3.0921380035579205\n[1, 1024, 7, 7, 2048, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 3.9088409766554832\nTH pt time = 3.130124807357788\n[1, 2048, 7, 7, 1024, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 4.0072557888925076\nTH pt time = 2.977220807224512\n[1, 1024, 7, 7, 1024, 3, 3, 1, 1, 1, 1, 32]\nMKLDNN pt time = 0.8867520093917847\nTH pt time = 3.1505179964005947\n[1, 1024, 7, 7, 2048, 1, 1, 1, 1, 0, 0, 1]\nMKLDNN pt time = 4.118196591734886\nTH pt time = 3.46621660515666\n```\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D22250817\n\nfbshipit-source-id: c9dc61b633e11a378a05810d711a696effd7f02b", "pr_number": "40610", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["fb-exported", "merged"]}, "abea7cd561": {"title": "msvc anonymous namespace bug (#41199)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41199\n\nworkaround for: https://developercommunity.visualstudio.com/content/problem/900452/variable-in-anonymous-namespace-has-external-linka.html\n\nTest Plan: CI green, ovrsource green\n\nReviewed By: malfet\n\nDifferential Revision: D22462050\n\nfbshipit-source-id: 11a2fd6a4db1f29ce350699cfc3121dc89ab7ef6", "pr_number": "41199", "files_changed": ["aten/src/ATen/native/cpu/ScatterGatherKernel.cpp"], "labels": ["fb-exported", "merged"]}, "ac3542fa59": {"title": "Define PSIMD_SOURCE_DIR when including FP16 (#41233)", "body": "Summary:\nAvoids a superflous redownload when *NNPACK is not used (e.g. on Power)\n\nExample: https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le/1128/consoleFull\nSearch for \"Downloading PSimd\"\n\nSee also https://github.com/pytorch/pytorch/issues/41178\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41233\n\nDifferential Revision: D22488833\n\nPulled By: malfet\n\nfbshipit-source-id: 637291419ddd3b2a8dc25e211a4ebbba955e5855", "pr_number": "41233", "files_changed": ["cmake/Dependencies.cmake"], "labels": ["merged", "open source", "triaged"]}, "67f5d68fdf": {"title": "Revert D22465221: [pytorch][PR] Reducing size of docker Linux image", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22465221 (https://github.com/pytorch/pytorch/commit/7c143e5d3e46a61b9188d3f2d89ba7c365778a23)\n\nOriginal commit changeset: 487542597294\n\nfbshipit-source-id: f085763a13497bd5ceea0ed6aa7676320c8806bf", "pr_number": null, "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": []}, "6cbb92494d": {"title": "Better THGeneric.h generation rules in bazel (#41285)", "body": "Summary:\nIt  doesn't do a good job of checking BLAS library capabilities, so hardcode the undef of BLAS_F2C\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41285\n\nDifferential Revision: D22489781\n\nPulled By: malfet\n\nfbshipit-source-id: 13a14f31e08d7f9ded49731e4fd23663bac75cd2", "pr_number": "41285", "files_changed": ["BUILD.bazel"], "labels": ["merged"]}, "877a59967f": {"title": "Ampere has CUDA_MAX_THREADS_PER_SM == 2048 (#41138)", "body": "Summary:\nSee: https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf\npage 44, table 5\n![image](https://user-images.githubusercontent.com/1032377/86958633-56051580-c111-11ea-94da-c726a61dc00a.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41138\n\nDifferential Revision: D22488904\n\nPulled By: malfet\n\nfbshipit-source-id: 97bd585d91e1a368f51aa6bd52081bc57d42dbf8", "pr_number": "41138", "files_changed": ["c10/macros/Macros.h"], "labels": ["merged", "open source", "triaged"]}, "1c098ae339": {"title": "Fix arg type annotations in jit.trace and onnx.export (#41093)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/40350\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41093\n\nDifferential Revision: D22477950\n\nPulled By: malfet\n\nfbshipit-source-id: f1141c129b6d9efb373d22291b441df86c529ddd", "pr_number": "41093", "files_changed": ["torch/jit/_trace.py", "torch/onnx/__init__.py"], "labels": ["merged", "module: docs", "oncall: jit", "open source"]}, "d1f06da9b7": {"title": "Solve log2(x:int) ambiguity by using log2(float(x)) (#41295)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41295\n\nDifferential Revision: D22490995\n\nPulled By: malfet\n\nfbshipit-source-id: 17037e551ce5986f3162389a61932099563c02a7", "pr_number": "41295", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cu"], "labels": ["merged"]}, "095886fa42": {"title": "[caffe2] Fix the issues when using CUB RadixSort (#41299)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41299\n\nWhen using `cub::DeviceRadixSort::SortPairs` (https://nvlabs.github.io/cub/structcub_1_1_device_radix_sort.html), the `end_bit` argument, or the most-significant bit index (exclusive) needed for key comparison, should be passed with  `int(log2(float(num_rows)) + 1)` instead of `int(log2(float(num_indice)) + 1)`. This is because all the values in indices array are guaranteed to be less than num_rows (hash_size), not num_indices. Thanks ngimel for pointing this point and thanks malfet for quickly fixing the log2() compilation issues.\n\nNote:\nAn optional bit subrange [begin_bit, end_bit) of differentiating key bits can be specified. This can reduce overall sorting overhead and yield a corresponding performance improvement.\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient \\(caffe2\\.caffe2\\.fb\\.net_transforms\\.tests\\.fuse_sparse_ops_test\\.TestFuseSparseOps\\)' --print-passing-details\n```\n\nReviewed By: malfet\n\nDifferential Revision: D22491662\n\nfbshipit-source-id: 4fdabe86244c948af6244f9bd91712844bf1dec1", "pr_number": "41299", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cu"], "labels": ["fb-exported", "merged"]}, "a1ed6e1eb3": {"title": "Revert D22467871: add check for duplicated op registration in JIT", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22467871 (https://github.com/pytorch/pytorch/commit/a548c6b18ff5a337b6329ed7b2381cfb4d3da2ed)\n\nOriginal commit changeset: 9b7a40a217e6\n\nfbshipit-source-id: b594d4d0a079f7e24ef0efb45476ded2838cbef1", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/operator.cpp"], "labels": []}, "e544bf2924": {"title": "fix the range of the random weights used in the int8fc test (#41303)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41303\n\nthe error came from I0710 18:02:48.025024 1780875 NNPIOptions.cpp:49]\n[NNPI_LOG][D] [KS] convert_base_kernel_ivp.cpp(524): Output Scale 108240.101562\nis out of valid range +-(Min 0.000061 Max 65504.000000)!!!\n\nSeems like the weights we are using are too small, thus generating scaling\nfactors out of the range of fp16 (>65k). I am tentatively increasing this\nfactor to a higher value to avoid this. (10x bigger)\n\nAlso increased max_examples to 100\n\nTest Plan: ran this test\n\nReviewed By: yinghai\n\nDifferential Revision: D22492481\n\nfbshipit-source-id: c0f9e59b0e70895ab787868ef1d87e6e80106554", "pr_number": "41303", "files_changed": ["caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py"], "labels": ["fb-exported", "merged"]}, "28291d3cf8": {"title": "[caffe2] Revert D22220798 (#41302)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41302\n\nTest Plan:\n```\nbuck test //caffe2/caffe2/fb/predictor:black_box_predictor_test\n```\n\nDifferential Revision: D22492356\n\nfbshipit-source-id: efcbc3c67abda5cb9da47e633804a4800d92f89b", "pr_number": "41302", "files_changed": ["caffe2/opt/tvm_transformer.cc", "caffe2/predictor/InferenceGraph.h", "caffe2/predictor/transforms.cc", "caffe2/predictor/transforms.h"], "labels": ["fb-exported", "merged"]}, "c864158475": {"title": "Add fp16 support to SparseLengthSum PyTorch operator (#41058)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41058\n\nSparseLengthSum PyTorch operator just accept float and double type before, this diff add fp16 support to SparseLengthSum PT operator.\n\nReviewed By: jianyuh\n\nDifferential Revision: D22387253\n\nfbshipit-source-id: 2a7d03ceaadbb7b04077cff72ab77da6457ba989", "pr_number": "41058", "files_changed": ["caffe2/operators/segment_reduction_op_gpu.cuh"], "labels": ["fb-exported", "merged"]}, "edcf2cdf86": {"title": "[quant] dequantize support list and tuple of tensors (#41079)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41079\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22420700\n\nfbshipit-source-id: bc4bf0fb47dcf8b94b11fbdc91e8d5a75142b7be", "pr_number": "41079", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/vararg_functions.cpp", "torch/csrc/jit/runtime/vararg_functions.h"], "labels": ["merged", "oncall: jit"]}, "106b0b6a62": {"title": "Op to create quant scheme blob (#40760)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40760\n\nAdd op to create a quant scheme.\n\nTest Plan:\nbuck test mode/opt caffe2/caffe2/quantization/server:int8_quant_scheme_blob_fill_test\n\n{F241838981}\n\nReviewed By: csummersea\n\nDifferential Revision: D22228154\n\nfbshipit-source-id: 1b7a02c06937c68e2fcccf77eb10a965300ed732", "pr_number": "40760", "files_changed": ["caffe2/quantization/server/int8_quant_scheme_blob_fill.cc", "caffe2/quantization/server/int8_quant_scheme_blob_fill.h", "caffe2/quantization/server/int8_quant_scheme_blob_fill_test.py", "caffe2/quantization/server/pybind.cc"], "labels": ["fb-exported", "merged"]}, "4b4184fc69": {"title": "[quant][graphmode] use RemoveMutation to remove append (#41161)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41161\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D22446714\n\nfbshipit-source-id: 15da28ef773300a141603d67a1c4524f1ec32239", "pr_number": "41161", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/finalize.cpp", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/insert_observers.cpp"], "labels": ["merged", "oncall: jit"]}, "402be850a8": {"title": "[quant] Adding zero point type check for per channel quantization (#40811)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40811\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22319417\n\nPulled By: z-a-f\n\nfbshipit-source-id: 7be3a511ddd33b5fe749a83166bbc5874d1bd539", "pr_number": "40811", "files_changed": ["aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp"], "labels": ["merged", "oncall: quantization"]}, "5e72ebeda3": {"title": "Fake Quantization Per Tensor Kernel Core Implementation (CPU) (#41029)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41029\n\nThis diff contains the core implementation for the fake quantizer per tensor kernel that supports back propagation on the scale and zero point.\n\nTest Plan:\nOn a devvm, use:\n- `buck test //caffe2/test:quantization -- learnable_forward_per_tensor`\n- `buck test //caffe2/test:quantization -- learnable_backward_per_tensor`\n\nReviewed By: z-a-f\n\nDifferential Revision: D22394145\n\nfbshipit-source-id: f6748b635b86679aa9174a8065e6be5e20a95d81", "pr_number": "41029", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/fake_quant_affine.h", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp", "test/quantization/test_workflow_module.py", "tools/autograd/derivatives.yaml"], "labels": ["fb-exported", "merged"]}, "fa153184c8": {"title": "Fake Quantization Per Channel Kernel Core Implementation (CPU) (#41037)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41037\n\nThis diff contains the core implementation for the fake quantizer per channel kernel that supports back propagation on the scale and zero point.\n\nTest Plan:\nOn a devvm, use:\n- `buck test //caffe2/test:quantization -- learnable_forward_per_channel`\n- `buck test //caffe2/test:quantization -- learnable_backward_per_channel`\n\nReviewed By: z-a-f\n\nDifferential Revision: D22395665\n\nfbshipit-source-id: 280c2405d04adfeda9fb9cfc94d89e8d868e0d41", "pr_number": "41037", "files_changed": ["aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/fake_quant_affine.h", "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp", "test/quantization/test_workflow_module.py", "tools/autograd/derivatives.yaml"], "labels": ["fb-exported", "merged"]}, "98df9781a7": {"title": "Impl for ParameterList (#41259)", "body": "Summary:\nThis is a new PR for https://github.com/pytorch/pytorch/issues/40850, https://github.com/pytorch/pytorch/issues/40987 and https://github.com/pytorch/pytorch/issues/41206(I unintentionally closed), as I have some issues for rebates for that one. Very sorry about that. And I have fixed the tests failed in that PR.\n\nThis diff contains the implementation of C++ API for ParameterList from https://github.com/pytorch/pytorch/issues/25883.\nRefer to the Python API: https://github.com/pytorch/pytorch/blob/bc9e8af21875dafafe9bbd25c8f542b20b2e660f/torch/nn/modules/container.py#L376\nNot sure about some naming difference between C++ API and Python API, like `append`, should it be called `push_back`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41259\n\nTest Plan: Add unit tests in this diff\n\nDifferential Revision: D22495780\n\nPulled By: glaringlee\n\nfbshipit-source-id: 79ea3592db640f35477d445ecdaeafbdad814bec", "pr_number": "41259", "files_changed": ["test/cpp/api/CMakeLists.txt", "test/cpp/api/parameterlist.cpp", "torch/csrc/api/include/torch/nn/modules.h", "torch/csrc/api/include/torch/nn/modules/container/parameterlist.h"], "labels": ["merged", "open source"]}, "67a4f375cd": {"title": "Pass the number of indices but not embedding size in PyTorch operator (#41315)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41315\n\nWe should pass the number of indices but not embedding size in SparseAdagrad fused PyTorch operator\n\nReviewed By: jianyuh\n\nDifferential Revision: D22495422\n\nfbshipit-source-id: ec5d3a5c9547fcd8f95106d912b71888217a5af0", "pr_number": "41315", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cuh"], "labels": ["fb-exported", "merged"]}, "dea39b596e": {"title": "reduce logging for layernorm (#41305)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41305\n\nadded a warning message when layernorm under/overflows, which is what\nnnpi does, reducing the frequency of the logging to every 1000\n\nTest Plan: compilation\n\nReviewed By: yinghai\n\nDifferential Revision: D22492726\n\nfbshipit-source-id: 9343beeae6e65bf3846c6b3d2edd2a08dac85ed6", "pr_number": "41305", "files_changed": ["caffe2/contrib/fakelowp/layernorm_fp16_fake_op.cc"], "labels": ["fb-exported", "merged"]}, "86d803a9da": {"title": ".cirlceci: Setup nvidia runtime for cu as well (#41268)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41268\n\nWe also want nvidia runtime packages to get installed when the\nBUILD_ENVIRONMENT also includes \"*cu*\"\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22505885\n\nPulled By: seemethere\n\nfbshipit-source-id: 4d8e70ed8aed9c6fd1828bc13cf7d5b0f8f50a0a", "pr_number": "41268", "files_changed": [".circleci/scripts/setup_ci_environment.sh"], "labels": ["ci/binaries", "merged", "module: ci"]}, "d04a2e4dae": {"title": "Back out \"Revert D22329069: Self binning histogram\" (#41313)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41313\n\nThis diff backs out the backout diff.  The failure was due to C++ `or`\nnot being supported in MSVC. This is now replaced with ||\n\nOriginal commit changeset: fc7f3f8c968d\n\nTest Plan: Existing unit tests, check github CI.\n\nReviewed By: malfet\n\nDifferential Revision: D22494777\n\nfbshipit-source-id: 3271288919dc3a6bfb82508ab9d021edc910ae45", "pr_number": "41313", "files_changed": ["caffe2/operators/self_binning_histogram_op.cc", "caffe2/operators/self_binning_histogram_op.h", "caffe2/python/operator_test/self_binning_histogram_test.py"], "labels": ["fb-exported", "merged"]}, "fb9e44f8dd": {"title": "Add support for float[]? arguments in native_functions.yaml (#37175)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37175\n\nghstack-source-id: 106938114\n\nTest Plan: Upcoming diffs use this for upsampling.\n\nDifferential Revision: D21209994\n\nfbshipit-source-id: 1a71c07e45e28772a2bbe450b68280dcc0fe2def", "pr_number": "37175", "files_changed": ["aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/native/TestOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "test/test_native_functions.py", "tools/autograd/gen_autograd_functions.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/Functions.h", "tools/autograd/templates/python_nn_functions.cpp", "tools/jit/gen_unboxing_wrappers.py", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tracer.h", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["merged", "oncall: jit"]}, "7183fd20f8": {"title": "Add interpolate-style overloads to aten::upsample* ops (#37176)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37176\n\nThe non-deprecated user-facing interface to these ops (F.interpolate)\nhas a good interface: output size and scale are both specified as\na scalar or list, and exactly one must be present.  These aten ops\nhave an older, clunkier interface where output size is required and\nscales are specified as separate optional scalars per dimension.\n\nThis change adds new overloads to the aten ops that match the interface\nof interpolate.  The plan is to eventually remove the old overloads,\nresulting in roughly net-zero code added.  I also believe it is possible\nto push this interface down further, eliminating multiple optional<double>\narguments, and simplifying the implementations.\n\nThe rollout plan is to land this, wait for a reasonable interval for\nforwards-compatibility (maybe 1 week?), land the change that updates\ninterpolate to call these overloads, wait for a reasonable interval\nfor backwards-compatibility (maybe 6 months?), then remove the old\noverloads.\n\nThis diff does not add the `.out` variants of the ops because they\nare not currently accessible through any user-facing API.\n\nghstack-source-id: 106938113\n\nTest Plan:\ntest_nn covers these ops fairly well, so that should prevent this diff\nfrom breaking anything on its own.\n\ntest_nn on the next diff in the stack actually uses these new overloads,\nso that should validate that they are actually correct.\n\nDifferential Revision: D21209989\n\nfbshipit-source-id: 2b74d230401f071364eb05e138cdaa55279cfe91", "pr_number": "37176", "files_changed": ["aten/src/ATen/native/UpSample.cpp", "aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/UpSampleBicubic2d.cpp", "aten/src/ATen/native/UpSampleBilinear2d.cpp", "aten/src/ATen/native/UpSampleLinear1d.cpp", "aten/src/ATen/native/UpSampleNearest1d.cpp", "aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/UpSampleTrilinear3d.cpp", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest2d.cu", "aten/src/ATen/native/cuda/UpSampleNearest3d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest2d.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest3d.cpp", "tools/autograd/derivatives.yaml", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["merged"]}, "c451ddaeda": {"title": "Add shape inference functions for int8 quantization related ops (#41215)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41215\n\nTo unblock int8 model productization on accelerators, we need the shape and type info for all the blobs after int8 quantization. This diff added shape inference functions for int8 quantization related ops.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/quantization/server:int8_gen_quant_params_test\nbuck test caffe2/caffe2/quantization/server:fully_connected_dnnlowp_op_test\n```\n\nReviewed By: hx89\n\nDifferential Revision: D22467487\n\nfbshipit-source-id: 8298abb0df3457fcb15df81f423f557c1a11f530", "pr_number": "41215", "files_changed": ["caffe2/quantization/server/fbgemm_pack_op.cc", "caffe2/quantization/server/fully_connected_dnnlowp_op_test.py", "caffe2/quantization/server/int8_gen_quant_params.cc", "caffe2/quantization/server/int8_gen_quant_params_test.py"], "labels": ["fb-exported", "merged"]}, "0c77bd7c0b": {"title": "Quantization: preserving pre and post forward hooks (#37233)", "body": "Summary:\n1. While do convert() preserve module's **pre and post forward** hooks\n2. While do fusion preserve only module's **pre forward** hooks (because after fusion output no longer the same)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37233\n\nDifferential Revision: D22425141\n\nPulled By: jerryzh168\n\nfbshipit-source-id: e69b81821d507dcd110d2ff3594ba94b9593c8da", "pr_number": "37233", "files_changed": ["test/quantization/test_quantize.py", "torch/quantization/fuse_modules.py", "torch/quantization/quantize.py"], "labels": ["merged", "oncall: quantization", "open source", "triaged"]}, "6e6931e234": {"title": "fix duplicate extern sdot and missing flags (#41195)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41195\n\n`BLAS_F2C` is set in `THGeneral.h`.\n`sdot` redefined with double return type in the case that `BLAS_F2C` is set and `BLAS_USE_CBLAS_DOT` is not.\n\nTest Plan: CircleCI green, ovrsource green\n\nReviewed By: malfet\n\nDifferential Revision: D22460253\n\nfbshipit-source-id: 75f17b3e47da0ed33fcadc2843a57ad616f27fb5", "pr_number": "41195", "files_changed": ["aten/src/ATen/native/BlasKernel.cpp"], "labels": ["fb-exported", "merged"]}, "4196605776": {"title": "helper function to print out all DDP-relevant env vars (#41297)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41297\n\nGH issue: https://github.com/pytorch/pytorch/issues/40105\n\nAdd a helper function to DDP to print out all relevant env vars for debugging\n\nTest Plan:\ntest through unittest, example output:\n ---\nenv:RANK=3\nenv:LOCAL_RANK=N/A\nenv:WORLD_SIZE=N/A\nenv:MASTER_PORT=N/A\nenv:MASTER_ADDR=N/A\nenv:CUDA_VISIBLE_DEVICES=N/A\nenv:GLOO_SOCKET_IFNAME=N/A\nenv:GLOO_DEVICE_TRANSPORT=N/A\nenv:NCCL_SOCKET_IFNAME=N/A\nenv:NCCL_BLOCKING_WAIT=N/A\n...\n ---\n\nReviewed By: mrshenli\n\nDifferential Revision: D22490486\n\nfbshipit-source-id: 5dc7d2a18111e5a5a12a1b724d90eda5d35acd1c", "pr_number": "41297", "files_changed": ["test/distributed/test_distributed.py", "torch/nn/parallel/distributed.py"], "labels": ["fb-exported", "merged"]}, "d601325de4": {"title": "update operators in the mapping to fp16 emulation", "body": "Summary: add logit and swish to this list\n\nTest Plan: f203925461\n\nReviewed By: amylittleyang\n\nDifferential Revision: D22506814\n\nfbshipit-source-id: b449e4ea16354cb76915adb01cf317cffb494733", "pr_number": null, "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": []}, "b6e1944d35": {"title": ".circleci: Explicitly remove nvidia apt repos (#41367)", "body": "Summary:\nThe nvidia apt repositories seem to be left over on the amd nodes so\nlet's just go ahead and remove them explicitly if we're not testing for\nCUDA\n\nExample: https://app.circleci.com/pipelines/github/pytorch/pytorch/190222/workflows/8f75b5cd-1afd-43dc-9fa7-f7b058f07b46/jobs/6223743/steps\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41367\n\nReviewed By: ezyang\n\nDifferential Revision: D22513844\n\nPulled By: seemethere\n\nfbshipit-source-id: 6da4dd8423de5f7ec80c7904187cf80c1b91ab14", "pr_number": "41367", "files_changed": [".circleci/scripts/setup_ci_environment.sh"], "labels": ["merged", "module: ci"]}, "ca1b8ebbcb": {"title": "move misc implementation out of `jit/__init__.py` (#41154)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41154\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D22445213\n\nPulled By: suo\n\nfbshipit-source-id: 200545715c5ef13beb1437f49e01efb21498ddb7", "pr_number": "41154", "files_changed": ["test/jit/test_models.py", "test/quantization/test_quantize.py", "test/test_jit.py", "test/test_jit_fuser.py", "test/test_jit_fuser_te.py", "torch/_jit_internal.py", "torch/_ops.py", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/distributed/rpc/api.py", "torch/jit/__init__.py", "torch/jit/_async.py", "torch/jit/_freeze.py", "torch/jit/_fuser.py", "torch/jit/_recursive.py", "torch/jit/_script.py", "torch/jit/_trace.py", "torch/jit/supported_ops.py", "torch/testing/_internal/distributed/rpc/rpc_test.py", "torch/testing/_internal/jit_metaprogramming_utils.py", "torch/testing/_internal/jit_utils.py"], "labels": ["merged", "oncall: jit"]}, "5f6c6ed157": {"title": "Fix FC issue (#41198)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41198\n\nhttps://github.com/pytorch/pytorch/pull/39611 unified signatures of some ops taking TensorOptions arguments by making them optional.\nThat has FC implications but only for models writting with a PyTorch version after that version (see explanation in description of that PR).\n\nHowever, it also changed the default from `pin_memory=False` to `pin_memory=None`, which actually breaks FC for preexisting models too if they're re-exported with a newer PyTorch,\nbecause we materialize default values when exporting. This is bad.\n\nThis PR reverts that particular part of https://github.com/pytorch/pytorch/pull/39611 to revert the FC breakage.\nghstack-source-id: 107475024\n\nTest Plan: waitforsandcastle\n\nReviewed By: bhosmer\n\nDifferential Revision: D22461661\n\nfbshipit-source-id: ba2776267c3bba97439df66ecb50be7c1971d20d", "pr_number": "41198", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "34e11b45c9": {"title": "Remove thrust casting from static_cast_with_inter_type (#39905)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39905\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22510307\n\nPulled By: ngimel\n\nfbshipit-source-id: 34357753fca4f2a8d5e2b1bbf8de8d642ca9bb20", "pr_number": "39905", "files_changed": ["c10/util/TypeCast.h"], "labels": ["merged", "module: complex", "open source", "triaged"]}, "80d5b3785b": {"title": "Add torch.logit function (#41062)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41062\n\nAdd torch.logit function\n\nTest Plan: buck test mode/dev-nosan //caffe2/test:torch -- \"logit\"\n\nReviewed By: hl475\n\nDifferential Revision: D22406912\n\nfbshipit-source-id: b303374f4c68850eb7477eb0645546a24b844606", "pr_number": "41062", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "benchmarks/operator_benchmark/pt/unary_test.py", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["fb-exported", "merged", "module: numpy"]}, "c20426f86d": {"title": "Fix torch.cuda.check_error type errors (#41330)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41330\n\n`torch.cuda.check_error` is annotated as taking an `int` as argument but when running `torch.cuda.check_error(34)` one would get:\n```\nTypeError: cudaGetErrorString(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch._C._cudart.cudaError) -> str\n\nInvoked with: 34\n```\nEven if one explicitly casted the argument, running `torch.cuda.check_error(torch._C._cudart.cudaError(34))` would give:\n```\nAttributeError: 'str' object has no attribute 'decode'\n```\n\nThis PR fixes both issues (thus allowing `check_error` to be called with a un-casted int) and adds a test.\nghstack-source-id: 107628709\n\nTest Plan: Unit tests\n\nReviewed By: ezyang\n\nDifferential Revision: D22500549\n\nfbshipit-source-id: 9170c1e466dd554d471e928b26eb472a712da9e1", "pr_number": "41330", "files_changed": ["test/test_cuda.py", "torch/csrc/cuda/shared/cudart.cpp", "torch/cuda/__init__.py"], "labels": ["merged"]}, "e888c3bca1": {"title": "Update torch.set_default_dtype doc (#41263)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41263\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22482989\n\nPulled By: anjali411\n\nfbshipit-source-id: 2aadfbb84bbab66f3111970734a37ba74d817ffd", "pr_number": "41263", "files_changed": ["torch/__init__.py"], "labels": ["merged"]}, "13dd53b3d2": {"title": "[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D22523334\n\nfbshipit-source-id: e687e26f68a4f923164a51ce0b69ec1d131b9022", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": []}, "befb22790f": {"title": "Fix a number of deprecation warnings (#40179)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40179\n\n- Pass no-psabi to shut up GCC about # Suppress \"The ABI for passing\n  parameters with 64-byte alignment has changed in GCC 4.6\"\n- Fix use of deprecated data() accessor (and minor optimization: hoist\n  accessor out of loop)\n- Undeprecate NetDef.num_workers, no one is serious about fixing these\n- Suppress warnings about deprecated pthreadpool types\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22234138\n\nPulled By: ezyang\n\nfbshipit-source-id: 6a1601b6d7551a7e6487a44ae65b19acdcb7b849", "pr_number": "40179", "files_changed": ["CMakeLists.txt", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "caffe2/proto/caffe2.proto"], "labels": ["merged"]}, "535e8814a4": {"title": "Add operators for LiteLMLSTM to Lite Interpreter (#41270)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41270\n\nThe Smart Keyboard model for Oculus requires operators previously not in the lite interpreter: aten::exp (for floats), aten::ord, aten::lower, aten::__contains__.str_list, aten::slice.str, aten::strip, aten::split.str, and aten::__getitem__.str.\n\nTest Plan:\nVerify smart keyboard model can be used:\nCheck out next diff in stack and follow test instructions there\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22289812\n\nfbshipit-source-id: df574d5af4d4fafb40f0e209b66a93fe02d83020", "pr_number": "41270", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp", "torch/csrc/jit/runtime/register_string_ops.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "87bf04fe12": {"title": "AvgPool: Ensure all cells are valid in ceil mode (#41368)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/36977\n\nThis avoid the division by zero that was causing NaNs to appear in the output. `AvgPooling2d` and `AvgPooling3d` both had this issue on CPU and CUDA.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41368\n\nReviewed By: ailzhang\n\nDifferential Revision: D22520013\n\nPulled By: ezyang\n\nfbshipit-source-id: 3ece7829f858f5bc17c2c1d905266ac510f11194", "pr_number": "41368", "files_changed": ["aten/src/ATen/native/AveragePool2d.cpp", "aten/src/ATen/native/AveragePool3d.cpp", "aten/src/ATen/native/cuda/AveragePool2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "test/test_nn.py"], "labels": ["merged", "open source"]}, "0e7b9d4ff8": {"title": "Fix logit doc (#41384)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41384\n\nFix logit doc\n\nTest Plan: unittest\n\nReviewed By: houseroad\n\nDifferential Revision: D22521730\n\nfbshipit-source-id: 270462008c6ac73cd90aecd77c5de112fc93ea8d", "pr_number": "41384", "files_changed": ["torch/_torch_docs.py"], "labels": ["fb-exported", "merged"]}, "4972cf06a2": {"title": "[JIT] Add out-of-source-tree to_backend tests (#41145)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41145\n\n**Summary**\nThis commit adds out-of-source-tree tests for `to_backend`. These tests check\nthat a Module can be lowered to a backend, exported, loaded (in both\nPython and C++) and executed.\n\n**Fixes**\nThis commit fixes #40067.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D22510076\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: f65964ef3092a095740f06636ed5b1eb0884492d", "pr_number": "41145", "files_changed": [".jenkins/pytorch/build.sh", ".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-test-helpers/test_custom_backend.bat", ".jenkins/pytorch/win-test.sh", "test/cpp/jit/test_backend.cpp", "test/custom_backend/CMakeLists.txt", "test/custom_backend/backend.py", "test/custom_backend/custom_backend.cpp", "test/custom_backend/custom_backend.h", "test/custom_backend/test_custom_backend.cpp", "test/custom_backend/test_custom_backend.py"], "labels": ["merged", "oncall: jit"]}, "5f146a4125": {"title": "fix include file path in unary ops", "body": "Summary: fix include file path in unary ops\n\nTest Plan: compile\n\nReviewed By: amylittleyang\n\nDifferential Revision: D22527312\n\nfbshipit-source-id: 589efd2231ff8bd3133cb7844738429927ecee68", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/unary_fp16_fake_op.cc"], "labels": []}, "c528faac7d": {"title": "[ROCm] Skip problematic mgpu tests on ROCm3.5 (#41409)", "body": "Summary:\nnccl tests and parallelize_bmuf_distributed test are failing on rocm3.5.1. Skipping these tests to upgrade the CI to rocm3.5.1\n\njeffdaily sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41409\n\nReviewed By: orionr\n\nDifferential Revision: D22528928\n\nPulled By: seemethere\n\nfbshipit-source-id: 928196b7a62a441d391e69f54b278313ecc75d77", "pr_number": "41409", "files_changed": ["caffe2/contrib/nccl/nccl_ops_test.py", "caffe2/python/parallelize_bmuf_distributed_test.py"], "labels": ["merged", "module: rocm", "open source"]}, "288ece89e1": {"title": "Enable TF32 support for cuBLAS (#40800)", "body": "Summary:\nBenchmark on a fully connected network and torchvision models (time in seconds) on GA100:\n\n| model              | batch size | forward(TF32) | forward(FP32) | backward(TF32) | backward(FP32) |\n|--------------------|------------|---------------|---------------|----------------|----------------|\n| FC 512-128-32-8    | 512        | 0.000211      | 0.000321      | 0.000499       | 0.000532       |\n| alexnet            | 512        | 0.0184        | 0.0255        | 0.0486         | 0.0709         |\n| densenet161        | 128        | 0.0665        | 0.204         | 0.108          | 0.437          |\n| googlenet          | 256        | 0.0925        | 0.110         | 0.269          | 0.326          |\n| inception_v3       | 256        | 0.155         | 0.214         | 0.391          | 0.510          |\n| mnasnet1_0         | 512        | 0.108         | 0.137         | 0.298          | 0.312          |\n| mobilenet_v2       | 512        | 0.114         | 0.294         | 0.133          | 0.303          |\n| resnet18           | 512        | 0.0722        | 0.100         | 0.182          | 0.228          |\n| resnext50_32x4d    | 256        | 0.170         | 0.237         | 0.373          | 0.479          |\n| shufflenet_v2_x1_0 | 512        | 0.0463        | 0.0473        | 0.125          | 0.123          |\n| squeezenet1_0      | 512        | 0.0870        | 0.0948        | 0.205          | 0.214          |\n| vgg16              | 256        | 0.167         | 0.234         | 0.401          | 0.502          |\n| wide_resnet50_2    | 512        | 0.186         | 0.310         | 0.415          | 0.638          |\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40800\n\nReviewed By: mruberry\n\nDifferential Revision: D22517785\n\nPulled By: ngimel\n\nfbshipit-source-id: 87334c8935616f72a6af5abbd3ae69f76923dc3e", "pr_number": "40800", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CublasHandlePool.cpp", "aten/src/ATen/native/cuda/MiscUtils.h", "aten/src/THC/THCBlas.cu", "docs/source/notes/cuda.rst", "test/test_cuda.py", "test/test_torch.py", "torch/backends/cuda/__init__.py", "torch/csrc/Module.cpp", "torch/testing/_internal/common_cuda.py"], "labels": ["merged", "open source"]}, "e2c4c2f102": {"title": "addmm: Reduce constant time overhead (#41374)", "body": "Summary:\nFixes the overhead reported by ngimel in https://github.com/pytorch/pytorch/pull/40927#issuecomment-657709646\n\nAs it turns out, `Tensor.size(n)` has more overhead than `Tensor.sizes()[n]`. Since addmm does a lot of introspection of the input matrix sizes and strides, this added up to a noticeable (~1 us) constant time overhead.\n\nWith this change, a 1x1 matmul takes 2.85 us on my machine compared to 2.90 us on pytorch 1.5.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41374\n\nReviewed By: ailzhang\n\nDifferential Revision: D22519924\n\nPulled By: ngimel\n\nfbshipit-source-id: b29504bee7de79ce42e5e50f91523dde42b073b7", "pr_number": "41374", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp"], "labels": ["merged", "open source"]}, "c17670ac50": {"title": "unsafe_split, unsafe_split_with_sizes, unsafe_chunk operations (#39299)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/36403\n\nCopy-paste of the issue description:\n\n* Escape hatch: Introduce unsafe_* version of the three functions above that have the current behavior (outputs not tracked as views). The documentation will explain in detail why they are unsafe and when it is safe to use them. (basically, only the outputs OR the input can be modified inplace but not both. Otherwise, you will get wrong gradients).\n* Deprecation: Use the CreationMeta on views to track views created by these three ops and throw warning when any of the views is modified inplace saying that this is deprecated and will raise an error soon. For users that really need to modify these views inplace, they should look at the doc of the unsafe_* version to make sure their usecase is valid:\n  * If it is not, then pytorch is computing wrong gradients for their use case and they should not do inplace anymore.\n  * If it is, then they can use the unsafe_* version to keep the current behavior.\n* Removal: Use the CreationMeta on view to prevent any inplace on these views (like we do for all other views coming from multi-output Nodes). The users will still be able to use the unsafe_ versions if they really need to do this.\n\nNote about BC-breaking:\n- This PR changes the behavior of the regular function by making them return proper views now. This is a modification that the user will be able to see.\n- We skip all the view logic for these views and so the code should behave the same as before (except the change in the `._is_view()` value).\n- Even though the view logic is not performed, we do raise deprecation warnings for the cases where doing these ops would throw an error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39299\n\nDifferential Revision: D22432885\n\nPulled By: albanD\n\nfbshipit-source-id: 324aef091b32ce69dd067fe9b13a3f17d85d0f12", "pr_number": "39299", "files_changed": ["aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensor_view.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source", "topic: bc-breaking", "triaged"]}, "05207b7371": {"title": ".circleci: Re-split postnightly into its own thing (#41354)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41354\n\nThe nightly pipeline has the potential to be flaky and thus the html\npages have the potential not to be updated.\n\nThis should actually be done as an automatic lambda job that runs\nwhenever the S3 bucket updates but this is intermediate step in order to\nget there.\n\nCloses https://github.com/pytorch/pytorch/issues/40998\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22530283\n\nPulled By: seemethere\n\nfbshipit-source-id: 0d80b7751ede83e6dd466690cc0a0ded68f59c5d", "pr_number": "41354", "files_changed": [".circleci/cimodel/data/binary_build_definitions.py", ".circleci/config.yml", ".circleci/generate_config_yml.py"], "labels": ["merged", "module: ci"]}, "c68c5ea0e6": {"title": "Upgrade cpp docs Sphinx/breathe/exhale to latest version (#41312)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41312\n\nI was hoping that exhale had gotten incremental recompilation\nin its latest version, but experimentally this does not seem\nto have been the case.  Still, I had gotten the whole shebang\nto be working on the latest version of these packages, so might\nas well land the upgrade.  There was one bug in Optional.h that\nI had to fix; see the cited bug report.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D22526349\n\nPulled By: ezyang\n\nfbshipit-source-id: d4169c2f48ebd8dfd8a593cc8cd232224d008ae9", "pr_number": "41312", "files_changed": [".circleci/scripts/cpp_doc_push_script.sh", ".gitignore", "c10/util/Optional.h", "docs/cpp/requirements.txt"], "labels": ["merged"]}, "144f04e7ef": {"title": "Fix qobserver test", "body": "Summary: Change the device config in qobserver test to a string to honor --device flag.\n\nTest Plan: buck run caffe2/benchmarks/operator_benchmark/pt:qobserver_test  -- --iterations 1 --device cpu\n\nReviewed By: ngimel\n\nDifferential Revision: D22536379\n\nfbshipit-source-id: 8926b2393be1f52f9183f8205959a3ff18e3ed2a", "pr_number": null, "files_changed": ["benchmarks/operator_benchmark/pt/qobserver_test.py"], "labels": []}, "359cdc20e2": {"title": "Revert D22432885: [pytorch][PR] unsafe_split, unsafe_split_with_sizes, unsafe_chunk operations", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22432885 (https://github.com/pytorch/pytorch/commit/c17670ac50cc0dc493ab7248a2fe452b092ae405)\n\nOriginal commit changeset: 324aef091b32\n\nfbshipit-source-id: 6b7c52bde46932e1cf77f61e7035d8a641b0beb6", "pr_number": null, "files_changed": ["aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensor_view.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/onnx/symbolic_opset9.py"], "labels": []}, "a0f110190c": {"title": "clamp Categorical logit from -inf to min_fifo when calculating entropy (#41002)", "body": "Summary:\nFixes gh-40553 by clamping logit values when calculating Categorical.entropy\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41002\n\nReviewed By: mruberry\n\nDifferential Revision: D22436432\n\nPulled By: ngimel\n\nfbshipit-source-id: 08b7c7b0c15ab4e5a56b3a8ec0d0237ad360202e", "pr_number": "41002", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "test/test_distributions.py", "torch/csrc/jit/ir/node_hashing.cpp", "torch/distributions/categorical.py"], "labels": ["merged", "open source", "triaged"]}, "4ddf27ba48": {"title": "[op-bench] check device attribute in user inputs", "body": "Summary: The device attribute in the op benchmark can only include 'cpu' or 'cuda'. So adding a check in this diff.\n\nTest Plan: buck run caffe2/benchmarks/operator_benchmark:benchmark_all_test -- --warmup_iterations 1 --iterations 1\n\nReviewed By: ngimel\n\nDifferential Revision: D22538252\n\nfbshipit-source-id: 3e5af72221fc056b8d867321ad22e35a2557b8c3", "pr_number": null, "files_changed": ["benchmarks/operator_benchmark/benchmark_utils.py"], "labels": []}, "0d4a110c28": {"title": "[JIT] Fix dead stores in JIT (#41202)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41202\n\nThis commit fixes dead stores in JIT surfaced by the Quality Analyzer.\n\nTest Plan: Continuous integration.\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22461492\n\nfbshipit-source-id: c587328f952054fb9449848e90b7d28a20aed4af", "pr_number": "41202", "files_changed": ["torch/csrc/jit/passes/fold_conv_bn.cpp", "torch/csrc/jit/passes/loop_unrolling.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "fd0329029f": {"title": "Fix flaky profiler and test_callback_simple RPC tests (#41287)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41287\n\nProfiler tests that test profiling with builtin functions and `test_callback_simple` test has been broken for a while. This diff fixes that by preferring c10 ops to non-c10 ops in our operation matching logic.\n\nThe result of this is that these ops go through the c10 dispatch and thus have profiling enabled. For `test_callback_simple` this results in the effect that we choose `aten::add.Tensor` over `aten::add.Int` which fixes the type issue.\n\nTest Plan:\nEnsured that the tests are no longer flaky by running them a bunch\nof times.\n\nReviewed By: vincentqb\n\nDifferential Revision: D22489197\n\nfbshipit-source-id: 8452b93e4d45703453f77d968350c0d32f3f63fe", "pr_number": "41287", "files_changed": ["torch/csrc/distributed/rpc/python_functions.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "0b73ea0ea2": {"title": "Change BCELoss size mismatch warning into an error (#41426)", "body": "Summary:\nBCELoss currently uses different broadcasting semantics than numpy. Since previous versions of PyTorch have thrown a warning in these cases telling the user that input sizes should match, and since the CUDA and CPU results differ when sizes do not match, it makes sense to upgrade the size mismatch warning to an error.\n\nWe can consider supporting numpy broadcasting semantics in BCELoss in the future if needed.\n\nCloses https://github.com/pytorch/pytorch/issues/40023\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41426\n\nReviewed By: zou3519\n\nDifferential Revision: D22540841\n\nPulled By: ezyang\n\nfbshipit-source-id: 6c6d94c78fa0ae30ebe385d05a9e3501a42b3652", "pr_number": "41426", "files_changed": ["test/test_nn.py", "torch/csrc/api/include/torch/nn/functional/loss.h", "torch/nn/functional.py"], "labels": ["merged", "open source", "topic: bc-breaking"]}, "96f124e623": {"title": "remove template arguments of layernorm", "body": "Summary:\nremove layernorm templates and make them float since that's the only variant\nminor fixes in logging and testing\n\nTest Plan: ran the test\n\nReviewed By: venkatacrc\n\nDifferential Revision: D22527359\n\nfbshipit-source-id: d6eec362a6e88e1c12fddf820ae629ede13fb2b8", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/layernorm_fp16_fake_op.cc", "caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h", "caffe2/contrib/fakelowp/test/test_layernorm_nnpi_fp16.py"], "labels": []}, "f074994a31": {"title": "vectorize rounding ops (#41439)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41439\n\nuse RoundToFloat16 on arrays\n\nTest Plan: layernorm unittest\n\nReviewed By: venkatacrc\n\nDifferential Revision: D22540118\n\nfbshipit-source-id: dc84fd22b5dc6a3bd15ad4ec1eecb9db13d64e97", "pr_number": "41439", "files_changed": ["caffe2/contrib/fakelowp/layernorm_fp16_fake_op.cc"], "labels": ["fb-exported", "merged"]}, "fcd6d91045": {"title": "Temporary fix for determinant bug on CPU (#35136)", "body": "Summary:\nChangelog:\n- Make diagonal contiguous\n\nTemporarily Fixes https://github.com/pytorch/pytorch/issues/34061\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/35136\n\nReviewed By: vincentqb\n\nDifferential Revision: D22516606\n\nPulled By: ezyang\n\nfbshipit-source-id: 7ea8299b9d2c1c244995955b333a1dffb0cdff73", "pr_number": "35136", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "20f3051f7d": {"title": "[adaptive_]max_pool{1,2,3}d: handle edge case when input is filled with -inf (#40665)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/40131\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40665\n\nDifferential Revision: D22463538\n\nPulled By: ezyang\n\nfbshipit-source-id: 7e08fd0205926911d45aa150012154637e64a8d4", "pr_number": "40665", "files_changed": ["aten/src/ATen/native/AdaptiveMaxPooling2d.cpp", "aten/src/ATen/native/AdaptiveMaxPooling3d.cpp", "aten/src/ATen/native/FractionalMaxPool2d.cpp", "aten/src/ATen/native/FractionalMaxPool3d.cpp", "benchmarks/operator_benchmark/pt/pool_test.py", "test/test_nn.py"], "labels": ["merged", "open source"]}, "44b9306d0a": {"title": "Export replaceAllUsesAfterNodeWith for PythonAPI (#41414)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41414\n\nThis diff exports replaceAllUsesAfterNodeWith to PythonAPI.\n\nTest Plan: Tested locally. Please let me know if there is a set of unit tests to be passed outside of the default ones triggered by Sandcastle.\n\nReviewed By: soumith\n\nDifferential Revision: D22523211\n\nfbshipit-source-id: 3f075bafa6208ada462abc57d495c15179a6e53d", "pr_number": "41414", "files_changed": ["torch/csrc/jit/python/python_ir.cpp"], "labels": ["merged", "oncall: jit"]}, "921d2a164f": {"title": "SparseAdagrad/RowWiseSparseAdagrad mean fusion on CPU & GPU and dedup version for RowWiseSparse mean fusion on GPU", "body": "Summary:\n1. Support SparseAdagradFusedWithSparseLengthsMeanGradient and RowWiseSparseAdagradFusedWithSparseLengthsMeanGradient on CPU and GPU\n2. Add the dedup implementation of fused RowWiseAdagrad op on GPUs for mean pooling\n\nReviewed By: xianjiec\n\nDifferential Revision: D22165603\n\nfbshipit-source-id: 743fa55ed5893c34bc6406ddfbbbb347b88091d1", "pr_number": null, "files_changed": ["caffe2/sgd/adagrad_fused.cc", "caffe2/sgd/adagrad_fused.h", "caffe2/sgd/adagrad_fused_op_gpu.cu", "caffe2/sgd/rowwise_adagrad_fused.cc", "caffe2/sgd/rowwise_adagrad_fused.h"], "labels": []}, "9552ec787c": {"title": "Revert D22516606: [pytorch][PR] Temporary fix for determinant bug on CPU", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22516606 (https://github.com/pytorch/pytorch/commit/fcd6d910450ca119091afbf62099e1fab10fe86e)\n\nOriginal commit changeset: 7ea8299b9d2c\n\nfbshipit-source-id: 41e19d5e1ba843cd70dce677869892f2e33fac09", "pr_number": null, "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "test/test_torch.py"], "labels": []}, "14f19ab833": {"title": "Port index_select to ATen (CUDA) (#39946)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/24578\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39946\n\nReviewed By: ngimel\n\nDifferential Revision: D22520160\n\nPulled By: mruberry\n\nfbshipit-source-id: 7eb3029e3917e793f3c020359acb0989d5deb61e", "pr_number": "39946", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.cu", "test/test_torch.py"], "labels": ["merged", "module: complex", "open source", "triaged"]}, "86a2bdc35e": {"title": "Adjust bound_shape_inferencer to take 4 inputs for FCs (#41452)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41452\n\nThe model exported from online training workflow with int8 quantization contains FCs with 4 inputs. The extra input is the quant_param blob. This diff is to adjust the bound_shape_inferencer to get shape info for the quant_param input.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/opt:bound_shape_inference_test\n```\n\nReviewed By: anurag16\n\nDifferential Revision: D22543215\n\nfbshipit-source-id: 0977fca06630e279d47292e6b44f3d8180a767a5", "pr_number": "41452", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/quantization/server/int8_quant_scheme_blob_fill.cc"], "labels": ["fb-exported", "merged"]}, "f153b35b9b": {"title": "Shape inference for SparseToDense in ExpertCombiner", "body": "Summary: Adding shape inference for SpraseToDense. Proposal impl of shape inference only works when data_to_infer_dim is given, otherwise SpraseToDense output dimension depends on max value of input tensor\n\nTest Plan:\nbuck test //caffe2/caffe2/python:sparse_to_dense_test\nbuck test //caffe2/caffe2/python:hypothesis_test -- test_sparse_to_dense\n\nDper3 Changes:\nf204594813\nbuck test dper3/dper3_models/ads_ranking/model_impl/sparse_nn/tests:sparse_nn_lib_test\n\nReviewed By: zhongyx12, ChunliF\n\nDifferential Revision: D22479511\n\nfbshipit-source-id: 8983a9baea8853deec53ad6f795c874c3fb93de0", "pr_number": null, "files_changed": ["caffe2/operators/sparse_to_dense_op.cc", "caffe2/python/hypothesis_test.py", "caffe2/python/sparse_to_dense_test.py"], "labels": []}, "8548a21c00": {"title": "Revert D22543215: Adjust bound_shape_inferencer to take 4 inputs for FCs", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22543215 (https://github.com/pytorch/pytorch/commit/86a2bdc35e03fbe4d66fa2666b49efc4f84855fc)\n\nOriginal commit changeset: 0977fca06630\n\nfbshipit-source-id: b440f9b1eaeb35ec8b08e899890691e7a77a9f6d", "pr_number": null, "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/quantization/server/int8_quant_scheme_blob_fill.cc"], "labels": []}, "3a63a939d4": {"title": "Revert D22517785: [pytorch][PR] Enable TF32 support for cuBLAS", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22517785 (https://github.com/pytorch/pytorch/commit/288ece89e16fe915ae0cfb2d948447e90ab03b1a)\n\nOriginal commit changeset: 87334c893561\n\nfbshipit-source-id: 0a0674f49c1bcfc98f7f88af5a8c7de93b76e458", "pr_number": null, "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CublasHandlePool.cpp", "aten/src/ATen/native/cuda/MiscUtils.h", "aten/src/THC/THCBlas.cu", "docs/source/notes/cuda.rst", "test/test_cuda.py", "test/test_torch.py", "torch/backends/cuda/__init__.py", "torch/csrc/Module.cpp", "torch/testing/_internal/common_cuda.py"], "labels": []}, "3971777ebb": {"title": "Krovatkin/reenable test tensorexpr (#41445)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41445\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22543075\n\nPulled By: Krovatkin\n\nfbshipit-source-id: fd8c0a94f5b3aff34d2b444dbf551425fdc1df04", "pr_number": "41445", "files_changed": [".jenkins/pytorch/test.sh", "test/test_tensorexpr.py"], "labels": ["merged", "oncall: jit"]}, "dddac948a3": {"title": "Add CUDA to pooling benchmark configs (#41438)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/41368\n\nThese benchmarks support CUDA already so there is no reason for it not to be in the benchmark config.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41438\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22540756\n\nPulled By: ezyang\n\nfbshipit-source-id: 621eceff37377c1ab06ff7483b39fc00dc34bd46", "pr_number": "41438", "files_changed": ["benchmarks/operator_benchmark/pt/pool_test.py"], "labels": ["merged", "open source"]}, "6ff306b8b5": {"title": "Add non-deterministic alert to CUDA operations that use `atomicAdd()` (#40056)", "body": "Summary:\nIssue https://github.com/pytorch/pytorch/issues/15359\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40056\n\nDifferential Revision: D22480638\n\nPulled By: ezyang\n\nfbshipit-source-id: 4cc913cb3ca6d4206de80f4665bbc9031aa3ca01", "pr_number": "40056", "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/GridSampler.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/ReflectionPad.cu", "aten/src/ATen/native/cuda/ReplicationPadding.cu", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "test/test_nn.py", "test/test_torch.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_nn.py"], "labels": ["merged", "open source", "triaged"]}, "563b60b890": {"title": "Fix flaky test_stream_event_nogil due to missing event sync (#41398)", "body": "Summary:\nThe test asserts that the stream is \"ready\" but doesn't wait for the\nevent to be \"executed\" which makes it fail on some platforms where the\n`query` call occurs \"soon enough\".\n\nFixes https://github.com/pytorch/pytorch/issues/38807\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41398\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22540012\n\nPulled By: ezyang\n\nfbshipit-source-id: 6f56d951e48133ce4f6a9a54534298b7d2877c80", "pr_number": "41398", "files_changed": ["test/test_cuda.py"], "labels": ["merged", "open source", "triaged"]}, "c86699d425": {"title": "[cmake] Use PROJECT_SOURCE_DIR instead of CMAKE_* (#41387)", "body": "Summary:\nAdd support for including pytorch via an add_subdirectory()\nThis requires using PROJECT_* instead of CMAKE_* which refer to\nthe top-most project including pytorch.\n\nTEST=add_subdirectory() into a pytorch checkout and build.\nThere are still some hardcoded references to TORCH_SRC_DIR, I will\nfix in a follow on commit. For now you can create a symlink to\n <pytorch>/torch/ in your project.\n\nChange-Id: Ic2a8aec3b08f64e2c23d9e79db83f14a0a896abc\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41387\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22539944\n\nPulled By: ezyang\n\nfbshipit-source-id: b7e9631021938255f0a6ea897a7abb061759093d", "pr_number": "41387", "files_changed": ["caffe2/CMakeLists.txt", "cmake/Codegen.cmake", "cmake/Dependencies.cmake"], "labels": ["merged", "open source", "triaged"]}, "d5ae4a07ef": {"title": "DDP Communication Hook Main Structure (#40848)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40848\n\nSub-tasks 1 and 2 of [39272](https://github.com/pytorch/pytorch/issues/39272)\nghstack-source-id: 107787878\n\nTest Plan:\n1\\. Perf tests to to validate new code (if conditions before `allreduce`) doesn't slow down today's DDP. Execute the following command with diff patched/unpatched (with V25):\n\n* **Unpatched Runs:**\n```\nhg checkout D22514243\nflow-cli canary pytorch.benchmark.main.workflow --parameters-json '{\"model_arch\": \"resnet50\", \"batch_size\": 32, \"world_size\": 1, \"use_fp16\": false, \"print_percentile\": true, \"backend\": \"gloo\"}' --entitlement pytorch_ftw_gpu --name test_torchelastic_gloo_masterD22514243 --run-as-secure-group pytorch_distributed\n```\n* **Run 1 (unpatched):** `elastic_gang:benchmark_single.elastic_operator` Ran for 2 mins 59 s\nf204539235\n```\nsum:\n8 GPUs: p25:  0.156   205/s  p50:  0.160   200/s  p75:  0.164   194/s  p90:  0.169   189/s  p95:  0.173   185/s\nfwds:\n8 GPUs: p25:  0.032  1011/s  p50:  0.032  1006/s  p75:  0.032  1000/s  p90:  0.032   992/s  p95:  0.033   984/s\nbwds:\n8 GPUs: p25:  0.121   265/s  p50:  0.125   256/s  p75:  0.129   248/s  p90:  0.134   239/s  p95:  0.137   232/s\nopts:\n8 GPUs: p25:  0.003  11840/s  p50:  0.003  11550/s  p75:  0.004  8037/s  p90:  0.006  5633/s  p95:  0.007  4631/s\n```\n* **Run 2 (unpatched):** `elastic_gang:benchmark_single.elastic_operator` Ran for 3 mins 1 s\nf204683840\n```\nsum:\n8 GPUs: p25:  0.145   220/s  p50:  0.147   217/s  p75:  0.150   213/s  p90:  0.154   207/s  p95:  0.157   204/s\nfwds:\n8 GPUs: p25:  0.032  1015/s  p50:  0.032  1009/s  p75:  0.032  1002/s  p90:  0.032   994/s  p95:  0.032   990/s\nbwds:\n8 GPUs: p25:  0.107   297/s  p50:  0.111   288/s  p75:  0.115   278/s  p90:  0.119   268/s  p95:  0.122   262/s\nopts:\n8 GPUs: p25:  0.003  11719/s  p50:  0.004  9026/s  p75:  0.006  5160/s  p90:  0.009  3700/s  p95:  0.010  3184/s\n```\n\n* **Patched Runs:**\n```\nhg checkout D22328310\nflow-cli canary pytorch.benchmark.main.workflow --parameters-json '{\"model_arch\": \"resnet50\", \"batch_size\": 32, \"world_size\": 1, \"use_fp16\": false, \"print_percentile\": true, \"backend\": \"gloo\"}' --entitlement pytorch_ftw_gpu --name test_torchelastic_gloo_localD22328310 --run-as-secure-group pytorch_distributed\n```\n* **Run 1 (patched):** `elastic_gang:benchmark_single.elastic_operator` Ran for 3 mins 30 s\nf204544541\n```\nsum:\n8 GPUs: p25:  0.148   216/s  p50:  0.152   210/s  p75:  0.156   205/s  p90:  0.160   200/s  p95:  0.163   196/s\nfwds:\n8 GPUs: p25:  0.032  1011/s  p50:  0.032  1005/s  p75:  0.032   999/s  p90:  0.032   991/s  p95:  0.033   984/s\nbwds:\n8 GPUs: p25:  0.112   286/s  p50:  0.116   275/s  p75:  0.120   265/s  p90:  0.125   256/s  p95:  0.128   250/s\nopts:\n8 GPUs: p25:  0.003  11823/s  p50:  0.003  10948/s  p75:  0.004  7225/s  p90:  0.007  4905/s  p95:  0.008  3873/s\n```\n* **Run 2 (patched):** `elastic_gang:benchmark_single.elastic_operator`\nRan for 3 mins 14 s\nf204684520\n```\nsum:\n8 GPUs: p25:  0.146   219/s  p50:  0.147   217/s  p75:  0.150   214/s  p90:  0.152   210/s  p95:  0.153   208/s\nfwds:\n8 GPUs: p25:  0.032  1013/s  p50:  0.032  1008/s  p75:  0.032  1002/s  p90:  0.032   996/s  p95:  0.032   990/s\nbwds:\n8 GPUs: p25:  0.107   299/s  p50:  0.110   290/s  p75:  0.114   280/s  p90:  0.117   274/s  p95:  0.119   269/s\nopts:\n8 GPUs: p25:  0.003  11057/s  p50:  0.005  6490/s  p75:  0.008  4110/s  p90:  0.010  3309/s  p95:  0.010  3103/s\n```\n* **Run 3 (patched):** `elastic_gang:benchmark_single.elastic_operator` Ran for 2 mins 54 s\nf204692872\n```\nsum:\n8 GPUs: p25:  0.145   220/s  p50:  0.147   217/s  p75:  0.150   213/s  p90:  0.154   207/s  p95:  0.156   204/s\nfwds:\n8 GPUs: p25:  0.032  1001/s  p50:  0.032   995/s  p75:  0.032   988/s  p90:  0.033   980/s  p95:  0.033   973/s\nbwds:\n8 GPUs: p25:  0.108   295/s  p50:  0.111   287/s  p75:  0.114   280/s  p90:  0.119   269/s  p95:  0.121   264/s\nopts:\n8 GPUs: p25:  0.003  11706/s  p50:  0.003  9257/s  p75:  0.005  6333/s  p90:  0.008  4242/s  p95:  0.009  3554/s\n```\n\n* **Memory:**\n   * Unpatched:\n```\nCUDA Memory Summary After                     first iteration: |===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |  428091 KB |    2892 MB |    9825 MB |    9407 MB |\n|       from large pool |  374913 KB |    2874 MB |    9752 MB |    9386 MB |\n|       from small pool |   53178 KB |      52 MB |      73 MB |      21 MB |\n|---------------------------------------------------------------------------|\n| Active memory         |  428091 KB |    2892 MB |    9825 MB |    9407 MB |\n|       from large pool |  374913 KB |    2874 MB |    9752 MB |    9386 MB |\n|       from small pool |   53178 KB |      52 MB |      73 MB |      21 MB |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |    3490 MB |    3490 MB |    3490 MB |       0 B  |\n|       from large pool |    3434 MB |    3434 MB |    3434 MB |       0 B  |\n|       from small pool |      56 MB |      56 MB |      56 MB |       0 B  |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |  315332 KB |  343472 KB |    2295 MB |    1987 MB |\n|       from large pool |  311166 KB |  340158 KB |    2239 MB |    1935 MB |\n|       from small pool |    4166 KB |    4334 KB |      56 MB |      52 MB |\n|---------------------------------------------------------------------------|\n| Allocations           |     704    |     705    |    1390    |     686    |\n|       from large pool |      60    |     131    |     395    |     335    |\n|       from small pool |     644    |     645    |     995    |     351    |\n|---------------------------------------------------------------------------|\n| Active allocs         |     704    |     705    |    1390    |     686    |\n|       from large pool |      60    |     131    |     395    |     335    |\n|       from small pool |     644    |     645    |     995    |     351    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |     102    |     102    |     102    |       0    |\n|       from large pool |      74    |      74    |      74    |       0    |\n|       from small pool |      28    |      28    |      28    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |      34    |      54    |     430    |     396    |\n|       from large pool |      15    |      48    |     208    |     193    |\n|       from small pool |      19    |      19    |     222    |     203    |\n|===========================================================================|\n\n```\n   * Patched:\n```\nCUDA Memory Summary After                     first iteration: |===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |  428091 KB |    2892 MB |    9825 MB |    9407 MB |\n|       from large pool |  374913 KB |    2874 MB |    9752 MB |    9386 MB |\n|       from small pool |   53178 KB |      52 MB |      73 MB |      21 MB |\n|---------------------------------------------------------------------------|\n| Active memory         |  428091 KB |    2892 MB |    9825 MB |    9407 MB |\n|       from large pool |  374913 KB |    2874 MB |    9752 MB |    9386 MB |\n|       from small pool |   53178 KB |      52 MB |      73 MB |      21 MB |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |    3490 MB |    3490 MB |    3490 MB |       0 B  |\n|       from large pool |    3434 MB |    3434 MB |    3434 MB |       0 B  |\n|       from small pool |      56 MB |      56 MB |      56 MB |       0 B  |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |  315332 KB |  343472 KB |    2295 MB |    1987 MB |\n|       from large pool |  311166 KB |  340158 KB |    2239 MB |    1935 MB |\n|       from small pool |    4166 KB |    4334 KB |      56 MB |      52 MB |\n|---------------------------------------------------------------------------|\n| Allocations           |     704    |     705    |    1390    |     686    |\n|       from large pool |      60    |     131    |     395    |     335    |\n|       from small pool |     644    |     645    |     995    |     351    |\n|---------------------------------------------------------------------------|\n| Active allocs         |     704    |     705    |    1390    |     686    |\n|       from large pool |      60    |     131    |     395    |     335    |\n|       from small pool |     644    |     645    |     995    |     351    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |     102    |     102    |     102    |       0    |\n|       from large pool |      74    |      74    |      74    |       0    |\n|       from small pool |      28    |      28    |      28    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |      34    |      54    |     431    |     397    |\n|       from large pool |      15    |      48    |     208    |     193    |\n|       from small pool |      19    |      19    |     223    |     204    |\n|===========================================================================|\n\n```\n\n2\\. As of v18: `python test/distributed/test_c10d.py`\n```\n....................s.....s.....................................................s................................\n----------------------------------------------------------------------\nRan 114 tests in 215.983s\n\nOK (skipped=3)\n\n```\n\n3\\. Additional tests in `python test/distributed/test_c10d.py`:\n* `test_ddp_comm_hook_future_passing_cpu`: This unit test verifies whether the Future object is passed properly. The callback function creates a Future object and sets a value to it.\n* `_test_ddp_comm_hook_future_passing_gpu`: This unit test verifies whether the Future object is passed properly. The callback function creates a Future object and sets a value to it.\n* `test_ddp_comm_hook_future_passing_gpu_gloo`: This unit test executes _test_ddp_comm_hook_future_passing_gpu using gloo backend.\n* `test_ddp_comm_hook_future_passing_gpu_nccl`: This unit test executes _test_ddp_comm_hook_future_passing_gpu using nccl backend.\n* `test_ddp_invalid_comm_hook_init`: This unit test makes sure that register_comm_hook properly checks the format of hook defined by user. The Python hook must be callable. This test also checks whether bucket annotation checked properly if defined.\n* `test_ddp_invalid_comm_hook_return_type`: This test checks whether return annotation checked properly if defined. It also checks whether an internal error is thrown if return type is incorrect and user hasn't specified any return type annotation.\n* `test_ddp_comm_hook_register_just_once`: DDP communication hook can only be registered once. This test validates whether the error is thrown properly when register_comm_hook is called more than once.\n\nReviewed By: ezyang\n\nDifferential Revision: D22328310\n\nfbshipit-source-id: 77a6a71808e7b6e947795cb3fcc68c8c8f024549", "pr_number": "40848", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/comm.cpp", "torch/csrc/distributed/c10d/comm.h", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/nn/parallel/distributed.py"], "labels": ["merged"]}, "008ab27b22": {"title": "[quant][pyper] Add embedding_bag weight quantize and dequantize ops (#41293)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41293\n\nAdd new operators that does quantize and packing for 8 bit and 4 bit embedding bag operators.\nThis is an initial change to help unblock testing. This will be follwed by adding graph mode passes to enable quantization of embedding_bag module\n\nNote to reviewers: Future PRs will replace this op with a separate quantize and pack operator and add support for floating point scale and zero point.\n\nTest Plan:\npython test/test_quantization.py TestQuantizedEmbeddingBag\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22506700\n\nfbshipit-source-id: 090cc85a8f56da417e4b7e45818ea987ae97ca8a", "pr_number": "41293", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py"], "labels": ["merged"]}, "954c260061": {"title": "Revert D22480638: [pytorch][PR] Add non-deterministic alert to CUDA operations that use `atomicAdd()`", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22480638 (https://github.com/pytorch/pytorch/commit/6ff306b8b5d9f0ca25ef8f1f4bcc3cfa1803ae0c)\n\nOriginal commit changeset: 4cc913cb3ca6\n\nfbshipit-source-id: e47fa14b5085bb2b74a479bd0830efc2d7604eea", "pr_number": null, "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/GridSampler.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/ReflectionPad.cu", "aten/src/ATen/native/cuda/ReplicationPadding.cu", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "test/test_nn.py", "test/test_torch.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "225289abc6": {"title": "Adding epsilon input argument to the Logit Op", "body": "Summary: Adding epsilon input argument to the Logit Op\n\nTest Plan: Added test_logit test case.\n\nReviewed By: hyuen\n\nDifferential Revision: D22537133\n\nfbshipit-source-id: d6f89afd1589fda99f09550a9d1b850cfc0b9ee1", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/unary_fp16_fake_op.cc"], "labels": []}, "4367a73399": {"title": "Cuda Support for Learnable Fake Quantize Per Tensor (GPU) (#41127)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41127\n\nIn this diff, implementation is provided to support the GPU kernel running the learnable fake quantize per tensor kernels.\n\nTest Plan: On a devvm, run `buck test //caffe2/test:quantization -- learnable` to test both the forward and backward for the learnable per tensor fake quantize kernels. The test will test the `cuda` version if a gpu is available.\n\nReviewed By: z-a-f\n\nDifferential Revision: D22435037\n\nfbshipit-source-id: 515afde13dd224d21fd47fb7cb027ee8d704cbdd", "pr_number": "41127", "files_changed": ["aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp", "test/quantization/test_workflow_module.py"], "labels": ["fb-exported", "merged"]}, "c62550e3f4": {"title": "Cuda Support for Learnable Fake Quantize Per Channel (GPU) (#41262)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41262\n\nIn this diff, implementation is provided to support the GPU kernel running the learnable fake quantize per tensor kernels.\n\nTest Plan: On a devvm, run `buck test //caffe2/test:quantization -- learnable` to test both the forward and backward for the learnable per tensor fake quantize kernels. The test will test the `cuda` version if a gpu is available.\n\nReviewed By: vkuzo\n\nDifferential Revision: D22478832\n\nfbshipit-source-id: 2731bd8b57bc83416790f6d65ef42d450183873c", "pr_number": "41262", "files_changed": ["aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu", "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp", "test/quantization/test_workflow_module.py"], "labels": ["fb-exported", "merged"]}, "8940a4e684": {"title": "Pull upstream select_compute_arch from cmake for Ampere (#41133)", "body": "Summary:\nThis pulls the following merge requests from CMake upstream:\n- https://gitlab.kitware.com/cmake/cmake/-/merge_requests/4979\n- https://gitlab.kitware.com/cmake/cmake/-/merge_requests/4991\n\nThe above two merge requests improve the Ampere build:\n- If `TORCH_CUDA_ARCH_LIST` is not set, it can now automatically pickup 8.0 as its part of its default value\n- If `TORCH_CUDA_ARCH_LIST=Ampere`, it no longer fails with `Unknown CUDA Architecture Name Ampere in CUDA_SELECT_NVCC_ARCH_FLAGS`\n\nCodes related to architecture < 3.5 are manually removed because PyTorch no longer supports it.\n\ncc: ngimel ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41133\n\nReviewed By: malfet\n\nDifferential Revision: D22540547\n\nPulled By: ezyang\n\nfbshipit-source-id: 6e040f4054ef04f18ebb7513497905886a375632", "pr_number": "41133", "files_changed": ["cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake"], "labels": ["merged", "open source"]}, "ff6e560301": {"title": "Add C++ end to end test for RPC and distributed autograd. (#36893)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36893\n\nAdding an end to end test for running a simple training loop in C++\nfor the distributed RPC framework.\n\nThe goal of this change is to enable LeakSanitizer and potentially catch memory\nleaks in the Future. Enabling LSAN with python multiprocessing is tricky and we\nhaven't found a solution for this. As a result, adding a C++ test that triggers\nmost of the critical codepaths would be good for now.\n\nAs an example, this unit test would've caught the memory leak fixed by:\nhttps://github.com/pytorch/pytorch/pull/31030\nghstack-source-id: 107781167\n\nTest Plan:\n1) Verify the test catches memory leaks.\n2) waitforbuildbot\n\nReviewed By: mrshenli\n\nDifferential Revision: D21112208\n\nfbshipit-source-id: 4eb2a6b409253108f6b6e14352e593d250c7a64d", "pr_number": "36893", "files_changed": [".jenkins/pytorch/build-asan.sh", ".jenkins/pytorch/test.sh", "caffe2/CMakeLists.txt", "test/cpp/rpc/CMakeLists.txt", "test/cpp/rpc/e2e_test_base.h", "test/cpp/rpc/test_e2e_process_group.cpp", "tools/build_variables.bzl", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_impl.h", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/request_callback_no_python.h", "torch/csrc/distributed/rpc/testing/faulty_process_group_agent.cpp"], "labels": ["merged"]}, "b48ee175e6": {"title": "[reland][DNNL]:enable conv3d (#40691)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40691\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22296548\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 8e2a7cf14e8bdfa2f29b735a89e8c83f6119e68d", "pr_number": "40691", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/mkldnn/Conv.cpp", "aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp", "aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_mkldnn.py", "test/test_nn.py", "torch/utils/mkldnn.py"], "labels": ["merged", "open source", "triaged"]}, "2b8db35c7e": {"title": "[reland][DNNL]:enable batchnorm3d (#40995)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40995\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22440765\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: b4bf427bbb7010ee234a54e81ade371627f9e82c", "pr_number": "40995", "files_changed": ["test/test_mkldnn.py", "torch/utils/mkldnn.py"], "labels": ["merged", "open source", "triaged"]}, "b7147fe6d7": {"title": "Learnable Fake Quantizer Benchmark Test (#41429)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41429\n\nThis diff contains the benchmark test to evaluate the speed of executing the learnable fake quantization operator, both in the forward path and the backward path, with respect to both per tensor and per channel usages.\n\nTest Plan:\nInside the path `torch/benchmarks/operator_benchmark` (The root directory will be `caffe2` inside `fbcode` if working on a devvm):\n- On a devvm, run the command `buck run pt:fake_quantize_learnable_test`\n- On a personal laptop, run the command `python3 -m pt.fake_quantize_learnable_test`\n\nBenchmark Results (Locally on CPU):\nEach sample has dimensions **3x256x256**; Each batch has 16 samples (`N=16`)\n- Per Tensor Forward: 0.023688 sec/sample\n- Per Tensor Backward: 0.165926 sec/sample\n- Per Channel Forward: 0.040432 sec / sample\n- Per Channel Backward: 0.173528 sec / sample\n\nReviewed By: vkuzo\n\nDifferential Revision: D22535252\n\nfbshipit-source-id: e8e953ff2de2107c6f2dde4c8d5627bdea67ef7f", "pr_number": "41429", "files_changed": ["benchmarks/operator_benchmark/benchmark_all_quantized_test.py", "benchmarks/operator_benchmark/pt/fake_quantize_learnable_test.py"], "labels": ["fb-exported", "merged"]}, "5bd71259ed": {"title": "remove blacklist reference (#41447)", "body": "Summary:\nReference : issue https://github.com/pytorch/pytorch/issues/41443\nRemoved blacklist reference\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41447\n\nReviewed By: ezyang\n\nDifferential Revision: D22542428\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 09728c7718bb99ff56b16fda6971ebd887a99c97", "pr_number": "41447", "files_changed": [".jenkins/pytorch/dirty.sh"], "labels": ["merged", "open source"]}, "71c3b397a6": {"title": "Reduce Image Size (2) (#41301)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41301\n\nReviewed By: malfet\n\nDifferential Revision: D22559626\n\nPulled By: ssylvain\n\nfbshipit-source-id: 32da88b7efe2e8d134f74b6ff2dff0bffede012c", "pr_number": "41301", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "488ee3790e": {"title": "Support @torch.jit.unused on a @torch.no_grad decorated function (#41496)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41496\n\nuse the wrapped function (instead of the wrapper) to obtain argument names\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test:jit -- 'test_unused_decorator \\(test_jit\\.TestScript\\)'\n```\n\nBefore:\n```\n> Traceback (most recent call last):\n>   File \"/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/test_jit.py\", line 3014, in test_unused_decorator\n>     torch.jit.script(MyMod())\n>   File \"/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/torch/jit/_script.py\", line 888, in script\n>     obj, torch.jit._recursive.infer_methods_to_compile\n>   File \"/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/torch/jit/_recursive.py\", line 317, in create_script_module\n>     return create_script_module_impl(nn_module, concrete_type, stubs_fn)\n>   File \"/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/torch/jit/_recursive.py\", line 376, in create_script_module_impl\n>     create_methods_from_stubs(concrete_type, stubs)\n>   File \"/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/torch/jit/_recursive.py\", line 292, in create_methods_from_stubs\n>     concrete_type._create_methods(defs, rcbs, defaults)\n> RuntimeError:\n> Non-static method does not have a self argument:\n>   File \"/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/test_jit.py\", line 3012\n>             def forward(self, x):\n>                 return self.fn(x)\n>                        ~~~~~~~ <--- HERE\n>\n```\n\nReviewed By: eellison\n\nDifferential Revision: D22554479\n\nfbshipit-source-id: 03e432ea92ed973cc57ff044da80ae7a36f6af4c", "pr_number": "41496", "files_changed": ["test/test_jit.py", "torch/jit/annotations.py"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "60f2fa6a84": {"title": "Updates serialization note to explain versioned symbols and dynamic versioning (#41395)", "body": "Summary:\nDoc update intended to clarify and expand our current serialization behavior, including explaining the difference between torch.save/torch.load, torch.nn.Module.state_dict/torch.nn.Module.load_state_dict, and torch.jit.save/torch.jit.load. Also explains, for the time, when historic serialized Torchscript behavior is preserved and our recommendation for preserving behavior (using the same PyTorch version to consume a model as produced it).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41395\n\nReviewed By: ngimel\n\nDifferential Revision: D22560538\n\nPulled By: mruberry\n\nfbshipit-source-id: dbc2f1bb92ab61ff2eca4888febc21f7dda76ba1", "pr_number": "41395", "files_changed": ["docs/source/notes/serialization.rst"], "labels": ["merged"]}, "404799d43f": {"title": "Disable failed caffe2 tests for BoundShapeInference on Windows (#41472)", "body": "Summary:\nRelated:\nhttps://github.com/pytorch/pytorch/issues/40861\nhttps://github.com/pytorch/pytorch/issues/41471\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41472\n\nReviewed By: yns88\n\nDifferential Revision: D22562385\n\nPulled By: malfet\n\nfbshipit-source-id: aebc600915342b984f4fc47cef0a1e79d8965c10", "pr_number": "41472", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc"], "labels": ["merged", "open source"]}, "d90fb72b5a": {"title": "remove use of the term \"blacklist\" from docs/cpp/source/Doxyfile (#41450)", "body": "Summary:\nAs requested in https://github.com/pytorch/pytorch/issues/41443\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41450\n\nReviewed By: ezyang\n\nDifferential Revision: D22561782\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: b38ab5e2725735d1f0c70a4d0012678636e992c3", "pr_number": "41450", "files_changed": ["docs/cpp/source/Doxyfile"], "labels": ["merged", "open source"]}, "1770937c9c": {"title": "Restore the contiguity preprocessing of linspace (#41286)", "body": "Summary:\nThe contiguity preprocessing was mistakenly removed in\ncd48fb503088af2c00884f1619db571fffbcdafa . It causes erroneous output\nwhen the output tensor is not contiguous. Here we restore this\npreprocessing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41286\n\nReviewed By: zou3519\n\nDifferential Revision: D22550822\n\nPulled By: ezyang\n\nfbshipit-source-id: ebad4e2ba83d2d808e3f958d4adc9a5513a95bec", "pr_number": "41286", "files_changed": ["aten/src/ATen/native/RangeFactories.cpp", "test/test_torch.py"], "labels": ["merge-this-please", "merged", "open source", "triaged"]}, "e44f460079": {"title": "[jit] Fix jit not round to even if const is folded (#40897)", "body": "Summary:\nFixed https://github.com/pytorch/pytorch/issues/40771\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40897\n\nReviewed By: Krovatkin\n\nDifferential Revision: D22543261\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 0bd4b1d910a42d5aa87e120c81acfdfb7ca895fa", "pr_number": "40897", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["merged", "oncall: jit", "open source"]}, "200c343184": {"title": "Implement gcd, lcm (#40651)", "body": "Summary:\nResolves https://github.com/pytorch/pytorch/issues/40018.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40651\n\nReviewed By: ezyang\n\nDifferential Revision: D22511828\n\nPulled By: mruberry\n\nfbshipit-source-id: 3ef251e45da4688b1b64c79f530fb6642feb63ab", "pr_number": "40651", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/Math.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged", "open source", "triaged"]}, "23174ca71b": {"title": "[reland] Enable TF32 support for cuBLAS (#41498)", "body": "Summary:\nfix rocm\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41498\n\nReviewed By: mruberry\n\nDifferential Revision: D22560572\n\nPulled By: ngimel\n\nfbshipit-source-id: 5ee79e96cb29e70d9180830d058efb53d1c6c041", "pr_number": "41498", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CublasHandlePool.cpp", "aten/src/ATen/native/cuda/MiscUtils.h", "aten/src/THC/THCBlas.cu", "docs/source/notes/cuda.rst", "test/test_cuda.py", "test/test_torch.py", "torch/backends/cuda/__init__.py", "torch/csrc/Module.cpp", "torch/testing/_internal/common_cuda.py"], "labels": ["merged", "open source"]}, "7a33d8b001": {"title": "[PyTorch Mobile] Modularize the autograd source files shared by mobile and full-jit (#41430)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41430\n\nTo avoid duplication at compile time, modularize the common autograd files used by both mobile and full-jit.\nghstack-source-id: 107742889\n\nTest Plan: CI\n\nReviewed By: kwanmacher\n\nDifferential Revision: D22531358\n\nfbshipit-source-id: 554f10be89b7ed59c9bde13387a0e1b08000c116", "pr_number": "41430", "files_changed": ["tools/build_variables.bzl"], "labels": ["merged"]}, "1fb2a7e5a2": {"title": "onnx export of fake quantize functions (#39738)", "body": "Summary:\nAs discussed in https://github.com/pytorch/pytorch/issues/39502.\n\nThis PR adds support for exporting  `fake_quantize_per_tensor_affine` to a pair of `QuantizeLinear` and `DequantizeLinear`.\n\nExporting `fake_quantize_per_channel_affine` to ONNX depends on https://github.com/onnx/onnx/pull/2772. will file another PR once ONNX merged the change.\n\nIt will generate ONNX graph like this:\n![image](https://user-images.githubusercontent.com/1697840/84180123-ddd90080-aa3b-11ea-81d5-eaf6f5f26715.png)\n\njamesr66a\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39738\n\nReviewed By: hl475\n\nDifferential Revision: D22517911\n\nPulled By: houseroad\n\nfbshipit-source-id: e998b4012e11b0f181b193860ff6960069a91d70", "pr_number": "39738", "files_changed": ["test/onnx/model_defs/op_test.py", "test/onnx/test_models.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset10.py"], "labels": ["merged", "open source", "triaged"]}, "f27e395a4a": {"title": "[Gloo] update gloo submodule for PyTorch (#41462)", "body": "Summary:\nTo include alltoall\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41462\n\nTest Plan: CI\n\nReviewed By: osalpekar\n\nDifferential Revision: D22544255\n\nPulled By: jiayisuse\n\nfbshipit-source-id: ad55a50a31e5e5affaf3e14e2401d38f99657dc9", "pr_number": "41462", "files_changed": ["third_party/gloo"], "labels": ["fb-exported", "merged"]}, "702140758f": {"title": "Move GLOG_ constants into c10 namespace (#41504)", "body": "Summary:\nDeclaring GLOG_ constants in google namespace causes a conflict in C++ project that uses GLOG and links with LibPyTorch compiled without GLOG.\nFor example, see https://github.com/facebookresearch/ReAgent/issues/288\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41504\n\nReviewed By: kaiwenw\n\nDifferential Revision: D22564308\n\nPulled By: malfet\n\nfbshipit-source-id: 2167bd2c6124bd14a67cc0a1360521d3c375e3c2", "pr_number": "41504", "files_changed": ["c10/util/Logging.cpp", "c10/util/logging_is_not_google_glog.h"], "labels": ["merged"]}, "b2e52186b9": {"title": "Rename capacity to nbytes in ShareExternalPointer to avoid confusion in future (#41461)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41461\n\ncapacity is misleading, and we have many wrong uses internally. Let's rename to nbytes to avoid the confusion in future. Ultimately, we could remove this parameter if possible.\nSo far I haven't seen any case this capacity is necessary.\n\nTest Plan: oss ci\n\nDifferential Revision: D22544189\n\nfbshipit-source-id: f310627f2ab8f4ebb294e0dd5eabc380926991eb", "pr_number": "41461", "files_changed": ["aten/src/ATen/test/tensor_interop_test.cpp", "caffe2/core/tensor.h"], "labels": ["fb-exported", "merged"]}, "04c0f2e3cc": {"title": "enable TE on windows (#41501)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41501\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22563872\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 2b5730017b34af27800cc03f3ba62f1cc8b4f240", "pr_number": "41501", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".jenkins/pytorch/win-test-helpers/test_python_jit_profiling.bat", ".jenkins/pytorch/win-test.sh"], "labels": ["merged"]}, "e6859ec78f": {"title": "resurrect single quantization op test (#41476)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41476\n\ndeleted this test by default, re-adding it in its own file to make it\nmore explicit\n\nTest Plan: ran the test\n\nReviewed By: yinghai\n\nDifferential Revision: D22550217\n\nfbshipit-source-id: 758e279b2bab3b23452a3d0ce75fb366f7afb7be", "pr_number": "41476", "files_changed": ["caffe2/contrib/fakelowp/test/test_int8_quant.py"], "labels": ["fb-exported", "merged"]}, "26790fb26d": {"title": "fix quantization mechanism to match nnpi (#41494)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41494\n\nrevert back to the changes from amylittleyang to make quantization work\n\nTest Plan:\nran against a dump from ctr_instagram, and verified that:\n-nnpi and fakelowp match bitwise\n-nnpi is different at most by 1 vs fbgemm, most likely due to the type of\nrounding\n\nReviewed By: venkatacrc\n\nDifferential Revision: D22555276\n\nfbshipit-source-id: 7074521d181f15ef6270985bb71c4b44d25d1c30", "pr_number": "41494", "files_changed": ["caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h"], "labels": ["fb-exported", "merged"]}, "d80e0c62be": {"title": "fix dequantization to match nnpi (#41505)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41505\n\nfix the dequantization to match the fixes from quantization\n\nTest Plan:\ntest is not conclusive, since only comparing emulation with reference collected from Amy's run\n\nrunning an evaluation workflow at the moment\n\nReviewed By: venkatacrc\n\nDifferential Revision: D22558092\n\nfbshipit-source-id: 3ff00ea15eac76007e194659c3b4949f07ff02a4", "pr_number": "41505", "files_changed": ["caffe2/contrib/fakelowp/int8_dequantize_op_nnpi.h"], "labels": ["fb-exported", "merged"]}, "b9442bb03e": {"title": "Doc note for complex (#41252)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41252\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D22553266\n\nPulled By: anjali411\n\nfbshipit-source-id: f6dc409da048496d72b29b0976dfd3dd6645bc4d", "pr_number": "41252", "files_changed": ["docs/source/complex_numbers.rst", "docs/source/index.rst", "docs/source/notes/autograd.rst"], "labels": ["merged"]}, "5bba973afd": {"title": "Reland split unsafe version (#41484)", "body": "Summary:\nReland of https://github.com/pytorch/pytorch/pull/39299\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41484\n\nReviewed By: glaringlee\n\nDifferential Revision: D22552377\n\nPulled By: albanD\n\nfbshipit-source-id: 1d1b713d2429ae162e04bda845ef0838c52df789", "pr_number": "41484", "files_changed": ["aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensor_view.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/onnx/symbolic_opset9.py"], "labels": ["merged"]}, "45c5bac870": {"title": "[WIP] Fix cpp grad accessor API (#40887)", "body": "Summary:\nUpdate the API to access grad in cpp to avoid unexpected thread safety issues.\nIn particular, with the current API, a check like `t.grad().defined()` is not thread safe.\n\n- This introduces `t.mutable_grad()` that should be used when getting a mutable version of the saved gradient. This function is **not** thread safe.\n- The `Tensor& grad()` API is now removed. We could not do a deprecation cycle as most of our call side use non-const Tensors that use the non-const overload. This would lead to most calls hitting the warning. This would be too verbose for all the users.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40887\n\nReviewed By: ezyang\n\nDifferential Revision: D22343932\n\nPulled By: albanD\n\nfbshipit-source-id: d5eb909bb743bc20caaf2098196e18ca4110c5d2", "pr_number": "40887", "files_changed": ["aten/src/ATen/templates/TensorBody.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "test/cpp/api/nn_utils.cpp", "test/cpp/api/optim.cpp", "test/custom_operator/test_custom_ops.cpp", "torch/csrc/api/src/nn/module.cpp", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/functions/accumulate_grad.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/variable.h", "torch/csrc/distributed/c10d/reducer.cpp"], "labels": ["merged", "topic: bc-breaking"]}, "2b14f2d368": {"title": "[reland][DNNL]:enable max_pool3d and avg_pool3d (#40996)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40996\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22440766\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 242711612920081eb4a7e5a7e80bc8b2d4c9f978", "pr_number": "40996", "files_changed": ["aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/mkldnn/Pooling.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_mkldnn.py", "torch/_overrides.py"], "labels": ["merged", "open source", "triaged"]}, "58244a9586": {"title": "Automated submodule update: FBGEMM (#40332)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/73ea1f5828d7a71162cd3c13b5e9d9898b04a3df\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40332\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: gchanan, yns88\n\nDifferential Revision: D22150737\n\nfbshipit-source-id: fe7e6787adef9e2fedee5d1a0a1e57bc4760b88c", "pr_number": "40332", "files_changed": ["third_party/fbgemm"], "labels": ["merged", "open source", "triaged"]}, "b2b8af9645": {"title": "Removes assertAlmostEqual (#41514)", "body": "Summary:\nThis test function is confusing since our `assertEqual` behavior allows for tolerance to be specified, and this is a redundant mechanism.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41514\n\nReviewed By: ngimel\n\nDifferential Revision: D22569348\n\nPulled By: mruberry\n\nfbshipit-source-id: 2b2ff8aaa9625a51207941dfee8e07786181fe9f", "pr_number": "41514", "files_changed": ["test/quantization/test_workflow_module.py", "test/test_bundled_inputs.py", "test/test_distributions.py", "test/test_jit_py3.py", "test/test_nn.py", "test/test_optim.py", "test/test_torch.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged"]}, "fef30220fd": {"title": "Runs CUDA test_istft_of_sine on CUDA (#41523)", "body": "Summary:\nThe test was always running on the CPU. This actually caused it to throw an error on non-MKL builds, since the CUDA test (which ran on the CPU) tried to execute but the test requires MKL (a requirement only checked for the CPU variant of the test).\n\nFixes https://github.com/pytorch/pytorch/issues/41402.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41523\n\nReviewed By: ngimel\n\nDifferential Revision: D22569344\n\nPulled By: mruberry\n\nfbshipit-source-id: e9908c0ed4b5e7b18cc7608879c6213fbf787da2", "pr_number": "41523", "files_changed": ["test/test_torch.py"], "labels": ["merged"]}, "9d92fa2679": {"title": "[NCCL] Add timeout to ProcessGroup Work Wait (#40944)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40944\n\nThis stack adds Work-level timeout for blocking wait.\n\nThis PR just changes the API to accept a default wait arg for the wait function in each ProcessGroup backend. The ProcessGroup superclass correctly waits for the given timeout by changing the CV wait to wait_for.\n\nCloses: https://github.com/pytorch/pytorch/issues/37571\nghstack-source-id: 107835735\n\nTest Plan: Tests in 4th PR in this stack\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22107135\n\nfbshipit-source-id: b38c07cb5e79e6c86c205e580336e7918ed96501", "pr_number": "40944", "files_changed": ["test/cpp_extensions/cpp_c10d_extension.cpp", "test/cpp_extensions/cpp_c10d_extension.hpp", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupMPI.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["merged"]}, "edf3dc73f2": {"title": "[NCCL] Support Wait Timeout in ProcessGroupNCCL (#40946)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40946\n\nAdds timeout to ProcessGroupNCCL::wait. Currently, WorkNCCL objects already have a timeout set during ProcessGroupNCCL construction. The new wait function will override the existing timeout with the user-defined timeout if one is provided. Timed out operations result in NCCL communicators being aborted and an exception being thrown.\nghstack-source-id: 107835739\n\nTest Plan: Test added to `ProcessGroupNCCLTest` in the next PR in this stack.\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22127898\n\nfbshipit-source-id: 543964855ac5b41e464b2df4bb6c211ef053e73b", "pr_number": "40946", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["merged"]}, "01dcef2e15": {"title": "[NCCL] Tests for WorkNCCL::wait with Timeouts (#40947)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40947\n\nThis PR adds tests for work-level timeouts in WorkNCCL objects. We kick off an allgather operation that waits for 1000ms before actually starting computation. We wait on completion of this allgather op with a timeout of 250ms, expecting the operation to timeout and throw a runtime error.\nghstack-source-id: 107835734\n\nTest Plan: This diff added tests - checking CI/Sandcastle for correctness. These are NCCL tests so they require at least 2 GPUs to run.\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22173101\n\nfbshipit-source-id: 8595e4b67662cef781b20ced0befdcc53d157c39", "pr_number": "40947", "files_changed": ["torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["merged"]}, "b979129cba": {"title": "[Gloo] Support work-level timeouts in ProcessGroupGloo (#40948)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40948\n\nAdd work-level timeouts to ProcessGroupGloo. This uses the timeout support in `waitSend` and `waitRecv` functions from Gloo's `unbound_buffer` construct.\nghstack-source-id: 107835738\n\nTest Plan: Tests are in the last PR in this stack\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22173763\n\nfbshipit-source-id: e0493231a23033464708ee2bc0e295d2b087a1c9", "pr_number": "40948", "files_changed": ["torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["merged"]}, "81e964904e": {"title": "[Gloo] Tests for Gloo Async Work Wait-level Timeouts (#41265)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41265\n\nThis PR adds tests for the Async Work wait-level timeouts that were added in the previous PR\nghstack-source-id: 107835732\n\nTest Plan: New tests are in this diff - Running on local machine and Sandcastle\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22470084\n\nfbshipit-source-id: 5552e384d384962e359c5f665e6572df03b6aa63", "pr_number": "41265", "files_changed": ["torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["merged"]}, "94e4248d80": {"title": "Split ASAN and ROCM tests into test1 and test2 (#41520)", "body": "Summary:\nThis should reduce end-to-end test runtime for 2 slowest configs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41520\n\nReviewed By: seemethere\n\nDifferential Revision: D22575028\n\nPulled By: malfet\n\nfbshipit-source-id: a65bfa5932fcda3cf0f4fdd97bcc7ebb3f54c281", "pr_number": "41520", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml"], "labels": ["merged"]}, "b5e32528d0": {"title": "Fix flaky test_udf_remote_message_delay_timeout_to_self (#41217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41217\n\nFixes this flaky test. Due to the possibility of callback\nfinishCreatingOwnerRRef running after request_callback has processed and\ncreated the owner RRef, we could actually end up with 0 owners on the node,\nsince the callback removes from the owners_ map. In this case, shutdown is fine\nsince there are no owners. On the other hand, if the callback runs first, there\nwill be 1 owner which we will delete in shutdown when we detect it has no\nforks. So either way, shutdown works fine and we don't need to enforce there to\nbe 1 owner.\nghstack-source-id: 107883497\n\nTest Plan: Ran the test 500 times with TSAN.\n\nReviewed By: ezyang\n\nDifferential Revision: D22469806\n\nfbshipit-source-id: 02290d6d5922f91a9e2d5ede21d1cf1c4598cb46", "pr_number": "41217", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "1ac4692489": {"title": "Remove unnecessary test in rpc_test.py (#41218)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41218\n\nThis test doesn't assert anything and was accidentally committed as\npart of a larger diff a few months ago.\nghstack-source-id: 107882848\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D22469852\n\nfbshipit-source-id: 0baa23da56b08200e16cf66df514566223dd9b15", "pr_number": "41218", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "6f5f455c54": {"title": "[Gloo] alltoall to ProcessGroupGloo (#41424)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41424\n\nAdding alltoall to Gloo process group\n\nTest Plan:\nbuck test caffe2/torch/lib/c10d:ProcessGroupGlooTest\n\nVerified on TSC as well D22141532\n\nReviewed By: osalpekar\n\nDifferential Revision: D22451929\n\nfbshipit-source-id: 695c4655c894c85229b16097fa63352ed04523ef", "pr_number": "41424", "files_changed": ["torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["fb-exported", "merged"]}, "09647e1287": {"title": "RandomSampler generates samples one at a time when replacement=True (#40026)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/32530\n\nI used the next() function to generate samples one at a time. To compensate replacement=False, I added a variable called \"sample_list\" to RandomSampler for random permutation.\n\ncc SsnL\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40026\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22519869\n\nPulled By: ezyang\n\nfbshipit-source-id: be65850025864d659a713b3bc461b25d6d0048a2", "pr_number": "40026", "files_changed": ["test/test_dataloader.py", "torch/utils/data/sampler.py"], "labels": ["merge-this-please", "merged", "open source"]}, "ba6b235461": {"title": "[RocM] Switch to rocm-3.5.1 image (#41273)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41273\n\nReviewed By: seemethere\n\nDifferential Revision: D22575277\n\nPulled By: malfet\n\nfbshipit-source-id: 6f43654c8c8c33adbc1de928dd43911931244978", "pr_number": "41273", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/cimodel/data/simple/util/docker_constants.py", ".circleci/config.yml"], "labels": ["merged"]}, "86590f226e": {"title": "Revert D22519869: [pytorch][PR] RandomSampler generates samples one at a time when replacement=True", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22519869 (https://github.com/pytorch/pytorch/commit/09647e1287f4490b4ef3ada2d34c1f9a82b342e9)\n\nOriginal commit changeset: be6585002586\n\nfbshipit-source-id: 31ca5ceb24dd0b291f46f427a6f30f1037252a5d", "pr_number": null, "files_changed": ["test/test_dataloader.py", "torch/utils/data/sampler.py"], "labels": []}, "f49d97a848": {"title": "Notes for lcm and gcd, formatting doc fixes (#41526)", "body": "Summary:\nA small PR fixing some formatting in lcm, gcd, and the serialization note. Adds a note to lcm and gcd explaining behavior that is not always defined.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41526\n\nReviewed By: ngimel\n\nDifferential Revision: D22569341\n\nPulled By: mruberry\n\nfbshipit-source-id: 5f5ff98c0831f65e82b991ef444a5cee8e3c8b5a", "pr_number": "41526", "files_changed": ["docs/source/notes/serialization.rst", "torch/_torch_docs.py"], "labels": ["merged"]}, "e324ea85ea": {"title": "Add tests to logical operation in BinaryOpsKernel.cpp (#41515)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41515\n\nadd test in atest.cpp to cover logical_and_kernel, logical_or_kernel and logical_nor_kernel in Aten/native/cpu/BinaryOpsKernel.cpp\n\nhttps://pxl.cl/1drmV\n\nTest Plan: CI\n\nReviewed By: malfet\n\nDifferential Revision: D22565235\n\nfbshipit-source-id: 7ad9fd8420d7fdd23fd9a703c75da212f72bde2c", "pr_number": "41515", "files_changed": ["aten/src/ATen/test/atest.cpp"], "labels": ["fb-exported", "merged"]}, "454cd3ea2e": {"title": "Fix RocM resource class allocation (#41553)", "body": "Summary:\nAdd Conf.is_test_stage() method to avoid duplicating state in ['test', 'test1', 'test2'] throughout the code\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41553\n\nTest Plan: Make sure that in modified config.yml ROCM tests jobs are assigned `pytorch/amd-gpu` resource class\n\nReviewed By: yns88\n\nDifferential Revision: D22580471\n\nPulled By: malfet\n\nfbshipit-source-id: 514555f0c0ac94c807bf837ba209560055335587", "pr_number": "41553", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml"], "labels": ["merged"]}, "a0e58996fb": {"title": "Makes the use of the term \"module\" consistent through the serialization note (#41563)", "body": "Summary:\nmodule -> torch.nn.Module or ScriptModule, as appropriate. + bonus grammar fix.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41563\n\nReviewed By: gchanan\n\nDifferential Revision: D22584173\n\nPulled By: mruberry\n\nfbshipit-source-id: 8c90f1f9a194bfdb277c97cf02c9b8c1c6ddc601", "pr_number": "41563", "files_changed": ["docs/source/notes/serialization.rst"], "labels": ["merged"]}, "9ed825746a": {"title": "Use c10::cuda:: primitives rather than make CUDA runtime calls directly (#41405)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41405\n\nTest Plan:\n**Imported from GitHub: all checks have passed**\n\n{F244195355}\n\n**The Intern Builds & Tests have 127 success, 5 no signals, and 1 failure. Double check the failed test log file, the failure is result differences:**\n- AssertionError: 0.435608434677124 != 0.4356083869934082\n- AssertionError: 0.4393022060394287 != 0.4393021583557129\n- AssertionError: 0.44707541465759276 != 0.44707536697387695\n\nThese are all very small numerical errors (within 0.0000001).\n\nReviewed By: malfet\n\nDifferential Revision: D22531486\n\nPulled By: threekindoms\n\nfbshipit-source-id: 21543ec76bb9b502885b5146c8ba5ede719be9ff", "pr_number": "41405", "files_changed": ["torch/csrc/cuda/Module.cpp"], "labels": ["merged"]}, "415ff0bceb": {"title": "Create lazy_dyndeps to avoid caffe2 import costs. (#41343)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41343\n\nCurrently caffe2.InitOpLibrary does the dll import uniliaterally. Instead if we make a lazy version and use it, then many pieces of code which do not need the caffe2urrenoperators get a lot faster.\n\nOne a real test, the import time went from 140s to 68s. 8s.\n\nThis also cleans up the algorithm slightly (although it makes a very minimal\ndifference), by parsing the list of operators once, rather than every time a\nnew operator is added, since we defer the RefreshCall until after we've\nimported all the operators.\n\nThe key way we maintain safety, is that as soon as someone does an operation\nwhich requires a operator (or could), we force importing of all available\noperators.\n\nFuture work could include trying to identify which code is needed for which\noperator and only import the needed ones. There may also be wins available by\nplaying with dlmopen (which opens within a namespace), or seeing if the dl\nflags have an impact (I tried this and didn't see an impact, but dlmopen may\nmake it better).\n\nNote that this was previously landed and reverted. The issue was that if a import failed and raised an exception, the specific library would not be removed from the lazy imports. This caused our tests which had libraries that failed to poison all other tests that ran after it. This has been fixed and a unit test has been added for this case (to help make it obvious what failed).\n\nTest Plan:\nI added a new test a lazy_dyndep_test.py (copied from all_compare_test.py).\nI'm a little concerned that I don't see any explicit tests for dyndep, but this\nshould provide decent coverage.\n\nI've added a specific test to handle the poisoning issues mentioned above, which caused the previous version to get reverted.\n\nDifferential Revision: D22506369\n\nfbshipit-source-id: 7395df4778e8eb0220630c570360b99a7d60eb83", "pr_number": "41343", "files_changed": ["caffe2/python/core.py", "caffe2/python/dyndep.py", "caffe2/python/lazy_dyndep.py", "caffe2/python/lazy_dyndep_test.py"], "labels": ["fb-exported", "merged"]}, "b1d4e33c8b": {"title": "Revert D22552377: [pytorch][PR] Reland split unsafe version", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22552377 (https://github.com/pytorch/pytorch/commit/5bba973afd81b8df5c4dd85fa6f2ec38e3549f04)\n\nOriginal commit changeset: 1d1b713d2429\n\nfbshipit-source-id: 8194458f99bfd5f077b7daa46ca3e81b549adc1b", "pr_number": null, "files_changed": ["aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensor_view.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/onnx/symbolic_opset9.py"], "labels": []}, "728fd37d92": {"title": "[JIT] make fastrnns runnable on cpu (#41483)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41483\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D22580275\n\nPulled By: eellison\n\nfbshipit-source-id: f2805bc7fa8037cfde7862b005d2940add3ac864", "pr_number": "41483", "files_changed": ["benchmarks/fastrnns/bench.py"], "labels": ["merged"]}, "5376785a70": {"title": "Run NO_AVX jobs on CPU (#41565)", "body": "Summary:\nDelete \"nogpu\" job since both \"AVX\" and \"AVX2\" jobs already act like one\nFix naming problem when NO_AVX_NO_AVX2 job and NO_AVX2 jobs were semantically identical, due to the following logic in test.sh:\n```\nif [[ \"${BUILD_ENVIRONMENT}\" == *-NO_AVX-* ]]; then\n  export ATEN_CPU_CAPABILITY=default\nelif [[ \"${BUILD_ENVIRONMENT}\" == *-NO_AVX2-* ]]; then\n  export ATEN_CPU_CAPABILITY=avx\nfi\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41565\n\nReviewed By: seemethere\n\nDifferential Revision: D22584743\n\nPulled By: malfet\n\nfbshipit-source-id: 783cce60f35947b5d1e8b93901db36371ef78243", "pr_number": "41565", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml"], "labels": ["merged"]}, "eb3bf96f95": {"title": "During inbatch broadcast, move Tile op after Fused8BitRowwiseQuantizedToFloat if applicable (#41464)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41464\n\nIf input is int8 rowwise quantized, currently we cannot low it to Glow. And previously, we had some error when running with inbatch broadcast. The main issue is that Tile op doesn't support uint8_t type, which is very easily added here. However, this will result in non-ideal situation that we will leave Tile -> Fused8BitRowwiseQuantizedToFloat on host side, which probably hurt the memory bw a lot. Even we later add the support to Fused8BitRowwiseQuantizedToFloat in Glow, it's still not ideal because we are doing redudant compute on identical columns. So the solution here is to swap the order of Fused8BitRowwiseQuantizedToFloat and Tile to make it Tile -> Fused8BitRowwiseQuantizedToFloat. In this way, it will resolve the error we saw immediately. For the short term, we can still run Tile in card. And for longer term, things runs faster on card.\n\nThe optimization is a heuristic. If in the net, there isn't such pattern, inbatch broadcast will work as it was before.\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\n```\nbuck test caffe2/caffe2/opt/custom:in_batch_broadcast_test\n```\n\nReviewed By: benjibc\n\nDifferential Revision: D22544162\n\nfbshipit-source-id: b6dd36a5925a9c8103b80f034e7730a7a085a6ff", "pr_number": "41464", "files_changed": ["caffe2/operators/tile_op.cc", "caffe2/opt/custom/in_batch_broadcast.cc", "caffe2/opt/custom/in_batch_broadcast.h", "caffe2/opt/custom/in_batch_broadcast_test.cc"], "labels": ["fb-exported", "merged"]}, "e3e58e20cd": {"title": "enable jit profiling tests on macos (#41550)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41550\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22579593\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 3e67bcf418ef266d5416b7fac413e94b1ac1ec7e", "pr_number": "41550", "files_changed": [".jenkins/pytorch/macos-test.sh"], "labels": ["merged"]}, "cb9029df9d": {"title": "Assert valid inner type for OptionalType creation (#41509)", "body": "Summary:\nAssert in OptionalType::create for valid TypePtr to catch all uses, as well as in python resolver to propagate slightly more helpful error message.\n\nCloses https://github.com/pytorch/pytorch/issues/40713.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41509\n\nReviewed By: suo\n\nDifferential Revision: D22563710\n\nPulled By: wconstab\n\nfbshipit-source-id: ee6314b1694a55c1ba7c8251260ea120be148b17", "pr_number": "41509", "files_changed": ["aten/src/ATen/core/jit_type.h", "torch/jit/annotations.py"], "labels": ["merged", "oncall: jit"]}, "1e230a5c52": {"title": "rewrite C++ __torch_function__ handling to work with TensorList operands (#41575)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41575\n\nFixes https://github.com/pytorch/pytorch/issues/34294\n\nThis updates the C++ argument parser to correctly handle `TensorList` operands. I've also included a number of updates to the testing infrastructure, this is because we're now doing a much more careful job of testing the signatures of aten kernels, using the type information about the arguments as read in from `Declarations.yaml`. The changes to the tests are required because we're now only checking for `__torch_function__` attributes on `Tensor`, `Optional[Tensor]` and elements of `TensorList` operands, whereas before we were checking for `__torch_function__` on all operands, so the relatively simplistic approach the tests were using before -- assuming all positional arguments might be tensors -- doesn't work anymore. I now think that checking for `__torch_function__` on all operands was a mistake in the original design.\n\nThe updates to the signatures of the `lambda` functions are to handle this new, more stringent checking of signatures.\n\nI also added override support for `torch.nn.functional.threshold` `torch.nn.functional.layer_norm`, which did not yet have python-level support.\n\nBenchmarks are still WIP.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34725\n\nReviewed By: mruberry\n\nDifferential Revision: D22357738\n\nPulled By: ezyang\n\nfbshipit-source-id: 0e7f4a58517867b2e3f193a0a8390e2ed294e1f3", "pr_number": "41575", "files_changed": [".gitignore", "caffe2/CMakeLists.txt", "test/test_overrides.py", "tools/autograd/gen_annotated_fn_args.py", "tools/autograd/templates/annotated_fn_args.py", "tools/setup_helpers/generate_code.py", "torch/_overrides.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/nn/functional.py", "torch/testing/_internal/generated/__init__.py"], "labels": ["fb-exported", "merged"]}, "71fdf748e5": {"title": "Add `torch.atleast_{1d/2d/3d}` (#41317)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/38349\n\nTODO:\n * [x] Docs\n * [x] Tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41317\n\nReviewed By: ngimel\n\nDifferential Revision: D22575456\n\nPulled By: mruberry\n\nfbshipit-source-id: cc79f4cd2ca4164108ed731c33cf140a4d1c9dd8", "pr_number": "41317", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/TensorTransformations.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/torch.rst", "test/test_autograd.py", "test/test_torch.py", "tools/pyi/gen_pyi.py", "torch/_overrides.py", "torch/functional.py"], "labels": ["merged", "open source", "triaged"]}, "c7798ddf7b": {"title": "Initial implementation of quantile operator (#39417)", "body": "Summary:\nImplementing the quantile operator similar to [numpy.quantile](https://numpy.org/devdocs/reference/generated/numpy.quantile.html).\n\nFor this implementation I'm reducing it to existing torch operators to get free CUDA implementation. It is more efficient to implement multiple quickselect algorithm instead of sorting but this can be addressed in a future PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39417\n\nReviewed By: mruberry\n\nDifferential Revision: D22525217\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 27a8bb23feee24fab7f8c228119d19edbb6cea33", "pr_number": "39417", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged", "module: numpy"]}, "241bc648c9": {"title": "Adding missing setting `state_.ptr()` and `hook_.ptr()` to `nullptr`. (#41537)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41537\n\nExplicitly setting PyObject* state_ and hook_ to nullptr to prevent py::object's dtor to decref on the PyObject again.\nReference PR [#40848](https://github.com/pytorch/pytorch/pull/40848).\nghstack-source-id: 107959254\n\nTest Plan: `python test/distributed/test_c10d.py`\n\nReviewed By: zou3519\n\nDifferential Revision: D22573858\n\nfbshipit-source-id: 84cc5949a370ffdb4ac3ca7a16a6f0f136563c1c", "pr_number": "41537", "files_changed": ["torch/csrc/distributed/c10d/comm.h"], "labels": ["merged"]}, "5d7046522b": {"title": "[JIT] Teach IRPrinter and IRParser to handle 'requires_grad' and 'device' as a part of type info. (#41507)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41507\n\nThese fields have always been a part of tensor types, this change just\nmakes them serializable through IR dumps.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin, ngimel\n\nDifferential Revision: D22563661\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: f01aaa130b7e0005bf1ff21f65827fc24755b360", "pr_number": "41507", "files_changed": ["aten/src/ATen/core/type.cpp", "test/cpp/jit/test_constant_pooling.cpp", "test/cpp/jit/test_irparser.cpp", "test/cpp/tensorexpr/test_kernel.cpp", "test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/test_jit.py", "torch/csrc/jit/frontend/schema_type_parser.cpp", "torch/csrc/jit/frontend/schema_type_parser.h"], "labels": ["merged", "oncall: jit"]}, "324c18fcad": {"title": "fix division by low precision scalar (#41446)", "body": "Summary:\nBefore, inverse for division by scalar is calculated in the precision of the non-scalar operands, which can lead to underflow:\n```\n>>> x = torch.tensor([3388.]).half().to(0)\n>>> scale = 524288.0\n>>> x.div(scale)\ntensor([0.], device='cuda:0', dtype=torch.float16)\n>>> x.mul(1. / scale)\ntensor([0.0065], device='cuda:0', dtype=torch.float16)\n```\nThis PR makes results of multiplication by inverse and division the same.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41446\n\nReviewed By: ezyang\n\nDifferential Revision: D22542872\n\nPulled By: ngimel\n\nfbshipit-source-id: b60e3244809573299c2c3030a006487a117606e9", "pr_number": "41446", "files_changed": ["aten/src/ATen/AccumulateType.h", "aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "test/test_torch.py"], "labels": ["merged"]}, "7eb71b4beb": {"title": "Profiler: Do not record zero duration kernel events (#41540)", "body": "Summary:\nChanges in the ROCm runtime have improved hipEventRecord.  The events no longer take ~4 usec to execute on the gpu stream, instead they appear instantaneous.  If you record two events, with no other activity in between, then they will have the same timestamp and the elapsed duration will be 0.\n\nThe profiler uses hip/cuda event pairs to infer gpu execution times.  It wraps functions whether they send work to the gpu or not.  Functions that send no gpu work will show as having zero duration.  Also they will show as running at the same time as neighboring functions.  On a trace, all those functions combine into a 'call stack' that can be tens of functions tall (when indeed they should be sequential).\n\nThis patch suppresses recording the zero duration 'kernel' events, leaving only the CPU execution part.  This means functions that do not use the GPU do not get an entry for how long they were using the GPU, which seams reasonable.  This fixes the 'stacking' on traces.  It also improves the signal to noise of the GPU trace beyond what was available previously.\n\nThis patch will not effect CUDA or legacy ROCm as those are not able to 'execute' eventRecord markers instantaneously.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41540\n\nReviewed By: zou3519\n\nDifferential Revision: D22597207\n\nPulled By: albanD\n\nfbshipit-source-id: 5e89de2b6d53888db4f9dbcb91a94478cde2f525", "pr_number": "41540", "files_changed": ["torch/autograd/profiler.py"], "labels": ["merged", "module: profiler", "module: rocm", "open source", "triaged"]}, "346c69a626": {"title": "[ONNX] Export embedding_bag (#41234)", "body": "Summary:\nEnable export of embedding_bag op to ONNX\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41234\n\nReviewed By: houseroad\n\nDifferential Revision: D22567470\n\nPulled By: bzinodev\n\nfbshipit-source-id: 2fcf74e54f3a9dee4588d7877a4ac9eb6c2a3629", "pr_number": "41234", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset10.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "319b20b7db": {"title": "[ONNX] Update ORT version (#41372)", "body": "Summary:\nUpdate ORT version [1.4 candidate].\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41372\n\nReviewed By: houseroad\n\nDifferential Revision: D22580050\n\nPulled By: bzinodev\n\nfbshipit-source-id: c66e3bab865b3221d52eea30db48e0870ae5b681", "pr_number": "41372", "files_changed": [".jenkins/caffe2/test.sh"], "labels": ["merged", "open source"]}, "43b1923d98": {"title": "Enable SLS FP32 accumulation SparseLengthsWeightedSumFused8BitRowwiseFakeFP32NNPI Op. (#41577)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41577\n\n* Remove skipping test\n* Use fma_avx_emulation\n* Increase test examples to 100\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: Tests are covered in test_sls_8bit_nnpi.py\n\nReviewed By: hyuen\n\nDifferential Revision: D22585742\n\nfbshipit-source-id: e1f62f47eb10b402b11893ffca7a6786e31daa79", "pr_number": "41577", "files_changed": ["caffe2/contrib/fakelowp/fp16_fma.cc", "caffe2/contrib/fakelowp/fp16_fma.h", "caffe2/contrib/fakelowp/lengths_reducer_fused_8bit_rowwise_fp16_fake_op.h", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp32.py"], "labels": ["fb-exported", "merged"]}, "f85a27e100": {"title": "[JIT] Replace \"blacklist\" in test_jit.py (#41453)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41453\n\n**Test Plan**\n`python test/test_jit.py`\n\n**Fixes**\nThis commit partially addresses #41443.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D22544268\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 8b6b94211a626209c3960fda6c860593148dcbf2", "pr_number": "41453", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "3b7c05b11b": {"title": "[JIT] Replace uses of \"blacklist\" in gen_unboxing_wrappers.py (#41454)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41454\n\n**Test Plan**\nContinuous integration (if this file is still used).\n\n**Fixes**\nThis commit partially addresses #41443.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D22544271\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 84a4d552745fe5163b2e3200103c3b1f2a9ffb2a", "pr_number": "41454", "files_changed": ["tools/jit/gen_unboxing_wrappers.py"], "labels": ["merged", "oncall: jit"]}, "c9bdf474d7": {"title": "[JIT] Replace use of \"blacklist\" in xnnpack_rewrite (#41455)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41455\n\n**Test Plan**\nContinuous integration.\n\n**Fixes**\nThis commit partially addresses #41443.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D22544275\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 5037b16e6ebc9e3b40dd03d2ce5a0671d7867892", "pr_number": "41455", "files_changed": ["torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.h"], "labels": ["merged", "oncall: jit"]}, "758edcd7df": {"title": "[JIT] Replace use of \"blacklist\" in python/init.cpp (#41456)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41456\n\n**Test Plan**\nContinuous integration.\n\n**Fixes**\nThis commit partially addresses #41443.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D22544270\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 649b30e1fcc6516a4def6b148a1da07bc3ce941d", "pr_number": "41456", "files_changed": ["torch/csrc/jit/python/init.cpp"], "labels": ["merged", "oncall: jit"]}, "bf0d0900a7": {"title": "[JIT] Replace uses of \"blacklist\" in jit/_recursive.py (#41457)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41457\n\n**Test Plan**\nContinuous integration.\n\n**Fixes**\nThis commit partially addresses #41443.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D22544274\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: ee74860c48d85d819d46c8b8848960e77bb5013e", "pr_number": "41457", "files_changed": ["torch/jit/_recursive.py"], "labels": ["merged", "oncall: jit"]}, "4f4e3a0f15": {"title": "[JIT] Replace uses of \"whitelist\" in jit/_script.py (#41458)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41458\n\n**Test Plan**\nContinuous integration.\n\n**Fixes**\nThis commit partially fixes #41443.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D22544273\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 8148e5338f90a5ef19177cf68bf36b56926d5a6c", "pr_number": "41458", "files_changed": ["torch/jit/_script.py"], "labels": ["merged", "oncall: jit"]}, "c2c2c1c106": {"title": "[JIT] Remove use of \"whitelist\" in quantization/helper.cpp (#41459)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41459\n\n**Test Plan**\nContinuous integration.\n\n**Fixes**\nThis commit partially addresses #41443.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22544269\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: d4bb7c0c9c71e953677a34f0530b66e5119447d0", "pr_number": "41459", "files_changed": ["torch/csrc/jit/passes/quantization/helper.cpp"], "labels": ["merged", "oncall: jit"]}, "fbd960801a": {"title": "[JIT] Replace use of \"whitelist\" in lower_tuples pass (#41460)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41460\n\n**Test Plan**\nContinuous integration.\n\n**Fixes**\nThis commit partially addresses #41443.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D22544272\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: b46940d1e24f81756daaace260bad7a1feda1e8f", "pr_number": "41460", "files_changed": ["torch/csrc/jit/passes/lower_tuples.cpp"], "labels": ["merged", "oncall: jit"]}, "3c862c80cf": {"title": "Move list size constants for profiler::Event and profiler::ProfilerConfig into (#40474)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40474\n\nThese constants are unnecessary since there is an enum, and we can add\nthe size at the end of the enum and it will be equal to the list size. I\nbelieve that this is the typical pattern used to represent enum sizes.\nghstack-source-id: 107969012\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D22147754\n\nfbshipit-source-id: 7064a897a07f9104da5953c2f87b58179df8ea84", "pr_number": "40474", "files_changed": ["torch/csrc/autograd/profiler.cpp"], "labels": ["merged"]}, "0f78e596ba": {"title": "ROCm: Fix linking of custom ops in load_inline (#41257)", "body": "Summary:\nPreviously we did not link against amdhip64 (roughly equivalent to cudart). Apparently, the recent RTDL_GLOBAL fixes prevent the extensions from finding the symbols needed for launching kernels.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41257\n\nReviewed By: zou3519\n\nDifferential Revision: D22573288\n\nPulled By: ezyang\n\nfbshipit-source-id: 89f9329b2097df26785e2f67e236d60984d40fdd", "pr_number": "41257", "files_changed": ["test/test_cpp_extensions_jit.py", "torch/utils/cpp_extension.py"], "labels": ["merge-this-please", "merged", "module: rocm", "open source", "triaged"]}, "a874c1e584": {"title": "Adds missing abs to lcm (#41552)", "body": "Summary:\nlcm was missing an abs. This adds it plus extends the test for NumPy compliance. Also includes a few doc fixes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41552\n\nReviewed By: ngimel\n\nDifferential Revision: D22580997\n\nPulled By: mruberry\n\nfbshipit-source-id: 5ce1db56f88df4355427e1b682fcf8877458ff4e", "pr_number": "41552", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "test/test_torch.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "cf811d2fb3": {"title": "retain undefined tensors in backward pass (#41490)", "body": "Summary:\nLeave undefined tensors / None returned from custom backward functions as undefined/None instead of creating a tensor full of zeros. This change improves performance in some cases.\n\n**This is BC-Breaking:** Custom backward functions that return None will now see it potentially being propagated all the way up to AccumulateGrad nodes. Potential impact is that .grad field of leaf tensors as well as the result of autograd.grad may be undefined/None where it used to be a tensor full of zeros. Also, autograd.grad may raise an error, if so, consider using allow_unused=True ([see doc](https://pytorch.org/docs/stable/autograd.html?highlight=autograd%20grad#torch.autograd.grad)) if it applies to your case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41490\n\nReviewed By: albanD\n\nDifferential Revision: D22578241\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: f4966f4cb520069294f8c5c1691eeea799cc0abe", "pr_number": "41490", "files_changed": ["test/cpp/api/autograd.cpp", "test/test_autograd.py", "torch/csrc/autograd/custom_function.h", "torch/csrc/autograd/python_function.cpp"], "labels": ["merged", "topic: bc-breaking"]}, "92b95e5243": {"title": "Fix NCCL version check when nccl.h in non-standard location. (#40982)", "body": "Summary:\nThe NCCL discovery process fails to compile detect_nccl_version.cc when nccl.h resides in a non-standard location.\nPass __NCCL_INCLUDE_DIRS__ to _try_run(... detect_nccl_version.cc)_ to fix this.\n\nCan reproduce with Dockerfile ..\n```Dockerfile\nFROM nvidia/cuda:10.2-cudnn7-devel-ubuntu18.04 as build\nWORKDIR /stage\n\n# install conda\nARG CONDA_VERSION=4.7.10\nARG CONDA_URL=https://repo.anaconda.com/miniconda/Miniconda3-${CONDA_VERSION}-Linux-x86_64.sh\nRUN cd /stage && curl -fSsL --insecure ${CONDA_URL} -o install-conda.sh &&\\\n    /bin/bash ./install-conda.sh -b -p /opt/conda &&\\\n    /opt/conda/bin/conda clean -ya\nENV PATH=/opt/conda/bin:${PATH}\n\n# install prerequisites\nRUN conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi\n\n# attempt compile\nENV CUDA_HOME=\"/usr/local/cuda\" \\\n    CUDNN_LIBRARY=\"/usr/lib/x86_64-linux-gnu\" \\\n    NCCL_INCLUDE_DIR=\"/usr/local/cuda/include\" \\\n    NCCL_LIB_DIR=\"/usr/local/cuda/lib64\" \\\n    USE_SYSTEM_NCCL=1\nRUN apt-get -y update &&\\\n    apt-get -y install git &&\\\n    cd /stage && git clone https://github.com/pytorch/pytorch.git &&\\\n    cd pytorch &&\\\n    git submodule update --init --recursive &&\\\n    python setup.py bdist_wheel\n```\n\nThis generates the following error ..\n```\n-- Found NCCL: /usr/local/cuda/include\n-- Determining NCCL version from /usr/local/cuda/include/nccl.h...\n-- Looking for NCCL_VERSION_CODE\n-- Looking for NCCL_VERSION_CODE - found\nCMake Error at cmake/Modules/FindNCCL.cmake:78 (message):\n  Found NCCL header version and library version do not match! (include:\n  /usr/local/cuda/include, library: /usr/local/cuda/lib64/libnccl.so) Please\n  set NCCL_INCLUDE_DIR and NCCL_LIB_DIR manually.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40982\n\nReviewed By: zou3519\n\nDifferential Revision: D22603911\n\nPulled By: malfet\n\nfbshipit-source-id: 084870375a270fb9c7daf3c2e731992a03614ad6", "pr_number": "40982", "files_changed": ["cmake/Modules/FindNCCL.cmake"], "labels": ["merged", "open source", "triaged"]}, "349c40507c": {"title": "Revert \"[CircleCI] Delete docker image after testing\" (#41601)", "body": "Summary:\nPer AMD request, this reverts commit 1e64bf4c40ef82d6bc3dcc42b3874353f7632be0.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41601\n\nReviewed By: ezyang\n\nDifferential Revision: D22603147\n\nPulled By: malfet\n\nfbshipit-source-id: f423d406601383f26ea83a51f1de37e60b53810e", "pr_number": "41601", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "39b4701d31": {"title": "[caffe2][redo] Reimplement RemoveOpsByType with SSA (#41606)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41606\n\nThe previous diff (D22220798 (https://github.com/pytorch/pytorch/commit/59294fbbb9efc7eec8352deae84966702ad24cf3) and D22220797) was recently reverted (D22492356 (https://github.com/pytorch/pytorch/commit/28291d3cf82d73c78ce871cb6946d1a6fee240db), D22492355) because of a bug associated with the op AsyncIf. The AsyncIf op has net_defs as args and the SSA rewriting didn't take that into account. It has a special path for the op If, but not for AsyncIf. Several changes I made to fix the bug:\n1) Add op AsyncIf to the special path for If op in SSA rewriting\n2) clear inputs/outputs of the netdefs that are args in If/AsyncIf ops because they're no longer valid\n3) revert renamed inputs/outputs in the arg netdefs that are in the external_outputs in the parent netdef\n\n2) and 3) are existing bugs in the `SsaRewrite` function that were just never exposed before.\n\nThe algorithm for `RemoveOpsByType` is the same as in my previous diff D22220798 (https://github.com/pytorch/pytorch/commit/59294fbbb9efc7eec8352deae84966702ad24cf3). The only new changes in this diff are in `onnx::SsaRewrite` and a few newly added unit tests.\n\n(Note: this ignores all push blocking failures!)\n\nReviewed By: yinghai\n\nDifferential Revision: D22588652\n\nfbshipit-source-id: ebb68ecd1662ea2bae14d4be8f61a75cd8b7e3e6", "pr_number": "41606", "files_changed": ["caffe2/onnx/onnx_exporter.cc", "caffe2/opt/tvm_transformer.cc", "caffe2/predictor/InferenceGraph.h", "caffe2/predictor/transforms.cc", "caffe2/predictor/transforms.h", "caffe2/utils/proto_utils.cc"], "labels": ["fb-exported", "merged"]}, "8fdea489af": {"title": "remediation of S205607", "body": "fbshipit-source-id: 5113fe0c527595e4227ff827253b7414abbdf7ac", "pr_number": null, "files_changed": [".circleci/cimodel/__init__.py", ".circleci/cimodel/data/__init__.py", ".circleci/cimodel/data/simple/__init__.py", ".circleci/cimodel/data/simple/util/__init__.py", ".circleci/cimodel/lib/__init__.py", ".github/PULL_REQUEST_TEMPLATE.md", ".python3", "aten/src/ATen/cudnn/Exceptions.h", "aten/src/ATen/native/LegacyBridge.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/dummy.c", "benchmarks/operator_benchmark/__init__.py", "benchmarks/operator_benchmark/c2/__init__.py", "benchmarks/operator_benchmark/common/__init__.py", "benchmarks/operator_benchmark/pt/__init__.py", "caffe2/__init__.py", "caffe2/contrib/__init__.py", "caffe2/contrib/aten/__init__.py", "caffe2/contrib/aten/docs/__init__.py", "caffe2/contrib/gloo/__init__.py", "caffe2/contrib/nccl/__init__.py", "caffe2/contrib/nnpack/__init__.py", "caffe2/contrib/playground/__init__.py", "caffe2/contrib/playground/resnetdemo/__init__.py", "caffe2/contrib/prof/__init__.py", "caffe2/contrib/script/__init__.py", "caffe2/contrib/script/examples/__init__.py", "caffe2/contrib/tensorboard/__init__.py", "caffe2/contrib/warpctc/__init__.py", "caffe2/core/__init__.py", "caffe2/core/nomnigraph/__init__.py", "caffe2/distributed/__init__.py", "caffe2/experiments/__init__.py", "caffe2/experiments/python/__init__.py", "caffe2/perfkernels/__init__.py", "caffe2/python/docs/__init__.py", "caffe2/python/examples/__init__.py", "caffe2/python/fakelowp/__init__.py", "caffe2/python/helpers/__init__.py", "caffe2/python/ideep/__init__.py", "caffe2/python/mint/__init__.py", "caffe2/python/mkl/__init__.py", "caffe2/python/modeling/__init__.py", "caffe2/python/models/__init__.py", "caffe2/python/models/seq2seq/.python3", "caffe2/python/models/seq2seq/__init__.py", "caffe2/python/onnx/__init__.py", "caffe2/python/onnx/bin/__init__.py", "caffe2/python/operator_test/__init__.py", "caffe2/python/predictor/__init__.py", "caffe2/python/serialized_test/__init__.py", "caffe2/python/test/__init__.py", "caffe2/python/trt/__init__.py", "caffe2/quantization/__init__.py", "caffe2/quantization/server/__init__.py", "test/cpp/__init__.py", "test/cpp/jit/__init__.py", "test/cpp/tensorexpr/__init__.py", "test/cpp_api_parity/__init__.py", "test/cpp_extensions/torch_test_cpp_extension/__init__.py", "test/distributed/nn/api/__init__.py", "test/distributed/nn/jit/__init__.py", "test/expect/__init__.py", "test/jit/__init__.py", "test/jit/_imported_class_test/__init__.py", "test/jit/_imported_class_test/very/__init__.py", "test/jit/_imported_class_test/very/very/__init__.py", "test/quantization/__init__.py", "third_party/BUILD", "tools/__init__.py", "tools/autograd/__init__.py", "tools/jit/__init__.py", "tools/pyi/__init__.py", "tools/rules/BUILD", "torch/contrib/__init__.py", "torch/csrc/empty.c", "torch/csrc/jit/tensorexpr/buffer.cpp", "torch/cuda/error.py", "torch/distributed/nn/api/__init__.py", "torch/distributed/nn/jit/__init__.py", "torch/distributed/nn/jit/templates/__init__.py", "torch/nn/backends/__init__.py", "torch/py.typed", "torch/testing/_internal/__init__.py", "torch/testing/_internal/codegen/__init__.py", "torch/testing/_internal/data/__init__.py", "torch/testing/_internal/distributed/__init__.py", "torch/testing/_internal/distributed/nn/__init__.py", "torch/testing/_internal/distributed/nn/api/__init__.py", "torch/testing/_internal/distributed/rpc/__init__.py", "torch/testing/_internal/distributed/rpc/jit/__init__.py", "torch/testing/_internal/generated/__init__.py", "torch/testing/_internal/test_module/__init__.py", "torch/utils/__init__.pyi", "torch/utils/bottleneck/__init__.py", "torch/utils/hipify/__init__.py"], "labels": []}, "b774ce54f8": {"title": "remediation of S205607", "body": "fbshipit-source-id: 798decc90db4f13770e97cdce3c0df7d5421b2a3", "pr_number": null, "files_changed": [".circleci/cimodel/__init__.py", ".circleci/cimodel/data/__init__.py", ".circleci/cimodel/data/simple/__init__.py", ".circleci/cimodel/data/simple/util/__init__.py", ".circleci/cimodel/lib/__init__.py", ".github/PULL_REQUEST_TEMPLATE.md", ".python3", "aten/src/ATen/cudnn/Exceptions.h", "aten/src/ATen/native/LegacyBridge.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/dummy.c", "benchmarks/operator_benchmark/__init__.py", "benchmarks/operator_benchmark/c2/__init__.py", "benchmarks/operator_benchmark/common/__init__.py", "benchmarks/operator_benchmark/pt/__init__.py", "caffe2/__init__.py", "caffe2/contrib/__init__.py", "caffe2/contrib/aten/__init__.py", "caffe2/contrib/aten/docs/__init__.py", "caffe2/contrib/gloo/__init__.py", "caffe2/contrib/nccl/__init__.py", "caffe2/contrib/nnpack/__init__.py", "caffe2/contrib/playground/__init__.py", "caffe2/contrib/playground/resnetdemo/__init__.py", "caffe2/contrib/prof/__init__.py", "caffe2/contrib/script/__init__.py", "caffe2/contrib/script/examples/__init__.py", "caffe2/contrib/tensorboard/__init__.py", "caffe2/contrib/warpctc/__init__.py", "caffe2/core/__init__.py", "caffe2/core/nomnigraph/__init__.py", "caffe2/distributed/__init__.py", "caffe2/experiments/__init__.py", "caffe2/experiments/python/__init__.py", "caffe2/perfkernels/__init__.py", "caffe2/python/docs/__init__.py", "caffe2/python/examples/__init__.py", "caffe2/python/fakelowp/__init__.py", "caffe2/python/helpers/__init__.py", "caffe2/python/ideep/__init__.py", "caffe2/python/mint/__init__.py", "caffe2/python/mkl/__init__.py", "caffe2/python/modeling/__init__.py", "caffe2/python/models/__init__.py", "caffe2/python/models/seq2seq/.python3", "caffe2/python/models/seq2seq/__init__.py", "caffe2/python/onnx/__init__.py", "caffe2/python/onnx/bin/__init__.py", "caffe2/python/operator_test/__init__.py", "caffe2/python/predictor/__init__.py", "caffe2/python/serialized_test/__init__.py", "caffe2/python/test/__init__.py", "caffe2/python/trt/__init__.py", "caffe2/quantization/__init__.py", "caffe2/quantization/server/__init__.py", "test/cpp/__init__.py", "test/cpp/jit/__init__.py", "test/cpp/tensorexpr/__init__.py", "test/cpp_api_parity/__init__.py", "test/cpp_extensions/torch_test_cpp_extension/__init__.py", "test/distributed/nn/api/__init__.py", "test/distributed/nn/jit/__init__.py", "test/expect/__init__.py", "test/jit/__init__.py", "test/jit/_imported_class_test/__init__.py", "test/jit/_imported_class_test/very/__init__.py", "test/jit/_imported_class_test/very/very/__init__.py", "test/quantization/__init__.py", "third_party/BUILD", "tools/__init__.py", "tools/autograd/__init__.py", "tools/jit/__init__.py", "tools/pyi/__init__.py", "tools/rules/BUILD", "torch/contrib/__init__.py", "torch/csrc/empty.c", "torch/csrc/jit/tensorexpr/buffer.cpp", "torch/cuda/error.py", "torch/distributed/nn/api/__init__.py", "torch/distributed/nn/jit/__init__.py", "torch/distributed/nn/jit/templates/__init__.py", "torch/nn/backends/__init__.py", "torch/py.typed", "torch/testing/_internal/__init__.py", "torch/testing/_internal/codegen/__init__.py", "torch/testing/_internal/data/__init__.py", "torch/testing/_internal/distributed/__init__.py", "torch/testing/_internal/distributed/nn/__init__.py", "torch/testing/_internal/distributed/nn/api/__init__.py", "torch/testing/_internal/distributed/rpc/__init__.py", "torch/testing/_internal/distributed/rpc/jit/__init__.py", "torch/testing/_internal/generated/__init__.py", "torch/testing/_internal/test_module/__init__.py", "torch/utils/__init__.pyi", "torch/utils/bottleneck/__init__.py", "torch/utils/hipify/__init__.py"], "labels": []}, "1734f24276": {"title": "Revert D22525217: [pytorch][PR] Initial implementation of quantile operator", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22525217 (https://github.com/pytorch/pytorch/commit/c7798ddf7b4ba77a331a81ca8e8d17dccb7f13eb)\n\nOriginal commit changeset: 27a8bb23feee\n\nfbshipit-source-id: 3beb3d4f8a4d558e993fbdfe977af12c7153afc8", "pr_number": null, "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": []}, "445e7eb01b": {"title": "Add quantized CELU operator by adding additional parameters to quantized ELU (#39199)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39199\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D21771202\n\nPulled By: durumu\n\nfbshipit-source-id: 910de6202fa3d5780497c5bf85208568a09297dd", "pr_number": "39199", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qelu.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "aten/src/ATen/native/quantized/library.cpp", "benchmarks/operator_benchmark/pt/qactivation_test.py", "test/quantization/test_quantized_op.py", "tools/autograd/derivatives.yaml", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/nn/quantized/functional.py"], "labels": ["merged"]}, "96ac12fdf4": {"title": "[PT] add overload name for int prim ops (#41578)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41578\n\nA new op aten::gcd(Tensor...) was added while the duplicated op name check was disabled. It's not a prime op, but it has the same name with one prime op aten::gcd(int, int).\n\nIt will be safer to enforce all prim ops have overload name, even there is no duplicated name right now. People may add tensor ops without overload name in the future.\n\nThis diff added the overload name for all ops defined using \"DEFINE_INT_OP\".\n\n```\naten::__and__\naten::__or__\naten::__xor__\naten::__lshift__\naten::__rshift__\naten::__round_to_zero_floordiv\naten::gcd\n```\n\nTest Plan: run full JIT predictor\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22593689\n\nfbshipit-source-id: b3335d356a774d33450a09d0a43ff947197f9b8a", "pr_number": "41578", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "torch/csrc/jit/runtime/register_ops_utils.h", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "d72c9f4200": {"title": "[PT] add check for duplicated op names in JIT (#41549)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41549\n\nD22467871 (https://github.com/pytorch/pytorch/commit/a548c6b18ff5a337b6329ed7b2381cfb4d3da2ed) was reverted due to double linking torch_mobile_train.\nRe-do this change after D22531358 (https://github.com/pytorch/pytorch/commit/7a33d8b001b1f8dc59c07d0701d615093a81a741).\n\nTest Plan:\nbuck install fb4a\nTrain mnist in Internal Settings.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22533824\n\nfbshipit-source-id: b36884531d41cea2e76b7fb1a567f21106c612b6", "pr_number": "41549", "files_changed": ["torch/csrc/jit/runtime/operator.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "581e9526bb": {"title": "[GradualGating] support better k value change (#41557)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41557\n\n - add new learning rate functor \"slope\"\n - use \"slope\" learning rate in gated_sparse_feature module\n\nTest Plan:\nbuck test dper3/dper3/modules/tests:core_modules_test -- test_gated_sparse_features_shape_num_warmup_tensor_k\nbuck test caffe2/caffe2/python/operator_test:learning_rate_op_test -- test_slope_learning_rate_op\n\nReviewed By: huayuli00\n\nDifferential Revision: D22544628\n\nfbshipit-source-id: f2fcae564e79e1d8bcd3a2305d0c11ca7c0d3b3c", "pr_number": "41557", "files_changed": ["caffe2/python/operator_test/learning_rate_op_test.py", "caffe2/sgd/learning_rate_functors.h", "caffe2/sgd/learning_rate_op.h"], "labels": ["fb-exported", "merged"]}, "c6d0fdd215": {"title": "torch.isreal (#41298)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/38349\n\nmruberry\nNot entirely sure if all the changes are necessary in how functions are added to Pytorch.\n\nShould it throw an error when called with a non-complex tensor? Numpy allows non-complex arrays in its imag() function which is used in its isreal() function but Pytorch's imag() throws an error for non-complex arrays.\n\nWhere does assertONNX() get its expected output to compare to?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41298\n\nReviewed By: ngimel\n\nDifferential Revision: D22610500\n\nPulled By: mruberry\n\nfbshipit-source-id: 817d61f8b1c3670788b81690636bd41335788439", "pr_number": "41298", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged", "module: complex", "module: operators", "open source", "triaged"]}, "e7a09b4d17": {"title": "RecordFunction in Dispatcher (#37587)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37587\n\nLifting RecordFunction up into the dispatcher code\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D21374246\n\nfbshipit-source-id: 19f9c1719e6fd3990e451c5bbd771121e91128f7", "pr_number": "37587", "files_changed": ["aten/src/ATen/SequenceNumber.cpp", "aten/src/ATen/SequenceNumber.h", "aten/src/ATen/ThreadLocalState.cpp", "aten/src/ATen/ThreadLocalState.h", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/boxing/impl/boxing.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/ObservedOperators.cpp", "aten/src/ATen/core/dispatch/ObservedOperators.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h", "test/cpp/jit/test_misc.cpp", "test/test_autograd.py", "test/test_jit.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/ProfiledType.cpp", "tools/autograd/templates/TraceType.cpp", "torch/autograd/profiler.py", "torch/csrc/autograd/function.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/python_function.cpp", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/jit/mobile/register_mobile_autograd.cpp", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["oncall: jit"]}, "c7bcb285f3": {"title": "Makes elementwise comparison docs more consistent (#41626)", "body": "Summary:\n- Removes outdated language like \"BoolTensor\"\n- Consistently labels keyword arguments, like out\n- Uses a more natural string to describe their return type\n- A few bonus fixes\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41626\n\nReviewed By: ngimel\n\nDifferential Revision: D22617322\n\nPulled By: mruberry\n\nfbshipit-source-id: 03cc3562b78a07ed30bd1dc7936d7a4f4e31f01d", "pr_number": "41626", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "module: docs"]}, "46eb8d997c": {"title": "Revert D22533824: [PT] add check for duplicated op names in JIT", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22533824 (https://github.com/pytorch/pytorch/commit/d72c9f42009bcf35e491259ca1a08bfcf69cbc0a)\n\nOriginal commit changeset: b36884531d41\n\nfbshipit-source-id: 8bf840a09b4001cc68858a5dc3540505a0e1abdc", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/operator.cpp"], "labels": []}, "4a3aad354a": {"title": "[1/N] Implement Enum JIT support (#41390)", "body": "Summary:\n* Add EnumType and AnyEnumType as first-class jit type\n* Add Enum-typed IValue\n* Enhanced aten::eq to support Enum\n\nSupported:\nEnum-typed function targuments\nusing Enum type and comparing them\n\nTODO:\nAdd PyThon sugared value for Enum\nSupport getting name/value attrs of enums\nSupport Enum-typed return values\nSupport enum values of different types in same Enum class\nSupport serialization and deserialization\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41390\n\nReviewed By: eellison\n\nDifferential Revision: D22524388\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 1627154a64e752d8457cd53270f3d14aea4b1150", "pr_number": "41390", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "aten/src/ATen/test/ivalue_test.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/jit/test_enum.py", "test/test_jit.py", "torch/_jit_internal.py", "torch/_overrides.py", "torch/csrc/jit/frontend/schema_type_parser.cpp", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_ir.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/serialization/unpickler.cpp", "torch/jit/annotations.py"], "labels": ["merged", "oncall: jit"]}, "a69a262810": {"title": "workaround segfault in deviceGuard construction (#41621)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41621\n\nPer title. In some situation, deviceGuard constructor in mul_kernel_cuda segfaults, so construct deviceGuard conditionally only when first argument is scalar.\nThis does not root cause why deviceGuard constructor segfaults, so the issue might come back.\n\nTest Plan: pytorch oss CI\n\nReviewed By: jianyuh\n\nDifferential Revision: D22616460\n\nfbshipit-source-id: b91bbe55c6eb0bbe80b8d6a61c41f09288752658", "pr_number": "41621", "files_changed": ["aten/src/ATen/native/cuda/BinaryMulDivKernel.cu"], "labels": ["fb-exported", "merged"]}, "bd42e1a082": {"title": "Doc language fixes (#41643)", "body": "Summary:\nUpdates doc for abs, acos, and isinf for clarity and consistency.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41643\n\nReviewed By: ngimel\n\nDifferential Revision: D22622957\n\nPulled By: mruberry\n\nfbshipit-source-id: 040f01b4e101153098577bf10dcd569b679aae2c", "pr_number": "41643", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "9600ed9af3": {"title": "typo fixes (#41632)", "body": "Summary:\ntypo fixes\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41632\n\nReviewed By: ezyang\n\nDifferential Revision: D22617827\n\nPulled By: mrshenli\n\nfbshipit-source-id: c2bfcb7cc36913a8dd32f13fc9adc3aa0a9b682f", "pr_number": "41632", "files_changed": ["torch/_lobpcg.py", "torch/autograd/profiler.py", "torch/cuda/__init__.py", "torch/distributed/nn/api/remote_module.py", "torch/distributed/rpc/functions.py", "torch/distributed/rpc/server_process_global_profiler.py", "torch/hub.py", "torch/jit/_trace.py", "torch/nn/functional.py", "torch/nn/modules/container.py", "torch/nn/quantized/functional.py", "torch/optim/swa_utils.py", "torch/quantization/observer.py", "torch/sparse/__init__.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py", "torch/utils/hipify/hipify_python.py", "torch/utils/tensorboard/writer.py"], "labels": ["merged", "oncall: jit", "open source"]}, "16dde6e3a0": {"title": "Augmenting Observers to Support Dynamic Quantization Range (#41113)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41113\n\nIn this diff, the `ObserverBase` class is augmented with 2 additional optional arguments qmin and qmax. Correspondingly the calculation of qmin and qmax and the related quantization parameters are modified to accommodate this additional flexibility should the number of bits for quantization be lower than 8 (the default value).\n\nAdditional logic in the base class `_calculate_qparams` function has also been modified to provide support for dynamic quantization range.\n\nTest Plan:\nTo ensure this modification is still backward compatible with past usages, numerics are verified by running the quantization unit test suite, which contains various observer tests. The following command executes the test suite, which also verifies the observer numerics:\n\n`buck test //caffe2/test:quantization -- observer`\n\nThis modified observer script can be tested within the experiments for lower bit fake quantization. Please see the following diffs for reference.\n- Single Fake Quantizer: D22337447\n- Single Conv Layer: D22338532\n\nReviewed By: z-a-f\n\nDifferential Revision: D22427134\n\nfbshipit-source-id: f405e633289322078b0f4a417f54b684adff2549", "pr_number": "41113", "files_changed": ["torch/quantization/observer.py"], "labels": ["fb-exported", "merged"]}, "6769b850b2": {"title": "Remove needless test duplication (#41583)", "body": "Summary:\nThe test loops over `upper` but does not use it effectively running the same test twice which increases test times for no gain.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41583\n\nReviewed By: soumith, seemethere, izdeby\n\nDifferential Revision: D22598475\n\nPulled By: zou3519\n\nfbshipit-source-id: d100f20143293a116ff3ba08b0f4eaf0cc5a8099", "pr_number": "41583", "files_changed": ["test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "ce443def01": {"title": "Grammar patch 1 (.md) (#41599)", "body": "Summary:\nA minor spell check!\nI have gone through a dozen of .md files to fix the typos.\nzou3519 take a look!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41599\n\nReviewed By: ezyang\n\nDifferential Revision: D22601629\n\nPulled By: zou3519\n\nfbshipit-source-id: 68d8f77ad18edc1e77874f778b7dadee04b393ef", "pr_number": "41599", "files_changed": [".circleci/README.md", "CONTRIBUTING.md", "caffe2/README.md", "docs/caffe2/README.md", "test/HowToWriteTestsUsingFileCheck.md", "third_party/miniz-2.0.8/ChangeLog.md", "torch/csrc/jit/tensorexpr/DesignOverview.md"], "labels": ["merged", "oncall: jit", "open source"]}, "26bbbeaea4": {"title": "[DOCS] Fix the docs for the inputs arg of trace_module func (#41586)", "body": "Summary:\nFix the docs for the `inputs` arg of `trace_module` func.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41586\n\nReviewed By: ezyang\n\nDifferential Revision: D22598453\n\nPulled By: zou3519\n\nfbshipit-source-id: c2d182238b5a51f6d0a7d0683372d72a239146c5", "pr_number": "41586", "files_changed": ["torch/jit/_trace.py"], "labels": ["merged", "oncall: jit", "open source"]}, "cc3c18edbc": {"title": "More LayerNorm Vectorization in calcMeanStd function. (#41618)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41618\n\nMore LayerNorm Vectorization in calcMeanStd function.\n\nTest Plan: test covered in test_layernorm_nnpi_fp16.py\n\nReviewed By: hyuen\n\nDifferential Revision: D22606585\n\nfbshipit-source-id: be773e62f0fc479dbc2d6735f60c2e98441916e9", "pr_number": "41618", "files_changed": ["caffe2/contrib/fakelowp/layernorm_fp16_fake_op.cc"], "labels": ["fb-exported", "merged"]}, "cfcee816f1": {"title": ".circleci: Prefix docker jobs with docker- (#41689)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41689\n\nIt's annoying not to know which jobs are actually related to docker\nbuilds so let's just add the prefix.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22631578\n\nPulled By: seemethere\n\nfbshipit-source-id: ac0cdd983ccc3bebcc360ba479b378d8f0eaa9c0", "pr_number": "41689", "files_changed": [".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml"], "labels": ["merged", "module: ci"]}, "6161730174": {"title": "[JIT] move remove mutation to its own test file (#41502)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41502\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D22629270\n\nPulled By: eellison\n\nfbshipit-source-id: fcec6ae4ff8f108164539d67427ef3d72fa07494", "pr_number": "41502", "files_changed": ["test/jit/test_functional_blocks.py", "test/jit/test_remove_mutation.py", "test/test_jit.py"], "labels": ["merged", "oncall: jit"]}, "de400fa5ac": {"title": "[JIT] handle specially mapped ops (#41503)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41503\n\nFix for https://github.com/pytorch/pytorch/issues/41192\n\nWe can map fill_ and zero_ to their functional equivalents full_like and zeros_like\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D22629269\n\nPulled By: eellison\n\nfbshipit-source-id: f1c62684dc55682c0b3845022e0461ec77d07179", "pr_number": "41503", "files_changed": ["test/jit/test_remove_mutation.py", "torch/csrc/jit/passes/remove_mutation.cpp"], "labels": ["merged", "oncall: jit"]}, "897cabc081": {"title": "Add operators for smart keyboard to lite interpreter (#41539)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41539\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22574746\n\nPulled By: ann-ss\n\nfbshipit-source-id: 3e2b78385149d7bde2598c975e60845a766ef86a", "pr_number": "41539", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["merged", "oncall: jit"]}, "f07816003a": {"title": "[2/n][Compute Meta] support analysis for null flag features", "body": "Summary:\n## TLDR\nSupport using NaN default value for missing dense features in RawInputProcessor for DPER2. In preparation for subsequent support for null flag features in compute meta. For train_eval this is already supported in DPER3 and we do not plan to support this in DPER2 train eval.\n\nDifferential Revision: D22439142\n\nfbshipit-source-id: 99ae9755bd41a5d5f43bf5a9a2819d64f3883005", "pr_number": null, "files_changed": ["caffe2/python/layer_model_helper.py", "caffe2/python/layers/feature_sparse_to_dense.py", "caffe2/python/utils.py"], "labels": []}, "30551ea7b2": {"title": "Update NCCL from 2.4.8 to 2.7.3 (#41608)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41608\n\nReviewed By: mrshenli, ngimel\n\nDifferential Revision: D22604953\n\nPulled By: malfet\n\nfbshipit-source-id: 28151e2d5b6ea360b79896cb79c761756687d121", "pr_number": "41608", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "third_party/nccl/nccl"], "labels": ["merged"]}, "1039bbf4eb": {"title": "add named parameters to mobile module (#41376)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41376\n\ntorch::jit::mobile::Module does not currently support accessing parameters via their attribute names, but torch::jit::Module does. This diff adds an equivalent functionality to mobile::Module.\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22609142\n\nPulled By: ann-ss\n\nfbshipit-source-id: 1a5272ff336f99a3c0bb6194c6a6384754f47846", "pr_number": "41376", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/test_lite_trainer.cpp", "test/cpp/jit/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h"], "labels": ["merged", "oncall: jit"]}, "3a9a64a4da": {"title": "Add non zero offset test cases for Quantize and Dequantize Ops. (#41693)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41693\n\nAdd non zero offset test cases for Quantize and Dequantize Ops.\n\nTest Plan: Added new test case test_int8_non_zero_offset_quantize part of the test_int8_ops_nnpi.py test file.\n\nReviewed By: hyuen\n\nDifferential Revision: D22633796\n\nfbshipit-source-id: be17ee7a0caa6e9bc7b175af539be2e6625ad47a", "pr_number": "41693", "files_changed": ["caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py"], "labels": ["fb-exported", "merged"]}, "5c50cb567c": {"title": "Generalized Learnable Fake Quantizer Module (#41535)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41535\n\nA generalized fake quantization module is built to support lower-bit fake quantization with back propagation on the scale and zero point. The module supports both per tensor and per channel fake quantization.\n\nTest Plan:\nPlease see diff D22337313 for a related experiment performed on the fake quantizer module.\n\nThe `_LearnableFakeQuantize` module supports the following use cases:\n- Per Tensor Fake Quantization or Per Channel Fake Quantization\n- Static Estimation from Observers or Quantization Parameter Learning through Back Propagation\n\nBy default, the module assumes per tensor affine fake quantization. To switch to per channel, during initialization, declare `channel_size` with the appropriate length. To toggle between utilizing static estimation and parameter learning with back propagation, you can invoke the call `enable_param_learning` or `enable_static_estimate`. For more information on the flags that support these operations, please see the doc string of the `_LearnableFakeQuantize` module.\n\nThe `_LearnableFakeQuantizer` module relies on 2 operators for its forward and backward paths: `_LearnableFakeQuantizePerTensorOp` and `_LearnableFakeQuantizePerChannelOp`. The backpropagation routine is developed based on the following literature:\n- Learned Step Size Quantization: https://openreview.net/pdf?id=rkgO66VKDS\n- Trained Quantization Thresholds: https://arxiv.org/pdf/1903.08066.pdf\n\nReviewed By: z-a-f\n\nDifferential Revision: D22573645\n\nfbshipit-source-id: cfd9ece8a959ae31c00d9beb1acf9dfed71a7ea1", "pr_number": "41535", "files_changed": ["torch/quantization/_learnable_fake_quantize.py"], "labels": ["fb-exported", "merged"]}, "65bd38127a": {"title": "GLOO process group GPU alltoall (#41690)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41690\n\nGloo alltoall for GPU\n\nTest Plan: buck test mode/dev-nosan caffe2/torch/lib/c10d:ProcessGroupGlooTest\n\nReviewed By: osalpekar\n\nDifferential Revision: D22631554\n\nfbshipit-source-id: 4b126d9d991a118f3925c005427f399fc60f92f7", "pr_number": "41690", "files_changed": ["torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["fb-exported", "merged"]}, "fe415589a9": {"title": "disable mkl for expm1 (#41654)", "body": "Summary:\nOn some systems/mkl versions it produces expm1(nan)=-1\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41654\n\nReviewed By: mruberry\n\nDifferential Revision: D22621333\n\nPulled By: ngimel\n\nfbshipit-source-id: 84544679fe96aed7de6873dce6f31f488e5e35dd", "pr_number": "41654", "files_changed": ["aten/src/ATen/cpu/vml.h"], "labels": ["merged"]}, "c89c294ef9": {"title": "Add Unflatten Module (#41564)", "body": "Summary:\nThis PR implements a feature extension discussed in https://github.com/pytorch/pytorch/issues/41516.\n\nI followed this other PR https://github.com/pytorch/pytorch/issues/22245 to add this other module. While I was at it, I also added `extra_repr()` method in `Flatten` which was missing.\n\nI see there are no unit tests for these modules. Should I add those too? If so, what is the best place I should place these?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41564\n\nReviewed By: gchanan\n\nDifferential Revision: D22636766\n\nPulled By: albanD\n\nfbshipit-source-id: f9efdefd3ffe7d9af9482087625344af8f990943", "pr_number": "41564", "files_changed": ["docs/source/nn.rst", "test/test_nn.py", "torch/nn/modules/__init__.py", "torch/nn/modules/flatten.py"], "labels": ["merged", "open source", "triaged"]}, "48569cc330": {"title": "Reland split (#41567)", "body": "Summary:\nTake 3\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41567\n\nReviewed By: zou3519\n\nDifferential Revision: D22586331\n\nPulled By: albanD\n\nfbshipit-source-id: ca08199da716d64a335455610edbce752fee224b", "pr_number": "41567", "files_changed": ["aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensor_view.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/onnx/symbolic_opset9.py"], "labels": ["merged"]}, "1f11e930d0": {"title": "[ROCm] skip test_streams on rocm. (#41697)", "body": "Summary:\nSkipping the test test_streams as it is flaky on rocm.\ncc: jeffdaily  sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41697\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22644600\n\nPulled By: malfet\n\nfbshipit-source-id: b1b16d496e58a91c44c40d640851fd62a5d7393d", "pr_number": "41697", "files_changed": ["test/test_cuda.py"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "523f80e894": {"title": ".circleci: Remove docker_hub_index_job, wasn't used (#41800)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41800\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: soumith\n\nDifferential Revision: D22645363\n\nPulled By: seemethere\n\nfbshipit-source-id: 35ed43ed5fb4053f71dc9525c4ed62f1c60eacc1", "pr_number": "41800", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/docker_jobs.yml", ".circleci/verbatim-sources/workflows/workflows-ecr-gc.yml"], "labels": ["merged", "module: ci"]}, "2da2b5c081": {"title": "update CONTRIBUTING.md for ccache (#41619)", "body": "Summary:\nccache now use cmake for building, update installation script.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41619\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22644594\n\nPulled By: malfet\n\nfbshipit-source-id: f894dd408822231f8aab36efbce188f06f004057", "pr_number": "41619", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged", "open source", "triaged"]}, "72a1146339": {"title": "Skip warning 4522 with MSVC (#41648)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41648\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22644623\n\nPulled By: malfet\n\nfbshipit-source-id: 7fb86f05b3d8cd6a4c7c0e3fdfd651b70a5094c9", "pr_number": "41648", "files_changed": ["aten/src/ATen/templates/TensorBody.h"], "labels": ["merged", "open source", "triaged"]}, "46808b49a8": {"title": "Change whitelist to allow in file test_quantized_op.py (#41771)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41751\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41771\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22641463\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 1a60af8d43ccdf1f35dc84dbf4a7bc64965eb44a", "pr_number": "41771", "files_changed": ["test/quantization/test_quantized_op.py"], "labels": ["merged", "open source", "triaged"]}, "341c4045df": {"title": "replaced blacklist with blocklist in test/test_type_hints.py (#41644)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41719.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41644\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22645479\n\nPulled By: zou3519\n\nfbshipit-source-id: 82710acae96ab508b8e9198dadb7d7911cb97235", "pr_number": "41644", "files_changed": ["test/test_type_hints.py"], "labels": ["merged", "open source", "triaged"]}, "03186a86d9": {"title": "Add test dependencies to CONTRIBUTING.md (#41799)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41799\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22645323\n\nPulled By: zou3519\n\nfbshipit-source-id: 0a695bffb57b29024461472dd1c8518a9a0d1d3b", "pr_number": "41799", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged", "module: docs", "open source"]}, "1ad7160a59": {"title": "fix backward compat (#41810)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41810\n\nReviewed By: malfet\n\nDifferential Revision: D22647763\n\nPulled By: albanD\n\nfbshipit-source-id: 8ce70ecb706bb98ed24b0b3e7e9ebf3d4c270964", "pr_number": "41810", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "62f4f87914": {"title": "Removed whitelist reference from tools/clang_format_ci.sh (#41636)", "body": "Summary:\nRemoved whitelist and blacklist references\nFixes https://github.com/pytorch/pytorch/issues/41753\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41636\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22648632\n\nPulled By: suo\n\nfbshipit-source-id: d22130a7cef96274f3fc73d00b50327dfcae332c", "pr_number": "41636", "files_changed": ["tools/clang_format_ci.sh"], "labels": ["merged", "open source", "triaged"]}, "dac393fa24": {"title": "[PT] enforce duplicate op name check on mobile", "body": "Summary: Enforce duplicate op name check on mobile\n\nTest Plan: run full/lite predictor\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22639758\n\nfbshipit-source-id: 2993c4bc1b14c833b273183f4f343ffad62121b3", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/operator.cpp"], "labels": []}, "7ffdd765c8": {"title": "[TensorExpr] more convenient outer Rfactor output (#40050)", "body": "Summary:\nAuto fuse the output loops of outer Rfactors, so it is in a more convenient format for binding GPU axes.\n\nAn example:\n```\n  Tensor* c = Reduce(\"sum\", {}, Sum(), b, {{m, \"m\"}, {n, \"n\"}, {k, \"k\"}});\n  LoopNest loop({c});\n  std::vector<For*> loops = loop.getLoopStmtsFor(c);\n  auto v = loops.at(0)->var();\n  loop.rfactor(c->body(), v);\n```\nBefore:\n```\n{\n  Allocate(tmp_buf, float, {m});\n  sum[0] = 0.f;\n  for (int m_1 = 0; m_1 < m; m_1++) {\n    tmp_buf[m_1] = 0.f;\n  }\n  for (int m_1 = 0; m_1 < m; m_1++) {\n    for (int n = 0; n < n_1; n++) {\n      for (int k = 0; k < k_1; k++) {\n        tmp_buf[m_1] = (tmp_buf[m_1]) + (b[((n_1 * m_1) * k_1 + k) + k_1 * n]);\n      }\n    }\n  }\n  for (int m_1 = 0; m_1 < m; m_1++) {\n    sum[0] = (sum[0]) + (tmp_buf[m_1]);\n  }\n  Free(tmp_buf);\n}\n```\n\nAfter:\n```\n{\n  sum[0] = 0.f;\n  for (int m = 0; m < m_1; m++) {\n    Allocate(tmp_buf, float, {m_1});\n    tmp_buf[m] = 0.f;\n    for (int n = 0; n < n_1; n++) {\n      for (int k = 0; k < k_1; k++) {\n        tmp_buf[m] = (tmp_buf[m]) + (b[((n_1 * m) * k_1 + k) + k_1 * n]);\n      }\n    }\n    sum[0] = (sum[0]) + (tmp_buf[m]);\n    Free(tmp_buf);\n  }\n}\n```\n\nThe existing Rfactor tests cover this case, although I did rename a few for clarity. This change broke the LLVMRFactorVectorizedReduction test because it now does what its intending to (vectorize a loop with a reduction in it) rather than nothing, and since that doesn't work it correctly fails. I've disabled it for now.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40050\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22605639\n\nPulled By: nickgg\n\nfbshipit-source-id: e359be53ea62d9106901cfbbc42d55d0e300e8e0", "pr_number": "40050", "files_changed": ["test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["merged", "oncall: jit"]}, "941069ca09": {"title": "[tensorexpr][trivial] Remove debug printing from test (#41806)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41806\n\nGenerally a good practice not to have tests spew output.\n\nTest Plan:\n`build/bin/test_tensorexpr`\n\nImported from OSS\n\nReviewed By: zheng-xq\n\nDifferential Revision: D22646833\n\nfbshipit-source-id: 444e883307d058fe77e7550d436fa61b7d91a701", "pr_number": "41806", "files_changed": ["test/cpp/tensorexpr/test_te_fuser_pass.cpp"], "labels": ["merged"]}, "60e2baf5e0": {"title": "[doc] Add LSTM non-deterministic workaround (#40893)", "body": "Summary:\nRelated: https://github.com/pytorch/pytorch/issues/35661\n\nPreview\n![image](https://user-images.githubusercontent.com/24860335/86861581-4b4c7100-c07c-11ea-950a-3145bfae9af9.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40893\n\nReviewed By: vincentqb\n\nDifferential Revision: D22535418\n\nPulled By: ngimel\n\nfbshipit-source-id: f194ddaff8ec6d03a3616c87466e2cbbe7e429a9", "pr_number": "40893", "files_changed": ["docs/source/cudnn_rnn_determinism.rst", "docs/source/notes/randomness.rst", "torch/nn/modules/rnn.py"], "labels": ["merged", "module: docs", "open source", "triaged"]}, "9e0c746b15": {"title": "Augmenting Concrete Observer Constructors to Support Dynamic Quantization Range; Modifying Utility Functions in _LearnableFakeQuantize Module for Better Logging and Baseline Construction. (#41815)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41815\n\n**All are minor changes to enable better simulations.**\n\nThe constructors of MinMaxObserver, MovingAverageMinMaxObserver, PerChannelMinMaxObserver, and MovingAveragePerChannelMinMaxObserver are augmented so they can utilize the dynamic quantization range support in the _ObserverBase class.\n\nIn addition, minor adjustments are made to the enable_static_observation function that allow observer to update parameters but do not fake quantize on the output (for constructing baseline).\n\nTest Plan:\nTo ensure this modification is still backward compatible with past usages, numerics are verified by running the quantization unit test suite, which contains various observer tests. The following command executes the test suite, which also verifies the observer numerics:\n```\nbuck test //caffe2/test:quantization -- observer\n```\n\nReviewed By: z-a-f\n\nDifferential Revision: D22649128\n\nfbshipit-source-id: 32393b706f9b69579dc2f644fb4859924d1f3773", "pr_number": "41815", "files_changed": ["torch/quantization/_learnable_fake_quantize.py", "torch/quantization/observer.py"], "labels": ["fb-exported", "merged"]}, "302e566205": {"title": "add max_and_min function and cpu kernel to speed up observers (#41570)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41570\n\nFor min/max based quantization observers, calculating min and max of a tensor\ntakes most of the runtime. Since the calculation of min and max is done\non the same tensor, we can speed this up by only reading the tensor\nonce, and reducing with two outputs.\n\nOne question I had is whether we should put this into the quantization\nnamespace, since the use case is pretty specific.\n\nThis PR implements the easier CPU path to get an initial validation.\nThere is some needed additional work in future PRs, which durumu will\ntake a look at:\n* CUDA kernel and tests\n* making this work per channel\n* benchmarking on observer\n* benchmarking impact on QAT overhead\n\nTest Plan:\n```\npython test/test_torch.py TestTorch.test_min_and_max\n```\n\nquick bench (not representative of real world use case):\nhttps://gist.github.com/vkuzo/7fce61c3456dbc488d432430cafd6eca\n```\n(pytorch) [vasiliy@devgpu108.ash6 ~/local/pytorch] OMP_NUM_THREADS=1 python ~/nfs/pytorch_scripts/observer_bench.py\ntensor(5.0390) tensor(-5.4485) tensor([-5.4485,  5.0390])\nmin and max separate 11.90243935585022\nmin and max combined 6.353186368942261\n% decrease 0.466228209277153\n(pytorch) [vasiliy@devgpu108.ash6 ~/local/pytorch] OMP_NUM_THREADS=4 python ~/nfs/pytorch_scripts/observer_bench.py\ntensor(5.5586) tensor(-5.3983) tensor([-5.3983,  5.5586])\nmin and max separate 3.468616485595703\nmin and max combined 1.8227086067199707\n% decrease 0.4745142294372342\n(pytorch) [vasiliy@devgpu108.ash6 ~/local/pytorch] OMP_NUM_THREADS=8 python ~/nfs/pytorch_scripts/observer_bench.py\ntensor(5.2146) tensor(-5.2858) tensor([-5.2858,  5.2146])\nmin and max separate 1.5707778930664062\nmin and max combined 0.8645427227020264\n% decrease 0.4496085496757899\n```\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D22589349\n\nfbshipit-source-id: c2e3f1b8b5c75a23372eb6e4c885f842904528ed", "pr_number": "41570", "files_changed": ["aten/src/ATen/cpu/vec256/functional.h", "aten/src/ATen/native/ReduceAllOps.cpp", "aten/src/ATen/native/ReduceAllOps.h", "aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": ["merged"]}, "ce8c7185de": {"title": "Add unittests to Comparison Operator Kernels in `BinaryOpsKernel.cpp` (#41809)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41809\n\nAdd new unittests to Operator Kernels.\nExplicitly announce function type in tests because it can't be inferred.\n\nTest Plan: CI\n\nReviewed By: malfet\n\nDifferential Revision: D22647221\n\nfbshipit-source-id: ef2f0e8c847841e90aa26d028753f23c8c53d6b0", "pr_number": "41809", "files_changed": ["aten/src/ATen/test/atest.cpp"], "labels": ["fb-exported", "merged"]}, "a0f2a5625f": {"title": "[quant][graphmode][fix] Make it work with CallMethod on non-Module objects (#41576)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41576\n\nPreviously we are assuming CallMethod only happens on module instances,\nbut it turns out this is not true, this PR fixes this issue.\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D22592789\n\nfbshipit-source-id: 48217626d9ea8e82536f00a296b8f9a471582ebe", "pr_number": "41576", "files_changed": ["torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/insert_observers.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": ["merged", "oncall: jit"]}, "5c9918e757": {"title": "Fix row-wise sparse SparseLengthSum and sparse adagrad fused operator (#41818)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41818\n\nFix row-wise sparse SparseLengthSum and sparse adagrad fused operator\n\nReviewed By: jianyuh\n\nDifferential Revision: D22345013\n\nfbshipit-source-id: 7c2d6c506b404f15a7aa8f1d0ccadb82e515a4c3", "pr_number": "41818", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cuh"], "labels": ["fb-exported", "merged"]}, "825a387ea2": {"title": "Fix bug on the backpropagation of LayerNorm when create_graph=True (#41595)", "body": "Summary:\nSolve an issue https://github.com/pytorch/pytorch/issues/41332\n\nI found the bug at https://github.com/pytorch/pytorch/issues/41332 is caused by LayerNorm.\n\nCurrent implementations of LayerNorm have a disparity between\n1. [`create_graph=False` CUDA implementation](https://github.com/BIT-silence/pytorch/blob/dde3d5f4a8f713ecc4649d776565b68ca75ae5c8/aten/src/ATen/native/cuda/layer_norm_kernel.cu#L145)\n2. [`create_graph=True` implementation](https://github.com/BIT-silence/pytorch/blob/dde3d5f4a8f713ecc4649d776565b68ca75ae5c8/tools/autograd/templates/Functions.cpp#L2536)\n\nWith this bug-fix, https://github.com/pytorch/pytorch/issues/41332 is solved.\n\nAiling BIT-silence\n\nSigned-off-by: Vinnam Kim <vinnamkim@gmail.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41595\n\nReviewed By: houseroad\n\nDifferential Revision: D22598415\n\nPulled By: BIT-silence\n\nfbshipit-source-id: 63e390724bd935dc8e028b4dfb75d34a80558c3a", "pr_number": "41595", "files_changed": ["test/test_nn.py", "tools/autograd/templates/Functions.cpp"], "labels": ["merged", "open source", "triaged"]}, "e17e55831d": {"title": "[pytorch] disable per-op profiling for internal mobile build (#41825)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41825\n\nAdd flag to gate D21374246 (https://github.com/pytorch/pytorch/commit/e7a09b4d17010e60bc95c25f8165ef479d1b9612) to mitigate mobile size regression.\nghstack-source-id: 108212047\n\nTest Plan: CI\n\nReviewed By: linbinyu\n\nDifferential Revision: D22650708\n\nfbshipit-source-id: ac9318af824ac31f519b7d5b4fe72df892d8d3f9", "pr_number": "41825", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["merged"]}, "fced54aa67": {"title": "[RPC tests] Fix test_init_(rpc|pg)_then_(rpc|pg) not shutting down RPC (#41558)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41558\n\nThe problem was due to non-deterministic destruction order of two global static variables: the mutexes used by glog and the RPC agent (which was still set because we didn't call `rpc.shutdown()`). When the TensorPipe RPC agent shuts down some callbacks may fire with an error and thus attempt to log something. If the mutexes have already been destroyed this causes a SIGABRT.\n\nFixes https://github.com/pytorch/pytorch/issues/41474\nghstack-source-id: 108231453\n\nTest Plan: Verified in https://github.com/pytorch/pytorch/issues/41474.\n\nReviewed By: fmassa\n\nDifferential Revision: D22582779\n\nfbshipit-source-id: 63e34d8a020c4af996ef079cfb7041b2474e27c9", "pr_number": "41558", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "fd62847eb2": {"title": "cross_layer_equalization (#41685)", "body": "Summary:\nThe goal is to implement cross layer equalization as described in section 4.1 in this paper: https://arxiv.org/pdf/1906.04721.pdf\nGiven two adjacent submodules in a trained model, A,B quantization might hurt one of the submodules more than the other. The paper poses the idea that a loss in accuracy from quantizing can be due to a difference in the channel ranges between the two submodules (the output channel range of A can be small, while the input channel range of B can be large). To minimize this source of error, we want to scale the tensors of A,B s.t. their channel ranges are equal (them being equal means no difference in ranges and minimizes this source of error).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41685\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D22630219\n\nPulled By: edmundw314\n\nfbshipit-source-id: ccc91ba12c10b652d7275222da8b85455b8a7cd5", "pr_number": "41685", "files_changed": ["test/quantization/test_equalize.py", "test/test_quantization.py", "torch/quantization/_equalize.py"], "labels": ["merged"]}, "9fbcfe848b": {"title": "Automated submodule update: FBGEMM (#41814)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/139c6f2292a876af9ef6264e6bb612b04539c711\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41814\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: dskhudia\n\nDifferential Revision: D22648844\n\nfbshipit-source-id: 4cfa8d83585407f870ea2bdee74e1c1f371082eb", "pr_number": "41814", "files_changed": ["third_party/fbgemm"], "labels": ["merged", "open source"]}, "5152633258": {"title": "[ROCm] update hip library name (#41813)", "body": "Summary:\nWith transition to hipclang, the HIP runtime library name was changed.  A symlink was added to ease the transition, but is going to be removed.  Conditionally set library name based on HIP compiler used.  Patch gloo submodule as part of build_amd.py script until its associated fix is available.\n\nCC ezyang xw285cornell sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41813\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22660077\n\nPulled By: xw285cornell\n\nfbshipit-source-id: c538129268d9947535b34523201f655b13c9e0a3", "pr_number": "41813", "files_changed": ["cmake/public/LoadHIP.cmake", "tools/amd_build/build_amd.py"], "labels": ["merged", "module: rocm", "open source"]}, "f03156f9df": {"title": "replace blacklist in caffe2/python/onnx/frontend.py (#41777)", "body": "Summary:\nClose https://github.com/pytorch/pytorch/issues/41712\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41777\n\nReviewed By: izdeby\n\nDifferential Revision: D22648532\n\nPulled By: yinghai\n\nfbshipit-source-id: 7f4c9f313e2887e70bb4eb1ab037aea6b549cec7", "pr_number": "41777", "files_changed": ["caffe2/python/onnx/frontend.py"], "labels": ["merged", "open source", "triaged"]}, "2da8c8df08": {"title": "[quant] Reaname from quantized... to ...quantized_cpu in the native_functions.yaml (#41071)", "body": "Summary:\nIssue https://github.com/pytorch/pytorch/issues/40315\n\nReaname from `quantized...` to `...quantized_cpu` in the native_functions.yaml\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41071\n\nReviewed By: z-a-f\n\nDifferential Revision: D22487087\n\nPulled By: jerryzh168\n\nfbshipit-source-id: f0d12907967739794839c1ffea44e78957f50b9b", "pr_number": "41071", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/QTensor.cpp", "aten/src/ATen/native/quantized/TensorCompare.cpp", "aten/src/ATen/native/quantized/cpu/int_repr_quant.cpp", "aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp", "aten/src/ATen/native/quantized/cpu/q_avgpool.cpp", "aten/src/ATen/native/quantized/cpu/q_avgpool3d.cpp", "aten/src/ATen/native/quantized/cpu/qadd.cpp", "aten/src/ATen/native/quantized/cpu/qchannel_shuffle.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/qconcat.cpp", "aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp", "aten/src/ATen/native/quantized/cpu/qreduction.cpp", "aten/src/ATen/native/quantized/cpu/qrelu.cpp", "aten/src/ATen/native/quantized/cpu/qsigmoid.cpp", "aten/src/ATen/native/quantized/cpu/qsort.cpp", "aten/src/ATen/native/quantized/cpu/qtanh.cpp", "aten/src/ATen/native/quantized/cpu/qthreshold.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest2d.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest3d.cpp", "aten/src/ATen/native/quantized/cuda/int_repr_quant.cu"], "labels": ["merged", "oncall: jit", "oncall: quantization", "open source"]}, "b87f0e5085": {"title": "Add NCCL Alltoall to PT NCCL process group (#39984)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39984\n\nAdd Alltoall and Alltoallv to PT NCCL process group using NCCL Send/Recv.\n\nReviewed By: jiayisuse\n\nDifferential Revision: D20781624\n\nfbshipit-source-id: 109436583ff69a3fea089703d32cfc5a75f973e0", "pr_number": "39984", "files_changed": ["test/distributed/test_distributed.py", "torch/lib/c10d/NCCLUtils.hpp", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/ProcessGroupRoundRobin.cpp", "torch/lib/c10d/ProcessGroupRoundRobin.hpp"], "labels": ["fb-exported", "merged"]}, "ca68dc7fa2": {"title": "replace std::clamp with shim (#41855)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41855\n\nreplace std::clamp with shim\n\nTest Plan: test_op_nnpi_fp16.py covers the testing.\n\nReviewed By: hyuen\n\nDifferential Revision: D22667645\n\nfbshipit-source-id: 5e7c94b499f381bde73f1984a6f0d01fb962a671", "pr_number": "41855", "files_changed": ["caffe2/contrib/fakelowp/unary_fp16_fake_op.cc"], "labels": ["fb-exported", "merged"]}, "61511aa1d6": {"title": "Remove zmath_std.h (#39835)", "body": "Summary:\nstd::complex is gone\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39835\n\nReviewed By: gchanan\n\nDifferential Revision: D22639834\n\nPulled By: anjali411\n\nfbshipit-source-id: 57da43d4e6c82261b1f9e5b876f1bbbdf9ae56ca", "pr_number": "39835", "files_changed": ["aten/src/ATen/native/cpu/zmath.h", "aten/src/ATen/native/cpu/zmath_std.h"], "labels": ["merged", "module: complex", "open source", "triaged"]}, "9c7ca89ae6": {"title": "Conda build (#38796)", "body": "Summary:\ncloses gh-37584. ~I think I need to do more to generate an image, but the `.circleci/README.md` is vague in the details. The first commit reflows and updates that document a bit, I will continue to update it as the PR progresses :)~ Dropped updating `.circleci/README.md`, will do that in a separate PR once this is merged.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38796\n\nReviewed By: gchanan\n\nDifferential Revision: D22627522\n\nPulled By: ezyang\n\nfbshipit-source-id: 99d5c19e942f15b9fc10f0de425790474a4242ab", "pr_number": "38796", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/cimodel/data/simple/util/docker_constants.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_conda.sh", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".circleci/verbatim-sources/workflows/workflows-ecr-gc.yml"], "labels": ["merged", "open source", "triaged"]}, "aa91a65b59": {"title": "[TensorExpr] Fix propagation of loop options when splitting loops (#40035)", "body": "Summary:\nFix a bug in SplitWithTail and SplitWithMask where loop_options such as Cuda block/thread bindings are overwritten by the split. This PR fixes this bug by propagating the loop options to the outer loop, which for axis bindings should be equivalent.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40035\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22080263\n\nPulled By: nickgg\n\nfbshipit-source-id: b8a9583fd90f69319fc4bb4db644e91f6ffa8e67", "pr_number": "40035", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["merged", "oncall: jit"]}, "ec683299eb": {"title": "Reland Add non-deterministic alert to CUDA operations that use `atomicAdd()` (#41538)", "body": "Summary:\nReland PR https://github.com/pytorch/pytorch/issues/40056\n\nA new overload of upsample_linear1d_backward_cuda was added in a recent commit, so I had to add the nondeterministic alert to it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41538\n\nReviewed By: zou3519\n\nDifferential Revision: D22608376\n\nPulled By: ezyang\n\nfbshipit-source-id: 54a2aa127e069197471f1feede6ad8f8dc6a2f82", "pr_number": "41538", "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/GridSampler.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/ReflectionPad.cu", "aten/src/ATen/native/cuda/ReplicationPadding.cu", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "test/test_nn.py", "test/test_torch.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_nn.py"], "labels": ["merged", "module: cuda", "open source", "triaged"]}, "b80ffd44b0": {"title": "Revert D20781624: Add NCCL Alltoall to PT NCCL process group", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20781624 (https://github.com/pytorch/pytorch/commit/b87f0e5085d30e1a1849436e6b9065edbe94e43d)\n\nOriginal commit changeset: 109436583ff6\n\nfbshipit-source-id: 03f6ee4d56baea93a1cf795d26dd92b7d6d1df28", "pr_number": null, "files_changed": ["test/distributed/test_distributed.py", "torch/lib/c10d/NCCLUtils.hpp", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/ProcessGroupRoundRobin.cpp", "torch/lib/c10d/ProcessGroupRoundRobin.hpp"], "labels": []}, "6ceb65f98c": {"title": "Document default dim for cross being None (#41850)", "body": "Summary:\nThe function torch.cross is a bit confusing, in particular the defaulting of the dim argument.\n\nThe default `dim` has been documented as -1 but it is actually `None`. This increases the confusion, in two possible ways depending on how carefully you read the rest. I also add a final warning to the final sentence.\n\nThis partially addresses https://github.com/pytorch/pytorch/issues/39310.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41850\n\nReviewed By: izdeby\n\nDifferential Revision: D22664625\n\nPulled By: albanD\n\nfbshipit-source-id: b8669e026fd01de9e4ec16da1414b9edfaa76bdd", "pr_number": "41850", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "47c57e8804": {"title": "rename TestFuser to TestTEFuser (#41542)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41542\n\nReviewed By: jamesr66a\n\nDifferential Revision: D22579606\n\nPulled By: Krovatkin\n\nfbshipit-source-id: f65b2cae996b42d55ef864bc0b424d9d43d8a2e2", "pr_number": "41542", "files_changed": ["test/test_jit_fuser_te.py"], "labels": ["merged"]}, "2d15b39745": {"title": "[Onnxifi] Support running with quantized int8 inputs (#41820)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41820\n\nPull Request resolved: https://github.com/pytorch/glow/pull/4721\n\nIn order to support int8 quantized tensor as an input to OnnxifiOp, we need to\n- Add support to recognize and extract shape meta from int8 tensor at input of OnnxifiOp\n- Make a copy of the input data and shift by 128 in Glow if input data is uint8 quantized tensor to get correct result because Glow uses int8 to represent the quantized data regardless.\n- Propagate correct quantization parameters to through shape info in C2.\n\nThis diff implements the above.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/contrib/fakelowp/test:test_int8_quantnnpi\n```\n\nReviewed By: jackm321\n\nDifferential Revision: D22650584\n\nfbshipit-source-id: 5e867f7ec7ce98bb066ec4128ceb7cad321b3392", "pr_number": "41820", "files_changed": ["caffe2/contrib/fakelowp/test/test_int8_quant.py", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h", "caffe2/opt/onnxifi_op.cc"], "labels": ["fb-exported", "merged"]}, "ad7133d3c1": {"title": "Patch for #40026 RandomSampler generates samples one at a time when replacement=True (#41682)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/32530\nFix/Patch https://github.com/pytorch/pytorch/pull/40026\n\nResubmit this patch and fix the type error.\n\nForce the input type to `manual_seed()` in `sampler.py` to be `int`.\n\nezyang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41682\n\nReviewed By: izdeby\n\nDifferential Revision: D22665477\n\nPulled By: ezyang\n\nfbshipit-source-id: 1725c8aa742c31e74321f20448f4b6a392afb38d", "pr_number": "41682", "files_changed": ["test/test_dataloader.py", "torch/utils/data/sampler.py"], "labels": ["merged", "open source", "triaged"]}, "af5d0bff00": {"title": "[ONNX] Add pass that fuses Conv and BatchNormalization (#40547)", "body": "Summary:\nAdd pass that fuses Conv and Batchnormalization nodes into one node Conv.\nThis pass is only applied in inference mode (training is None or TrainingMode.Eval).\nSince this pass needs access to param_dict it is written outside peephole file where these kind of passes (fusing multiple nodes into one) is usually placed.\n\nThis PR also adds wrapper skipIfNoEmbed to skip debug_embed_params test:\nPass that fuses Conv and Batchnorm changes the params of resnet model and parameters of onnx and pytorch model won't match. Since parameters are not matching, debug_embed_params test for test_resnet will fail and that is expected, therefore debug_embed_params test for test_resnet should be skipped.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40547\n\nReviewed By: gchanan\n\nDifferential Revision: D22631687\n\nPulled By: bzinodev\n\nfbshipit-source-id: fe45812400398a32541e797f727fd8697eb6d8c0", "pr_number": "40547", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_utility_funs.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/onnx/eval_peephole.cpp", "torch/csrc/jit/passes/onnx/eval_peephole.h", "torch/csrc/jit/python/init.cpp", "torch/onnx/utils.py"], "labels": ["merged", "oncall: jit", "open source", "triaged"]}, "dfa914a90c": {"title": "Modify lazy_dyndep loading to trigger inside workspace. (#41687)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41687\n\nSpecifically, this makes a new library (lazy), which can be used from both core\nand workspace.\n\nThis allows workspace.Createnet to trigger lazy loading of dyndep dependencies.\n\nTest Plan: Added a unit test specifically for workspace.CreateNet\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D22441877\n\nfbshipit-source-id: 3a9d1af9962585d08ea2566c9c85bec7377d39f2", "pr_number": "41687", "files_changed": ["caffe2/python/core.py", "caffe2/python/lazy.py", "caffe2/python/lazy_dyndep.py", "caffe2/python/lazy_dyndep_test.py", "caffe2/python/workspace.py"], "labels": ["fb-exported", "merged"]}, "dbc6a2904b": {"title": "[quant][graphmode][fix] Remove assert for uses == 1 in remove dequantize pass (#41859)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41859\n\nA value can be used multiple times in the same node, we don't really need to assert uses of dequantize == 1\n\nTest Plan: Imported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D22673525\n\nfbshipit-source-id: 2c4a770e0ddee722ca54e68d310c395e7f418b3b", "pr_number": "41859", "files_changed": ["torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": ["merged", "oncall: jit"]}, "4e16be9073": {"title": "[MemLeak] Fix memory leak from releasing unique ptr (#41883)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41883\n\nFix memory leak from releasing unique ptr\n\nTest Plan:\nTested serialization with and without the change.\n\nHeap profile without change:\n```\nWelcome to jeprof!  For help, type 'help'.\n(jeprof) top\nTotal: 7298.4 MB\n  4025.2  55.2%  55.2%   4025.2  55.2% c10::alloc_cpu (inline)\n  3195.3  43.8%  98.9%   3195.3  43.8% caffe2::SerializeUsingBytesOrInt32\n    63.6   0.9%  99.8%     63.6   0.9% __gnu_cxx::new_allocator::allocate (inline)\n     5.0   0.1%  99.9%      5.0   0.1% google::protobuf::RepeatedField::Reserve\n     2.5   0.0%  99.9%      2.5   0.0% folly::aligned_malloc (inline)\n     1.2   0.0%  99.9%      1.2   0.0% caffe2::detail::CopyFromProtoWithCast (inline)\n     1.0   0.0%  99.9%      1.0   0.0% __new_exitfn\n     1.0   0.0% 100.0%      1.0   0.0% std::_Function_base::_Base_manager::_M_init_functor (inline)\n     0.5   0.0% 100.0%      0.5   0.0% folly::HHWheelTimerBase::newTimer (inline)\n     0.5   0.0% 100.0%      0.5   0.0% std::__detail::_Hashtable_alloc::_M_allocate_node\n```\n\nHeap profile with change:\n```\nWelcome to jeprof!  For help, type 'help'.\n(jeprof) top\nTotal: 6689.2 MB\n  4025.2  60.2%  60.2%   4025.2  60.2% c10::alloc_cpu (inline)\n  2560.0  38.3%  98.4%   2560.0  38.3% caffe2::::HugePagesArena::alloc_huge (inline)\n    90.9   1.4%  99.8%     90.9   1.4% __gnu_cxx::new_allocator::allocate (inline)\n     5.0   0.1%  99.9%      5.0   0.1% google::protobuf::RepeatedField::Reserve\n     2.0   0.0%  99.9%      2.0   0.0% prof_backtrace_impl (inline)\n     1.0   0.0%  99.9%     20.3   0.3% std::__cxx11::basic_string::_M_construct (inline)\n     1.0   0.0%  99.9%      1.0   0.0% std::_Function_base::_Base_manager::_M_init_functor (inline)\n     0.5   0.0%  99.9%      0.5   0.0% folly::UnboundedQueue::allocNextSegment (inline)\n     0.5   0.0% 100.0%      0.5   0.0% folly::aligned_malloc (inline)\n     0.5   0.0% 100.0%      0.5   0.0% __new_exitfn\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D22662093\n\nfbshipit-source-id: d0b8ff1ed26c72b14bb02fb1146c51ef11a7e519", "pr_number": "41883", "files_changed": ["caffe2/core/blob_serialization.cc"], "labels": ["fb-exported", "merged"]}, "b40ef422d3": {"title": ".circleci: Separate out docs build from push (#41871)", "body": "Summary:\nSeparates out the docs build from the push and limits when the push\nactually happens.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41871\n\nReviewed By: yns88\n\nDifferential Revision: D22673716\n\nPulled By: seemethere\n\nfbshipit-source-id: fff8b35ba8465dc15832214c4c9ef03ce12faa48", "pr_number": "41871", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/cpp_doc_push_script.sh", ".circleci/scripts/python_doc_push_script.sh", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["merged", "module: ci"]}, "586b7f991c": {"title": "Enable skipped tests from test_torch on ROCm (#41611)", "body": "Summary:\nThis pull request enables the following tests from test_torch, previously skipped on ROCm:\ntest_pow_-2_cuda_float32/float64\ntest_sum_noncontig_cuda_float64\ntest_conv_transposed_large\n\nThe first two tests experienced precision issues on earlier ROCm version, whereas the conv_transposed test was hitting a bug in MIOpen which is fixed with the version shipping with ROCm 3.5\n\nezyang jeffdaily\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41611\n\nReviewed By: xw285cornell\n\nDifferential Revision: D22672690\n\nPulled By: ezyang\n\nfbshipit-source-id: 5585387c048f301a483c4c0566eb9665555ef874", "pr_number": "41611", "files_changed": ["test/test_torch.py"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "ca3ba1095e": {"title": "Do not chown files inside docker for pytorch-job-tests (#41884)", "body": "Summary:\nThey are already owned by `jenkins` user after the build\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41884\n\nReviewed By: orionr\n\nDifferential Revision: D22682441\n\nPulled By: malfet\n\nfbshipit-source-id: daf99532d300d30a5de591ad03af4597e145fdfc", "pr_number": "41884", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "2a3ab71f28": {"title": "[quant][graphmode][fix] Remove useQuantizable check for dynamic quant (#41892)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41892\n\nCurrently the input of batch_norm is considered as dynamically quantizable but it shouldn't be\nthis PR fixes that\n\nTest Plan:\ninternal models\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22681423\n\nfbshipit-source-id: 7f428751de0c4af0a811b9c952e1d01afda42d85", "pr_number": "41892", "files_changed": ["torch/csrc/jit/passes/quantization/helper.cpp"], "labels": ["merged", "oncall: jit"]}, "0ec7ba4088": {"title": "[iOS] Bump up the cocoapods version (#41895)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41895\n\n### Summary\n\nThe iOS binary for 1.6.0 has been uploaded to AWS. This PR bumps up the version for cocoapods.\n\n### Test Plan\n\n- Check CI\n\nTest Plan: Imported from OSS\n\nReviewed By: husthyc\n\nDifferential Revision: D22683787\n\nPulled By: xta0\n\nfbshipit-source-id: bb95b670a7945d823d55e9c65b357765753f295a", "pr_number": "41895", "files_changed": ["ios/LibTorch.podspec"], "labels": ["merged"]}, "30ce7b3740": {"title": "Fix bug when compiling with caffe2 (#41868)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41868\n\nFix bug when compiling with caffe2\n\nReviewed By: jianyuh\n\nDifferential Revision: D22670707\n\nfbshipit-source-id: aa654d7b9004257e0288c8ae8819ca5752eea443", "pr_number": "41868", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cuh"], "labels": ["fb-exported", "merged"]}, "4b4273a04e": {"title": "Update Adam documentation (#41679)", "body": "Summary:\nThis PR fixes https://github.com/pytorch/pytorch/issues/41477\n\nAdam implementation is doing L2 regularization and not decoupled weight decay. However, the change mentioned in https://github.com/pytorch/pytorch/issues/41477 was motivated by Line 12 of algorithm 2 in [Decoupled Weight Decay Regularization](https://arxiv.org/pdf/1711.05101.pdf) paper.\n\nPlease let me know if you have other suggestions about how to deliver this info in the docs.\ncc ezyang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41679\n\nReviewed By: izdeby\n\nDifferential Revision: D22671329\n\nPulled By: vincentqb\n\nfbshipit-source-id: 2caf60e4f62fe31f29aa35a9532d1c6895a24224", "pr_number": "41679", "files_changed": ["torch/optim/adam.py"], "labels": ["merged", "open source", "triaged"]}, "e831299bae": {"title": "Fix typing error of torch/optim/lr_scheduler.pyi (#41775)", "body": "Summary:\n* add `_LRScheduler.get_last_lr` type stub.\n* remove `CosineAnnealingWarmRestarts.step` because its signature is same with `_LRScheduler`'s.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41775\n\nReviewed By: izdeby\n\nDifferential Revision: D22649350\n\nPulled By: vincentqb\n\nfbshipit-source-id: 5355dd062a5af437f4fc153244dda793a2382e7e", "pr_number": "41775", "files_changed": ["torch/optim/lr_scheduler.pyi"], "labels": ["merged", "module: optimizer", "open source", "triaged"]}, "272fb3635f": {"title": "Add regression test for ONNX exports of modules that embed an Embedding layer inside a Sequential (#32598)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/19227\n\nThis PR adds a regression test for ONNX exports where a module has a sequential that references an Embedding layer\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32598\n\nReviewed By: izdeby\n\nDifferential Revision: D22672790\n\nPulled By: ezyang\n\nfbshipit-source-id: c88beb29a36b07378c28b0e4546efe887fcbc3be", "pr_number": "32598", "files_changed": ["test/onnx/model_defs/emb_seq.py", "test/onnx/test_models.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "c0e3839845": {"title": "fix #36801 (#41607)", "body": "Summary:\nunittest actually did stdout testname like (test_accumulate_grad (__main__.TestAutograd) ... ) first befor test start running. export PYTHONUNBUFFERED=1 or python -u could record this msg. ezyang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41607\n\nReviewed By: izdeby\n\nDifferential Revision: D22673930\n\nPulled By: ezyang\n\nfbshipit-source-id: 18512b6f5f80485c2b0d812f2ebdecc1fdc4b4ec", "pr_number": "41607", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged", "open source"]}, "266657182a": {"title": "Add `torch.movedim` (#41480)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/38349 #36048\n\nTODO:\n* [x] Tests\n* [x] Docs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41480\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22649917\n\nPulled By: zou3519\n\nfbshipit-source-id: a7f3920a24bae16ecf2ad731698ca65ca3e8c1ce", "pr_number": "41480", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensor_view.rst", "test/test_autograd.py", "test/test_torch.py", "tools/autograd/gen_autograd.py", "torch/_overrides.py", "torch/_torch_docs.py"], "labels": ["merged", "module: numpy", "open source", "triaged"]}, "fab1795577": {"title": "move benchmark utils into torch namespace (#41506)", "body": "Summary:\nMove the timing utils to `torch.utils._benchmark`. I couldn't figure out how to get setuptools to pick it up and put it under `torch` unless it is in the `torch` directory. (And I think it has to be for `setup.py develop` anyway.)\n\nI also modified the record function benchmark since `Timer` and `Compare` should always be available now.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41506\n\nReviewed By: ngimel\n\nDifferential Revision: D22601460\n\nPulled By: robieta\n\nfbshipit-source-id: 9cea7ff1dcb0bb6922c15b99dd64833d9631c37b", "pr_number": "41506", "files_changed": ["benchmarks/experimental_components/README.md", "benchmarks/experimental_components/examples/compare.py", "benchmarks/experimental_components/examples/end_to_end.py", "benchmarks/experimental_components/examples/fuzzer.py", "benchmarks/experimental_components/examples/op_benchmark.py", "benchmarks/experimental_components/examples/prepare_e2e.sh", "benchmarks/experimental_components/examples/simple_timeit.py", "benchmarks/experimental_components/op_fuzzers/binary.py", "benchmarks/experimental_components/op_fuzzers/unary.py", "benchmarks/experimental_components/utils/__init__.py", "benchmarks/experimental_components/utils/common.py", "benchmarks/experimental_components/utils/compare.py", "benchmarks/experimental_components/utils/fuzzer.py", "benchmarks/experimental_components/utils/timer.py", "benchmarks/record_function_benchmark/record_function_bench.py", "test/test_utils.py", "torch/utils/_benchmark/README.md", "torch/utils/_benchmark/__init__.py", "torch/utils/_benchmark/examples/compare.py", "torch/utils/_benchmark/examples/end_to_end.py", "torch/utils/_benchmark/examples/fuzzer.py", "torch/utils/_benchmark/examples/op_benchmark.py", "torch/utils/_benchmark/examples/prepare_e2e.sh", "torch/utils/_benchmark/examples/simple_timeit.py", "torch/utils/_benchmark/op_fuzzers/__init__.py", "torch/utils/_benchmark/op_fuzzers/binary.py", "torch/utils/_benchmark/op_fuzzers/unary.py", "torch/utils/_benchmark/utils/__init__.py", "torch/utils/_benchmark/utils/common.py", "torch/utils/_benchmark/utils/compare.py", "torch/utils/_benchmark/utils/fuzzer.py", "torch/utils/_benchmark/utils/timer.py"], "labels": ["merged"]}, "37e7f0caf6": {"title": "Fix docstring in Unflatten (#41835)", "body": "Summary:\nI'd like to amend the docstring introduced in https://github.com/pytorch/pytorch/issues/41564. It's not rendering correctly on the web, and this should fix it.\n\ncc albanD\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41835\n\nReviewed By: izdeby\n\nDifferential Revision: D22672368\n\nPulled By: albanD\n\nfbshipit-source-id: f0b03c2b2a4c79b790d54f7c8f2ae28ef9d76a75", "pr_number": "41835", "files_changed": ["torch/nn/modules/flatten.py"], "labels": ["merged", "open source"]}, "17f76f9a78": {"title": "Verbose param for schedulers that don't have it #38726 (#41580)", "body": "Summary:\nVerbose param for schedulers that don't have it https://github.com/pytorch/pytorch/issues/38726\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41580\n\nReviewed By: izdeby\n\nDifferential Revision: D22671163\n\nPulled By: vincentqb\n\nfbshipit-source-id: 53a6c9e929141d411b6846bc25f3fe7f46fdf3be", "pr_number": "41580", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged", "open source", "triaged"]}, "77db93228b": {"title": "Temporary fix for determinant bug on CPU (#35136)", "body": "Summary:\nChangelog:\n- Make diagonal contiguous\n\nTemporarily Fixes https://github.com/pytorch/pytorch/issues/34061\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/35136\n\nReviewed By: izdeby\n\nDifferential Revision: D22673153\n\nPulled By: ezyang\n\nfbshipit-source-id: 850f537483f929fcb43bcdef9d4ec264a7c3d354", "pr_number": "35136", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "1978188639": {"title": "Remove two \"return\"s that return \"void\" (#41811)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41811\n\nReviewed By: izdeby\n\nDifferential Revision: D22673690\n\nPulled By: ezyang\n\nfbshipit-source-id: 10d4aff90e2e051116e682fa51fb9494af8482c1", "pr_number": "41811", "files_changed": ["aten/src/ATen/native/cpu/Loops.h"], "labels": ["merged", "open source"]}, "01c406cc22": {"title": "[pytorch] bump up variable version regardless of differentiability (#41269)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41269\n\nThe ultimate goal is to move things that are not gated with `if (compute_requires_grad(...))`\nor `if (grad_fn)` out from VariableType so that VariableType kernels can be enabled/disabled\nbased upon `GradMode`. Then we can merge `AutoNonVariableTypeMode` and `NoGradGuard`.\n\nWe've moved profiling / tracing logic out from VariableType. One remaining thing that's\nnot gated with the if-statement is the `increment_version` call.\n\nHowever, the `gen_variable_type.py` does use bits from `derivatives.yaml` to determine whether\nto emit the `increment_version` call. If an output is never going to be differentiable (not based\nupon runtime property of the variable but based upon static property, e.g. it's integral type)\nthen it would never emit the increment_version call.\n\nHypothetically, increment_version for a tensor can be orthogonal to its differentiability.\n\nThis PR is to make the change and test its impact. Making this logical simplification would\nallow us to move this out from VariableType to aten codegen.\nghstack-source-id: 108318746\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D22471643\n\nfbshipit-source-id: 3e3a442c7fd851641eb4a9c4f024d1f5438acdb8", "pr_number": "41269", "files_changed": ["tools/autograd/gen_variable_type.py"], "labels": ["merged", "topic: bc-breaking"]}, "3626473105": {"title": "NCCL Backend support for torch.bool (#41318)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41318\n\nCloses https://github.com/pytorch/pytorch/issues/24137.\n\nThis PR adds support for the `torch.bool` tensor type to ProcessGroupNCCL. For most types we use the existing mapping, but since `bool` is not supported as a native `ncclDataType_t`, we add the following logic:\n1) Map `at::kBool` to `ncclUint8`\n2) During reduction (allreduce for example), if the operation is SUM, we instead override to to a MAX, to avoid overflow issues. The rest of the operations work with no changes. In the boolean case, changing sum to max makes no correctness difference since they both function as a bitwise OR.\n\nThe reduction logic (for example for reduce/allreduce) is as follows:\nsum, max = bitwise or\nproduct, min = bitwise and\n\nTests are added to ensure that the reductions work as expected.\nghstack-source-id: 108315417\n\nTest Plan: Added unittests\n\nReviewed By: mrshenli\n\nDifferential Revision: D22496604\n\nfbshipit-source-id: a1a15381ec41dc59923591885d40d966886ff556", "pr_number": "41318", "files_changed": ["test/distributed/test_distributed.py", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged"]}, "b85df3709a": {"title": "Add __main__ entrypoint to test_futures.py (#41826)", "body": "Summary:\nPer comment in run_test.py, every test module must have a __main__ entrypoint:\nhttps://github.com/pytorch/pytorch/blob/60e2baf5e056b63414a31e40ac7a1024ed0b0ce0/test/run_test.py#L237-L238\nAlso disable test_wait_all on Windows, as it fails with an uncaught exception:\n```\n  test_wait_all (__main__.TestFuture) ... Traceback (most recent call last):\n  File \"run_test.py\", line 744, in <module>\n    main()\n  File \"run_test.py\", line 733, in main\n    raise RuntimeError(err)\nRuntimeError: test_futures failed!\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41826\n\nReviewed By: seemethere, izdeby\n\nDifferential Revision: D22654899\n\nPulled By: malfet\n\nfbshipit-source-id: ab7fdd7adce3f32c53034762ae37cf35ce08cafc", "pr_number": "41826", "files_changed": ["test/test_futures.py"], "labels": ["merged"]}, "dfe7d27d0e": {"title": "implement lite parameter serializer (#41403)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41403\n\nTest Plan: Imported from OSS\n\nReviewed By: kwanmacher\n\nDifferential Revision: D22611633\n\nPulled By: ann-ss\n\nfbshipit-source-id: b391e8c96234b2e69f350119a11f688e920c7817", "pr_number": "41403", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_lite_trainer.cpp", "test/cpp/jit/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/mobile/export.cpp", "torch/csrc/jit/mobile/import.h", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/module.h"], "labels": ["merged", "oncall: jit"]}, "da3ff5e473": {"title": "[JIT] dont count constants in subgraph size (#41436)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41436\n\nConstants are not executed as instructions, we should ignore them when counting subgraph size, as we ignore them in counting block size for loop unrolling.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin, ZolotukhinM\n\nDifferential Revision: D22600608\n\nPulled By: eellison\n\nfbshipit-source-id: 9770b21c936144a3d6a1df89cf3be5911095187e", "pr_number": "41436", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/create_autodiff_subgraphs.cpp"], "labels": ["merged", "oncall: jit"]}, "25b6e2e5ee": {"title": "[JIT] optimize autodiff subgraph slicing (#41437)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41437\n\n[copied from commented code]\nthe IR has many nodes which can never be reordered around, such as a\nprim::Bailout. if a node N is surrounded by two nodes which cannot be\nreordered, A and B, then a differentiable subgraph that is created from N\ncan only contain nodes from [A, B] The nodes from A to B represent one\nwork block for the subgraph slicer to work on. By creating these up\nfront, we avoid retraversing the whole graph block any time scanNode\nreturns, and we can also avoid attempting to create differentiable\nsubgraphs in work blocks that do not contain a minimum number of differentiable nodes\n\nThis improved compilation time of e of densenet (the model with the slowest compilation time we're tracking) from 56s  -> 28s, and for mobilenet from 8s -> 6s.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin, ZolotukhinM\n\nDifferential Revision: D22600607\n\nPulled By: eellison\n\nfbshipit-source-id: e5ab6ed87bf6820b4e22c86eabafd9d17bf7cedc", "pr_number": "41437", "files_changed": ["torch/csrc/jit/passes/create_autodiff_subgraphs.cpp"], "labels": ["merged", "oncall: jit"]}, "b898bdd4d3": {"title": "[JIT] Don't re run CSE on every block (#41479)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41479\n\nPreviously we were re-running CSE every time we recursed into a new block, which in turn created a new Alias Db for the whole graph. This was O(# Nodes * # Blocks).\n\nFor graphs which don't have any autodiff opportunities, such as Densenet,  create_autodiff_subgraphs is now linear in number of nodes. For Densenet this pass was measured at ~.1 seconds.\n\nThis pass is still non-linear for models which actually do create autodiff subgraphs, because in the\n```\n      bool any_changed = true;\n      while (any_changed) {\n        AliasDb aliasDb(graph_);\n        any_changed = false;\n        for (auto it = workblock.end()->reverseIterator();\n             it != workblock.begin()->reverseIterator();) {\n          bool changed;\n          std::tie(it, changed) = scanNode(*it, aliasDb);\n          any_changed |= changed;\n        }\n      }\n```\nloop we recreate the AliasDb (which is O(N)) every time we merge something and scan node returns. I will make that linear in next PR in the stack.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D22600606\n\nPulled By: eellison\n\nfbshipit-source-id: b08abfde2df474f168104c5b477352362e0b7b16", "pr_number": "41479", "files_changed": ["torch/csrc/jit/passes/create_autodiff_subgraphs.cpp"], "labels": ["merged", "oncall: jit"]}, "dbe6bfbd7e": {"title": "Revert D22496604: NCCL Backend support for torch.bool", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22496604 (https://github.com/pytorch/pytorch/commit/3626473105afff2598c30f0c04734488d79d9937)\n\nOriginal commit changeset: a1a15381ec41\n\nfbshipit-source-id: 693c2f9fd1df568508cbcf8c734c092cec3b0a72", "pr_number": null, "files_changed": ["test/distributed/test_distributed.py", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": []}, "a4b831a86a": {"title": "Replace if(NOT ${var}) by if(NOT var) (#41924)", "body": "Summary:\nAs explained in https://github.com/pytorch/pytorch/issues/41922 using `if(NOT ${var})\" is usually wrong and can lead to issues like https://github.com/pytorch/pytorch/issues/41922 where the condition is wrongly evaluated to FALSE instead of TRUE. Instead the unevaluated variable name should be used in all cases, see the CMake docu for details.\n\nThis fixes the `NOT ${var}` cases by using a simple regexp replacement. It seems `pybind11_PREFER_third_party` is the only variable really prone to causing an issue as all others are set. However due to CMake evaluating unquoted strings in `if` conditions as variable names I recommend to never use unquoted `${var}` in an if condition. A similar regexp based replacement could be done on the whole codebase but as that does a lot of changes I didn't include this now. Also `if(${var})` will likely lead to a parser error if `var` is unset instead of a wrong result\n\nFixes https://github.com/pytorch/pytorch/issues/41922\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41924\n\nReviewed By: seemethere\n\nDifferential Revision: D22700229\n\nPulled By: mrshenli\n\nfbshipit-source-id: e2b3466039e4312887543c2e988270547a91c439", "pr_number": "41924", "files_changed": ["cmake/Dependencies.cmake", "cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake", "cmake/ProtoBuf.cmake", "cmake/Utils.cmake", "cmake/public/cuda.cmake", "torch/CMakeLists.txt"], "labels": ["merged", "open source"]}, "183b43f323": {"title": "Clarify Python 3.5 is the minimum supported version in the installation section. (#41937)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41937\n\nReviewed By: izdeby\n\nDifferential Revision: D22702924\n\nPulled By: mrshenli\n\nfbshipit-source-id: 67306435e80f80236b585f1d5406444daec782d6", "pr_number": "41937", "files_changed": ["README.md"], "labels": ["merged", "open source"]}, "c5fdcd85c7": {"title": "check pruned attributes before deleting (#41913)", "body": "Summary:\nI copyed a pruned model after deleteing the derived tensors. In order to be able to reparameter the model, we should check the existence of the tensors here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41913\n\nReviewed By: izdeby\n\nDifferential Revision: D22703248\n\nPulled By: mrshenli\n\nfbshipit-source-id: f5274d2c634a4c9a038100d8a6e837f132eabd34", "pr_number": "41913", "files_changed": ["torch/nn/utils/prune.py"], "labels": ["merged", "open source"]}, "7646f3c77f": {"title": "Fix type annotation for CosineAnnealingLR (#41866)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41866\n\nReviewed By: izdeby\n\nDifferential Revision: D22703576\n\nPulled By: mrshenli\n\nfbshipit-source-id: 10a0f593ffaaae82a2923a42815c36793a9043d5", "pr_number": "41866", "files_changed": ["torch/optim/lr_scheduler.pyi"], "labels": ["merged", "open source"]}, "b6690eb29a": {"title": "Might be good for newcomers to read what N means (#41851)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41851\n\nReviewed By: izdeby\n\nDifferential Revision: D22703602\n\nPulled By: mrshenli\n\nfbshipit-source-id: 44905f43cdf53b38e383347e5002a28c9363a446", "pr_number": "41851", "files_changed": ["torch/nn/functional.py"], "labels": ["merged", "open source"]}, "a1cfcd4d22": {"title": "Change whitelist to another context in binary_smoketest.py (#41822)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/41740\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41822\n\nReviewed By: izdeby\n\nDifferential Revision: D22703682\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1df82fd43890142dfd261eb7bf49dbd128295e03", "pr_number": "41822", "files_changed": [".circleci/cimodel/data/simple/binary_smoketest.py"], "labels": ["merged", "open source"]}, "401ac2dd39": {"title": "Replaced whitelisted with allowed (#41867)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/41746\nCloses https://github.com/pytorch/pytorch/issues/41745\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41867\n\nReviewed By: izdeby\n\nDifferential Revision: D22703533\n\nPulled By: mrshenli\n\nfbshipit-source-id: 915895463a92e18f36db93b8884d9fd432c0997d", "pr_number": "41867", "files_changed": ["caffe2/transforms/common_subexpression_elimination.cc", "caffe2/transforms/common_subexpression_elimination.h"], "labels": ["caffe2", "merged", "open source", "triaged"]}, "36fb14b68b": {"title": "[quant] Add Graph Mode Passes to quantize EmbeddingBag operators (#41612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41612\n\nThis change adds preliminary support to quantize the EmbeddingBag operators. We currently support 4-bit and 8-bit quantization+packing of the weights.\n\nTo quantize these operators, specify the operator name in the `custom_op_name` field of the NoopObserver. Based on the op name (4bit or 8bit) we call the corresponding quantization functions.\nRefer to the testplan for how to invoke the qconfig for the embedding_bag ops.\n\nFuture versions of this will support 4-bit and 2-bit qtensors with native support to observe and quantize it.\n\nNB - This version assumes that the weights in the EmbeddingBag Module reside on the same device.\n\nTest Plan:\npython test/test_quantization.py TestQuantizeDynamicJitOps.test_embedding_bag\n\nImported from OSS\n\nReviewed By: vkuzo, jerryzh168\n\nDifferential Revision: D22609342\n\nfbshipit-source-id: 23e33f44a451c26719e6e283e87fbf09b584c0e6", "pr_number": "41612", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp", "torch/quantization/observer.py"], "labels": ["merged", "oncall: jit"]}, "97ab33d47c": {"title": "Fix memory leak in XNNPACK/MaxPool2D. (#41874)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41874\n\nTest Plan: Imported from OSS\n\nReviewed By: ann-ss\n\nDifferential Revision: D22699598\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: fec59ed3d5d23bd9197349057fcf2ce56a2b278b", "pr_number": "41874", "files_changed": ["aten/src/ATen/native/xnnpack/MaxPooling.cpp"], "labels": ["merged"]}, "f00a37dd71": {"title": "Make setup.py Python-2 syntactically correct (#41960)", "body": "Summary:\nImport __future__ to make `print(*args)` a syntactically correct statement under Python-2\nOtherwise, if once accidentally invokes setup.py using Python-2 interpreter they will be greeted by:\n```\n  File \"setup.py\", line 229\n    print(*args)\n          ^\nSyntaxError: invalid syntax\n```\ninstead of:\n```\nPython 2 has reached end-of-life and is no longer supported by PyTorch.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41960\n\nReviewed By: orionr, seemethere\n\nDifferential Revision: D22710174\n\nPulled By: malfet\n\nfbshipit-source-id: ffde3ddd585707ba1d39e57e0c6bc9c4c53f8004", "pr_number": "41960", "files_changed": ["setup.py"], "labels": ["merged"]}, "2da69081d7": {"title": "Fix one error message format of torch.dot() (#41963)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41963\n\nthe error message of dot(CUDA) was copied from dot(CPU), however, they both are easy to cause confusion\n\nTest Plan: wait for unittests\n\nReviewed By: ngimel\n\nDifferential Revision: D22710822\n\nfbshipit-source-id: 565b51149ff4bee567ef0775e3f8828579565f8a", "pr_number": "41963", "files_changed": ["aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu"], "labels": ["fb-exported", "merged"]}, "e42eab4b1c": {"title": "Update PULL_REQUEST_TEMPLATE.md (#41812)", "body": "Summary:\n**Summary**\nThis commit updates the repository's pull request template to remind contributors to tag the issue that their pull request addresses.\n\n**Fixes**\nThis commit fixes https://github.com/pytorch/pytorch/issues/35319.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41812\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D22667902\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: cda5ff7cbbbfeb89c589fd0dfd378bf73a59d77b", "pr_number": "41812", "files_changed": [".github/PULL_REQUEST_TEMPLATE.md"], "labels": ["merged"]}, "6a8c9f601f": {"title": "Removed whitelist references from test/backward_compatibility/check_b\u2026 (#41691)", "body": "Summary:\nRemoved whitelist reference\nFixes https://github.com/pytorch/pytorch/issues/41733.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41691\n\nReviewed By: houseroad\n\nDifferential Revision: D22641467\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 72899b7410d4fc8454d87ca0c042f1ede7cf73de", "pr_number": "41691", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged", "open source", "triaged"]}, "750d9dea49": {"title": "move min/max tests to TestTorchDeviceType (#41908)", "body": "Summary:\nso that testing _min_max on the different devices is easier, and min/max operations have better CUDA test coverage.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41908\n\nReviewed By: mruberry\n\nDifferential Revision: D22697032\n\nPulled By: ngimel\n\nfbshipit-source-id: a796638fdbed8cda90a23f7ff4ee167f45530914", "pr_number": "41908", "files_changed": ["test/test_torch.py"], "labels": ["merged"]}, "c0bfa45f9d": {"title": "Enable typechecking for `torch.futures` (#41675)", "body": "Summary:\nAdd typing declarations for torch._C.Future and torch._C._collect_all\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41675\n\nReviewed By: izdeby\n\nDifferential Revision: D22627539\n\nPulled By: malfet\n\nfbshipit-source-id: 29b87685d65dd24ee2094bae8a84a0fe3787e7f8", "pr_number": "41675", "files_changed": ["mypy.ini", "test/test_futures.py", "torch/_C/__init__.pyi.in", "torch/futures/__init__.py"], "labels": ["merged"]}, "79cdd84c81": {"title": "Downloading different sccache binary in case of ROCm build (#41958)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41958\n\nReviewed By: colesbury\n\nDifferential Revision: D22717509\n\nPulled By: malfet\n\nfbshipit-source-id: 96c94512f12193fa549ec84cd51f17978f221bc6", "pr_number": "41958", "files_changed": [".circleci/docker/common/install_cache.sh"], "labels": ["merged", "open source", "triaged"]}, "1b55e2b043": {"title": "add prefetch_factor for multiprocessing prefetching process (#41130)", "body": "Summary:\nfix https://github.com/pytorch/pytorch/issues/40604\nAdd parameter to Dataloader to configure the per-worker prefetch number.\nBefore this edit, the prefetch process always prefetch 2 * num_workers data items, this commit help us make this configurable, e.x. you can specify to prefetch 10 * num_workers data items.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41130\n\nReviewed By: izdeby\n\nDifferential Revision: D22705288\n\nPulled By: albanD\n\nfbshipit-source-id: 2c483fce409735fef1351eb5aa0b033f8e596561", "pr_number": "41130", "files_changed": ["docs/source/data.rst", "test/test_dataloader.py", "torch/utils/data/dataloader.py"], "labels": ["merged", "open source", "triaged"]}, "e9e6cc8c83": {"title": "Added Prehook option to prepare method (#41863)", "body": "Summary:\nAdded a logic so that if a prehook is passed into the prepare method during quantization, then the hook will be added as a prehook to all leaf nodes (and modules specified in the non_leaf_module_list).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41863\n\nTest Plan:\nSmall demo, made simple module then called prepare with prehook parameter set to the numeric suite logger, printed the results to verify its what we wanted\n{F245156246}\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22671288\n\nPulled By: edmundw314\n\nfbshipit-source-id: ce65a00830ff03360a82c0a075b3b6d8cbc4362e", "pr_number": "41863", "files_changed": ["torch/quantization/quantize.py"], "labels": ["merged"]}, "2e95b29988": {"title": "restore at::Half support for caffe2 SumOp (#41952)", "body": "Summary:\nPR https://github.com/pytorch/pytorch/issues/40379 added long support but removed at::Half support.  Restore at::Half support.\n\nCC ezyang xw285cornell neha26shah\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41952\n\nReviewed By: colesbury\n\nDifferential Revision: D22720656\n\nPulled By: xw285cornell\n\nfbshipit-source-id: be83ca7fe51fc43d81bc0685a3b658353d42f8ea", "pr_number": "41952", "files_changed": ["caffe2/operators/utility_ops.cu"], "labels": ["merged", "open source"]}, "d904ea5972": {"title": "[NCCL] DDP communication hook: getFuture() (#41596)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41596\n\nWe've modified the previous design of `convert_dist_work_to_future` API in the GH Issue [#39272](https://github.com/pytorch/pytorch/issues/39272).\n\n1. Whenever we create a `WorkNCCL` object, create a `Future` associated with `WorkNCCL` and store it with the object.\n2. Add an API `c10::intrusive_ptr<c10::ivalue::Future> getFuture()` to `c10d::ProcessGroup::Work`.\n3. This API will only be supported by NCCL in the first version, the default implementation will throw UnsupportedOperation.\n4. To mark the future associated with WorkNCCL completed, implement a `cudaStreamCallback` function.\n\n`cudaStreamAddCallback` is marked as deprecated. An alternative is `cudaLaunchHostFunc`, but it is supported for CUDA > 10 and may not be deprecated until there's a reasonable alternative available according to [this discussion](https://stackoverflow.com/questions/56448390/how-to-recover-from-cuda-errors-when-using-cudalaunchhostfunc-instead-of-cudastr).\nghstack-source-id: 108409748\n\nTest Plan:\nRun old  python test/distributed/test_c10d.py.\nSome additional tests:\n`test_ddp_comm_hook_allreduce_hook_nccl`: This unit test verifies whether a DDP communication hook that just calls allreduce gives the same result result with the case of no hook registered.  Without the then callback, the future_value in reducer is no longer a PyObject, and this unit test verifies future_value is properly checked.\n`test_ddp_comm_hook_allreduce_then_mult_ten_hook_nccl`: This unit test verifies whether a DDP communication hook that calls allreduce and then multiplies the result by ten gives the expected result.\n\nAs of v10:\n```\n........................s.....s.....................................................s...............................\n----------------------------------------------------------------------\nRan 116 tests\n\nOK (skipped=3)\n```\n`flow-cli` performance validation using a stacked diff where `bucket.work` is completely replaced with `bucket.future_work` in `reducer`. See PR [#41840](https://github.com/pytorch/pytorch/pull/41840) [D22660198](https://www.internalfb.com/intern/diff/D22660198/).\n\nReviewed By: izdeby\n\nDifferential Revision: D22583690\n\nfbshipit-source-id: 8c059745261d68d543eaf21a5700e64826e8d94a", "pr_number": "41596", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/comm.cpp", "torch/csrc/distributed/c10d/comm.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/nn/parallel/distributed.py"], "labels": ["merged"]}, "890b52e09f": {"title": "Reduce instability in runCleanUpPasses by reordering passes. (#41891)", "body": "Summary:\nCurrently constant pooling runs before const propagation, which can create more constants that need pooling. This can get in the way of serialization/deserialization stability because each time user serializes and deserializes a module, runCleanUpPasses is called upon it. Doing so multiple times would lead to different saved module.\n\nThis PR moves constant pooling after const propagation, which may slow down const propagation a little bit, but would otherwise side-step aforementioned problem.\n\ntest_constant_insertion in test_jit.py is also updated because after fixing the pass ordering, the number of constants is no longer a constant and it is extremely difficult to get the exact number with the current convoluted test structure. So for now, I changed the test to check only that CSE doesn't change number of \"prim::constant\" rather than comparing against a known number. Also left a TODO to improve this test.\n\nConstantPropagation pass is replaced by ConstantPropagationImmutableTypes because the latter is used in runCleanUpPasses. If not replaced, the former would create new CSE opportunities by folding more constants. This voids the purpose of the test case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41891\n\nReviewed By: colesbury\n\nDifferential Revision: D22701540\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 8e60dbdcc54a93dac111d81b8d88fb39387224f5", "pr_number": "41891", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_cleanup_passes.cpp", "test/cpp/jit/tests.h", "test/test_jit.py", "tools/build_variables.bzl", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/python/init.cpp"], "labels": ["merged", "oncall: jit"]}, "d4736ef95f": {"title": "Add done() API to Future (#42013)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42013\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D22729596\n\nPulled By: mrshenli\n\nfbshipit-source-id: ed31021a35af6e2c3393b9b14e4572cf51013bc0", "pr_number": "42013", "files_changed": ["test/test_futures.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/futures/__init__.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged", "oncall: jit"]}, "7e84913233": {"title": ".circleci: Make sure to install expect for docs push (#41964)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41964\n\nSince we're not executing this in a docker container we should go ahead\nan install expect explicitly\n\nThis is a follow up PR to #41871\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D22736738\n\nPulled By: seemethere\n\nfbshipit-source-id: a56e19c1ee13c2f6e2750c2483202c1eea3b558a", "pr_number": "41964", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["merged"]}, "becc1b26dd": {"title": "updated white list/allow list (#41789)", "body": "Summary:\ncloses https://github.com/pytorch/pytorch/issues/41758\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41789\n\nReviewed By: izdeby\n\nDifferential Revision: D22648038\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 5abc895789d8803ca542dfc0c62069350c6977c4", "pr_number": "41789", "files_changed": ["test/test_autograd.py"], "labels": ["merged", "open source", "triaged"]}, "42a0b51f71": {"title": "Easier english updated tech docs (#42016)", "body": "Summary:\nJust added a easier way to understand the tech docs\n\n![Screenshot from 2020-07-24 21-48-07](https://user-images.githubusercontent.com/55920093/88412562-6991cb00-cdf7-11ea-9612-5f69146ea233.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42016\n\nReviewed By: colesbury\n\nDifferential Revision: D22735752\n\nPulled By: mrshenli\n\nfbshipit-source-id: 8e3dfb721f51ee0869b0df66bf856d9949553453", "pr_number": "42016", "files_changed": ["docs/source/named_tensor.rst"], "labels": ["merged", "open source"]}, "0c0864c6be": {"title": "update tests to run back-compat check using new binary (#41949)", "body": "Summary:\ninstead exporting schemas using the current binary being tested, install nightly and export its schemas to use in a back-compat test run by the current binary being tested.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41949\n\nReviewed By: houseroad\n\nDifferential Revision: D22731054\n\nPulled By: bradleyhd\n\nfbshipit-source-id: 68a7e7637b9be2604c0ffcde2a40dd208057ba72", "pr_number": "41949", "files_changed": [".gitignore", ".jenkins/pytorch/test.sh", "test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "976e614915": {"title": "caffe2: add PIPELINE tag (#41482)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41482\n\nThis adds a new tag for use with pipeline parallelism.\n\nTest Plan: CI\n\nReviewed By: heslami\n\nDifferential Revision: D22551487\n\nfbshipit-source-id: 90910f458a9bce68f7ef684773322a49aa24494a", "pr_number": "41482", "files_changed": ["caffe2/python/layers/tags.py"], "labels": ["fb-exported", "merged"]}, "dede71d6e3": {"title": "Support aarch32 neon backend for Vec256 (#41267)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41267\n\nDue to llvm bug and some unsupported intrinsics we could not directly\nuse intrinsics for implementing aarch32 neon back end for Vec256.\nInstead we resort to inline assembly.\n\nTest Plan:\nvec256_test run on android phone.\n\nImported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22482196\n\nfbshipit-source-id: 1c22cf67ec352942c465552031e9329550b27b3e", "pr_number": "41267", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_float_neon.h"], "labels": ["merged"]}, "cf7e7909d5": {"title": "NCCL must depend on librt (#41978)", "body": "Summary:\nSince NCCL makes calls to shm_open/shm_close it must depend on librt on Linux\n\nThis should fix `DSO missing from command line` error on some platforms\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41978\n\nReviewed By: colesbury\n\nDifferential Revision: D22721430\n\nPulled By: malfet\n\nfbshipit-source-id: d2ae08ce9da3979daaae599e677d5e4519b080f0", "pr_number": "41978", "files_changed": ["cmake/External/nccl.cmake"], "labels": ["merged"]}, "45e6f2d600": {"title": "Enable ProcessGroupGlooTest in CI (#41985)", "body": "Summary:\nPartially addresses https://github.com/pytorch/pytorch/issues/41143\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41985\n\nReviewed By: rohan-varma\n\nDifferential Revision: D22741514\n\nPulled By: malfet\n\nfbshipit-source-id: 738d2e27f52334e402b65b724b8ba3b0b41372ee", "pr_number": "41985", "files_changed": [".jenkins/pytorch/test.sh", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["merged"]}, "6287f9ed65": {"title": "Remove AllGatherTestWithTimeout (#41945)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41945\n\nThis test previously did a thread sleep before launching the allgather operation, and then waited on the work object. Since the sleep was done before the work object was created, it did not affect the allgather call, and thus, did not test work-level timeouts as intended.\n\nI am removing this test for now. In the future we can add this test back, but would need to somehow inject a `cudaSleep` call before the  allgather (so the collective operation itself is delayed). This may require overriding the `ProcessGroupNCCL::collective`, so it's a bit more heavy-weight.\n\nIn the meantime, we can remove this test - work-level timeouts are still thoroughly tested with Gloo.\nghstack-source-id: 108370178\n\nTest Plan: Ran ProcessGroupNCCL tests on devGPU\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22702291\n\nfbshipit-source-id: a36ac3d83abfab6351c0476046a2f3b04a80c44d", "pr_number": "41945", "files_changed": ["torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["merged"]}, "8e03c38a4f": {"title": "Add prim::EnumName and prim::EnumValue ops (#41965)", "body": "Summary:\n[2/N] Implement Enum JIT support\n\nAdd prim::EnumName and prim::EnumValue and their lowerings to support getting `name` and `value` attribute of Python enums.\n\nSupported:\nEnum-typed function targuments\nusing Enum type and comparing them\nSupport getting name/value attrs of enums\n\nTODO:\nAdd PyThon sugared value for Enum\nSupport Enum-typed return values\nSupport enum values of different types in same Enum class\nSupport serialization and deserialization\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41965\n\nReviewed By: eellison\n\nDifferential Revision: D22714446\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: db8c4e26b657e7782dbfc2b58a141add1263f76e", "pr_number": "41965", "files_changed": ["aten/src/ATen/core/interned_strings.h", "aten/src/ATen/core/jit_type.h", "test/jit/test_enum.py", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/ir.h", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["merged", "oncall: jit"]}, "38580422bb": {"title": "Allow specifying PYTHON executable to build_android (#41927)", "body": "Summary:\nbuild_android.sh should check PYTHON environment variable before trying to use default python executable.\nEven in that case, try to pick python3 over python2 when available.\n\nCloses https://github.com/pytorch/pytorch/issues/41795\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41927\n\nReviewed By: seemethere\n\nDifferential Revision: D22696850\n\nPulled By: malfet\n\nfbshipit-source-id: be236c2baf54a1cd111e55ee7743cdc93cb6b9d7", "pr_number": "41927", "files_changed": ["scripts/build_android.sh"], "labels": ["merged"]}, "366c014a77": {"title": "[Resubmit #41318] NCCL backend support for torch bool (#41959)", "body": "Summary:\nResubmit of https://github.com/pytorch/pytorch/issues/41318 pushed to ci-all branch.\n\nOriginal description:\nCloses https://github.com/pytorch/pytorch/issues/24137.\nThis PR adds support for the torch.bool tensor type to ProcessGroupNCCL. For most types we use the existing mapping, but since bool is not supported as a native ncclDataType_t, we add the following logic:\n\nMap at::kBool to ncclUint8\nDuring reduction (allreduce for example), if the operation is SUM, we instead override to to a MAX, to avoid overflow issues. The rest of the operations work with no changes. In the boolean case, changing sum to max makes no correctness difference since they both function as a bitwise OR.\nThe reduction logic (for example for reduce/allreduce) is as follows:\nsum, max = bitwise or\nproduct, min = bitwise and\n\nNote that this PR doesn't add support for BAND/BOR/BXOR. That is because these reduction ops currently are not supported by NCCL backend, see https://github.com/pytorch/pytorch/issues/41362\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41959\n\nReviewed By: mrshenli\n\nDifferential Revision: D22719665\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 8bc4194a8d1268589640242277124f277d2ec9f1", "pr_number": "41959", "files_changed": ["test/distributed/test_distributed.py", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged"]}, "12cd083fd7": {"title": "Updates torch.tensor, torch.as_tensor, and sparse ctors to use the device of inputs tensors they're given, by default (#41984)", "body": "Summary:\n**BC-Breaking Note**\n\nThis PR changes the behavior of the torch.tensor, torch.as_tensor, and sparse constructors. When given a tensor as input and a device is not explicitly specified, these constructors now always infer their device from the tensor. Historically, if the optional dtype kwarg was provided then these constructors would not infer their device from tensor inputs. Additionally, for the sparse ctor a runtime error is now thrown if the indices and values tensors are on different devices and the device kwarg is not specified.\n\n**PR Summary**\nThis PR's functional change is a single line:\n\n```\nauto device = device_opt.has_value() ? *device_opt : (type_inference ? var.device() : at::Device(computeDeviceType(dispatch_key)));\n```\n=>\n```\nauto device = device_opt.has_value() ? *device_opt : var.device();\n```\n\nin `internal_new_from_data`. This line entangled whether the function was performing type inference with whether it inferred its device from an input tensor, and in practice meant that\n\n```\nt = torch.tensor((1, 2, 3), device='cuda')\ntorch.tensor(t, dtype=torch.float64)\n```\n\nwould return a tensor on the CPU, not the default CUDA device, while\n\n```\nt = torch.tensor((1, 2, 3), device='cuda')\ntorch.tensor(t)\n```\n\nwould return a tensor on the device of `t`!\n\nThis behavior is niche and odd, but came up while aocsa was fixing https://github.com/pytorch/pytorch/issues/40648.\n\nAn additional side affect of this change is that the indices and values tensors given to a sparse constructor must be on the same device, or the sparse ctor must specify the dtype kwarg. The tests in test_sparse.py have been updated to reflect this behavior.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41984\n\nReviewed By: ngimel\n\nDifferential Revision: D22721426\n\nPulled By: mruberry\n\nfbshipit-source-id: 909645124837fcdf3d339d7db539367209eccd48", "pr_number": "41984", "files_changed": ["test/test_sparse.py", "test/test_torch.py", "torch/csrc/utils/tensor_new.cpp"], "labels": ["merged", "topic: bc-breaking"]}, "c5b4f60fc2": {"title": "Move qconfig removal into convert() (#41930)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41930\n\nAs title\nghstack-source-id: 108517079\n\nTest Plan: CI\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22698386\n\nfbshipit-source-id: 4f748c9bae4a0b615aa69c7cc8d8e451e5d26863", "pr_number": "41930", "files_changed": ["test/quantization/test_quantize.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "b00c05c86c": {"title": "update cub submodule (#42042)", "body": "Summary:\nPer title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42042\n\nReviewed By: mruberry\n\nDifferential Revision: D22752345\n\nPulled By: ngimel\n\nfbshipit-source-id: 363735bfe3d49bab12fedef43b68c9dc9e372815", "pr_number": "42042", "files_changed": ["third_party/cub"], "labels": ["merged"]}, "47e6d4b3c8": {"title": "Revert D22741514: [pytorch][PR] Enable ProcessGroupGlooTest in CI", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22741514 (https://github.com/pytorch/pytorch/commit/45e6f2d60012df37393f5c5153e805209713ef19)\n\nOriginal commit changeset: 738d2e27f523\n\nfbshipit-source-id: 0381105ed0ab676b0abd1927f602a35b1b264a6a", "pr_number": null, "files_changed": [".jenkins/pytorch/test.sh", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": []}, "6af659629a": {"title": "DOC: fix two build warnings (#41334)", "body": "Summary:\nxref gh-38011.\n\nFixes two warnings when building documentation by\n- using the external link to torchvision\n- install tensorboard before building documentation\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41334\n\nReviewed By: ngimel\n\nDifferential Revision: D22753083\n\nPulled By: mruberry\n\nfbshipit-source-id: 876377e9bd09750437fbfab0378664b85701f827", "pr_number": "41334", "files_changed": ["docs/requirements.txt", "docs/source/index.rst"], "labels": ["merged", "module: docs", "open source", "triaged"]}, "b7bda236d1": {"title": "DOC: split quantization.rst into smaller pieces (#41321)", "body": "Summary:\nxref gh-38010 and gh-38011.\n\nAfter this PR, there should be only two warnings:\n```\npytorch/docs/source/index.rst:65: WARNING: toctree contains reference to nonexisting \\\n      document 'torchvision/index'\nWARNING: autodoc: failed to import class 'tensorboard.writer.SummaryWriter' from module \\\n     'torch.utils'; the following exception was raised:\nNo module named 'tensorboard'\n```\n\nIf tensorboard and torchvision are prerequisites to building docs, they should be added to the `requirements.txt`.\n\nAs for breaking up quantization into smaller pieces: I split out the list of supported operations and the list of modules to separate documents. I think this makes the page flow better, makes it much \"lighter\" in terms of page cost, and also removes some warnings since the same class names appear in multiple sub-modules.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41321\n\nReviewed By: ngimel\n\nDifferential Revision: D22753099\n\nPulled By: mruberry\n\nfbshipit-source-id: d504787fcf1104a0b6e3d1c12747ec53450841da", "pr_number": "41321", "files_changed": ["docs/Makefile", "docs/source/jit.rst", "docs/source/quantization-support.rst", "docs/source/quantization.rst", "docs/source/torch.nn.intrinsic.qat.rst", "docs/source/torch.nn.intrinsic.quantized.rst", "docs/source/torch.nn.intrinsic.rst", "docs/source/torch.nn.qat.rst", "docs/source/torch.nn.quantized.dynamic.rst", "docs/source/torch.nn.quantized.rst", "docs/source/torch.quantization.rst", "docs/source/type_info.rst", "torch/csrc/api/include/torch/nn/modules/loss.h", "torch/jit/_script.py", "torch/nn/modules/loss.py"], "labels": ["merged", "oncall: jit", "open source"]}, "96aaa311c0": {"title": "Grammar Changes (#42076)", "body": "Summary:\nSmall grammatical updates.\n![Screenshot (188)](https://user-images.githubusercontent.com/56619747/88471271-02723480-cf25-11ea-8fd1-ae98d5ebcc86.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42076\n\nReviewed By: mrshenli\n\nDifferential Revision: D22756651\n\nPulled By: ngimel\n\nfbshipit-source-id: e810eb7397a5831d801348c8fff072854658830e", "pr_number": "42076", "files_changed": ["torch/__init__.py"], "labels": ["merged", "open source"]}, "fbdaa555a2": {"title": "Enable ProcessGroupGlooTest in CI (take 2) (#42086)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42073\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42086\n\nReviewed By: ngimel\n\nDifferential Revision: D22765777\n\nPulled By: malfet\n\nfbshipit-source-id: ebbcd44f448a1e7f9a3d18fa9967461129dd1dcd", "pr_number": "42086", "files_changed": [".jenkins/pytorch/test.sh", "torch/lib/c10d/test/CUDATest.cu", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["merged"]}, "11e5174926": {"title": "Added support for Huber Loss (#37599)", "body": "Summary:\nCurrent losses in PyTorch only include a (partial) implementation of Huber loss through `smooth l1` based on Fast RCNN - which essentially uses a delta value of 1. Changing/Renaming the [`_smooth_l1_loss()`](https://github.com/pytorch/pytorch/blob/3e1859959a3f720bb5f5e47c3ca15fb3cbfae4da/torch/nn/functional.py#L2487) and refactoring to include delta, enables to use the actual function.\n\nSupplementary to this, I have also made a functional and criterion versions for anyone that wants to set the delta explicitly - based on the functional `smooth_l1_loss()` and the criterion `Smooth_L1_Loss()`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37599\n\nDifferential Revision: D21559311\n\nPulled By: vincentqb\n\nfbshipit-source-id: 34b2a5a237462e119920d6f55ba5ab9b8e086a8c", "pr_number": "37599", "files_changed": ["torch/nn/functional.py", "torch/nn/modules/loss.py"], "labels": ["merged", "open source", "triaged"]}, "4290d0be60": {"title": "Remove settings for the logit test case. (#42114)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42114\n\nRemove settings for the logit test case.\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: test_op_nnpi_fp16.py test case.\n\nReviewed By: hyuen\n\nDifferential Revision: D22766728\n\nfbshipit-source-id: 2fe8404b103c613524cf1beddf1a0eb9068caf8a", "pr_number": "42114", "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py"], "labels": ["fb-exported", "merged"]}, "3e121d9688": {"title": "Amend docstring and add test for Flatten module (#42084)", "body": "Summary:\nI've noticed when PR https://github.com/pytorch/pytorch/issues/22245 introduced `nn.Flatten`, the docstring had a bug where it wouldn't render properly on the web, and this PR addresses that. Additionally, it adds a unit test for this module.\n\n**Actual**\n![image](https://user-images.githubusercontent.com/13088001/88483672-cf896a00-cf3f-11ea-8b1b-a30d152e1368.png)\n\n**Expected**\n![image](https://user-images.githubusercontent.com/13088001/88483642-86391a80-cf3f-11ea-8333-0964a027a172.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42084\n\nReviewed By: mrshenli\n\nDifferential Revision: D22756662\n\nPulled By: ngimel\n\nfbshipit-source-id: 60c58c18c9a68854533196ed6b9e9fb0d4f83520", "pr_number": "42084", "files_changed": ["test/test_nn.py", "torch/nn/modules/flatten.py"], "labels": ["merged", "open source"]}, "e62bf89273": {"title": "Renaming variables from dX to dY in Learnable Fake Quantize kernels for Better Clarity (#42032)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42032\n\nIn this diff, the arguments `dX` within the C++ kernels are named as `dY` for clarity and avoid confusion since it doesn't represent the gradient with respect to the input.\n\nTest Plan:\nTo test all related fake quantize kernel operators, on a devvm, run the command:\n\n`buck test //caffe2/test:quantization -- learnable`\n\nReviewed By: z-a-f, jerryzh168\n\nDifferential Revision: D22735429\n\nfbshipit-source-id: 9d6d967f08b98a720eca39a4d2280ca8109dcdd6", "pr_number": "42032", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu", "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp"], "labels": ["fb-exported", "merged"]}, "c261a894d1": {"title": "Updates to Python Module for Calculation of dX and Addition of Unit Tests (#42033)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42033\n\nIn this diff, the Python `_LearnableFakeQuantize` module is updated where the gradient with respect to the input `x` is actually computed instead of passed through. Argument naming is also updated for better clarity; and unit tests on the `PerTensor` and `PerChannel` operators are added for asserting correctness.\n\nTest Plan:\nOn a devvm, execute the command:\n\n`buck test //caffe2/test:quantization -- learnable_py_module`\n\nTo include `cuda` tests as well, run:\n\n`buck test mode/dev-nosan //caffe2/test:quantization -- learnable_py_module`\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22735580\n\nfbshipit-source-id: 66bea7e9f8cb6422936e653500f917aa597c86de", "pr_number": "42033", "files_changed": ["test/quantization/test_workflow_module.py", "torch/quantization/_learnable_fake_quantize.py"], "labels": ["fb-exported", "merged"]}, "5a6d88d503": {"title": "Updates to Scale and Zero Point Gradient Calculation (#42034)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42034\n\nIn this diff, scale and zero point gradient calculations are updated to correctly reflect the actual backpropagation equation (instead of `dScale * dX`, the near-final output should be `dScale * dY`; the same applies to zero point).\n\nTest Plan:\nTo execute the unit tests for all affected learnable fake quantize modules and kernels, on a devvm, execute the following command:\n\n`buck test //caffe2/test:quantization -- learnable`\n\nTo enable the `cuda` tests, execute the following command:\n\n`buck test mode/dev-nosan //caffe2/test:quantization -- learnable`\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22735668\n\nfbshipit-source-id: 45c1e0fd38cbb2d8d5e60be4711e1e989e9743b4", "pr_number": "42034", "files_changed": ["aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp", "test/quantization/test_workflow_module.py", "torch/quantization/_learnable_fake_quantize.py"], "labels": ["fb-exported", "merged"]}, "d4735ff490": {"title": "Avoid refcount bump in IValue::toStringRef() (#42019)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42019\n\nAccording to benchmarks, this makes IValue::toStringRef() 3-4x as fast.\nghstack-source-id: 108451154\n\nTest Plan: unit tests\n\nReviewed By: ezyang\n\nDifferential Revision: D22731354\n\nfbshipit-source-id: 3ca3822ea7310d8593e38b1d3e6014d6d80963db", "pr_number": "42019", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["merged"]}, "6367a9d2b0": {"title": "[vulkan] Shaders caching (#39384)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39384\n\nIntroducing `ComputeUnitFactory` which is responsible for providing `ComputeUnit`s (Shaders),\nit caches it, using shader name (glsl file name)+workGroupSize as a cacheKey, just `std::map<string, std::shared_ptr>`\n\nMacro GLSL_SPV changed to have literal name for cache key as a first argument.\n\nAll constructors of ComputeUnit are changed to use `ComputeUnitFactory`\n\nOwnership model:\nComputeUnitFactory also owns `vkPipelineCache` that is internal vulkan cache object ( https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VkPipelineCache.html )\n\n`VContext` (global object) owns ComputeUnitFactory, that owns ComputeUnits, vkPipelineCache, for destruction of them we need valid VkDevice, so it should be destructed before `vkDestryDevice` in `~VContext` => As members of the class will be destructed only after destructor - forcing destruction of ComputeUnitFactory before `vkDestroyDevice`, doing `unique_ptr<ComputeUnitFactory>.reset()`\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D21962430\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: effe60538308805f317c11448b31dbcf670487e8", "pr_number": "39384", "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.cpp", "aten/src/ATen/native/vulkan/Vulkan.h", "aten/src/ATen/native/vulkan/VulkanOps.cpp"], "labels": ["merged"]}, "4281240cb5": {"title": "Raise error for duplicate params in param group #40967 (#41597)", "body": "Summary:\nThis PR fixes an issue in https://github.com/pytorch/pytorch/issues/40967 where duplicate parameters across different parameter groups are not allowed, but duplicates inside the same parameter group are accepted. After this PR, both cases are treated equally and raise `ValueError`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41597\n\nReviewed By: zou3519\n\nDifferential Revision: D22608019\n\nPulled By: vincentqb\n\nfbshipit-source-id: 6df41dac62b80db042cfefa6e53fb021b49f4399", "pr_number": "41597", "files_changed": ["test/test_optim.py", "torch/optim/optimizer.py"], "labels": ["merged", "open source", "topic: deprecation"]}, "d6f1346c37": {"title": "Add a new op for converting the dense feature to sparse representation", "body": "Summary: we need this op to avoid the splicing of a dense tensor and then use the Mergesinglescaler op\n\nTest Plan: integrated test with dper2\n\nDifferential Revision: D22677523\n\nfbshipit-source-id: f4f9a1f06841b0906ec8cbb435482ae0a89e1721", "pr_number": null, "files_changed": ["caffe2/operators/feature_maps_ops.cc", "caffe2/operators/feature_maps_ops.h", "caffe2/python/operator_test/feature_maps_ops_test.py"], "labels": []}, "e59db43313": {"title": "Find hip properly (#42064)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41886\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42064\n\nReviewed By: seemethere\n\nDifferential Revision: D22757115\n\nPulled By: malfet\n\nfbshipit-source-id: 9c8805e6eb0b7d7defe0ecb08c1e45dcc775a237", "pr_number": "42064", "files_changed": ["cmake/public/LoadHIP.cmake"], "labels": ["merged", "open source"]}, "5246bc4e87": {"title": "register parameters correctly in c++ MultiheadAttention (#42037)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42037\n\nThis is to fix #41951\n\nTest Plan: Imported from OSS\n\nReviewed By: yf225\n\nDifferential Revision: D22764717\n\nPulled By: glaringlee\n\nfbshipit-source-id: e6da0aeb05a2356f52446e6d5fad391f2cd1cf6f", "pr_number": "42037", "files_changed": ["test/cpp/api/modules.cpp", "torch/csrc/api/src/nn/modules/activation.cpp"], "labels": ["merged"]}, "ed822de0fc": {"title": "change 2 instances of blacklist to blocklist in tools/pyi/gen_pyi.py (#41979)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41722\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41979\n\nReviewed By: ngimel\n\nDifferential Revision: D22764112\n\nPulled By: zou3519\n\nfbshipit-source-id: 3f8580c96cf45078a9df3cd9ca6fdb10d58e143f", "pr_number": "41979", "files_changed": ["tools/pyi/gen_pyi.py"], "labels": ["merged", "open source", "triaged"]}, "f7d50f50b9": {"title": ".circleci: Prefer netrc for docs push (#42136)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42136\n\nExpect was giving weird issues so let's just use netrc since it doesn't\nrely on janky expect behavior\n\nAnother follow up for: https://github.com/pytorch/pytorch/pull/41964\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: yns88\n\nDifferential Revision: D22778940\n\nPulled By: seemethere\n\nfbshipit-source-id: 1bdf879a5cfbf68a7d2d34b6966c20f95bd0a3b5", "pr_number": "42136", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["merged", "module: ci"]}, "330a107199": {"title": "Refactor lite serializer dependencies from full jit (#42127)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42127\n\nThis diff renames core_autograd_sources to core_trainer_sources and moves/adds dependencies for the lite trainer in order to build the serializer functionality internally.\nghstack-source-id: 108589416\n\nTest Plan: Manually tested serializer functionality from the internal lite trainer and verified that data is written correctly.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22738293\n\nfbshipit-source-id: 992beb0c4368b2395f5bd5563fb2bc12ddde39a1", "pr_number": "42127", "files_changed": ["tools/build_variables.bzl"], "labels": ["merged"]}, "6ca5421a8f": {"title": "Enable non-synchronizing cub scan for cum* operations (#42036)", "body": "Summary:\nThis uses cub for cum* operations, because, unlike thrust, cub is non-synchronizing.\nCub does not support more than `2**31` element tensors out of the box (in fact, due to cub bugs the cutoff point is even smaller)\nso to support that I split the tensor into `2**30` element chunks, and modify the first value of the second and subsequent chunks to contain the cumsum result of the previous chunks. Since modification is done inplace on the source tensor, if something goes wrong and we error out before the source tensor is reverted back to its original state, source tensor will be corrupted, but in most cases errors will invalidate the full coda context.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42036\n\nReviewed By: ajtulloch\n\nDifferential Revision: D22749945\n\nPulled By: ngimel\n\nfbshipit-source-id: 9fc9b54d466df9c8885e79c4f4f8af81e3f224ef", "pr_number": "42036", "files_changed": ["aten/src/ATen/native/cuda/ScanKernels.cu", "test/test_torch.py"], "labels": ["merged"]}, "cb9c2049cd": {"title": "replace blacklist in aten/src/ATen/native/cudnn/Conv.cpp (#41627)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41700.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41627\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D22678492\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 75b82bd10059754d8e6c25fc20e9dde775d54698", "pr_number": "41627", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp"], "labels": ["merged", "open source", "triaged"]}, "d198fb3efe": {"title": "changed white-allowlisted (#41796)", "body": "Summary:\ncloses https://github.com/pytorch/pytorch/issues/41749\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41796\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D22718991\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 6c2d2b0e3b1e79fd515f9bdd395335a32f525a26", "pr_number": "41796", "files_changed": ["test/test_namedtuple_return_api.py"], "labels": ["merged", "module: tests", "open source", "triaged"]}, "1df35ba61e": {"title": "Back out \"Support aarch32 neon backend for Vec256\"", "body": "Summary: Original commit changeset: 1c22cf67ec35\n\nTest Plan: sandcastle, testing on Portal\n\nReviewed By: currybeef\n\nDifferential Revision: D22774614\n\nfbshipit-source-id: 8897aec5df32092c4df86c0d54b0d2fe58d66e66", "pr_number": null, "files_changed": ["aten/src/ATen/cpu/vec256/vec256_float_neon.h"], "labels": []}, "646042e0fb": {"title": "Add suggestion to enumerate ModuleDict in error message (#41946)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41946\n\nReviewed By: ngimel\n\nDifferential Revision: D22774243\n\nPulled By: wconstab\n\nfbshipit-source-id: 5cfbe52b5b1c540f824593e67ae6ba4973458bb5", "pr_number": "41946", "files_changed": ["torch/csrc/jit/python/python_sugared_value.cpp"], "labels": ["merged", "oncall: jit"]}, "509c18a096": {"title": "Documentation for `torch.optim.swa_utils` (#41228)", "body": "Summary:\nThis PR adds a description of `torch.optim.swa_utils` added in https://github.com/pytorch/pytorch/pull/35032 to the docs at `docs/source/optim.rst`. Please let me know what you think!\n\nvincentqb andrewgordonwilson\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41228\n\nReviewed By: ngimel\n\nDifferential Revision: D22609451\n\nPulled By: vincentqb\n\nfbshipit-source-id: 8dd98102c865ae4a074a601b047072de8cc5a5e3", "pr_number": "41228", "files_changed": ["docs/source/optim.rst"], "labels": ["merged", "module: docs", "open source", "triaged"]}, "d5de616a4a": {"title": "Enable c10d Store tests in CI (#42128)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42128\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22774445\n\nPulled By: mrshenli\n\nfbshipit-source-id: 6e5e56f42833414ef375b6cd23fdb3260cb07be9", "pr_number": "42128", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "bcd75bd683": {"title": "[ModelLints] Refine dropout lint message. (#42046)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42046\n\nRefine dropout lint message as we have enabled dropout operator removal in optimize_for_mobile method.\nghstack-source-id: 108607182\n\nTest Plan: buck test ai_infra/ai_mobile_infra/tests:mobile_model_util_tests\n\nReviewed By: kimishpatel\n\nDifferential Revision: D22741132\n\nfbshipit-source-id: 8f87356aae2bd9c89d1cad0d7be7286278bb14ad", "pr_number": "42046", "files_changed": ["torch/utils/mobile_optimizer.py"], "labels": ["merged"]}, "c76fada4a8": {"title": "Let DDP.train() return self to stay consistent with nn.Module (#42131)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42131\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22775311\n\nPulled By: mrshenli\n\nfbshipit-source-id: ac9e6cf8b2381036a2b6064bd029dca361a81777", "pr_number": "42131", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["merged"]}, "f805184165": {"title": "onnxifi: make it work with AsyncIf", "body": "Summary:\nthe onnxifi path didn't handle the input/output name rewrite for ssa correctly for AsyncIf op. Add support for it.\n\nAlso fixed a place where we lose the net type while doing onnxifi transform.\n\nTest Plan: Load 163357582_593 which is a multi feed model that uses AsyncIf. This used to fail with c2 not finding some blobs in workspace. Now it works.\n\nReviewed By: dhe95\n\nDifferential Revision: D21268230\n\nfbshipit-source-id: ce7ec0e952513d0f251df1bfcfb2b0250f51fd94", "pr_number": null, "files_changed": ["caffe2/onnx/onnx_exporter.cc", "caffe2/opt/onnxifi_transformer.cc"], "labels": []}, "c062cdbd90": {"title": "Log the net if blob doesn't exist when setting output record (#41971)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41971\n\nReviewed By: wx1988\n\nDifferential Revision: D22490309\n\nfbshipit-source-id: d967ee211b610f5523a307b5266b9fcb0277a21c", "pr_number": "41971", "files_changed": ["caffe2/python/core.py"], "labels": ["fb-exported", "merged"]}, "83762844e5": {"title": "Make `run_binary_ops_test` function generic and Add tests to add_kernel function (#42101)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42101\n\n1. Add test fixture `atest class` to store global variables\n2. Make `run_binary_ops_test` function generic: can dispose different dtypes and different numbers of parameters\n3. add test to `add_kernel`\n\nTest Plan:\nRun locally to check cover the corresponding code part in `BinaryOpsKernel.cpp`.\nCI\n\nReviewed By: malfet\n\nDifferential Revision: D22760015\n\nfbshipit-source-id: 95b47732f661124615c0856efa827445dd714125", "pr_number": "42101", "files_changed": ["aten/src/ATen/test/atest.cpp"], "labels": ["fb-exported", "merged"]}, "0a0960126c": {"title": "If we don't collect tracing, always free the trace data (#42118)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42118\n\nWe toggle trace on with a certain probablility. In the case of 3 inferences with trace on/off/on. We leak the trace from the first inference. Always clean up the trace will fix it.\n\nTest Plan:\npredictor\n\nI created a tiny repro here: D22786551\n\nWith this fix, this issue is gone.\n\nReviewed By: gcatron\n\nDifferential Revision: D22768382\n\nfbshipit-source-id: 9ee0bbcb2bc5f76107dae385759fe578909a683d", "pr_number": "42118", "files_changed": ["caffe2/opt/onnxifi_op.cc"], "labels": ["fb-exported", "merged"]}, "4f723825b4": {"title": "[vulkan] adaptive_avg_pool2d (#41220)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41220\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22754943\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 91a94f32db005ebb693384f4d27efe66e2c33a14", "pr_number": "41220", "files_changed": ["aten/src/ATen/native/AdaptiveAveragePooling.cpp", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanAten.h", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/VulkanOps.h", "aten/src/ATen/native/vulkan/glsl/adaptive_avg_pool2d.glsl", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "5336ccc1b2": {"title": "[BugFix] Fix bug in onnx::SsaRewrite (#42148)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42148\n\nDifferential Revision: D22687388\n\nfbshipit-source-id: facf7a186dd48d6f919d0ff5d42f756977c3f9f4", "pr_number": "42148", "files_changed": ["caffe2/onnx/onnx_exporter.cc"], "labels": ["fb-exported", "merged"]}, "3c6fae6567": {"title": "[caffe2][tpx] Use logger instead of print", "body": "Test Plan: CI?\n\nDifferential Revision: D22790238\n\nfbshipit-source-id: c0a801cdf7f0da489c67708a0eb1b498ff104c64", "pr_number": null, "files_changed": ["test/distributed/test_nccl.py"], "labels": []}, "6bd88f581a": {"title": "Revert D22790238: [caffe2][tpx] Use logger instead of print", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22790238 (https://github.com/pytorch/pytorch/commit/3c6fae6567230da0e08788b968cfb466ec629d65)\n\nOriginal commit changeset: c0a801cdf7f0\n\nfbshipit-source-id: cadfbd22f7d3ce656624483c9a19062f7c9a5b61", "pr_number": null, "files_changed": ["test/distributed/test_nccl.py"], "labels": []}, "2bc7dae2fc": {"title": "Use new sccache for RocM builds (#42134)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42134\n\nReviewed By: seemethere\n\nDifferential Revision: D22782146\n\nPulled By: malfet\n\nfbshipit-source-id: 85ba69a705600e30ae0eddbf654298b3dc6f96ed", "pr_number": "42134", "files_changed": [".jenkins/pytorch/build.sh"], "labels": ["merged"]}, "5124436af4": {"title": "Fix const correctness for VmapPhysicalView struct methods (#41940)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41940\n\nSee title. I marked methods that don't mutate the VmapPhysicalView as\n`const`.\n\nTest Plan: - wait for tests\n\nReviewed By: albanD\n\nDifferential Revision: D22764102\n\nPulled By: zou3519\n\nfbshipit-source-id: 40f957ad61c85f0e5684357562a541a2712b1f38", "pr_number": "41940", "files_changed": ["aten/src/ATen/VmapTransforms.cpp", "aten/src/ATen/VmapTransforms.h"], "labels": ["merged"]}, "1994ab1473": {"title": "Optimize alignBatchDimsAtFront (#41941)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41941\n\nIf we know that the tensor already has the desired aligned size, we\ndon't need to put in the effort to align it.\n\nTest Plan: - `./build/bin/vmap_test`, `pytest test/test_vmap.py -v`\n\nReviewed By: albanD\n\nDifferential Revision: D22764101\n\nPulled By: zou3519\n\nfbshipit-source-id: a2ab7ce7b98d405ae905f7fd98db097210bfad65", "pr_number": "41941", "files_changed": ["aten/src/ATen/VmapTransforms.cpp"], "labels": ["merged"]}, "0571cfd875": {"title": "Implement `MultiBatchVmapTransform::logicalToPhysical(TensorList)` (#41942)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41942\n\nThis function:\n- permutes all batch dims to the front of the tensors\n- aligns all the batch dims to the collective levels of all the tensors\n- expands all of the batch dims such that they are present in each of\nthe result tensors\n\nThis function is useful for the next diff up on the stack (which is\nimplementing a fallback kernel for BatchedTensor). It's also useful in\ngeneral for implementing batching rules on operators that take in\nmultiple batch dimensions at the front of each tensor (but we don't have\ntoo many of those in PyTorch).\n\nTest Plan: - `./build/bin/vmap_test`\n\nReviewed By: ezyang\n\nDifferential Revision: D22764104\n\nPulled By: zou3519\n\nfbshipit-source-id: d42cc8824a1bcf258687de164b7853af52852f53", "pr_number": "41942", "files_changed": ["aten/src/ATen/VmapTransforms.cpp", "aten/src/ATen/VmapTransforms.h", "aten/src/ATen/test/vmap_test.cpp"], "labels": ["merged"]}, "e179966248": {"title": "[caffe2][tpx] log to stderr (#42162)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42162\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D22791440\n\nfbshipit-source-id: 14f16cd7a94a57161c5724177b518527f486232d", "pr_number": "42162", "files_changed": ["test/distributed/test_nccl.py"], "labels": ["fb-exported", "merged"]}, "1a8269a566": {"title": "Replace blacklist with blocklist in test/run_test.py file. (#42011)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41716\ntest/run_test.py file updated with an appropriate replacement for blacklist and whitelist.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42011\n\nReviewed By: pbelevich\n\nDifferential Revision: D22791836\n\nPulled By: malfet\n\nfbshipit-source-id: 8139649c5b70c876b711e25c33f3051ea8461063", "pr_number": "42011", "files_changed": ["test/run_test.py"], "labels": ["merged", "open source", "triaged"]}, "b282297559": {"title": "Replace whitelist with allowlist (#42067)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41757\n\nI've replaced all the whitelist with allowlist for this issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42067\n\nReviewed By: pbelevich\n\nDifferential Revision: D22791690\n\nPulled By: malfet\n\nfbshipit-source-id: 638c13cf49915f5c83bd79c7f4a39b8390cc15b4", "pr_number": "42067", "files_changed": ["CMakeLists.txt", "caffe2/CMakeLists.txt", "cmake/Allowlist.cmake", "cmake/Whitelist.cmake"], "labels": ["merged", "open source", "triaged"]}, "672ed3c06b": {"title": "replace onnx producer_version when updating results (#41910)", "body": "Summary:\nxref gh-39002 which handled the reading but not the writing of the onnx expect files, and the last comment in that PR which points out `XXX` was suboptimal.\nxref [this comment](https://github.com/pytorch/pytorch/pull/37091#discussion_r456460168) which pointed out the problem.\n\nThis PR:\n- replaces `XXX` with `CURRENT_VERSION` in the stored files\n- ensures that updating the results with the `--accept` flag will maintain the change\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41910\n\nReviewed By: pbelevich\n\nDifferential Revision: D22758671\n\nPulled By: ezyang\n\nfbshipit-source-id: 47c345c66740edfc8f0fb9ff358047a41e19b554", "pr_number": "41910", "files_changed": ["test/onnx/expect/TestOperators.test_acos.expect", "test/onnx/expect/TestOperators.test_add_broadcast.expect", "test/onnx/expect/TestOperators.test_add_left_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_right_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_singleton_broadcast.expect", "test/onnx/expect/TestOperators.test_addconstant.expect", "test/onnx/expect/TestOperators.test_addmm.expect", "test/onnx/expect/TestOperators.test_arange_dynamic.expect", "test/onnx/expect/TestOperators.test_argmax.expect", "test/onnx/expect/TestOperators.test_asin.expect", "test/onnx/expect/TestOperators.test_at_op.expect", "test/onnx/expect/TestOperators.test_atan.expect", "test/onnx/expect/TestOperators.test_avg_pool2d.expect", "test/onnx/expect/TestOperators.test_baddbmm.expect", "test/onnx/expect/TestOperators.test_basic.expect", "test/onnx/expect/TestOperators.test_batchnorm.expect", "test/onnx/expect/TestOperators.test_batchnorm_1d.expect", "test/onnx/expect/TestOperators.test_batchnorm_noaffine.expect", "test/onnx/expect/TestOperators.test_batchnorm_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_batchnorm_training.expect", "test/onnx/expect/TestOperators.test_bitshift.expect", "test/onnx/expect/TestOperators.test_c2_op.expect", "test/onnx/expect/TestOperators.test_chunk.expect", "test/onnx/expect/TestOperators.test_clip.expect", "test/onnx/expect/TestOperators.test_clip_max.expect", "test/onnx/expect/TestOperators.test_clip_min.expect", "test/onnx/expect/TestOperators.test_concat2.expect", "test/onnx/expect/TestOperators.test_conv.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4_opset8.expect", "test/onnx/expect/TestOperators.test_convtranspose.expect", "test/onnx/expect/TestOperators.test_cos.expect", "test/onnx/expect/TestOperators.test_cumsum.expect", "test/onnx/expect/TestOperators.test_det.expect", "test/onnx/expect/TestOperators.test_dict.expect", "test/onnx/expect/TestOperators.test_dict_str.expect", "test/onnx/expect/TestOperators.test_dim.expect", "test/onnx/expect/TestOperators.test_dropout.expect", "test/onnx/expect/TestOperators.test_dropout_default.expect", "test/onnx/expect/TestOperators.test_dropout_opset12.expect", "test/onnx/expect/TestOperators.test_dropout_training.expect", "test/onnx/expect/TestOperators.test_dropout_training_opset12.expect", "test/onnx/expect/TestOperators.test_elu.expect", "test/onnx/expect/TestOperators.test_embedding_bags.expect", "test/onnx/expect/TestOperators.test_empty_like.expect", "test/onnx/expect/TestOperators.test_empty_like_opset7.expect", "test/onnx/expect/TestOperators.test_equal.expect", "test/onnx/expect/TestOperators.test_erf.expect", "test/onnx/expect/TestOperators.test_exp.expect", "test/onnx/expect/TestOperators.test_expand.expect", "test/onnx/expect/TestOperators.test_flatten.expect", "test/onnx/expect/TestOperators.test_flatten2D.expect", "test/onnx/expect/TestOperators.test_fmod.expect", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_full.expect", "test/onnx/expect/TestOperators.test_full_like.expect", "test/onnx/expect/TestOperators.test_gather.expect", "test/onnx/expect/TestOperators.test_gather_opset11.expect", "test/onnx/expect/TestOperators.test_ge.expect", "test/onnx/expect/TestOperators.test_gelu.expect", "test/onnx/expect/TestOperators.test_gt.expect", "test/onnx/expect/TestOperators.test_hardtanh.expect", "test/onnx/expect/TestOperators.test_implicit_expand.expect", "test/onnx/expect/TestOperators.test_index.expect", "test/onnx/expect/TestOperators.test_isnan.expect", "test/onnx/expect/TestOperators.test_layer_norm_aten.expect", "test/onnx/expect/TestOperators.test_le.expect", "test/onnx/expect/TestOperators.test_linear.expect", "test/onnx/expect/TestOperators.test_log_sigmoid.expect", "test/onnx/expect/TestOperators.test_logsoftmax.expect", "test/onnx/expect/TestOperators.test_lt.expect", "test/onnx/expect/TestOperators.test_master_opset.expect", "test/onnx/expect/TestOperators.test_max.expect", "test/onnx/expect/TestOperators.test_maxpool.expect", "test/onnx/expect/TestOperators.test_maxpool_dilations.expect", "test/onnx/expect/TestOperators.test_maxpool_indices.expect", "test/onnx/expect/TestOperators.test_mean.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_min.expect", "test/onnx/expect/TestOperators.test_mm.expect", "test/onnx/expect/TestOperators.test_narrow.expect", "test/onnx/expect/TestOperators.test_ne.expect", "test/onnx/expect/TestOperators.test_nonzero.expect", "test/onnx/expect/TestOperators.test_norm_p1.expect", "test/onnx/expect/TestOperators.test_norm_p2.expect", "test/onnx/expect/TestOperators.test_ones_like.expect", "test/onnx/expect/TestOperators.test_pad.expect", "test/onnx/expect/TestOperators.test_params.expect", "test/onnx/expect/TestOperators.test_params_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_permute2.expect", "test/onnx/expect/TestOperators.test_pixel_shuffle.expect", "test/onnx/expect/TestOperators.test_pow.expect", "test/onnx/expect/TestOperators.test_prelu.expect", "test/onnx/expect/TestOperators.test_prod.expect", "test/onnx/expect/TestOperators.test_rand.expect", "test/onnx/expect/TestOperators.test_randn.expect", "test/onnx/expect/TestOperators.test_reduce_sum_negative_indices.expect", "test/onnx/expect/TestOperators.test_reduced_mean.expect", "test/onnx/expect/TestOperators.test_reduced_mean_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_prod.expect", "test/onnx/expect/TestOperators.test_reduced_prod_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_sum.expect", "test/onnx/expect/TestOperators.test_reduced_sum_keepdim.expect", "test/onnx/expect/TestOperators.test_reducemax.expect", "test/onnx/expect/TestOperators.test_reducemin.expect", "test/onnx/expect/TestOperators.test_remainder.expect", "test/onnx/expect/TestOperators.test_repeat.expect", "test/onnx/expect/TestOperators.test_repeat_dim_overflow.expect", "test/onnx/expect/TestOperators.test_retain_param_name_disabled.expect", "test/onnx/expect/TestOperators.test_round.expect", "test/onnx/expect/TestOperators.test_rrelu.expect", "test/onnx/expect/TestOperators.test_rsqrt.expect", "test/onnx/expect/TestOperators.test_rsub.expect", "test/onnx/expect/TestOperators.test_scatter_add.expect", "test/onnx/expect/TestOperators.test_scatter_add_opset11.expect", "test/onnx/expect/TestOperators.test_selu.expect", "test/onnx/expect/TestOperators.test_sign.expect", "test/onnx/expect/TestOperators.test_sin.expect", "test/onnx/expect/TestOperators.test_slice.expect", "test/onnx/expect/TestOperators.test_slice_dynamic.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_3d.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_3d_none.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_4d.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_ignore_index.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_weights.expect", "test/onnx/expect/TestOperators.test_split.expect", "test/onnx/expect/TestOperators.test_split_with_sizes.expect", "test/onnx/expect/TestOperators.test_sqrt.expect", "test/onnx/expect/TestOperators.test_std.expect", "test/onnx/expect/TestOperators.test_sum.expect", "test/onnx/expect/TestOperators.test_tan.expect", "test/onnx/expect/TestOperators.test_topk.expect", "test/onnx/expect/TestOperators.test_topk_smallest_unsorted.expect", "test/onnx/expect/TestOperators.test_transpose.expect", "test/onnx/expect/TestOperators.test_type_as.expect", "test/onnx/expect/TestOperators.test_unfold.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/onnx/expect/TestOperators.test_unsqueeze.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale_default_scale_factor.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_size.expect", "test/onnx/expect/TestOperators.test_view.expect", "test/onnx/expect/TestOperators.test_view_flatten.expect", "test/onnx/expect/TestOperators.test_zeros_like.expect", "torch/testing/_internal/common_utils.py"], "labels": ["merged", "open source"]}, "86492410bc": {"title": "Don't run tests with custom arguments with pytest (#41397)", "body": "Summary:\nThis patch basically removes the `-m pytest` parameters when `extra_unittest_args` is used (e.g. `--subprocess`)\n\nFixes https://github.com/pytorch/pytorch/issues/41393\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41397\n\nReviewed By: pbelevich\n\nDifferential Revision: D22792133\n\nPulled By: ezyang\n\nfbshipit-source-id: 29930d703666f4ecc0d727356bbab4a5f7ed4860", "pr_number": "41397", "files_changed": ["test/run_test.py"], "labels": ["merged", "module: tests", "open source", "triaged"]}, "14e75fbdb9": {"title": "Remove py2 specific code from test_utils.py (#42105)", "body": "Summary:\nAs https://github.com/pytorch/pytorch/issues/23795 mentioned drop Python 2 support. albanD\nFixes https://github.com/pytorch/pytorch/issues/31796\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42105\n\nReviewed By: ngimel\n\nDifferential Revision: D22765768\n\nPulled By: mrshenli\n\nfbshipit-source-id: bae114a21cd5598004c7f92d313938ad826b4a24", "pr_number": "42105", "files_changed": ["test/test_utils.py"], "labels": ["merged", "open source"]}, "3acd6b7359": {"title": "Document formatting (#42065)", "body": "Summary:\nApply syntax highlighting to the command in `README.md`. This makes `README.md` easier to read.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42065\n\nReviewed By: pbelevich\n\nDifferential Revision: D22753418\n\nPulled By: mrshenli\n\nfbshipit-source-id: ebfa90fdf60478c34bc8a7284d163e0254cfbe3b", "pr_number": "42065", "files_changed": ["README.md"], "labels": ["merged", "open source"]}, "f0c46878c6": {"title": "Fix the issue GPU skip message(#41378) (#41973)", "body": "Summary:\nRelated https://github.com/pytorch/pytorch/issues/41378\n\nFix the issue GPU skip message\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41973\n\nReviewed By: pbelevich\n\nDifferential Revision: D22753459\n\nPulled By: mrshenli\n\nfbshipit-source-id: d24b531926e28b860ae90b9ae07e8ca3438d21db", "pr_number": "41973", "files_changed": ["torch/testing/_internal/common_distributed.py"], "labels": ["merged", "open source"]}, "e7ed0b3fae": {"title": "Avoid zero division in _cubic_interpolate (#42093)", "body": "Summary:\nI encountered a zero division problem when using LBFGS:\n\nFile \"/home/yshen/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\", line 118, in _strong_wolfe\n    bracket[1], bracket_f[1], bracket_gtd[1])\nFile \"/home/yshen/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\", line 21, in _cubic_interpolate\n    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)\nZeroDivisionError: float division by zero\n\nMy solution is to determine whether \"line-search bracket is so small\" before calling _cubic_interpolate\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42093\n\nReviewed By: pbelevich\n\nDifferential Revision: D22770667\n\nPulled By: mrshenli\n\nfbshipit-source-id: f8fdfcbd3fd530235901d255208fef8005bf898c", "pr_number": "42093", "files_changed": ["torch/optim/lbfgs.py"], "labels": ["open source"]}, "2de549518e": {"title": "Make fmod work with zero divisors consistently (#41948)", "body": "Summary:\nCurrently `torch.tensor(1, dtype=torch.int).fmod(0)` crashes (floating point exception).\n\nThis PR should fix this issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41948\n\nReviewed By: ngimel\n\nDifferential Revision: D22771081\n\nPulled By: ezyang\n\nfbshipit-source-id: a94dd35d6cd85daa2d51cae8362004e31f97989e", "pr_number": "41948", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "test/test_torch.py"], "labels": ["merge-this-please", "merged", "open source"]}, "73ff252913": {"title": "Back out \"[NCCL] DDP communication hook: getFuture()\" (#42152)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42152\n\nOriginal commit changeset: 8c059745261d\n\nTest Plan: .\n\nReviewed By: ajtulloch, jianyuh\n\nDifferential Revision: D22786183\n\nfbshipit-source-id: 51155389d37dc82ccb4d2fa20d350f9d14abeaca", "pr_number": "42152", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/comm.cpp", "torch/csrc/distributed/c10d/comm.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/nn/parallel/distributed.py"], "labels": ["fb-exported", "merged"]}, "2f61aca17b": {"title": "Skip DataIO tests relying on LevelDB if compiled without it (#42169)", "body": "Summary:\nFound while trying to get RocM Caffe2 job green\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42169\n\nReviewed By: seemethere\n\nDifferential Revision: D22791896\n\nPulled By: malfet\n\nfbshipit-source-id: 9df6233876aec5ead056365499bab970aa7e8bdc", "pr_number": "42169", "files_changed": ["caffe2/python/dataio_test.py", "caffe2/python/operator_test/checkpoint_test.py"], "labels": ["merged"]}, "5aa2b572ff": {"title": "replace black list with block (#42091)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41729\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42091\n\nReviewed By: pbelevich\n\nDifferential Revision: D22792096\n\nPulled By: ezyang\n\nfbshipit-source-id: caafa42d12cbad377b67ddbaba8f84a2b8c98066", "pr_number": "42091", "files_changed": ["torch/utils/hipify/hipify_python.py"], "labels": ["merged", "open source", "triaged"]}, "b0424a895c": {"title": "Raise RuntimeError for zero stride pooling (#41819)", "body": "Summary:\nClose https://github.com/pytorch/pytorch/issues/41767\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41819\n\nReviewed By: mrshenli\n\nDifferential Revision: D22780634\n\nPulled By: ngimel\n\nfbshipit-source-id: 376ce5229ad5bd60804d839340d2c6505cf3288d", "pr_number": "41819", "files_changed": ["aten/src/ATen/native/Pool.h", "test/test_nn.py", "torch/nn/modules/pooling.py"], "labels": ["merged", "module: nn", "open source", "triaged"]}, "b3a9e21a29": {"title": "[vulkan] mm op through addmm (#41221)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41221\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22754938\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: f9a0f48d7943a85b7dbb3fc9edf9e214ba07543b", "pr_number": "41221", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/VulkanOps.h", "aten/src/ATen/native/vulkan/glsl/addmm.glsl", "aten/src/ATen/native/vulkan/glsl/mm.glsl", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "f666be7bc1": {"title": "[vulkan] support add for dim < 4 (#41222)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41222\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22754937\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: f8c5e55c965c0a805e75c63b21f410fb0c323515", "pr_number": "41222", "files_changed": ["aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "48ae5945de": {"title": "Skip TestExtractPredictorNet if compiled without OpenCV (#42168)", "body": "Summary:\nFound while trying to get RocM Caffe2 CI green\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42168\n\nReviewed By: seemethere\n\nDifferential Revision: D22791879\n\nPulled By: malfet\n\nfbshipit-source-id: 8f7ef9711bdc5941b2836e4c8943bb95c72ef8af", "pr_number": "42168", "files_changed": ["caffe2/python/core_test.py"], "labels": ["merged"]}, "5ed7cd0025": {"title": "Allow drop_last option in DistributedSampler (#41171)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41171\n\nDistributedSampler allows data to be split evenly across workers in\nDDP, but it has always added additional samples in order for the data to be\nevenly split in the case that the # of samples is not evenly divisible by the\nnumber of workers. This can cause issues such as when doing distributed\nvalidation accuracy, where multiple samples could be considered twice.\n\nThis PR adds a drop_last option where the tail of the data is dropped such that\nthe effective dataset size is still evenly divisible across the workers. This\nensures that DDP can train fine (there is no uneven inputs) and each replica\ngets an equal number of data indices.\nghstack-source-id: 108617516\n\nTest Plan: Added unittest\n\nReviewed By: mrshenli\n\nDifferential Revision: D22449974\n\nfbshipit-source-id: e3156b751f5262cc66437b9191818b78aee8ddea", "pr_number": "41171", "files_changed": ["test/distributed/test_distributed.py", "torch/utils/data/distributed.py"], "labels": ["merged"]}, "64965c4572": {"title": "Replaced blacklist with blocklist (#42097)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/41726\n\nFixes https://github.com/pytorch/pytorch/issues/41726\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42097\n\nReviewed By: ngimel\n\nDifferential Revision: D22779535\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 1d414af22a1b3e856a11d64cff4b4d33160d957b", "pr_number": "42097", "files_changed": ["torch/csrc/utils/python_arg_parser.cpp"], "labels": ["merged", "open source", "triaged"]}, "030ab2bda5": {"title": "Replaced whitelist reference with allowlist (#42071)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41741\n\nReplaced whitelist reference with allowlist.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42071\n\nReviewed By: pbelevich\n\nDifferential Revision: D22795176\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: bcf1b8afe516b9684ce0298bc257ef81152ba20c", "pr_number": "42071", "files_changed": [".github/workflows/clang_format.yml"], "labels": ["merged", "open source", "triaged"]}, "4d17ecb071": {"title": "Changed Blacklisted to Blocklisted (#42100)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41703\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42100\n\nReviewed By: ngimel\n\nDifferential Revision: D22780380\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: d465c41f1d4951ab6de55cb827c7ef53975209af", "pr_number": "42100", "files_changed": ["caffe2/opt/backend_transformer_base.h"], "labels": ["merged", "open source", "triaged"]}, "fd9205e14b": {"title": "Enable caffe2 tests for RocM jobs (#41604)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41604\n\nReviewed By: ezyang\n\nDifferential Revision: D22603703\n\nPulled By: malfet\n\nfbshipit-source-id: 789ccf2bb79668a5a68006bb877b2d88fb569809", "pr_number": "41604", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".jenkins/caffe2/test.sh", "caffe2/python/ideep/elementwise_sum_op_test.py", "caffe2/python/ideep/expanddims_squeeze_op_test.py", "caffe2/python/ideep/fc_op_test.py", "caffe2/python/ideep/pool_op_test.py", "caffe2/python/ideep/relu_op_test.py"], "labels": ["merged"]}, "8ddd2c4e1b": {"title": "[pytorch] fix code analyzer for LLVM 9 & 10 (#42135)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42135\n\nTested the code analyzer with LLVM 9 & 10 and fixed a couple issues:\n- Rename local demangle() which is available as public API since LLVM 9;\n- Fix falsely associated op registrations due to the `phi` instruction;\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22795508\n\nPulled By: ljk53\n\nfbshipit-source-id: 2d47af088acd3312a7ea5fd9361cdccd48940fe6", "pr_number": "42135", "files_changed": ["test/mobile/op_deps/simple_ops.cpp", "tools/code_analyzer/op_deps_pass.cpp"], "labels": ["merged"]}, "4c7fb8c2b6": {"title": "make FusionCallback refer to specified GraphFuser context (#41560)", "body": "Summary:\nFixes issue where\n - top level fuser's block_ was captured by callback due to [&] capture,\n - recursive/nested fusers would compare erroneously to top-level block_ instead of own block_\n\nCloses (https://github.com/pytorch/pytorch/issues/39810)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41560\n\nReviewed By: Krovatkin\n\nDifferential Revision: D22583196\n\nPulled By: wconstab\n\nfbshipit-source-id: 8f543cd9ea00e116cf3e776ab168cdd9fed69632", "pr_number": "41560", "files_changed": ["torch/csrc/jit/passes/graph_fuser.cpp"], "labels": ["merged", "oncall: jit"]}, "e2344db886": {"title": "Use Python3.7 when running OSX builds/tests (#42191)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42191\n\nReviewed By: seemethere\n\nDifferential Revision: D22801091\n\nPulled By: malfet\n\nfbshipit-source-id: b589343ef1bc6896d3d6d8d863f75aa3a102d985", "pr_number": "42191", "files_changed": [".jenkins/pytorch/macos-common.sh"], "labels": ["merged"]}, "3c084fd358": {"title": "Dequant => Swish => Quant Test case. (#41976)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41976\n\nDequant => Swish => Quant Test case.\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: test_deq_swish_quant_nnpi.py.\n\nReviewed By: hyuen\n\nDifferential Revision: D22718593\n\nfbshipit-source-id: 1cee503a27e339af6d89c819007511b90bb6610c", "pr_number": "41976", "files_changed": ["caffe2/contrib/fakelowp/test/test_deq_swish_quant_nnpi.py"], "labels": ["fb-exported", "merged"]}, "deac621ae2": {"title": "Stop building PyTorch for VS2017 (#42144)", "body": "Summary:\nAnd since CUDA-9.2 is incompatible with VS2019, disable CUDA-9.2 for Windows as well\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42144\n\nReviewed By: pbelevich\n\nDifferential Revision: D22794475\n\nPulled By: malfet\n\nfbshipit-source-id: 24fc980e6fc75240664b9de8a4a63b1153f8d8ee", "pr_number": "42144", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/build-parameters/pytorch-build-params.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "30eacb5fb6": {"title": "[quant][graphmode] Support stack (#42187)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42187\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22801229\n\nfbshipit-source-id: 7d1758c4fb1c8f742a275c3a631605f0f0d08e44", "pr_number": "42187", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/helper.cpp"], "labels": ["merged", "oncall: jit"]}, "8a644f0c13": {"title": "[Shape Inference] Fix InferFC", "body": "Summary: Sometimes first dim of X in FC is BATCH_OF_FEATURE_MAX instead of BATCH. This caused an issue in f207899183 (when first dim of X is 64 but is set to 1 in inferFC). Change the check from `!= BATCH` to `== UNKNOWN`\n\nTest Plan: unit test\n\nReviewed By: yinghai\n\nDifferential Revision: D22784691\n\nfbshipit-source-id: eb66ba361d6fe75672b13edbac2fbd269a7e7a00", "pr_number": null, "files_changed": ["caffe2/opt/bound_shape_inferencer.cc"], "labels": []}, "b2ef7fa359": {"title": "Add a flag to enforce fp32 to fp16 conversion for all inputs of the onnxifi net. (#39931)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39931\n\nATT.\n\nReviewed By: yinghai, ChunliF\n\nDifferential Revision: D21993492\n\nfbshipit-source-id: ff386e6e9b95a783906fc1ae6a62462e6559a20b", "pr_number": "39931", "files_changed": ["caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/onnxifi_transformer.h"], "labels": ["fb-exported", "merged"]}, "e4c3f526c8": {"title": "Fixed Skipping Logic in ProcessGroupNCCLErrors tests (#42192)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42192\n\nThis PR fixes the complicated skipping logic for ProcessGroupNCCLErrors Tests - it correctly logs the reason for skipping tests when GPUs are not available or the NCCL version is too old.\n\nThis is part of a broader effort to improve the testing of the ProcessGroup and Collectives tests.\nghstack-source-id: 108620568\n\nTest Plan: Tested on devGPU and devvm. Tests are run correctly on GPU and skipped on CPU as expected.\n\nReviewed By: mrshenli\n\nDifferential Revision: D22782856\n\nfbshipit-source-id: 6071dfdd9743f45e59295e5cee09e89c8eb299c9", "pr_number": "42192", "files_changed": ["torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp"], "labels": ["merged"]}, "b6a9f42758": {"title": "Add appropriate error messages for ProcessGroupNCCLTest (#42143)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42143\n\nReplaces the original makeshift error messages in ProcessGroupNCCLTest\nwith more appropriate ones.\nghstack-source-id: 108711579\n\nTest Plan: Ran the tests on DevGPU\n\nReviewed By: mrshenli\n\nDifferential Revision: D22778505\n\nfbshipit-source-id: 27109874f0b474a74b09f588cf6e7528d2069702", "pr_number": "42143", "files_changed": ["torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["merged"]}, "8deb4fe809": {"title": "Fix flaky NCCL error handling tests. (#42149)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42149\n\nSome of these tests were flaky since we could kill the process in some\nway without cleaning up the ProcessGroup. This resulted in issues where the\nFileStore didn't clean up appropriately resulting in other processes in the\ngroup to crash.\n\nFixed this by explicitly deleting the process_group before we bring a process\ndown forcibly.\nghstack-source-id: 108629057\n\nTest Plan: waitforbuildbot\n\nReviewed By: mrshenli\n\nDifferential Revision: D22785042\n\nfbshipit-source-id: c31d0f723badbc23b7258e322f75b57e0a1a42cf", "pr_number": "42149", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["merged"]}, "8fc5adc88e": {"title": "Remove dead named_tensors_unsupported_error definitions. (#42171)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42171\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D22794980\n\nPulled By: ezyang\n\nfbshipit-source-id: 250b6566270e19240361d758db55101d6fcb33e9", "pr_number": "42171", "files_changed": ["aten/src/ATen/templates/LegacyTHFunctions.cpp", "aten/src/ATen/templates/SparseTypeDerived.cpp", "aten/src/ATen/templates/TypeDefault.cpp", "aten/src/ATen/templates/TypeDerived.cpp"], "labels": ["merged"]}, "4b108ca763": {"title": "refactor save_data as non member function (#42045)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42045\n\nThis PR changes the save_data() member functions of torch::jit::mobile::Module which was introduced in #41403 to be the non member function torch::jit::mobile::_save_parameters() (taking a mobile Module as its first argument).\n\nIn addition, this PR:\n* adds a getter function _ivalue() for the mobile::Module object\n* renames torch::jit::mobile::_load_mobile_data() to torch::jit::mobile_load_parameters()\n* refactors the import.h header file into import.h and import_data.h\n\nTest Plan: Imported from OSS\n\nReviewed By: kwanmacher, iseeyuan\n\nDifferential Revision: D22766781\n\nPulled By: ann-ss\n\nfbshipit-source-id: 5cabae31927187753a958feede5e9a28d71d9e92", "pr_number": "42045", "files_changed": ["test/cpp/jit/test_lite_trainer.cpp", "torch/csrc/jit/mobile/export.cpp", "torch/csrc/jit/mobile/export.h", "torch/csrc/jit/mobile/import.h", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/import_data.h", "torch/csrc/jit/mobile/module.h"], "labels": ["merged", "oncall: jit"]}, "8c653e05ff": {"title": "DOC: fail to build if there are warnings (#41335)", "body": "Summary:\nMerge after gh-41334 and gh-41321 (EDIT: both are merged).\nCloses gh-38011\n\nThis is the last in a series of PRs to build documentation without warnings. It adds `-WT --keepgoing` to the shpinx build which will [fail the build if there are warnings](https://www.sphinx-doc.org/en/master/man/sphinx-build.html#cmdoption-sphinx-build-W), print a [trackeback on error](https://www.sphinx-doc.org/en/master/man/sphinx-build.html#cmdoption-sphinx-build-T) and [finish the build](https://www.sphinx-doc.org/en/master/man/sphinx-build.html#cmdoption-sphinx-build-keep-going) even when there are warnings.\n\nIt should fail now, but pass once the PRs mentioned at the top are merged.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41335\n\nReviewed By: pbelevich\n\nDifferential Revision: D22794425\n\nPulled By: mruberry\n\nfbshipit-source-id: eb2903e50759d1d4f66346ee2ceebeecfac7b094", "pr_number": "41335", "files_changed": [".circleci/scripts/python_doc_push_script.sh", "docs/Makefile", "docs/source/jit.rst", "docs/source/optim.rst", "torch/__init__.py", "torch/functional.py", "torch/serialization.py"], "labels": ["merged", "open source", "triaged"]}, "6b3f335641": {"title": "Enables torch.full bool and integer type inference (#41912)", "body": "Summary:\nAfter being deprecated in 1.5 and throwing a runtime error in 1.6, we can now enable torch.full inferring its dtype when given bool and integer fill values. This PR enables that inference and updates the tests and docs to reflect this.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41912\n\nReviewed By: pbelevich\n\nDifferential Revision: D22790718\n\nPulled By: mruberry\n\nfbshipit-source-id: 8d1eb01574b1977f00bc0696974ac38ffdd40d9e", "pr_number": "41912", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "test/jit/test_save_load.py", "test/test_torch.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "1c5c289b62": {"title": "[pt] Add incude_last_offset option to EmbeddingBag mean and max (#42215)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42215\n\nSpecifically on https://github.com/pytorch/pytorch/pull/27477#discussion_r371402079\n\nWe would like to supported with include_last=True overall for other reduction types like mean and max. It now causes further code fragmentation in DPER (https://www.internalfb.com/intern/diff/D22794469/).\n\nMore details: https://www.internalfb.com/intern/diff/D22794469/?dest_fbid=309597093427021&transaction_id=631457624153457\n\nghstack-source-id: 108733009\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test:nn -- \"test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu\"\n```\n\n```\n(base) [jianyuhuang@devbig281.ftw3.facebook.com: ~/fbsource/fbcode/caffe2/test] $ TORCH_SHOW_CPP_STACKTRACES=1 buck test mode/dev-nosan //caffe2/test:\nnn -- \"test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu\" --print-passing-details\nParsing buck files: finished in 1.2 sec\nBuilding: finished in 5.5 sec (100%) 10130/10130 jobs, 2 updated\n  Total time: 6.7 sec\nMore details at https://www.internalfb.com/intern/buck/build/dbdc2063-69d8-45cb-9146-308a9e8505ef\nFirst unknown argument: --print-passing-details.\nFalling back to TestPilot classic.\nTrace available for this run at /tmp/testpilot.20200728-195414.1422748.log\nTestPilot test runner for Facebook. See https://fburl.com/testpilot for details.\nTestpilot build revision cd2638f1f47250eac058b8c36561760027d16add fbpkg f88726c8ebde4ba288e1172a348c7f46 at Mon Jul 27 18:11:43 2020 by twsvcscm from /usr/local/fbprojects/packages/testinfra.testpilot/887/t.par\nDiscovering tests\nRunning 1 test\nStarted new test run: https://our.intern.facebook.com/intern/testinfra/testrun/844425097242375\n      \u2713 caffe2/test:nn - test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu (test_nn.TestNNDeviceTypeCPU) 0.162 1/1 (passed)\nTest output:\n> /data/users/jianyuhuang/fbsource/fbcode/buck-out/dev/gen/caffe2/test/nn#binary,link-tree/torch/_utils_internal.py:103: DeprecationWarning: This is a NOOP in python >= 3.7, its just too dangerous with how we write code at facebook. Instead we patch os.fork and multiprocessing which can raise exceptions if a deadlock would happen.\n>   threadSafeForkRegisterAtFork()\n> /usr/local/fbcode/platform007/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__\nand __path__\n>   return f(*args, **kwds)\n> test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu (test_nn.TestNNDeviceTypeCPU) ... Couldn't download test skip set, leaving all tests enabled...\n> ok\n>\n> ----------------------------------------------------------------------\n> Ran 1 test in 0.162s\n>\n> OK\nFinished test run: https://our.intern.facebook.com/intern/testinfra/testrun/844425097242375\nSummary (total time 5.54s):\n  PASS: 1\n  FAIL: 0\n  SKIP: 0\n  FATAL: 0\n  TIMEOUT: 0\n  OMIT: 0\nDid _not_ run with tpx. See https://fburl.com/tpx for details.\n```\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D22801881\n\nfbshipit-source-id: 80a624465727081bb9bf55c28419695a3d79c6e5", "pr_number": "42215", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "test/test_nn.py", "torch/csrc/api/include/torch/nn/options/embedding.h", "torch/nn/modules/sparse.py"], "labels": ["merged"]}, "90074bbfa6": {"title": "implement numpy-like functionality isposinf, isneginf (#41588)", "body": "Summary:\nRelated https://github.com/pytorch/pytorch/issues/38349\n\nNumpy-like functionalities `isposinf` and `isneginf` are implemented.\n\nTest-Plan:\n- pytest test/test_torch.py -k \"test_isposinf_isneginf\"\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41588\n\nReviewed By: ngimel\n\nDifferential Revision: D22770732\n\nPulled By: mruberry\n\nfbshipit-source-id: 7448653e8fb8df6b9cd4604a4739fe18a1135578", "pr_number": "41588", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged", "open source", "triaged"]}, "460970483d": {"title": "Revert D22790718: [pytorch][PR] Enables torch.full bool and integer type inference", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22790718 (https://github.com/pytorch/pytorch/commit/6b3f335641e1874c5b0ae0aa719687ed5f42ab45)\n\nOriginal commit changeset: 8d1eb01574b1\n\nfbshipit-source-id: c321177cce129a6c83f1a7b26bd5ed94a343ac0f", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "test/jit/test_save_load.py", "test/test_torch.py", "torch/_torch_docs.py"], "labels": []}, "b45b82b006": {"title": "Fix type annotation for DistributedDataParallel (#42231)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42231\n\nReviewed By: albanD\n\nDifferential Revision: D22816589\n\nPulled By: mrshenli\n\nfbshipit-source-id: a355f7e2fa895617bf81ef681b051f074d39ab8c", "pr_number": "42231", "files_changed": ["torch/nn/parallel/distributed.pyi"], "labels": ["merged", "open source"]}, "382781221d": {"title": "Extending Learnable Fake Quantize module to support gradient scaling and factory (partial) construction (#41969)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41969\n\nIn this diff, the `_LearnableFakeQuantize` module is extended to provide support for gradient scaling where the gradients for both scale and zero point are multiplied by a constant `g` (in some cases, can help with quicker convergence). In addition, it is also augmented to provide a factory method via `_with_args` such that a partial constructor of the module can be built.\n\nTest Plan:\nFor correctness of the fake quantizer operators, on a devvm, enter the following command:\n```\nbuck test //caffe2/torch:quantization -- learnable_py_module\n```\n\nReviewed By: z-a-f\n\nDifferential Revision: D22715629\n\nfbshipit-source-id: ff8e5764f81ca7264bf9333789f57e0b0cec7a72", "pr_number": "41969", "files_changed": ["torch/quantization/_learnable_fake_quantize.py"], "labels": ["fb-exported", "merged"]}, "48acdfd505": {"title": "add tests to BinaryOpsKernel -- max/min kernel (#42198)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42198\n\n1. add tests to max/min kernel\n\nTest Plan:\n1. Run locally to check cover the corresponding code part in BinaryOpsKernel.cpp.\n2. CI\n\nReviewed By: malfet\n\nDifferential Revision: D22796019\n\nfbshipit-source-id: 84c8d7df509de453c4ec3c5e38977733b0ef3457", "pr_number": "42198", "files_changed": ["aten/src/ATen/test/atest.cpp"], "labels": ["fb-exported", "merged"]}, "01b794f169": {"title": "Operator-level Benchmark Test for Per Tensor and Per Channel Fake Quantization (#41974)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41974\n\nIn this diff, 2 new sets of benchmark tests are added to the `quantization` benchmark suite where operator-level benchmarking is conducted for the learnable Python operators, the learnable c++ kernels, and the original non-backprop c++ kernels.\n\nTest Plan:\nInside the path `torch/benchmarks/operator_benchmark` (The root directory will be `caffe2` inside `fbcode` if working on a devvm):\n- On a devvm, run the command `buck run pt:fake_quantize_learnable_test`\n- On a personal laptop, run the command `python3 -m pt.fake_quantize_learnable_test`\n\nBenchmark Results (On devGPU with 0% volatile utilization -- all GPUs are free):\nEach sample has dimensions **3x256x256**;\n\n### In **microseconds** (`1e-6` second),\n\n|                           | Python Module | C++ Kernel | Non-backprop C++ Kernel |\n|---------------------------|---------------|------------|-------------------------|\n| Per Tensor CPU Forward    | 3112.666      | 3270.740   | 3596.864                |\n| Per Tensor Cuda Forward   | 797.258       | 258.961    | 133.953                 |\n| Per Channel CPU Forward   | 6587.693      | 6931.461   | 6352.417                |\n| Per Channel Cuda Forward  | 1579.576      | 555.723    | 479.016                 |\n| Per Tensor CPU Backward   | 72278.390     | 22466.648  | 12922.195               |\n| Per Tensor Cuda Backward  | 6512.280      | 1546.218   | 652.942                 |\n| Per Channel CPU Backward  | 74138.545     | 41212.777  | 14131.576               |\n| Per Channel Cuda Backward | 6795.173      | 4321.351   | 1052.066                |\n\nReviewed By: z-a-f\n\nDifferential Revision: D22715683\n\nfbshipit-source-id: 8be528b790663413cbeeabd4f68bbca00be052dd", "pr_number": "41974", "files_changed": ["benchmarks/operator_benchmark/benchmark_all_quantized_test.py", "benchmarks/operator_benchmark/pt/fake_quantize_learnable_test.py", "benchmarks/operator_benchmark/pt/quantization_test.py"], "labels": ["fb-exported", "merged"]}, "91546a4b0f": {"title": "Environment variable for controlling type verbosity in debug output (#41906)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41906\n\nFixes #41770\n\nTest Plan:\nExample:\n```\nimport torch\ndef bar():\n    def test(a):\n        return a\n    x = torch.ones(10,10, device='cpu')\n    print(torch.jit.trace(test, (x)).graph)\nbar()\n```\n\nBash:\n```\nfor i in 0 1 2 3; do\n  PYTORCH_JIT_TYPE_VERBOSITY=$i python test.py\ndone\n```\n\nOutput:\n```\ngraph(%0):\n  return (%0)\n\ngraph(%0 : Float(10, 10)):\n  return (%0)\n\ngraph(%0 : Float(10:10, 10:1)):\n  return (%0)\n\ngraph(%0 : Float(10:10, 10:1, requires_grad=0, device=cpu)):\n  return (%0)\n```\n\nImported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22687966\n\nfbshipit-source-id: cd395257d79a4baa35245c778a74a55d1ea2a842", "pr_number": "41906", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "torch/csrc/jit/ir/ir.cpp"], "labels": ["merged", "oncall: jit"]}, "60f51542dc": {"title": "[Caffe2] Fix spatial_bn bug for computing running_var on CPU or on CUDA without CuDNN (#42151)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42151\n\nPreviously our Caffe2 SpatialBN op impl was incorrect for computing running_var without unbias coefficent. Actually it should fail the test because the output will be different with CuDNN's output. However, our tests are too weak to find this bug. This diff fix all of them.\n\nTest Plan: buck test mode/dev-nosan //caffe2/caffe2/python/operator_test:spatial_bn_op_test\n\nReviewed By: houseroad\n\nDifferential Revision: D22786127\n\nfbshipit-source-id: db80becb67d60c44faae180c7e4257cb136a266d", "pr_number": "42151", "files_changed": ["caffe2/operators/spatial_batch_norm_op.h", "caffe2/operators/spatial_batch_norm_op_impl.cuh", "caffe2/python/operator_test/spatial_bn_op_test.py"], "labels": ["fb-exported", "merged"]}, "029007c8b6": {"title": "Improved coverage for unboxed->boxed kernel wrappers (#38999)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38999\n\nAdds boxing for inplace and outplace kernels, itemizes\nremaining unsupported cases, and fails compilation when\nnew unsupported types are introduced in op signatures.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D21718547\n\nPulled By: bhosmer\n\nfbshipit-source-id: 03295128b21d1843e86789fb474f38411b26a8b6", "pr_number": "38999", "files_changed": ["aten/src/ATen/core/QuantizerBase.h", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/impl/boxing.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/quantized/Quantizer.h", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.cpp", "c10/test/util/Metaprogramming_test.cpp", "c10/test/util/TypeList_test.cpp", "c10/util/Metaprogramming.h", "c10/util/TypeList.h"], "labels": ["merged"]}, "4b6e5f42a4": {"title": "Creates spectral ops test suite (#42157)", "body": "Summary:\nIn preparation for creating the new torch.fft namespace and NumPy-like fft functions, as well as supporting our goal of refactoring and reducing the size of test_torch.py, this PR creates a test suite for our spectral ops.\n\nThe existing spectral op tests from test_torch.py and test_cuda.py are moved to test_spectral_ops.py and updated to run under the device generic test framework.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42157\n\nReviewed By: albanD\n\nDifferential Revision: D22811096\n\nPulled By: mruberry\n\nfbshipit-source-id: e5c50f0016ea6bb8b093cd6df2dbcef6db9bb6b6", "pr_number": "42157", "files_changed": ["test/run_test.py", "test/test_cuda.py", "test/test_spectral_ops.py", "test/test_torch.py"], "labels": ["merged"]}, "79cfd85987": {"title": "grad detach_ only when it has grad_fn in zero_grad call (#41283)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41283\n\nin optimizer.zero_grad(), detach_ is useful to avoid memory leak only when grad has grad_fn, so add check to call grad.detach_ only when the grad has grad_fn in zero_grad() function\nghstack-source-id: 108702289\n\nTest Plan: unit test\n\nReviewed By: mrshenli\n\nDifferential Revision: D22487315\n\nfbshipit-source-id: 861909b15c8497f1da57f092d8963d4920c85e38", "pr_number": "41283", "files_changed": ["torch/nn/modules/module.py", "torch/nn/parallel/distributed.py", "torch/optim/optimizer.py"], "labels": ["merged"]}, "7cdf786a07": {"title": "fix typo in GradScaler docstring (#42236)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/42226.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42236\n\nReviewed By: albanD\n\nDifferential Revision: D22817980\n\nPulled By: ngimel\n\nfbshipit-source-id: 4326fe028dba1dbeed454edc4e4d4fffa56f51d6", "pr_number": "42236", "files_changed": ["torch/cuda/amp/grad_scaler.py"], "labels": ["merged", "open source"]}, "27b03d62de": {"title": "[HT] Clear the device placement tag for the auto gen sum so that we could break the component for FC sharing the same input (#42219)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42219\n\nIntroduce a new extra info that is tagged on the forward net for the operators sharing the same input. The effect is that the auto gen sum of gradient for the input will not follow the tag of the operator tags in the forward net. This allow more flexible device allocation.\n\nTest Plan:\n# unit test\n`./buck-out/gen/caffe2/caffe2/python/core_gradients_test#binary.par -r  testMultiUseInputAutoGenSumDevice`\n\nReviewed By: xianjiec, boryiingsu\n\nDifferential Revision: D22609080\n\nfbshipit-source-id: d558145e5eb36295580a70e1ee3a822504dd439a", "pr_number": "42219", "files_changed": ["caffe2/python/core.py", "caffe2/python/core_gradients_test.py"], "labels": ["fb-exported", "merged"]}, "6c251f74b2": {"title": "replace black_list/blacklist with blocklist/block_list (#42089)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41734\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42089\n\nReviewed By: pbelevich\n\nDifferential Revision: D22794556\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 4404845b6293b076b3c8cc02b135b20c91397a79", "pr_number": "42089", "files_changed": ["caffe2/python/pybind_state.cc"], "labels": ["merged", "open source", "triaged"]}, "c18223f9ef": {"title": "add Dimname support to IValue (#42054)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42054\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D22750398\n\nPulled By: bhosmer\n\nfbshipit-source-id: 7028268093f86b33c4117868b0edcb9e1ca6f7ee", "pr_number": "42054", "files_changed": ["aten/src/ATen/core/boxing/impl/boxing.h", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h"], "labels": ["merged"]}, "fe4f19e164": {"title": "[CUDA] max_pool2d NCHW performance improvement (#42182)", "body": "Summary:\nFix the regression introduced in https://github.com/pytorch/pytorch/issues/38953.\n\nPlease see https://github.com/xwang233/code-snippet/blob/master/max-pool2d-nchw-perf/max-pool2d.ipynb for detailed before & after performance comparisons.\n\nPerformance improvement for backward max_pool2d before and after this PR (negative value means speed up)\n\n![image](https://user-images.githubusercontent.com/24860335/88712204-363c8e00-d0ce-11ea-8586-057e09b16103.png)\n\nSeems like the forward modulo doesn't benefit much from a similar change, so I did not change forward. https://github.com/pytorch/pytorch/pull/42182/commits/1718f0ccfd84ed33229b69826e8e1c53dc5725f7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42182\n\nReviewed By: albanD\n\nDifferential Revision: D22829498\n\nPulled By: ngimel\n\nfbshipit-source-id: 4c81968fe072f4e264e70c70ade4c32d760a3af4", "pr_number": "42182", "files_changed": ["aten/src/ATen/native/cuda/DilatedMaxPool2d.cu"], "labels": ["merged", "open source"]}, "872237c1f2": {"title": "Output to stderr in distributed tests. (#42139)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42139\n\nA bunch of tests were failing with buck since we would output to\nstdout and buck would fail parsing stdout in some cases.\n\nMoving these print statements to stderr fixes this issue.\nghstack-source-id: 108606579\n\nTest Plan: Run the offending unit tests.\n\nReviewed By: mrshenli\n\nDifferential Revision: D22779135\n\nfbshipit-source-id: 789af3b16a03b68a6cb12377ed852e5b5091bbad", "pr_number": "42139", "files_changed": ["test/distributed/test_c10d.py", "test/distributed/test_c10d_spawn.py", "test/distributed/test_distributed.py", "test/distributed/test_nccl.py", "test/test_cuda.py", "test/test_cuda_primary_ctx.py", "torch/testing/_internal/dist_utils.py"], "labels": ["merged"]}, "7459da268e": {"title": "Add typing annotations to torch.random (#42234)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42234\n\nReviewed By: ezyang\n\nDifferential Revision: D22816933\n\nPulled By: malfet\n\nfbshipit-source-id: 9e2124ad16fed339abd507f6e474cb63feb7eada", "pr_number": "42234", "files_changed": ["mypy.ini", "torch/random.py"], "labels": ["merged", "module: typing"]}, "9ea7476d9c": {"title": "Add test to lerp function (#42266)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42266\n\nfunction `lerp_kernel_scalar` and `lerp_kernel_tensor` are not covered in `Aten/native/cpu/LerpKernel.cpp`, add tests to cover them\n\nTest Plan:\n1. Test locally to check new lines are covered\n2. CI\n\nhttps://pxl.cl/1fXPd\n\nReviewed By: malfet\n\nDifferential Revision: D22832164\n\nfbshipit-source-id: b1eaabbf8bfa08b4dedc1a468abfdfb619a50e3c", "pr_number": "42266", "files_changed": ["test/cpp/api/operations.cpp"], "labels": ["fb-exported", "merged"]}, "0444bac940": {"title": "Add test to cross function", "body": "Summary: function `cross_kernel_scalar` is not covered in `Aten/native/cpu/CrossKernel.cpp`, add tests to cover it\n\nTest Plan:\n1. Test locally to check new lines are covered\n2. CI\n\nhttps://pxl.cl/1fZjG\n\nReviewed By: malfet\n\nDifferential Revision: D22834122\n\nfbshipit-source-id: 0d50f3a3e6aee52cb6fdee2b9f5883f542c7b6e2", "pr_number": null, "files_changed": ["test/cpp/api/operations.cpp"], "labels": []}, "f30ac66e79": {"title": "[caffe2] Fix a performance bug in Dedup SparseAdagrad op (#42287)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42287\n\nWe shouldn't use block_size for thread dimensions in linear_index_weight_offsets_dedup_kernel, since the kernel doesn't iterate the embedding dimensions.\nghstack-source-id: 108834058\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient \\(caffe2\\.caffe2\\.fb\\.net_transforms\\.tests\\.fuse_sparse_ops_test\\.TestFuseSparseOps\\)' --print-passing-details\n```\n\nReviewed By: jspark1105\n\nDifferential Revision: D22800959\n\nfbshipit-source-id: 641d52a51070715c04f9fd286e7e22ac62001f61", "pr_number": "42287", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cu"], "labels": ["merged"]}, "4f163df41a": {"title": "[caffe2] Special handling of If/AsyncIf op in RemoveOpsByType (#42286)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42286\n\nOne more bug to fix. Operators such as If and AsyncIf need special treatment not just in `onnx::SsaRewrite`, but also in `RemoveOpsByType`. The solution needs two steps:\n1) add external inputs/outputs of the subnets of If/AsyncIf op to the inputs/outputs of the op\n2) if the inputs/outputs of the If/AsyncIf op need to be renamed as a result, the same inputs/outputs of the subnets need to be renamed as well.\n\nI also added unit tests to cover this corner case.\n\nTest Plan:\n```\nbuck test //caffe2/caffe2/fb/predictor:black_box_predictor_test\n\nmkdir /tmp/models\nrm -rf /tmp/$USER/snntest\nrm -rf /tmp/snntest\nbuck run mode/opt admarket/lib/ranking/prediction_replayer/snntest_replayer_test/tools:snntest_replay_test -- --serving_paradigm=USER_AD_PRECOMPUTATION_DSNN\n```\n\nDifferential Revision: D22834028\n\nfbshipit-source-id: c070707316cac694f452a96e5c80255abf4014bc", "pr_number": "42286", "files_changed": ["caffe2/onnx/onnx_exporter.cc", "caffe2/onnx/onnx_exporter.h", "caffe2/predictor/transforms.cc"], "labels": ["fb-exported", "merged"]}, "2335430086": {"title": "Update TensorPipe submodule (#42225)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42225\n\nMain changes:\n- Consolidated CMake files to have a single entry point, rather than having a specialized one for PyTorch.\n- Changed the way the preprocessor flags are provided, and changed their name.\n\nThere were a few instances in PyTorch's CMake files where we were directly adding TensorPipe's source directory as an include path, which however doesn't contain the auto-generated header we now added. We fix that by adding the `tensorpipe` CMake target as a dependency, so that the include paths defined by TensorPipe are used, which contain that auto-generated header.\n\nI'm turning off SHM and CMA for now because they have never been covered by the CI. I'll enable them in a separate PR so that if they turn out to be flaky we can revert that change without reverting this one.\n\nTest Plan: CircleCI is all green.\n\nReviewed By: beauby\n\nDifferential Revision: D22812445\n\nfbshipit-source-id: e6d824bb28f5afe75fd765de0430968174f3531f", "pr_number": "42225", "files_changed": ["caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "test/cpp/rpc/CMakeLists.txt", "third_party/tensorpipe", "third_party/tensorpipe.BUILD", "torch/CMakeLists.txt", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["fb-exported", "merged"]}, "269ec767ca": {"title": "[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D22838806\n\nfbshipit-source-id: 29039585c82bb214db860d582cc4e269ab990c85", "pr_number": null, "files_changed": ["torch/csrc/jit/passes/create_autodiff_subgraphs.cpp"], "labels": []}, "547bbdac86": {"title": "Add MSFT Owners to the Windows Maintainership (#42280)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42280\n\nReviewed By: albanD\n\nDifferential Revision: D22836782\n\nPulled By: soumith\n\nfbshipit-source-id: a38f91e381abc0acf3ab41e05ff70611926091ac", "pr_number": "42280", "files_changed": ["docs/source/community/persons_of_interest.rst"], "labels": ["merged"]}, "f15af2fe4f": {"title": "Remove unused variable \"schema\" (#42245)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42245\n\nReviewed By: albanD\n\nDifferential Revision: D22835223\n\nPulled By: mrshenli\n\nfbshipit-source-id: 94f0cbddb36feefc8a136ef38b0a74d22b305680", "pr_number": "42245", "files_changed": ["aten/src/ATen/core/op_registration/op_registration.cpp"], "labels": ["merged", "open source"]}, "8e3d1908b6": {"title": "Fix minor typo in comment (#42184)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42184\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D22809375\n\nPulled By: ezyang\n\nfbshipit-source-id: 322a4c2059b612a10c6257013bbf2fd207e75df7", "pr_number": "42184", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["merged"]}, "7cd92aaa6b": {"title": "Disable validation layers in non-debug builds. (#42122)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42122\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D22835545\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: b0eee550c8d727c79b5d45a7e1d603379ae3af5c", "pr_number": "42122", "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.h"], "labels": ["merged"]}, "ee2150370e": {"title": "Add Vulkan Test to ATen Mobile Tests. (#42123)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42123\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D22835544\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 08bce5d94ed8c966d25707f69e51b16d5b45febd", "pr_number": "42123", "files_changed": ["aten/src/ATen/CMakeLists.txt"], "labels": ["merged"]}, "d0ed1e303f": {"title": "Add missing header guards. (#42272)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42272\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D22835546\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: c880199acaf0ad11c3db4ac9f9f2d000038f98f1", "pr_number": "42272", "files_changed": ["aten/src/ATen/native/vulkan/VulkanAten.h", "aten/src/ATen/native/vulkan/VulkanOps.h"], "labels": ["merged"]}, "ce546328a3": {"title": "Const-correctness, variable initialization, and error checking. (#42124)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42124\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D22835543\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 29b7619b7bc6dd346eec91b8a2b6cc6a76769bcf", "pr_number": "42124", "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.cpp", "aten/src/ATen/native/vulkan/Vulkan.h", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanConvolution.cpp", "aten/src/ATen/native/vulkan/VulkanConvolution.h", "aten/src/ATen/native/vulkan/VulkanOpContext.cpp", "aten/src/ATen/native/vulkan/VulkanOpContext.h"], "labels": ["merged"]}, "c35faae10d": {"title": "[pytorch][ci] install nightly instead of stable libtorch for mobile CIs (#42220)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42220\n\nMobile custom build CI jobs need desktop version libtorch to prepare\nmodels and dump root ops.\n\nIdeally we should use the libtorch built on the PR so that backward\nincompatible changes won't break this script - but it will significantly\nslow down mobile CI jobs.\n\nThis PR changed it to install nightly instead of stable so that we have\nan option to temporarily skip mobile CI jobs on BC-breaking PRs until\nthey are in nightly.\n\nTest Plan: Imported from OSS\n\nReviewed By: seemethere\n\nDifferential Revision: D22810484\n\nPulled By: ljk53\n\nfbshipit-source-id: eb5f7b762a969d1cfeeac2648816be546bd291b6", "pr_number": "42220", "files_changed": [".jenkins/pytorch/build-mobile.sh"], "labels": ["merged"]}, "26d58503c2": {"title": "Implementing NumPy-like function torch.signbit() (#41589)", "body": "Summary:\n- Related with https://github.com/pytorch/pytorch/issues/38349\n- Implementing the NumPy-like function `torch.signbit()` .\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41589\n\nReviewed By: albanD\n\nDifferential Revision: D22835249\n\nPulled By: mruberry\n\nfbshipit-source-id: 7988f7fa8f591ce4b6a23ac884ee7b3aa718bcfd", "pr_number": "41589", "files_changed": ["aten/src/ATen/core/NamedRegistrations.cpp", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnarySignKernels.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_overrides.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged", "module: numpy", "open source", "triaged"]}, "c489bbe122": {"title": "Add typing support to torch._six (#42232)", "body": "Summary:\nAlso add __prepare__ method metaclass created by `with_metaclass` to conform with PEP 3115\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42232\n\nReviewed By: ezyang\n\nDifferential Revision: D22816936\n\nPulled By: malfet\n\nfbshipit-source-id: a47d054b2f061985846d0db6b407f4e5df97b0d4", "pr_number": "42232", "files_changed": ["mypy.ini", "torch/_six.py"], "labels": ["merged"]}, "344defc973": {"title": "Let bfloat16 support promotion with other types (#41698)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/40580\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41698\n\nReviewed By: albanD\n\nDifferential Revision: D22824042\n\nPulled By: mruberry\n\nfbshipit-source-id: 7dad9c12dc51d8f88c3ca963ae9c5f8aa2f72277", "pr_number": "41698", "files_changed": ["c10/core/ScalarType.h", "test/test_torch.py", "test/test_type_promotion.py"], "labels": ["merged", "open source", "triaged"]}, "5ff54ff4ff": {"title": "import freeze (#42319)", "body": "Summary:\ntorch.jit.freeze was broken with https://github.com/pytorch/pytorch/pull/41154/files#diff-9084cd464651f7fa1ff030d2edd9eb55R1\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42319\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22845476\n\nPulled By: eellison\n\nfbshipit-source-id: bc9e50678d0e0ffca4062854ccc71bbef2e1a97b", "pr_number": "42319", "files_changed": ["test/jit/test_freezing.py", "torch/jit/__init__.py"], "labels": ["merged", "oncall: jit"]}, "153673c33b": {"title": "fix quantized elu benchmark (#42318)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42318\n\nWe forgot to update this benchmark when quantized elu's signature\nchanged to require observation, fixing.\n\nTest Plan:\n```\ncd benchmarks/operator_benchmark\npython -m pt.qactivation_test\n```\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D22845251\n\nfbshipit-source-id: 1443f6f0deac695715b1f2bd47f0f22b96dc72ca", "pr_number": "42318", "files_changed": ["benchmarks/operator_benchmark/pt/qactivation_test.py"], "labels": ["merged"]}, "7d6c4f62ef": {"title": "Remove 4 unused variables in lp_pool_op.cc (#42329)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42329\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22850894\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1e91380a432525b83c0bb0bfef0d5067c767cb67", "pr_number": "42329", "files_changed": ["caffe2/operators/lp_pool_op.cc"], "labels": ["merged", "open source"]}, "b5fcd89479": {"title": "Add tests to `sigmoid_backward` and `fmod` (#42289)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42289\n\n`sigmoid_backward` and `fmod` are not covered by neither in `test/cpp/api` nor in `Aten/test`. Add test functions to cover them\n\nTest Plan:\n1. Test locally and check new lines are covred\n2. CI\n\nReviewed By: malfet\n\nDifferential Revision: D22804912\n\nfbshipit-source-id: ea50ef0ef3dcf3940ac950d74f6f1cb38d8547a7", "pr_number": "42289", "files_changed": ["aten/src/ATen/test/atest.cpp"], "labels": ["fb-exported", "merged"]}, "27c22b9b3c": {"title": "Modify function to takes dtype as argument", "body": "Summary: To avoid repeating to() casts for every argument of the function\n\nTest Plan: CI\n\nReviewed By: malfet\n\nDifferential Revision: D22833521\n\nfbshipit-source-id: ae0a8f70339cd6adfeea2f552d35bbcd48b11cf7", "pr_number": null, "files_changed": ["aten/src/ATen/test/atest.cpp"], "labels": []}, "fbb052c2cc": {"title": "BlackList to BlockList (#42279)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41701 blackList convention to blockList convention\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42279\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22843178\n\nPulled By: malfet\n\nfbshipit-source-id: c9be5a5f084dfd0e46545d4a3d1124ef59277604", "pr_number": "42279", "files_changed": ["caffe2/onnx/onnx_exporter.cc", "caffe2/onnx/onnx_exporter.h"], "labels": ["merged", "open source"]}, "1c8217a7a6": {"title": "Abstract cuda calls made from `torch_python` (#42251)", "body": "Summary:\n* Make c10::cuda functions regular non-inlined functions\n* Add driver_version() and device_synchronize() functions\n\nWith this change I don't see anymore direct calls to CUDA API when look at Modules.cpp.obj\n\nFYI malfet\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42251\n\nReviewed By: malfet\n\nDifferential Revision: D22826505\n\nPulled By: ziab\n\nfbshipit-source-id: 8dc2f3e209d3710e2ce78411982a10e8c727573c", "pr_number": "42251", "files_changed": ["c10/cuda/CMakeLists.txt", "c10/cuda/CUDAFunctions.cpp", "c10/cuda/CUDAFunctions.h", "torch/csrc/cuda/Module.cpp", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["merged"]}, "31d41f987a": {"title": "torch.where : Scalar Support (#40336)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/38349 #9190\n\nTODO\n* [x] Add Tests\n* [x] Update Docs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40336\n\nReviewed By: albanD\n\nDifferential Revision: D22813834\n\nPulled By: mruberry\n\nfbshipit-source-id: 67c1693c059a301b249213afee3c25cea9f64fec", "pr_number": "40336", "files_changed": ["aten/src/ATen/ScalarOps.h", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_autograd.py", "test/test_torch.py", "torch/_torch_docs.py"], "labels": ["merged", "module: numpy", "open source", "triaged"]}, "e54f268a7a": {"title": "Enables torch.full bool and integer type inference (#41912)", "body": "Summary:\nAfter being deprecated in 1.5 and throwing a runtime error in 1.6, we can now enable torch.full inferring its dtype when given bool and integer fill values. This PR enables that inference and updates the tests and docs to reflect this.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41912\n\nReviewed By: albanD\n\nDifferential Revision: D22836802\n\nPulled By: mruberry\n\nfbshipit-source-id: 33dfbe4d4067800c418b314b1f60fab8adcab4e7", "pr_number": "41912", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "test/jit/test_save_load.py", "test/test_torch.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "2f840b1662": {"title": "Warns when TensorIterator would resize its output (#42079)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/issues/41027.\n\nThis adds a helper to resize output to ATen/native/Resize.* and updates TensorIterator to use it. The helper throws a warning if a tensor with one or more elements needs to be resized. This warning indicates that these resizes will become an error in a future PyTorch release.\n\n There are many functions in PyTorch that will resize their outputs and don't use TensorIterator. For example,\n\nhttps://github.com/pytorch/pytorch/blob/985fd970aad85799a568b76c2a1b75a1c422fe53/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu#L243\n\nAnd these functions will need to be updated to use this helper, too. This PR avoids their inclusion since the work is separable, and this should let us focus on the function and its behavior in review. A TODO appears in the code to reflect this.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42079\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22846851\n\nPulled By: mruberry\n\nfbshipit-source-id: d1a413efb97e30853923bce828513ba76e5a495d", "pr_number": "42079", "files_changed": ["aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/Resize.h", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "test/test_torch.py", "test/test_type_promotion.py"], "labels": ["merged"]}, "0adb584376": {"title": "Make resize_ use normal device dispatch (#42240)", "body": "Summary:\n`resize_` only requires manual registration to `Autograd` key and its device kernels can safely live together with our normal device dispatch in `native_functions.yaml`.\nBut currently we do manual registration for `CPU/CUDA` kernels (and leaves no dispatch in native_functions.yaml) which makes `resize_` non-overrideable from backend point of view. While it indeed should dispatch at device level, this caused xla to whitelist `resize_` and register a lowering to XLA key. This PR moves the device dispatch of `resize_` back to `native_functions.yaml` so that it shows up as `abstract` method properly for downstream extensions.\nNote that we also do manual registration for `copy_/detach_/resize_as_/etc` in aten but they are slightly different than `resize_` since for them we only register `catchAll` kernels instead of device kernels. I'll need to investigate and send a followup PR for those ops.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42240\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22846311\n\nPulled By: ailzhang\n\nfbshipit-source-id: 10b6cf99c4ed3d62fc4e1571f4a2a463d1b88c81", "pr_number": "42240", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp"], "labels": ["merged"]}, "4c6878c97d": {"title": "[gloo] change ProcessGroupGlooAsyncTest to use gtest (#42313)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42313\n\nChanges the tests in `ProcessGroupGlooAsyncTest.cpp` to use the Gtest testing framework.\n\nReviewed By: malfet\n\nDifferential Revision: D22821577\n\nfbshipit-source-id: 326b24a334ae84a16434d0d5ef27d16ba4b90d5d", "pr_number": "42313", "files_changed": ["torch/lib/c10d/test/CMakeLists.txt", "torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp"], "labels": ["fb-exported", "merged"]}, "832b1659e7": {"title": "Fix missing attribute when loading model from older version (#42242) (#42290)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42242\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42290\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22844096\n\nPulled By: albanD\n\nfbshipit-source-id: 707e552e0ed581fbe00f1527ab7426880edaed64", "pr_number": "42290", "files_changed": ["torch/nn/modules/module.py"], "labels": ["merged", "open source"]}, "352e15f1a2": {"title": "Revert D22812445: Update TensorPipe submodule", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22812445 (https://github.com/pytorch/pytorch/commit/2335430086fd1e23db161f5f0782d8b04885702d)\n\nOriginal commit changeset: e6d824bb28f5\n\nfbshipit-source-id: 606632a9aaf2513b5ac949e4d6687aa7563eae5d", "pr_number": null, "files_changed": ["caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "test/cpp/rpc/CMakeLists.txt", "third_party/tensorpipe", "third_party/tensorpipe.BUILD", "torch/CMakeLists.txt", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": []}, "a9e7e787f8": {"title": "[jit] make clone works for interface type (#42121)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42121\n\nThis PR changes the Module API to allow register a module with module\ninterface type, and therefore allows Module::clone works on the case\nwhere there's a module interface type being shared by two submodules.\n\ninterface type will be shared by the new cloned instance in the same\ncompilation unit bc it only\ncontains a list of functionSchema, which does not involve any\nattributes compared to classType.\n\nfixes https://github.com/pytorch/pytorch/issues/41882\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D22781205\n\nPulled By: wanchaol\n\nfbshipit-source-id: f97f4b75970f0b434e38b5a1f778eda2c4e5109b", "pr_number": "42121", "files_changed": ["test/cpp/jit/test_module_api.cpp", "test/cpp/jit/tests.h", "test/quantization/test_quantize_jit.py", "torch/csrc/jit/api/module.cpp", "torch/csrc/jit/passes/quantization/insert_observers.cpp"], "labels": ["merged", "oncall: jit"]}, "bdd9ef1981": {"title": "Support RowWiseSparseAdam on GPU (#35404)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/35404\n\nImplement RowWiseSparseAdam on CUDA\n\nReviewed By: xw285cornell\n\nDifferential Revision: D20650225\n\nfbshipit-source-id: 5f871e2f259e362b713c9281b4d94534453995cf", "pr_number": "35404", "files_changed": ["caffe2/python/operator_test/adam_test.py", "caffe2/sgd/adam_op.cc", "caffe2/sgd/adam_op_gpu.cu"], "labels": ["fb-exported", "merged"]}, "a9eebaf693": {"title": "[quant] Add saturate_to_fp16 op for FP16 quant support (#42147)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42147\n\nOp to check the range of a tensor and clamp the values to fp16 range\nThis operator will be inserted into the graph in subsequent diffs.\n\nTest Plan:\npython test/test_quantization.py TestQuantizedTensor.test_fp16_saturate_op\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22849221\n\nfbshipit-source-id: 0da3298e179750f6311e3a09596a7b8070509096", "pr_number": "42147", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/quantized/cpu/quant_utils.h", "test/quantization/test_quantized_tensor.py"], "labels": ["merged"]}, "8c5bf10264": {"title": "[quant] Add FP16Observer for fp16 quant support (#42221)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42221\n\nAdds a new observer that emits a warning if the range of tensor is beyond fp16 range. This will be further used in graph mode quantization to insert the cast to fp16 ops in the graph\n\nTest Plan:\npython test/test_quantizaton.py TestObserver.test_fp16_observer\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22849222\n\nfbshipit-source-id: a301281ce38ba4d4e7a009308400d34a08c113d2", "pr_number": "42221", "files_changed": ["test/quantization/test_workflow_module.py", "torch/quantization/observer.py"], "labels": ["merged"]}, "6bd46b583e": {"title": "[quant][graph] Add support for FP16 dynamic quant (#42222)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42222\n\nThis change adds the necessary passes to perform FP16 dynamic quantization.\nWe skip inserting observers for activations based on the dtype (torch.float16) and only insert the Fp16Observer for weights\n\nTest Plan:\npython test/test_quantization.py TestQuantizeJitOps\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22849220\n\nfbshipit-source-id: 2c53594ecd2485e9e3dd0b380eceaf7c5ab5fc50", "pr_number": "42222", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/insert_observers.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp", "torch/csrc/jit/passes/quantization/quantization_patterns.h", "torch/quantization/qconfig.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged", "oncall: jit"]}, "38bf5be24f": {"title": "[quant] Use PlaceholderObserver instead of Fp16Observer and NoopObserver (#42348)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42348\n\nUse the dtype info in placeholderObserver to decide what ops to insert in the graph\nIn the next PR we can delete NoopObserver\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22859457\n\nfbshipit-source-id: a5c618f22315534ebd9a2df77b14a0aece196989", "pr_number": "42348", "files_changed": ["test/quantization/test_quantize_jit.py", "test/quantization/test_workflow_module.py", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp", "torch/quantization/observer.py", "torch/quantization/qconfig.py"], "labels": ["merged", "oncall: jit"]}, "a01e91e6b2": {"title": "[pytorch] include all overloads for OSS custom build", "body": "Summary:\nFor mobile custom build, we only generate code for ops that are used by\nspecific models to reduce binary size.\n\nThere multiple places where we apply the op filtering:\n- generated_unboxing_wrappers_*.cpp\n- autograd/VariableType*.cpp\n- c10 op registration (in aten/gen.py)\n\nFor c10 op registration, we filter by the main op name - all overloads\nthat match the main op name part will be kept.\n\nFor generated_unboxing_wrappers_*, we filter by the full op name - only\nthose having exactly the same overload name will be kept.\n\nThis PR changes generated_unboxing_wrappers_* and autograd/VariableType*.cpp\ncodegen to also filter by the main op name.\n\nThe reasons are:\n- keeping all overloads can have better backward compatibility;\n- generated_unboxing_wrappers_* are relatively small as it only contains\n  thin wrappers for root ops.\n- generated_unboxing_wrappers_* will be replaced by c10 op registration\n  soon anyway.\n- autograd/VariableType*.cpp are not included in OSS build.\n\nWhy it offers better backward compatibility? #40737 is an example:\nIt introduced a new `_convolution` overload and renamed the original one\nto `_convolution.deprecated`. Before this PR, the model prepared by the\nold version PyTorch won't be able to run on the custom mobile build\ngenerated on the PR because `_convolution.deprecated` won't be kept in\nthe custom build due to full op name matching policy. By relaxing it to\npartial matching policy, the mobile custom build CI on the PR can pass.\n\nWill test the size impact for FB production build before landing.\n\nDifferential Revision: D22809564\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nPulled By: ljk53\n\nfbshipit-source-id: e2fc017da31f38b9430cc2113f33e6d21a0eaf0b", "pr_number": null, "files_changed": ["tools/autograd/gen_autograd.py", "tools/autograd/utils.py", "tools/jit/gen_unboxing_wrappers.py", "tools/setup_helpers/generate_code.py"], "labels": []}, "4fc525e729": {"title": "[Dper3] Implementation of squeezed input to DC++", "body": "Summary:\nThis Diff provides an option for DC++ module to use the squeezed sparse feature embeddings to generate attention weights, with the purpose of reducing the network size to achieve QPS gains. There are 3 squeeze options: sum, max, and mean, along the embedding dimension and are provided for both the attention weights and resnet generation.\nExample workflow: f208474456\n\n{F257199459}\n\nTest Plan:\n1. Test single ops\nbuck test dper3/dper3/modules/low_level_modules/tests:single_operators_test -- test_reduce_back_mean\nbuck test dper3/dper3/modules/low_level_modules/tests:single_operators_test -- test_reduce_back_max\n2. Test DC++ module\nbuck test dper3/dper3/modules/tests:core_modules_test -- test_dc_pp_arch_one_layer_compressed_embeddings_only_squeeze_input\nbuck test dper3/dper3/modules/tests:core_modules_test -- test_dc_pp_arch_shared_input_squeeze_input\nbuck test dper3/dper3/modules/tests:core_modules_test -- test_dc_pp_input_compress_embeddings_squeeze_input\n3. Test Arch\nbuck test dper3/dper3_models/ads_ranking/model_impl/sparse_nn/tests:sparse_nn_lib_test -- test_dense_sparse_interaction_compress_dot_arch_dot_compress_pp_squeezed_input\n4. e2e test\nbuck test dper3/dper3_models/ads_ranking/tests:model_paradigm_e2e_tests -- test_sparse_nn_compress_dot_attention_fm_max_fc_size_squeeze_input\n\nReviewed By: taiqing\n\nDifferential Revision: D22825069\n\nfbshipit-source-id: 29269ea22cb47d487a1c92a1f6daae1055f54cfc", "pr_number": null, "files_changed": ["caffe2/opt/bound_shape_inferencer.cc"], "labels": []}, "2285a2fc11": {"title": "refactor canonical ordering to also be able to do isAfter checks (#42140)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42140\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22798378\n\nPulled By: eellison\n\nfbshipit-source-id: d1a549f43b28fe927729597818a46674c58fe81d", "pr_number": "42140", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/canonicalize.cpp", "torch/csrc/jit/passes/canonicalize.h", "torch/csrc/jit/python/python_ir.cpp"], "labels": ["merged", "oncall: jit"]}, "f502290e91": {"title": "[JIT] Make create autodiff subgraphs do in place updates to aliasDb (#42141)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42141\n\nUpdate alias db in-place instead of having to construct alias db from scratch on each change, causing O(n^2) behavior.\n\nDescription from https://github.com/pytorch/pytorch/pull/37106 holds pretty well:\n\"\"\"\nRecomputing the aliasdb on every fusion iteration + in every subblock\nis hugely expensive. Instead, update it in-place when doing fusion.\n\nThe graph fuser pass operates by pushing nodes into a fusion group. So\nwe start with\n\n`x, y = f(a, b, c)`\n\nand end with:\n```\nx_out, y_out = prim::fusionGroup(a, b, c)\n   x_in, y_in = f(a_in, b_in, c_in)\n   -> x_in, y_in\n```\n\nWe destroy the x and y Value*s in the process. This operation is\neasy to express as an update to the aliasDb--x_out just takes on all\nthe aliasing information x used to have. In particular, since we know\nf and prim::fusionGroup are purely functional, we don't have to mess\nwith any write information.\n\"\"\"\n\nThe one difficulty here is mapping x, y to x_out, y_out is not trivial in merging nodes into the autodiff subgraph node.\nThere are a few options:\n- attempt to make all subgraph utils & ir cloning logic update a map\n- mirror the subgraph utils implementation in create_autodiff_subgraph\n- uniquely map x, y and x_in, y_in so you can back out the correspondence.\n\nI went with the third option.\n\nThis shouldn't affect the results of the pass at all. LMK if you think there's anything else I should be doing to test, I was thinking about maybe exposing an option to run create autodiff subgraphs without the post processor and check that the alias db was correctly updated.\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22798377\n\nPulled By: eellison\n\nfbshipit-source-id: 9a133bcaa3b051c0fb565afb23a3eed56dbe71f9", "pr_number": "42141", "files_changed": ["test/jit/test_autodiff_subgraph_slicing.py", "torch/csrc/jit/passes/create_autodiff_subgraphs.cpp", "torch/testing/_internal/jit_utils.py"], "labels": ["merged", "oncall: jit"]}, "3a19af2427": {"title": "Make operators with optional Tensor? arguments c10-full (#41610)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41610\n\nPreviously, operators that have a `Tensor?` (i.e. optional tensor) in their schema implemented it using `Tensor` in C++ and filled in an undefined tensor for the None case.\nThe c10 operator library, however, expects `Tensor?` to be represented as `optional<Tensor>`, so those operators couldn't be c10-full yet and still had to use codegenerated unboxing instead of templated unboxing.\n\nThis PR changes that. It extends the `hacky_wrapper_for_legacy_signatures` to not only take case of TensorOptions, but now also map between signatures taking `Tensor` and `optional<Tensor>`.\nFor this, it requires an additional template parameter, the expected signature, and it uses that to go argument-by-argument and unwrap any optionals it finds.\nghstack-source-id: 108873701\n\nTest Plan: waitforsandcastle\n\nReviewed By: bhosmer\n\nDifferential Revision: D22607879\n\nfbshipit-source-id: 57b2fb01a294b804f82cd55cd70f0ef4a478e14f", "pr_number": "41610", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen_backend_select_register.py", "aten/src/ATen/native/native_functions.yaml", "c10/test/util/TypeList_test.cpp", "c10/util/TypeList.h", "test/cpp_extensions/msnpu_extension.cpp", "test/mobile/op_deps/simple_ops.cpp", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "1542c41a67": {"title": "Change C++ frontend to take optional<Tensor> arguments (#41947)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41947\n\nPreviously, if an op took an optional `Tensor?` argument, the C++ frontend (i.e. `at::op()` and `Tensor::op()`)\nwere generated to take `Tensor`. A previous PR (https://github.com/pytorch/pytorch/pull/41610) changed the kernels\nto be written with `c10::optional<Tensor>` instead of `Tensor`, but that did not touch the C++ frontend yet.\n\nThis PR changes the C++ frontend API to take `c10::optional<Tensor>` instead of `Tensor` as well.\nThis should be mostly bc conserving. Since `Tensor` implicitly converts to `c10::optional<Tensor>`, any old code\ncalling an op with a `Tensor` would still work. There are likely corner cases that get broken though.\nFor example, C++ only ever does *one* implicit conversion. So if you call an op with a non-tensor object\nthat gets implicitly converted to a `Tensor`, then that previously worked since the API took a `Tensor` and\nC++ allows one implicit conversion. Now it wouldn't work anymore because it would require two implicit conversions\n(to `Tensor` and then to `c10::optional<Tensor>`) and C++ doesn't do that.\n\nThe main reasons for doing this are\n- Make the C++ API more sane. Those arguments are optional and that should be visible from the signature.\n- Allow easier integration for XLA and Autocast. Those backends generate code to wrap operators and forward\n  operator arguments to calls to at::op(). After https://github.com/pytorch/pytorch/pull/41610, there was\n  a mismatch because they had to implement operators with `optional<Tensor>` but call `at::op()` with `Tensor`,\n  so they had to manually convert between those. After this PR, they can just forward the `optional<Tensor>`\n  in their call to `at::op()`.\nghstack-source-id: 108873705\n\nTest Plan: unit tests\n\nReviewed By: bhosmer\n\nDifferential Revision: D22704832\n\nfbshipit-source-id: f4c00d457b178fbc124be9e884a538a3653aae1f", "pr_number": "41947", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/README.md", "aten/src/ATen/templates/Functions.cpp", "tools/autograd/gen_autograd_functions.py", "tools/autograd/gen_python_functions.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/Functions.cpp", "tools/autograd/templates/VariableType.h", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/function.h", "torch/csrc/autograd/functions/utils.h", "torch/csrc/autograd/saved_variable.cpp", "torch/csrc/autograd/saved_variable.h", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tracer.h", "torch/csrc/jit/mobile/register_mobile_autograd.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["merged", "oncall: jit", "topic: bc-breaking"]}, "ff91b169c7": {"title": "Changes to match Fused Op: Dequantize->Swish->Quantize (#42255)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42255\n\nChanges to match Fused Op: Dequantize->Swish->Quantize\n* Changes to scale handling\n\nResults showing matching intermediate and final Swish_Int8 Op.\nP137389801\n\nTest Plan: test case test_deq_swish_quant_nnpi.py\n\nReviewed By: hyuen\n\nDifferential Revision: D22827499\n\nfbshipit-source-id: b469470ca66f6405ccc89696694af372ce6ce89e", "pr_number": "42255", "files_changed": ["caffe2/contrib/fakelowp/int8_dequantize_op_nnpi.h", "caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h", "caffe2/contrib/fakelowp/int8_swish_op_nnpi.cc", "caffe2/contrib/fakelowp/int8_swish_op_nnpi.h", "caffe2/contrib/fakelowp/test/test_deq_swish_quant_nnpi.py"], "labels": ["fb-exported", "merged"]}, "655f376460": {"title": "Implement Enum sugared value and Enum constant support (#42085)", "body": "Summary:\n[3/N] Implement Enum JIT support\n\n* Add enum value as constant support\n* Add sugared value for EnumClass\n\nSupported:\nEnum-typed function arguments\nusing Enum type and comparing them\nSupport getting name/value attrs of enums\nUsing Enum value as constant\n\nTODO:\nAdd PyThon sugared value for Enum\nSupport Enum-typed return values\nSupport serialization and deserialization\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42085\n\nReviewed By: eellison\n\nDifferential Revision: D22758042\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 5c6e571686c0b60d7fbad59503f5f94b3b3cd125", "pr_number": "42085", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/test/ivalue_test.cpp", "test/jit/test_enum.py", "torch/csrc/jit/ir/constants.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h"], "labels": ["merged", "oncall: jit"]}, "f8c5800bb5": {"title": "[TensorExpr] Add debug dumps to kernel.cpp. (#42196)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42196\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22803676\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 109372ca45d86478826190b868d005d2fb2c9ba7", "pr_number": "42196", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["merged", "oncall: jit"]}, "f41bb1f92b": {"title": "[TensorExpr] Explicitly cast to bool results of comparison ops in kernel.cpp. (#42201)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42201\n\nPreviously, we've been using operators <, >, ==, et al. and relied on\nthe dtype to be picked automatically. It led to a wrong dtype being\npicked for the result, but that choice was overwritten by the type\nexplicitly specified in JIT IR, which we were lowering. Now we are\nmoving towards using shape inference instead of relying on all types\nbeing specified in the IR, and that made this issue to immediately pop\nup.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D22806428\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 89d2726340efa2bb3da45d1603bedc53955e14b9", "pr_number": "42201", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["merged", "oncall: jit"]}, "2decccea2e": {"title": "[TensorExpr] Implement shape inference for TE. (#41451)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41451\n\nSince TE operates on a limited subset of ops with a well-defined\nsemantics, we can easily infer shapes of intermediate and output tensors\ngiven shapes of the inputs.\n\nThere is a couple of ops that are not yet supported in the shape\ninference, once we add them we could relax the shape info requirements\nin the TE fuser: currently it requires all values in the fusion group to\nhave shapes known and we can change it to only inputs.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D22543470\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 256bae921028cb6ec3af91977f12bb870c385f40", "pr_number": "41451", "files_changed": ["test/cpp/tensorexpr/test_kernel.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["merged", "oncall: jit"]}, "dcc4d11ffa": {"title": "[TensorExpr] Make tensorOrConstant non-templatized function. (#42202)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42202\n\nCurrently we used the template in order to be able to take both\n`std::vector<ExprHandle>` and `std::vector<VarHandle>`. However,\nsemantics of this function tells that the only allowed option should be\nthe former one: we're specifying indices for the tensor access we want\nto generate. While it could be convenient to avoid conversion from\nvector of vars to a vector of exprs at the callsites, it makes the code\nless explicit and thus more difficult to reason about.\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22806429\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 8403af5fe6947c27213050a033e79a09f7075d4c", "pr_number": "42202", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["merged", "oncall: jit"]}, "f47e00bdc3": {"title": "[NNC] Bounds Inference: make inferred bounds respect gaps (#42185)", "body": "Summary:\nA heavy refactor of bounds inference to fix some issues and bugs blocking using it to analyze cross thread interactions:\n* We were merging all accesses to a Buf into a single bounds info entry, even if they did not overlap. E.g. if we accessed a[0:2] and a[5:6] we would merge that into a bound of a[0:6]. I've changed this behaviour to merge only overlapping bounds.\n* We were not separating bounds of different kinds (e.g. Load vs Store) and would merge a Store bounds into a Load bounds, losing the information about what kind of access it was. E.g. this loop would produce bounds: [{Load, 0, 10}] and now produces bounds [{Load, 0, 9}, {Store, 1, 10}]:\n```\nfor i in 1 to 10...\n  x[i] = x[i-1]\n```\n* Both ComputeAt and Rfactor relied on the overzealous merging and only used a single entry in the bounds list to determine the bounds of temporary buffers they created, which could result in temporary buffers allocated smaller than accesses to them. I've fixed Rfactor, but *not* ComputeAt - however all ComputeAt tests still pass (may require loop fusion to trigger this issue) - I will come back to it.\n\nBeing more precise about bounds is more complex, rather than taking the minimum of starts and maximum of stops we now need to determine if two bounds overlap or are adjacent. There are many edge cases and so I've added a bunch of test coverage of the merging method.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42185\n\nReviewed By: mruberry\n\nDifferential Revision: D22870391\n\nPulled By: nickgg\n\nfbshipit-source-id: 3ee34fcbf0740a47259defeb44cba783b54d0baa", "pr_number": "42185", "files_changed": ["test/cpp/tensorexpr/test_boundsinference.cpp", "test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/bounds_inference.cpp", "torch/csrc/jit/tensorexpr/bounds_inference.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.h", "torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["merged", "oncall: jit"]}, "44b018ddeb": {"title": "Convert ProcessGroupNCCLTest.cpp to gtest unittest (#42365)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42365\n\nConverting the test\n\nReviewed By: malfet\n\nDifferential Revision: D22855087\n\nfbshipit-source-id: dc917950dcf99ec7036e48aaa4264d2c455cb19e", "pr_number": "42365", "files_changed": ["torch/lib/c10d/test/CMakeLists.txt", "torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["fb-exported", "merged"]}, "206db5c127": {"title": "Improve `torch.norm` functionality, errors, and tests (#41956)", "body": "Summary:\n**BC-Breaking Note:**\nBC breaking changes in the case where keepdim=True. Before this change, when calling `torch.norm` with keepdim=True and p='fro' or p=number, leaving all other optional arguments as their default values, the keepdim argument would be ignored. Also, any time `torch.norm` was called with p='nuc', the result would have one fewer dimension than the input, and the dimensions could be out of order depending on which dimensions were being reduced. After the change, for each of these cases, the result has the same number and order of dimensions as the input.\n\n**PR Summary:**\n\n* Fix keepdim behavior\n* Throw descriptive errors for unsupported sparse norm args\n* Increase unit test coverage for these cases and for complex inputs\n\nThese changes were taken from part of PR https://github.com/pytorch/pytorch/issues/40924. That PR is not going to be merged because it overrides `torch.norm`'s interface, which we want to avoid. But these improvements are still useful.\n\nIssue https://github.com/pytorch/pytorch/issues/24802\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41956\n\nReviewed By: albanD\n\nDifferential Revision: D22837455\n\nPulled By: mruberry\n\nfbshipit-source-id: 509ecabfa63b93737996f48a58c7188b005b7217", "pr_number": "41956", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/LinearAlgebraUtils.h", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "test/test_sparse.py", "test/test_torch.py", "torch/_overrides.py", "torch/functional.py"], "labels": ["merged", "open source", "topic: bc-breaking", "topic: linear algebra", "triaged"]}, "2912390662": {"title": "Limits cpu scalar error message to where it's appropriate (#42360)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/40986.\n\nTensorIterator's test for a CUDA kernel getting too many CPU scalar inputs was too permissive. This update limits the check to not consider outputs and to only be performed if the kernel can support CPU scalars.\n\nA test is added to verify the appropriate error message is thrown in a case where the old error message was thrown previously.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42360\n\nReviewed By: ngimel\n\nDifferential Revision: D22868536\n\nPulled By: mruberry\n\nfbshipit-source-id: 2bc8227978f8f6c0a197444ff0c607aeb51b0671", "pr_number": "42360", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "test/test_torch.py"], "labels": ["merged"]}, "115d226498": {"title": "Pin NumPy version on MacOS testers to 1.18.5 (#42409)", "body": "Summary:\nOtherwise numba linking by clang-9 fails with:\n```\nld: in /Users/distiller/workspace/miniconda3/lib/python3.7/site-packages/numpy/core/lib/libnpymath.a(npy_math.o), could not parse object file /Users/distiller/workspace/miniconda3/lib/python3.7/site-packages/numpy/core/lib/libnpymath.a(npy_math.o): 'Unknown attribute kind (61) (Producer: 'LLVM10.0.0' Reader: 'LLVM APPLE_1_902.0.39.2_0')', using libLTO version 'LLVM version 9.1.0, (clang-902.0.39.2)' for architecture x86_64\n```\nBecause conda's numpy-1.19.1 is compiled with clang-10\nThis should fix MacOS regressions in CIrcleCI\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42409\n\nReviewed By: xw285cornell\n\nDifferential Revision: D22887683\n\nPulled By: malfet\n\nfbshipit-source-id: d58ee9bf53772b57c59e18f71151916d4f0a3c7d", "pr_number": "42409", "files_changed": [".jenkins/pytorch/macos-common.sh"], "labels": ["merged"]}, "5769b06ab5": {"title": "[Caffe2] Remove explicitly divide by zero in SpatialBN training mode (#42380)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42380\n\n[Caffe2] Remove explicitly divide by zero in SpatialBN training mode\n\nTest Plan: buck test mode/dev-nosan //caffe2/caffe2/python/operator_test:spatial_bn_op_test\n\nReviewed By: houseroad\n\nDifferential Revision: D22873214\n\nfbshipit-source-id: 70b505391b5db02b45fc46ecd7feb303e50c6280", "pr_number": "42380", "files_changed": ["caffe2/operators/spatial_batch_norm_op.h", "caffe2/operators/spatial_batch_norm_op_impl.cuh", "caffe2/python/operator_test/spatial_bn_op_test.py"], "labels": ["fb-exported", "merged"]}, "bdcf320bed": {"title": "Support custom exception message (#41907)", "body": "Summary:\nRaise and assert used to have a hard-coded error message \"Exception\". User provided error message was ignored. This PR adds support to represent user's error message in TorchScript.\n\nThis breaks backward compatibility because now we actually need to script the user's error message, which can potentially contain unscriptable expressions. Such programs can break when scripting, but saved models can still continue to work.\n\nIncreased an op count in test_mobile_optimizer.py because now we need aten::format to form the actual exception message.\n\nThis is built upon an WIP PR:  https://github.com/pytorch/pytorch/pull/34112 by driazati\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41907\n\nReviewed By: ngimel\n\nDifferential Revision: D22778301\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 2b94f0db4ae9fe70c4cd03f4048e519ea96323ad", "pr_number": "41907", "files_changed": ["test/expect/TestScript.test_python_frontend_py3.expect", "test/test_jit.py", "test/test_mobile_optimizer.py", "torch/_jit_internal.py", "torch/_linalg_utils.py", "torch/_lobpcg.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/frontend/tree_views.h", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h", "torch/csrc/jit/python/python_tree_views.cpp", "torch/jit/annotations.py", "torch/jit/quantized.py", "torch/nn/functional.py", "torch/nn/modules/rnn.py", "torch/nn/quantized/dynamic/modules/rnn.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["oncall: jit", "topic: bc-breaking"]}, "d403983695": {"title": "Support List[str].index (#39210) (#40348)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40348\n\nTest Plan: Imported from OSS\n\nReviewed By: wanchaol\n\nDifferential Revision: D22757035\n\nPulled By: firstprayer\n\nfbshipit-source-id: 4fadf8beabf8d5bdfa5b0a185075f7caf9ba8b02", "pr_number": "40348", "files_changed": ["aten/src/ATen/core/List.h", "aten/src/ATen/core/List_inl.h", "test/jit/test_list_dict.py", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["merged", "oncall: jit"]}, "4cbf18ccc3": {"title": "Enables integer -> float type promotion in TensorIterator (#42359)", "body": "Summary:\nMany ufuncs (mostly unary ufuncs) in NumPy promote integer inputs to float. This typically occurs when the results of the function are not representable as integers.\n\nFor example:\n\n```\na = np.array([1, 2, 3], dtype=np.int64)\nnp.sin(a)\n: array([0.84147098, 0.90929743, 0.14112001])\n```\n\nIn PyTorch we only have one function, `torch.true_divide`, which exhibits this behavior today, and it did it by explicitly pre-casting its inputs to the default (float) scalar type where necessary before calling TensorIterator.\n\nThis PR lets TensorIterator understand and implement this behavior directly, and it updates `torch.true_divide` to verify the behavior is properly implemented. This will be convenient when implementing more integer->float promotions later (like with `torch.sin`), and also saves copies on CUDA, where the cast from one dtype to another is fused with the computation.\n\nThe mechanism for this change is simple. A new flag, `promote_integer_inputs_to_float_` is added to TensorIteratorConfig, and it requires `promote_integer_inputs_to_float_` be true if it's set. When the new flag is set, after the TensorIterator's \"common dtype\" (AKA \"computation type\") is computed it's checked for being an integral (boolean included) type and, if it is, changed to the default (float) scalar type, instead. Only `torch.true_divide` sets this flag (for now).\n\nIn the future we'll likely...\n- provide helpers (`binary_float_op`, `unary_float_op`) to more easily construct functions that promote int->float instead of requiring they build their own TensorIteratorConfigs.\n- update torch.atan2 to use `binary_float_op`\n- update many unary ufuncs, like `torch.sin` to use `unary_float_op` and support unary ops having different input and result type (this will also require a small modification to some of the \"loops\" code)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42359\n\nReviewed By: ngimel\n\nDifferential Revision: D22878394\n\nPulled By: mruberry\n\nfbshipit-source-id: b8de01e46be859321522da411aed655e2c40e5b9", "pr_number": "42359", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h"], "labels": ["merged", "module: numpy", "topic: type promotion"]}, "91c80d122a": {"title": "torch.gcd: Do not use std::abs() because it does not have an unsigned integer overload (#42254)", "body": "Summary:\n`abs` doesn't have an signed overload across all compilers, so applying abs on uint8_t can be ambiguous: https://en.cppreference.com/w/cpp/numeric/math/abs\n\nThis may cause unexpected issue when the input is uint8 and is greater\nthan 128. For example, on MSVC, applying `std::abs` on an unsigned char\nvariable\n\n```c++\n#include <cmath>\n\nunsigned char a(unsigned char x) {\n    return std::abs(x);\n}\n```\n\ngives the following warning:\n\n    warning C4244: 'return': conversion from 'int' to 'unsigned char',\n    possible loss of data\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42254\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22860505\n\nPulled By: mruberry\n\nfbshipit-source-id: 0076d327bb6141b2ee94917a1a21c22bd2b7f23a", "pr_number": "42254", "files_changed": ["aten/src/ATen/native/Math.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "test/test_torch.py"], "labels": ["merged", "open source"]}, "bfa94487b9": {"title": "Remove register_mobile_autograd.cpp. (#42397)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42397\n\nSince the autograd registration is unified to code-gen, we don't need to keep a manual registration file for mobile.\nRemove it to avoid extra maintenance.\n\nTest Plan: Imported from OSS\n\nReviewed By: ljk53\n\nDifferential Revision: D22883153\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 6db0bd89369beab9eed6e9a9692dd46f5bd1ff48", "pr_number": "42397", "files_changed": ["caffe2/CMakeLists.txt", "tools/autograd/templates/VariableType.cpp", "tools/build_variables.bzl", "torch/csrc/jit/mobile/register_mobile_autograd.cpp"], "labels": ["merged", "oncall: jit"]}, "ebfff31e19": {"title": "[distributedhogwild] Introducing new tags for distributed hogwild. (#42381)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42381\n\nIntroduce new tag to support distributed hogwild.\n\nReviewed By: boryiingsu\n\nDifferential Revision: D20484099\n\nfbshipit-source-id: 5973495589e0a7ab185d3867b37437aa747f408a", "pr_number": "42381", "files_changed": ["caffe2/python/layers/tags.py"], "labels": ["fb-exported", "merged"]}, "192487d716": {"title": "Update MAGMA to 2.5.3 for Windows (#42410)", "body": "Summary:\nIn order to introduce CUDA 11 build jobs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42410\n\nReviewed By: malfet\n\nDifferential Revision: D22892025\n\nPulled By: ezyang\n\nfbshipit-source-id: 11bd7507f623d654a589ba00a138f6b947990f4c", "pr_number": "42410", "files_changed": [".jenkins/pytorch/win-test-helpers/installation-helpers/install_magma.bat", "docs/source/notes/windows.rst"], "labels": ["merged", "open source"]}, "2f8d5b68fa": {"title": "vmap fallback kernel (#41943)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41943\n\nIf an operator doesn't have a batching rule implemented then we fallback\nto this implementation. The fallback only works on out-of-place operators\nthat return only tensors with new memory. (e.g., no in-place operators,\nno view operations).\n\nThe fallback effectively takes all of the BatchedTensors in `stack`,\nslices them, and runs `op` on all of the corresponding slices to produce slices\nof the outputs. The output slices then get `torch.stack`ed to create the\nfinal returns.\n\nThe performance of the fallback is not very good because it introduces\nan extra copy from stacking the sliced outputs. Because of this, we prefer\nto write batching rules for operators whenever possible.\n\nIn the future, I'd like to disable the fallback kernel for random\nfunctions until we have a better random story for vmap. I will probably\nadd a blocklist of operators to support that.\n\nTest Plan: - `pytest test/test_vmap.py -v`\n\nReviewed By: ezyang\n\nDifferential Revision: D22764103\n\nPulled By: zou3519\n\nfbshipit-source-id: b235833f7f27e11fb76a8513357ac3ca286a638b", "pr_number": "41943", "files_changed": ["aten/src/ATen/BatchedFallback.cpp", "aten/src/ATen/BatchedFallback.h", "aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "4cdbe5c495": {"title": "Implement batching rules for some view ops (#42248)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42248\n\nIncluding:\n- torch.diagonal\n- torch.t\n- torch.select\n- Tensor.expand_as\n- Tensor slicing.\n\nPlease let me know in the future if it would be easier to review these\nseparately (I put five operators into this PR because each\nimplementation is relatively simple).\n\nTest Plan:\n- new tests in `test/test_vmap.py`.\n- I would like to have a more structured/automated way of testing but\nmy previous attempts at making something resulted in something very\ncomplicated.\n\nReviewed By: ezyang\n\nDifferential Revision: D22846273\n\nPulled By: zou3519\n\nfbshipit-source-id: 8e45ebe11174512110faf1ee0fdc317a25e8b7ac", "pr_number": "42248", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "ebde590864": {"title": "Remove debug vestige (#42277)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42277\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D22853481\n\nPulled By: mrshenli\n\nfbshipit-source-id: 74e58c532d8f872c1dd830573b2a4c4c86410de2", "pr_number": "42277", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "326d777e53": {"title": "Convert _wait_all_workers to _all_gather (#42276)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42276\n\nThis commit converts `_wait_all_workers()` to `_all_gather()` by\nallowing each worker to provide its own data object. The `_all_gather()`\nfunction blocks and returns the gathered results. This API can be\nconverted to `rpc.barrier()` latter.\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D22853480\n\nPulled By: mrshenli\n\nfbshipit-source-id: 9d506813b9fd5b7c144885e2b76a863cbd19466a", "pr_number": "42276", "files_changed": ["torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "ed44269edc": {"title": "Add missing space after -> for topk.values (#42321)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42321\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22846520\n\nPulled By: ezyang\n\nfbshipit-source-id: 7c0ab0b019d05a13309c3b8d770582414795799f", "pr_number": "42321", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["merged"]}, "fa6e900e8c": {"title": "Let TensorIterator::nullary_op support check_mem_overlap option (#38693)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/38693\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22291237\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 5bc96e617ed36ed076da73e3d019699f2efd6e4e", "pr_number": "38693", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h"], "labels": ["merged", "open source", "triaged"]}, "34025eb826": {"title": "Vectorize arange (#38697)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38697\n\nBenchmark (gcc 8.3, Debian Buster, turbo off, Release build, Intel(R)\nXeon(R) E-2136, Parallelization using OpenMP):\n\n```python\nimport timeit\nfor dtype in ('torch.double', 'torch.float', 'torch.uint8', 'torch.int8', 'torch.int16', 'torch.int32', 'torch.int64'):\n    for n, t in [(40_000, 50000),\n                (400_000, 5000)]:\n        print(f'torch.arange(0, {n}, dtype={dtype}) for {t} times')\n        print(timeit.timeit(f'torch.arange(0, {n}, dtype={dtype})', setup=f'import torch', number=t))\n```\n\nBefore:\n\n```\ntorch.arange(0, 40000, dtype=torch.double) for 50000 times\n1.587841397995362\ntorch.arange(0, 400000, dtype=torch.double) for 5000 times\n0.47885190199303906\ntorch.arange(0, 40000, dtype=torch.float) for 50000 times\n1.5519152240012772\ntorch.arange(0, 400000, dtype=torch.float) for 5000 times\n0.4733216500026174\ntorch.arange(0, 40000, dtype=torch.uint8) for 50000 times\n1.426058754004771\ntorch.arange(0, 400000, dtype=torch.uint8) for 5000 times\n0.43596178699226584\ntorch.arange(0, 40000, dtype=torch.int8) for 50000 times\n1.4289699140063021\ntorch.arange(0, 400000, dtype=torch.int8) for 5000 times\n0.43451592899509706\ntorch.arange(0, 40000, dtype=torch.int16) for 50000 times\n0.5714442400058033\ntorch.arange(0, 400000, dtype=torch.int16) for 5000 times\n0.14837959500437137\ntorch.arange(0, 40000, dtype=torch.int32) for 50000 times\n0.5964003179979045\ntorch.arange(0, 400000, dtype=torch.int32) for 5000 times\n0.15676555599202402\ntorch.arange(0, 40000, dtype=torch.int64) for 50000 times\n0.8390555799996946\ntorch.arange(0, 400000, dtype=torch.int64) for 5000 times\n0.23184613398916554\n```\n\nAfter:\n\n```\ntorch.arange(0, 40000, dtype=torch.double) for 50000 times\n0.6895066159922862\ntorch.arange(0, 400000, dtype=torch.double) for 5000 times\n0.16820953000569716\ntorch.arange(0, 40000, dtype=torch.float) for 50000 times\n1.3640095089940587\ntorch.arange(0, 400000, dtype=torch.float) for 5000 times\n0.39255041000433266\ntorch.arange(0, 40000, dtype=torch.uint8) for 50000 times\n0.3422072059911443\ntorch.arange(0, 400000, dtype=torch.uint8) for 5000 times\n0.0605111670010956\ntorch.arange(0, 40000, dtype=torch.int8) for 50000 times\n0.3449254590086639\ntorch.arange(0, 400000, dtype=torch.int8) for 5000 times\n0.06115841199061833\ntorch.arange(0, 40000, dtype=torch.int16) for 50000 times\n0.7745441729930462\ntorch.arange(0, 400000, dtype=torch.int16) for 5000 times\n0.22106765500211623\ntorch.arange(0, 40000, dtype=torch.int32) for 50000 times\n0.720475220005028\ntorch.arange(0, 400000, dtype=torch.int32) for 5000 times\n0.20230313099455088\ntorch.arange(0, 40000, dtype=torch.int64) for 50000 times\n0.8144655400101328\ntorch.arange(0, 400000, dtype=torch.int64) for 5000 times\n0.23762561299372464\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22291236\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 134dd08b77b11e631d914b5500ee4285b5d0591e", "pr_number": "38697", "files_changed": ["aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp", "test/test_torch.py"], "labels": ["merge-this-please", "merged", "open source", "triaged"]}, "0eb513beef": {"title": "Set a proper type for a variable (#42453)", "body": "Summary:\n`ninputs` variable was always used as a `size_t` but declared as an `int32_t`\n\nNow, some annoying warnings are fixed\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42453\n\nReviewed By: agolynski\n\nDifferential Revision: D22898282\n\nPulled By: mrshenli\n\nfbshipit-source-id: b62d6b07f0bc3717482906df6010d88762ae0ccd", "pr_number": "42453", "files_changed": ["torch/csrc/jit/runtime/argument_spec.h"], "labels": ["merged", "oncall: jit", "open source"]}, "1b9cd747cf": {"title": "Revert \"Conda build (#38796)\" (#42472)", "body": "Summary:\nThis reverts commit 9c7ca89ae637a9cea52b4fee0877adc7485f4eb7.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42472\n\nReviewed By: ezyang, agolynski\n\nDifferential Revision: D22903382\n\nPulled By: seemethere\n\nfbshipit-source-id: e2b01537bcdf6c50d967329833cb6450a75b8247", "pr_number": "42472", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/cimodel/data/simple/util/docker_constants.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_conda.sh", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".circleci/verbatim-sources/workflows/workflows-ecr-gc.yml"], "labels": ["merged"]}, "c3236b6649": {"title": "[quant] Expose register activation post process hook function to user (#42342)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42342\n\nTest Plan: Imported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D22856711\n\nfbshipit-source-id: d6ad080c82b744ae1147a656c321c448ac5e7f10", "pr_number": "42342", "files_changed": ["torch/quantization/__init__.py", "torch/quantization/quantize.py"], "labels": ["merged"]}, "f0fd1cc873": {"title": "Calculate inverse of output scale first. (#41342)", "body": "Summary:\nThis is to unify how output scale calculation is to be done between\nfbgemm and qnnpack (servers vs mobile).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41342\n\nTest Plan: Quantization tests.\n\nReviewed By: vkuzo\n\nDifferential Revision: D22506347\n\nPulled By: kimishpatel\n\nfbshipit-source-id: e14d22f13c6e751cafa3e52617e76ecd9d39dad5", "pr_number": "41342", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack_utils.h"], "labels": ["merged"]}, "dbdd28207c": {"title": "Expose a generic shape info struct for ONNXIFI Python interface (#42421)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42421\n\nPreviously, we can only feed shape info from Python with float dtype, and batch based dim type when we do onnxifi from Python. This diff removes this limitation and uses TensorBoundShapes protobuf as a generic shape info struct. This will make the onnxifi interface in Python more flexible.\n\nReviewed By: ChunliF\n\nDifferential Revision: D22889781\n\nfbshipit-source-id: 1a89f3a68c215a0409738c425b4e0d0617d58245", "pr_number": "42421", "files_changed": ["caffe2/opt/shape_info.cc", "caffe2/opt/shape_info.h", "caffe2/python/onnx/onnxifi.py", "caffe2/python/pybind_state.cc"], "labels": ["fb-exported", "merged"]}, "d3acfe3ba8": {"title": "Fix segfault in `THPGenerator_dealloc` (#42490)", "body": "Summary:\nSegfault happens when one tries to deallocate unintialized generator\n\nAdd `TestTorch.test_invalid_generator_raises` that validates that Generator created on invalid device is handled correctly\n\nFixes https://github.com/pytorch/pytorch/issues/42281\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42490\n\nReviewed By: seemethere\n\nDifferential Revision: D22908795\n\nPulled By: malfet\n\nfbshipit-source-id: c5b6a35db381738c0fc984aa54e5cab5ef2cbb76", "pr_number": "42490", "files_changed": ["test/test_torch.py", "torch/csrc/Generator.cpp"], "labels": ["merged"]}, "934b68f866": {"title": "ecr_gc: Iterate through all tags, reduce prints (#42492)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42492\n\nThere's a potential for multiple tags to be created for the same digest\nso we should iterate through all potential tags so that we're not\ndeleting digests that are associated with tags that we actually want.\n\nAlso, reduced the number of prints in this script to only the absolutely\nnecessary prints. (i.e. only the deleted images)\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22909248\n\nPulled By: seemethere\n\nfbshipit-source-id: 7f2e540d133485ed6464e413b01ef67aa73df432", "pr_number": "42492", "files_changed": [".circleci/ecr_gc_docker/gc.py"], "labels": ["merged", "module: ci"]}, "d707d4bf6d": {"title": "Implement a light SGD optimizer (#42137)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42137\n\nThis PR implements an SGD optimizer class similar to torch::optim::SGD, but it doesn't inherit from torch::optim::Optimizer, for use on mobile devices (or other lightweight use case).\n\nAdding Martin's comment for visibility: \"SGD may be the only optimizer used in near future. If more client optimizers are needed, refactoring the full optim codes and reusing the existing code would be an option.\"\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22846514\n\nPulled By: ann-ss\n\nfbshipit-source-id: f5f46804aa021e7ada7c0cd3f16e24404d10c7eb", "pr_number": "42137", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_lite_trainer.cpp", "test/cpp/jit/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/mobile/optim/sgd.cpp", "torch/csrc/jit/mobile/optim/sgd.h"], "labels": ["merged", "oncall: jit"]}, "7a5708832f": {"title": "fix masked_select for discontiguous outputs (#41841)", "body": "Summary:\nThis fixes https://github.com/pytorch/pytorch/issues/41473 for discontiguous input, mask and out. Tests to follow. Reverting https://github.com/pytorch/pytorch/issues/33269 is not a great solution because I'm told masked_select was needed for printing complex tensors.\ncc gchanan , zou3519, ezyang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41841\n\nReviewed By: mruberry\n\nDifferential Revision: D22706943\n\nPulled By: ngimel\n\nfbshipit-source-id: 413d7fd3f3308b184de04fd56b8a9aaabcad22fc", "pr_number": "41841", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h", "test/test_torch.py"], "labels": ["merged"]}, "fb56299d4a": {"title": "Fix check highlight in filecheck. (#42417)", "body": "Summary:\n* It originally failed to check for cases where highlight token appears more than once.\n* Now it repeated tries to find highlight token if one doesn't seem correctly highlighted until end of error message.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42417\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22889411\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 994835db32849f3d7e98ab7f662bd5c6b8a1662e", "pr_number": "42417", "files_changed": ["torch/csrc/jit/testing/file_check.cpp"], "labels": ["merged", "oncall: jit"]}, "c000b890a8": {"title": "[ONNX] Export torch.eye to ONNX::EyeLike (#41357)", "body": "Summary:\nExport dynamic torch.eye, i.e. commonly from another tensor, shape for torch.eye is not known at export time.\nStatic torch.eye where n,m are constants is exported as constant tensor directly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41357\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22845220\n\nPulled By: bzinodev\n\nfbshipit-source-id: 6e5c331fa28ca542022ea16f9c88c69995a393b2", "pr_number": "41357", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "04e55d69f9": {"title": "[ONNX] Enable scripting tests and update jit passes (#41413)", "body": "Summary:\nThis PR initiates the process of updating the torchsciprt backend interface used by ONNX exporter.\n\n- Replace jit lower graph pass by freeze module pass\n\n- Enable ScriptModule tests for ONNX operator tests (ORT backend) and model tests by default.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41413\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22845258\n\nPulled By: bzinodev\n\nfbshipit-source-id: d57fd4086f27bd0c3bf5f70af7fd0daa39a2814a", "pr_number": "41413", "files_changed": ["test/jit/test_onnx_export.py", "test/onnx/test_models.py", "test/onnx/test_models_onnxruntime.py", "test/onnx/test_pytorch_common.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/utils.py"], "labels": ["merged", "open source", "triaged"]}, "1b18adb7e8": {"title": "[ONNX] Export static as_strided (#41569)", "body": "Summary:\n`as_strided` creates a view of an existing tensor with specified `sizes`, `strides`, and `storage_offsets`. This PR supports the export of `as_strided` with static argument `strides`. The following scenarios will not be supported:\n* Calling on tensor of dynamic shape, i.e. the tensor shape differs between model runs and different model inputs.\n* In-place operations, i.e. updates to the original tensor that are expected to reflect in the `as_strided` output, and vice versa.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41569\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22845295\n\nPulled By: bzinodev\n\nfbshipit-source-id: 7d1aa88a810e6728688491478dbf029f17ae7201", "pr_number": "41569", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source", "triaged"]}, "24199e0768": {"title": "tuple_map / tuple_concat (#42326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42326\n\nghstack-source-id: 108868289\n\nTest Plan: Unit tests\n\nReviewed By: smessmer\n\nDifferential Revision: D22846504\n\nfbshipit-source-id: fa9539d16e21996bbd80db3e3c524b174b22069e", "pr_number": "42326", "files_changed": ["c10/test/util/Metaprogramming_test.cpp", "c10/util/Metaprogramming.h"], "labels": ["merged"]}, "c8cb5e5bcb": {"title": "Relax cusparse windows guard on cuda 11 (#42412)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42406\n\n### cusparse Xcsrmm2 API:\n\n(https://github.com/pytorch/pytorch/issues/37202)\n\n- new: https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-spmm\n- old (deprecated in cuda 11): https://docs.nvidia.com/cuda/archive/10.2/cusparse/index.html#csrmm2\n\nBefore:\n\n|cuda ver | windows | linux |\n|--|--|--|\n| 10.1 | old api | old api  |\n| 10.2 | old api | new api |\n| 11    | old api (build error claimed in https://github.com/pytorch/pytorch/issues/42406) | new api |\n\nAfter:\n\n|cuda ver | windows | linux |\n|--|--|--|\n| 10.1 | old api | old api  |\n| 10.2 | old api | **old api** |\n| 11    | **new api** | new api |\n\n### cusparse bmm-sparse-dense API\n\n<details><summary>reverted, will be revisited in the future</summary>\n(cc kurtamohler https://github.com/pytorch/pytorch/issues/33430)\n\n- new: https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-spmm\n\nBefore:\n\n|cuda ver | windows | linux |\n|--|--|--|\n| 10.1 | not supported | new api  |\n| 10.2 | not supported | new api |\n| 11    | not supported | new api |\n\nAfter:\n\n|cuda ver | windows | linux |\n|--|--|--|\n| 10.1 | not supported | new api  |\n| 10.2 | not supported | new api |\n| 11    | **new api** | new api |\n\n</details>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42412\n\nReviewed By: agolynski\n\nDifferential Revision: D22892032\n\nPulled By: ezyang\n\nfbshipit-source-id: cded614af970f0efdc79c74e18e1d9ea8a46d012", "pr_number": "42412", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu"], "labels": ["merged", "open source"]}, "dc1f87c254": {"title": "Add typing_extensions as a dependency. (#42431)", "body": "Summary:\nCloses gh-38221.\n\nThe related pytorch/builder PR: https://github.com/pytorch/builder/pull/475\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42431\n\nReviewed By: malfet\n\nDifferential Revision: D22916499\n\nPulled By: ezyang\n\nfbshipit-source-id: c8fe9413b62fc7a6b829fc82aaf32531b55994d1", "pr_number": "42431", "files_changed": ["README.md", "requirements.txt", "setup.py"], "labels": ["merged", "module: typing", "open source"]}, "0cb86afd72": {"title": "Revert D22908795: [pytorch][PR] Fix segfault in `THPGenerator_dealloc`", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22908795 (https://github.com/pytorch/pytorch/commit/d3acfe3ba8c74cd040fb6ea68228ae5ba00750d6)\n\nOriginal commit changeset: c5b6a35db381\n\nfbshipit-source-id: c7559c382fced23cef683c8c90cff2d6012801ec", "pr_number": null, "files_changed": ["test/test_torch.py", "torch/csrc/Generator.cpp"], "labels": []}, "49e06e305f": {"title": "[ONNX] Updating input node removal in ONNX function_substitution pass. (#42146)", "body": "Summary:\nONNX pass `torch._C._jit_pass_onnx_function_substitution(graph)` inlines the function with the compiled torch graph. But while it removes all connections with the compiled function node (e.g. see below - `%6 : Function = prim::Constant[name=\"f\"]()`), it does not remove the function node itself. For example, if the input graph is:\n```\ngraph(%0 : Long(requires_grad=0, device=cpu),\n      %1 : Long(requires_grad=0, device=cpu)):\n  %6 : Function = prim::Constant[name=\"f\"]()\n  %7 : Tensor = prim::CallFunction(%6, %0, %1)\n  return (%7)\n```\nThe output graph is:\n```\ngraph(%0 : Long(requires_grad=0, device=cpu),\n      %1 : Long(requires_grad=0, device=cpu)):\n  %6 : Function = prim::Constant[name=\"f\"]()\n  %8 : int = prim::Constant[value=1]()\n  %z.1 : Tensor = aten::sub(%0, %1, %8) # test/onnx/test_utility_funs.py:790:20\n  %10 : Tensor = aten::add(%0, %z.1, %8) # test/onnx/test_utility_funs.py:791:23\n  return (%10)\n```\nNote that the `%6 : Function = prim::Constant[name=\"f\"]()` has not been removed (though it is not being used).\n\nThis PR updates the pass to remove the function node completely. The updated graph looks as follows:\n```\ngraph(%0 : Long(requires_grad=0, device=cpu),\n      %1 : Long(requires_grad=0, device=cpu)):\n  %8 : int = prim::Constant[value=1]()\n  %z.1 : Tensor = aten::sub(%0, %1, %8) # test/onnx/test_utility_funs.py:790:20\n  %10 : Tensor = aten::add(%0, %z.1, %8) # test/onnx/test_utility_funs.py:791:23\n  return (%10)\n```\n\nA test point has also been added for this scenario.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42146\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22845314\n\nPulled By: bzinodev\n\nfbshipit-source-id: 81fb351f0a36f47204e5327b60b84d7a91d3bcd9", "pr_number": "42146", "files_changed": ["test/onnx/test_utility_funs.py", "torch/csrc/jit/passes/onnx/function_substitution.cpp"], "labels": ["merged", "oncall: jit", "open source", "triaged"]}, "55d2a732cd": {"title": "Skip part of test_figure[_list] if Matplotlib-3.3.0 is installed (#42500)", "body": "Summary:\nSee https://github.com/matplotlib/matplotlib/issues/18163 for more details\nFixes https://github.com/pytorch/pytorch/issues/41680\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42500\n\nReviewed By: ezyang\n\nDifferential Revision: D22915857\n\nPulled By: malfet\n\nfbshipit-source-id: 4f8858b7b0018c6958a49f908de81a13a29e6046", "pr_number": "42500", "files_changed": ["test/test_tensorboard.py"], "labels": ["merged"]}, "842759591d": {"title": "[ONNX] Refactor ONNX fixup for Loop and If (#40943)", "body": "Summary:\n* move both under new file `fixup_onnx_controlflow`\n* move the fixup to where the ONNX loop/if node is created, as oppose to running the fixup as postpass. This will help with enable onnx shape inference later.\n* move `fuseSequenceSplitConcat` to `Peephole`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40943\n\nReviewed By: mrshenli\n\nDifferential Revision: D22709999\n\nPulled By: bzinodev\n\nfbshipit-source-id: 51d316991d25dc4bb4047a6bb46ad1e2401d3d2d", "pr_number": "40943", "files_changed": ["tools/build_variables.bzl", "torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.h", "torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.h", "torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_loop.h", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/python/init.cpp", "torch/onnx/utils.py"], "labels": ["merged", "module: onnx", "oncall: jit", "open source", "triaged"]}, "ae67f4c8b8": {"title": "Revert D22845258: [pytorch][PR] [ONNX] Enable scripting tests and update jit passes", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22845258 (https://github.com/pytorch/pytorch/commit/04e55d69f94f0e74bfb2bb0e64e13dbfbf5761d5)\n\nOriginal commit changeset: d57fd4086f27\n\nfbshipit-source-id: 15aa5cdae496a5e8ce2d8739a06dd4a7edc2200c", "pr_number": null, "files_changed": ["test/jit/test_onnx_export.py", "test/onnx/test_models.py", "test/onnx/test_models_onnxruntime.py", "test/onnx/test_pytorch_common.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/utils.py"], "labels": []}, "8850fd1952": {"title": "Add python inferface to create OfflineTensor (#42516)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42516\n\natt. We need it for some scripts.\n\nReviewed By: houseroad\n\nDifferential Revision: D22918112\n\nfbshipit-source-id: 8a1696ceeeda67a34114bc57cb52c925711cfb4c", "pr_number": "42516", "files_changed": ["caffe2/onnx/offline_tensor.h", "caffe2/python/pybind_state.cc", "caffe2/python/workspace.py"], "labels": ["fb-exported", "merged"]}, "d21e345ef0": {"title": "Fix segfault in `THPGenerator_dealloc` (take 2) (#42510)", "body": "Summary:\nSegfault happens when one tries to deallocate uninitialized generator.\nMake `THPGenerator_dealloc` UBSAN-safe by moving implicit cast in the struct definition to reinterpret_cast\n\nAdd `TestTorch.test_invalid_generator_raises` that validates that Generator created on invalid device is handled correctly\n\nFixes https://github.com/pytorch/pytorch/issues/42281\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42510\n\nReviewed By: pbelevich\n\nDifferential Revision: D22917469\n\nPulled By: malfet\n\nfbshipit-source-id: 5eaa68eef10d899ee3e210cb0e1e92f73be75712", "pr_number": "42510", "files_changed": ["test/test_torch.py", "torch/csrc/Generator.cpp"], "labels": ["merged"]}, "0c48aa1e07": {"title": "Add typing annotations to hub.py and _jit_internal.py (#42252)", "body": "Summary:\nxref: https://github.com/pytorch/pytorch/wiki/Guide-for-adding-type-annotations-to-PyTorch\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42252\n\nReviewed By: malfet\n\nDifferential Revision: D22916480\n\nPulled By: ezyang\n\nfbshipit-source-id: 392ab805b0023640a3b5cdf600f70638b375f84f", "pr_number": "42252", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in", "torch/_jit_internal.py", "torch/hub.py"], "labels": ["merged", "module: typing", "open source"]}, "01cd613e7e": {"title": "Batching rules for: T, view, view_as, reshape, reshape_as (#42458)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42458\n\nTest Plan: - `pytest test/test_vmap.py -v -k \"Operators\"`\n\nReviewed By: ezyang\n\nDifferential Revision: D22898715\n\nPulled By: zou3519\n\nfbshipit-source-id: 47f374962697dcae1d5aec80a41085679d016f92", "pr_number": "42458", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "f1d7f001b9": {"title": "Batching rules for: torch.movedim, torch.narrow, Tensor.unfold (#42474)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42474\n\nTest Plan: - `pytest test/test_vmap.py -v -k \"Operators\"`\n\nReviewed By: ezyang\n\nDifferential Revision: D22903513\n\nPulled By: zou3519\n\nfbshipit-source-id: 06b3fb0c7d12b9a045c73a5c5a4f4e3207e07b02", "pr_number": "42474", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "f3e8fff0d2": {"title": "Batching rules for: chunk, split, unbind (#42480)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42480\n\nThese are grouped together because they all return a tuple of multiple\ntensors.\n\nThis PR implements batching rules for chunk, split, and unbind. It also\nupdates the testing logic. Previously, reference_vmap was not able to\nhandle multiple outputs, now, it does.\n\nTest Plan: - `pytest test/test_vmap.py -v -k \"Operators\"`\n\nReviewed By: ezyang\n\nDifferential Revision: D22905401\n\nPulled By: zou3519\n\nfbshipit-source-id: 9963c943d035e9035c866be74dbdf7ab1989f8c4", "pr_number": "42480", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "aten/src/ATen/VmapTransforms.cpp", "aten/src/ATen/VmapTransforms.h", "test/test_vmap.py"], "labels": ["merged"]}, "b56db305cf": {"title": "Improve the documentation of DistributedDataParallel (#42471)", "body": "Summary:\nFixes #{issue number}\n\nIt's not clear by illustrating 'gradients from each node are averaged' in the documentation of DistributedDataParallel. Many people, including me, have a totally wrong understanding on this part. I add a note into the documentation to make it more straight forward and more user friendly.\n\nHere is some toy code to illustrate my point:\n\n* non-DistributedDataParallel version\n    ```python\n    import torch\n    import torch.nn as nn\n\n    x = torch.tensor([-1, 2, -3, 4], dtype=torch.float).view(-1, 1)\n    print(\"input:\", x)\n\n    model = nn.Linear(in_features=1, out_features=1, bias=False)\n    model.weight.data.zero_()\n    model.weight.data.add_(1.0)\n\n    opti = torch.optim.SGD(model.parameters(), lr=0.001)\n    opti.zero_grad()\n\n    y = model(x)\n\n    label = torch.zeros(4, 1, dtype=torch.float)\n    loss = torch.sum((y - label)**2)\n\n    loss.backward()\n    opti.step()\n\n    print(\"grad:\", model.weight.grad)\n    print(\"updated weight:\\n\", model.weight)\n\n    # OUTPUT\n    # $ python test.py\n    # input: tensor([[-1.],\n    #         [ 2.],\n    #         [-3.],\n    #         [ 4.]])\n    # grad: tensor([[60.]])\n    # updated weight:\n    #  Parameter containing:\n    # tensor([[0.9400]], requires_grad=True)\n    ```\n\n* DistributedDataParallel version\n    ```python\n    import os\n    import torch\n    import torch.nn as nn\n    import torch.distributed as dist\n    from torch.multiprocessing import Process\n\n    def run(rank, size):\n        x = torch.tensor([-(1 + 2 * rank), 2 + 2 * rank], dtype=torch.float).view(-1, 1)\n        print(\"input:\", x)\n\n        model = nn.Linear(in_features=1, out_features=1, bias=False)\n        model.weight.data.zero_()\n        model.weight.data.add_(1.0)\n        model = torch.nn.parallel.DistributedDataParallel(model)\n\n        opti = torch.optim.SGD(model.parameters(), lr=0.001)\n        opti.zero_grad()\n\n        y = model(x)\n\n        label = torch.zeros(2, 1, dtype=torch.float)\n        loss = torch.sum((y.view(-1, 1) - label)**2)\n\n        loss.backward()\n        opti.step()\n\n        if rank == 0:\n            print(\"grad:\", model.module.weight.grad)\n            print(\"updated weight:\\n\", model.module.weight)\n\n    def init_process(rank, size, fn, backend=\"gloo\"):\n        os.environ['MASTER_ADDR'] = '127.0.0.1'\n        os.environ['MASTER_PORT'] = '29500'\n        dist.init_process_group(backend, rank=rank, world_size=size)\n        fn(rank, size)\n\n    if __name__ == \"__main__\":\n        size = 2\n        process = []\n        for rank in range(size):\n            p = Process(target=init_process, args=(rank, size, run))\n            p.start()\n            process.append(p)\n\n        for p in process:\n            p.join()\n\n    # OUTPUT\n    # $ python test_d.py\n    # input: tensor([[-3.],\n    #         [ 4.]])input: tensor([[-1.],\n    #         [ 2.]])\n\n    # grad: tensor([[30.]])\n    # updated weight:\n    #  Parameter containing:\n    # tensor([[0.9700]], requires_grad=True)\n    ```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42471\n\nReviewed By: glaringlee\n\nDifferential Revision: D22923340\n\nPulled By: mrshenli\n\nfbshipit-source-id: 40b8c8ba63a243f857cd5976badbf7377253ba82", "pr_number": "42471", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["merged", "open source"]}, "ecb88c5d11": {"title": "Add NCCL Alltoall to PT NCCL process group (#42514)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42514\n\nAdd Alltoall and Alltoallv to PT NCCL process group using NCCL Send/Recv.\n\nReviewed By: mrshenli\n\nDifferential Revision: D22917967\n\nfbshipit-source-id: 402f2870915bc237845864a4a27c97df4351d975", "pr_number": "42514", "files_changed": ["test/distributed/test_distributed.py", "torch/lib/c10d/NCCLUtils.hpp", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/ProcessGroupRoundRobin.cpp", "torch/lib/c10d/ProcessGroupRoundRobin.hpp"], "labels": ["fb-exported", "merged"]}, "ec898b1ab5": {"title": "fix discontiguous inputs/outputs for cummin/cummax (#42507)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42363\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42507\n\nReviewed By: mruberry\n\nDifferential Revision: D22917876\n\nPulled By: ngimel\n\nfbshipit-source-id: 05f3f4a55bcddf6a853552184c9fafcef8d36270", "pr_number": "42507", "files_changed": ["aten/src/ATen/native/cuda/ScanKernels.cu", "test/test_torch.py"], "labels": ["merged"]}, "d2a2ac4eea": {"title": "Fix read/write bulk data (#42504)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42504\n\nReviewed By: glaringlee\n\nDifferential Revision: D22922750\n\nPulled By: mrshenli\n\nfbshipit-source-id: 9008fa22c00513bd75c3cf88a3081184cd72b0e3", "pr_number": "42504", "files_changed": ["torch/lib/c10d/FileStore.cpp"], "labels": ["merged", "open source"]}, "94e8676a70": {"title": "Initialize uninitialized variable (#42419)", "body": "Summary:\nFixes internal T70924595\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42419\n\nReviewed By: allwu, Krovatkin\n\nDifferential Revision: D22889325\n\nPulled By: wconstab\n\nfbshipit-source-id: 108b6a6c6bb7c98d77e22bae9974a6c00bc296f0", "pr_number": "42419", "files_changed": ["torch/csrc/jit/passes/graph_fuser.cpp"], "labels": ["merged", "oncall: jit"]}, "4b42a5b5a1": {"title": "Remove redundant kernels calling TypeDefault in VariableType codegen. (#42031)", "body": "Summary:\nWe have code snippet like below in VariableType_X.cpp\n```\nTensor __and___Scalar(const Tensor & self, Scalar other) {\n  auto result = TypeDefault::__and___Scalar(self, other);\n  return result;\n}\n TORCH_LIBRARY_IMPL(aten, Autograd, m) {\n  m.impl(\"__and__.Scalar\",\n         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(VariableType::__and___Scalar))\n  );\n```\nWe already register TypeDefault kernels as catchAll so they're not needed to be wrapped and register to Autograd key in VariableType.cpp. This PR removes the wrapper and registration in VariableType.cpp. (The ones in other files like TracedType.cpp remains the same).\nHere's a [diff in generated VariableTypeEverything.cpp](https://gist.github.com/ailzhang/18876edec4dad54e43a1db0c127c5707)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42031\n\nReviewed By: agolynski\n\nDifferential Revision: D22903507\n\nPulled By: ailzhang\n\nfbshipit-source-id: 04e6672b6c79e079fc0dfd95c409ebca7f9d76fc", "pr_number": "42031", "files_changed": ["tools/autograd/gen_variable_type.py"], "labels": ["merged"]}, "5939d8a3e0": {"title": "Revert \"Revert D22360735: .circleci: Build docker images as part of C\u2026 (#40950)", "body": "Summary:\n\u2026I workflow\"\n\nThis reverts commit 3c6b8a64964b0275884359dd6a5bf484655d8c7c.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40950\n\nReviewed By: malfet\n\nDifferential Revision: D22909883\n\nPulled By: seemethere\n\nfbshipit-source-id: 93c070400d7fbe1753f88c3291ab5eba4ab237fa", "pr_number": "40950", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/cimodel/data/simple/android_definitions.py", ".circleci/cimodel/data/simple/bazel_definitions.py", ".circleci/cimodel/data/simple/mobile_definitions.py", ".circleci/cimodel/data/simple/nightly_android.py", ".circleci/cimodel/data/simple/util/docker_constants.py", ".circleci/config.yml", ".circleci/docker/build_docker.sh", ".circleci/docker/common/install_android.sh", ".circleci/docker/common/install_travis_python.sh", ".circleci/generate_config_yml.py", ".circleci/validate-docker-version.py", ".circleci/verbatim-sources/commands.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs/docker_jobs.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".github/workflows/lint.yml"], "labels": ["merged", "module: ci"]}, "38a9984451": {"title": "[TensorExpr] Properly handle all dtypes in evaluation of CompareSelect exprs. (#42493)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42493\n\nTest Plan: Imported from OSS\n\nReviewed By: nickgg\n\nDifferential Revision: D22910754\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: cf7073d6ea792998a9fa3989c7ec486419476de0", "pr_number": "42493", "files_changed": ["test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/eval.h"], "labels": ["merged", "oncall: jit"]}, "c334ebf1aa": {"title": "[TensorExpr] Properly handle all dtypes in evaluation of Intrinsics exprs. (#42494)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42494\n\nNote that we're currently assuming that dtypes of all the arguments and\nthe return value is the same.\n\nTest Plan: Imported from OSS\n\nReviewed By: nickgg\n\nDifferential Revision: D22910755\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 7f899692065428fbf2ad05d22b4ca39cab788ae5", "pr_number": "42494", "files_changed": ["test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/eval.h"], "labels": ["merged", "oncall: jit"]}, "b3ffebda7a": {"title": "[TensorExpr] Properly handle all dtypes of the condition in evaluation of IfThenElse exprs. (#42495)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42495\n\nTest Plan: Imported from OSS\n\nReviewed By: nickgg\n\nDifferential Revision: D22910753\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: f9ffd3dc4c50fb3fb84ce6d6916c1fbfd3201c8f", "pr_number": "42495", "files_changed": ["test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/eval.h"], "labels": ["merged", "oncall: jit"]}, "2b8e7e2f2d": {"title": "Moving ProcessGroupNCCLTest to Gtest (#42208)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42208\n\nProcessGroupNCCLTest is currently written without any testing framework, and all tests are simply called from the main function and throw exceptions upon failure. As a result, it is hard to debug and pinpoint which tests have succeeded/failed.\n\nThis PR moves ProcessGroupNCCLTest to gtest with appropriate setup and skipping functionality in the test superclass.\nghstack-source-id: 109097246\n\nTest Plan: Working Correctly on devGPU and devvm.\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22782673\n\nfbshipit-source-id: 85bd407f4534f3d339ddcdd65ef3d2022aeb7064", "pr_number": "42208", "files_changed": [".jenkins/pytorch/test.sh", "torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["merged"]}, "3ca361791f": {"title": "TearDown function for ProcessGroupNCCLTest Initializer (#42209)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42209\n\nThis PR adds a TearDown function to the testing superclass to ensure that the NCCL_BLOCKING_WAIT environment variable is reset after each test case.\nghstack-source-id: 109097247\n\nTest Plan: Working on devGPU and devvm.\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22782672\n\nfbshipit-source-id: 8f919a96d7112f9f167e90ce3df59886c88f3514", "pr_number": "42209", "files_changed": ["torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["merged"]}, "e97e87368e": {"title": "Clean up CUDA Sleep and Tensor Initialization in ProcessGroupNCCLTest (#42211)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42211\n\nHelper functions for launching CUDA Sleep and Tensor Value Initialization for the collective test functions.\n\nThis is more of a code cleanup fix compared to the previous diffs.\nghstack-source-id: 109097243\n\nTest Plan: working on devGPU and devvm\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22782671\n\nfbshipit-source-id: 7d88f568a4e08feae778669affe69c8d638973db", "pr_number": "42211", "files_changed": ["torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["merged"]}, "0d1a689764": {"title": "[vulkan] reshape op (#41223)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41223\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22754942\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 99fc5888803d6afe2a73bb5bbed6651d2ea98313", "pr_number": "41223", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/vulkan/Vulkan.cpp", "aten/src/ATen/native/vulkan/Vulkan.h", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanAten.h", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/VulkanOps.h", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "91d87292a6": {"title": "[vulkan][asan] Fix Invalid Memory ops (#41224)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41224\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22754940\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: f012b78a57f5f88897b2b6b91713090c8984a0bc", "pr_number": "41224", "files_changed": ["aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "a0695b34cd": {"title": ".circleci: Have python docs always push to site (#42552)", "body": "Summary:\nWas getting an error when attempting to push to master for\npytorch/pytorch.github.io since the main branch on that repository is\nactually site and not master.\n\nGet rid of the loop too since the loop wasn't going to work with a\nconditional and conditionals on a two variable loop just isn't worth the\nreadability concerns\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42552\n\nReviewed By: malfet\n\nDifferential Revision: D22929503\n\nPulled By: seemethere\n\nfbshipit-source-id: acdd26b86718304eac9dcfc81761de0b3e609004", "pr_number": "42552", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["merged", "module: ci"]}, "e995c3d21e": {"title": "Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar) (#41554)", "body": "Summary:\nInitial PR for the Tensor List functionality.\n\n**Motivation**\n[GitHub issue](https://github.com/pytorch/pytorch/issues/38655)\nCurrent PyTorch optimizer implementations are not efficient in cases when we work with a lot of small feature tensors. Starting a lot of kernels slows down the whole process. We need to reduce the number of kernels that we start.\nAs an example, we should be looking at [NVIDIAs Apex](https://github.com/NVIDIA/apex).\nIn order to track progress, we will pick PyTorchs DCGAN model with Adam optimizer and once the optimizer is reimplemented with tensor lists, benchmark the model performance against original model version, Apexs version with original Adam optimizer and it\u2019s FusedAdam optimizer.\n\n**In this PR**\n- Adding `multi_tensor_apply` mechanism which will help to efficiently apply passed functor on a given list of tensors on CUDA.\n- Adding a first private API - `std::vector<Tensor> _foreach_add(TensorList tensors, Scalar scalar)`\n\n**Tests**\nTested via unit tests\n\n**Plan for the next PRs**\n\n1. Cover these ops with `multi_tensor_apply` support\n- exponent\n- division\n- mul_\n- add_\n- addcmul_\n- addcdiv_\n- Sqrt\n\n2. Rewrite PyTorch optimizers to use for-each operators in order to get performance gains.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41554\n\nReviewed By: cpuhrsch\n\nDifferential Revision: D22829724\n\nPulled By: izdeby\n\nfbshipit-source-id: 47febdbf7845cf931958a638567b7428a24782b1", "pr_number": "41554", "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/cuda/ForeachTensorAddScalar.cu", "aten/src/ATen/native/cuda/ForeachUtils.cuh", "aten/src/ATen/native/cuda/MultiTensorApply.cuh", "aten/src/ATen/native/native_functions.yaml", "test/run_test.py", "test/test_foreach.py"], "labels": ["merged", "module: windows"]}, "56fc7d0345": {"title": "Fix doc build (#42559)", "body": "Summary:\nAdd space between double back quotes and left curly bracket\n\nOtherwise doc generation failed with `Inline literal start-string without end-string.`\n\nThis regression was introduced by https://github.com/pytorch/pytorch/commit/b56db305cf441453a84ae0b55f708ac0998cafc0\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42559\n\nReviewed By: glaringlee\n\nDifferential Revision: D22931527\n\nPulled By: malfet\n\nfbshipit-source-id: 11c04a92dbba48592505f704d77222cf92a81055", "pr_number": "42559", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["merged"]}, "317b9d3bfc": {"title": "Implement sort for string in aten (#42398)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42375\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42398\n\nReviewed By: ailzhang\n\nDifferential Revision: D22884849\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: e53386949f0a5e166f3d1c2aa695294340bd1440", "pr_number": "42398", "files_changed": ["test/test_jit.py", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["merged", "oncall: jit"]}, "b9e68e03c4": {"title": "Fix the bug in THCTensor_(baddbmm) and ATen's addmm_cuda for strided views input (#42425)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42418.\n\nThe problem was that the non-contiguous batched matrices were passed to `gemmStridedBatched`.\n\nThe following code fails on master and works with the proposed patch:\n```python\nimport torch\nx = torch.tensor([[1., 2, 3], [4., 5, 6]], device='cuda:0')\nc = torch.as_strided(x, size=[2, 2, 2], stride=[3, 1, 1])\ntorch.einsum('...ab,...bc->...ac', c, c)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42425\n\nReviewed By: glaringlee\n\nDifferential Revision: D22925266\n\nPulled By: ngimel\n\nfbshipit-source-id: a72d56d26c7381b7793a047d76bcc5bd45a9602c", "pr_number": "42425", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/THC/generic/THCTensorMathBlas.cu", "test/test_torch.py"], "labels": ["merged", "open source"]}, "ccc831ae35": {"title": "test: Disable test_strided_grad_layout on ROCM (#42561)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42561\n\nRegression was introduced as part of https://github.com/pytorch/pytorch/commit/5939d8a3e09b55a4889f6e43b15c1dc87c1f31ce, logs: https://app.circleci.com/pipelines/github/pytorch/pytorch/196558/workflows/9a2dd56e-86af-4d0f-9fb9-b205dcd12f93/jobs/6502042\n\nGoing to go ahead and disable the test to give rocm folks time to investigate what's going on\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D22932615\n\nPulled By: seemethere\n\nfbshipit-source-id: 41150f3085f848cce75990716362261fea9391a0", "pr_number": "42561", "files_changed": ["test/distributed/test_data_parallel.py"], "labels": ["merged"]}, "afa489dea9": {"title": "[ONNX] Enable lower_tuple pass for custom layer (#41548)", "body": "Summary:\nCustom layer by `torch.autograd.Function` appears in the lower_tuple as `prim::PythonOp`. Adding this op type to the allowed list to enable lower_tuple pass. This helps with exporting custom layer with tuple outputs.\n\nE.g.\n```python\nimport torch\nclass CustomFunction(torch.autograd.Function):\n    staticmethod\n    def symbolic(g, input):\n        return g.op('CustomNamespace::Custom', input, outputs=2)\n    staticmethod\n    def forward(ctx, input):\n        return input, input\nclass Custom(torch.nn.Module):\n    def forward(self, input):\n        return CustomFunction.apply(input)\n\nmodel = Custom()\nbatch = torch.FloatTensor(1, 3)\ntorch.onnx.export(model, batch, \"test.onnx\", verbose=True)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41548\n\nReviewed By: glaringlee\n\nDifferential Revision: D22926143\n\nPulled By: bzinodev\n\nfbshipit-source-id: ce14d1d3c70a920154a8235d635ab31ddf0c46f3", "pr_number": "41548", "files_changed": ["test/onnx/test_utility_funs.py", "torch/csrc/jit/passes/lower_tuples.cpp"], "labels": ["merged", "module: onnx", "oncall: jit", "open source", "triaged"]}, "29700c0092": {"title": "[JIT] Fix torch.jit.is_tracing() (#42486)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42486\n\n**Summary**\nThis commit fixes a small bug in which `torch.jit.is_tracing()` returns\n`torch._C.is_tracing`, the function object, instead of calling the\nfunction and returning the result.\n\n**Test Plan**\nContinuous integration?\n\n**Fixes**\nThis commit fixes #42448.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D22911062\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: b94eca0c1c65ca6f22acc6c5542af397f2dc37f0", "pr_number": "42486", "files_changed": ["torch/jit/_trace.py"], "labels": ["merged", "oncall: jit"]}, "61027a1a59": {"title": "Install typing_extensions in PyTorch CI (#42551)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42551\n\nReviewed By: seemethere\n\nDifferential Revision: D22929256\n\nPulled By: malfet\n\nfbshipit-source-id: 9a6f8c56ca1c0fb8a8569614a34a12f2769755f3", "pr_number": "42551", "files_changed": [".jenkins/pytorch/macos-common.sh", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat"], "labels": ["merged"]}, "d45e2d3ef9": {"title": "Reduce the output overhead of OutputColumnMaxHistogramObserver by enabling changing bin_nums, Update the observer_test.py", "body": "Summary: Current OutputColumnMaxHistogramObserver will output 2048 bins for each column. The file will be extremely large and the dumping time is quite long. However, we only use the min and max finally. This diff enables changing bin_nums by adding an argument. And the default value is set to 16 to reduce dumping overhead. When we need more bins to analyze the results, we only need to change this argument\n\nTest Plan:\nbuck run caffe2/caffe2/quantization/server:observer_test\n\n{F263843430}\n\nReviewed By: hx89\n\nDifferential Revision: D22918202\n\nfbshipit-source-id: bda34449355b269b24c55802012450ebaa4d280c", "pr_number": null, "files_changed": ["caffe2/quantization/server/observer_test.py", "caffe2/quantization/server/pybind.cc"], "labels": []}, "78f4cff8fe": {"title": "handle multiple returns properly in boxing wrappers (#42437)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42437\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D22894191\n\nPulled By: bhosmer\n\nfbshipit-source-id: fd4c7bc605a4b20bb3882f71e3b8874150671324", "pr_number": "42437", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/impl/boxing.h"], "labels": ["merged"]}, "3c7fccc1c2": {"title": "Reenable cusparse SpMM on cuda 10.2 (#42556)", "body": "Summary:\nThis fixes feature regression introduced by https://github.com/pytorch/pytorch/issues/42412 which limited all the use of the API to CUDA-11.0+\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42556\n\nReviewed By: ngimel\n\nDifferential Revision: D22932129\n\nPulled By: malfet\n\nfbshipit-source-id: 2756e0587456678fa1bc7deaa09d0ea482dfd19f", "pr_number": "42556", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu"], "labels": ["merged"]}, "882ad117cf": {"title": "[TensorExpr] Fix a way we were createing np arrays in tests. (#42575)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42575\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D22939119\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 3388270af8eae9fd4747f06202f366887aaf5f36", "pr_number": "42575", "files_changed": ["test/test_tensorexpr.py"], "labels": ["merged"]}, "0f358fab6b": {"title": "Hide cudnn symbols in libtorch_cuda.so when statically linking cudnn (#41986)", "body": "Summary:\nThis PR intends to fix https://github.com/pytorch/pytorch/issues/32983.\n\nThe initial (one-line) diff causes statically linked cudnn symbols in `libtorch_cuda.so` to have local linkage (such that they shouldn't be visible to external libraries during dynamic linking at load time), at least in my source build on Ubuntu 20.04.\n\nProcedure I used to verify:\n```\nexport USE_STATIC_CUDNN=ON\npython3 setup.py install\n...\n```\nthen\n```\nmcarilli@mcarilli-desktop:~/Desktop/mcarilli_github/pytorch/torch/lib$ nm libtorch_cuda.so | grep cudnnCreate\n00000000031ff540 t cudnnCreate\n00000000031fbe70 t cudnnCreateActivationDescriptor\n```\nBefore the diff they were marked with capital `T`s indicating external linkage.\n\nCaveats:\n- The fix is gcc-specific afaik.  I have no idea how to enable it for Windows or other compilers.\n- Hiding the cudnn symbols will break external C++ applications that rely on linking `libtorch.so` to supply cudnn symbol definitions.  IMO this is \"off menu\" usage so I don't think it's a major concern.  Hiding the symbols _won't_ break applications that call cudnn indirectly through torch functions, which IMO is the \"on menu\" way.\n- I know _very little_ about the build system.  The diff's intent is to add a link option that applies to any Pytorch `.so`s that statically link cudnn, and does so on Linux only.  I'm blindly following soumith 's recommendation https://github.com/pytorch/pytorch/issues/32983#issuecomment-662056151, and post-checking the built libs (I also added `set(CMAKE_VERBOSE_MAKEFILE ON)` to the top-level CMakeLists.txt at one point to confirm `-Wl,--exclude-libs,libcudnn_static.a` was picked up by the command that linked `libtorch_cuda.so`).\n- https://github.com/pytorch/pytorch/issues/32983 (which used a Pytorch 1.4 binary build) complained about `libtorch.so`, not `libtorch_cuda.so`:\n    ```\n    nvpohanh@ubuntu:~$ nm /usr/local/lib/python3.5/dist-packages/torch/lib/libtorch.so | grep ' cudnnCreate'\n    000000000f479c30 T cudnnCreate\n    000000000f475ff0 T cudnnCreateActivationDescriptor\n    ```\n  In my source build, `libtorch.so` ends up small, containing no cudnn symbols (this is true with or without the PR's diff), which contradicts https://github.com/pytorch/pytorch/issues/32983.  Maybe the symbol organization (what goes in   `libtorch.so` vs `libtorch_cuda/cpu/whatever.so`) changed since 1.4.  Or maybe the symbol organization is different for source vs binary builds, in which case I have no idea if this PR's diff has the same effect for a binary build.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41986\n\nReviewed By: glaringlee\n\nDifferential Revision: D22934926\n\nPulled By: malfet\n\nfbshipit-source-id: 711475834e0f8148f0e5f2fe28fca5f138ef494b", "pr_number": "41986", "files_changed": ["cmake/public/cuda.cmake"], "labels": ["merged", "module: cuda", "open source", "triaged"]}, "b85216887b": {"title": "[vulkan] max_pool2d (#41379)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41379\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22754944\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 5261337bb731a207a1532e6423c0d33f1307e413", "pr_number": "41379", "files_changed": ["aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanAten.h", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/VulkanOps.h", "aten/src/ATen/native/vulkan/glsl/max_pool2d.glsl", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "0cf71eb547": {"title": "Unconditinally use typing extensions in jit_internal (#42538)", "body": "Summary:\nSince https://github.com/pytorch/pytorch/issues/38221 is closed now, `typing_extensions` module should always be available\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42538\n\nReviewed By: ezyang\n\nDifferential Revision: D22942153\n\nPulled By: malfet\n\nfbshipit-source-id: edabbadde13800a3412d14c19ca55ef206ada5e1", "pr_number": "42538", "files_changed": ["torch/_jit_internal.py"], "labels": ["merged", "module: typing"]}, "924a1dbe9b": {"title": "Revert D22939119: [TensorExpr] Fix a way we were createing np arrays in tests.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22939119 (https://github.com/pytorch/pytorch/commit/882ad117cfcb2d4f63dc6016d7fdbd432de21e20)\n\nOriginal commit changeset: 3388270af8ea\n\nfbshipit-source-id: 7c8d159586ce2c4c21184fd84aa6da5183bc71ea", "pr_number": null, "files_changed": ["test/test_tensorexpr.py"], "labels": []}, "db52cd7322": {"title": ".circleci: Hardcode rocm image to previous tag (#42603)", "body": "Summary:\nThere were some inconsistencies with the newer docker images so it'd be\nbest to stick with something that works without reverting the entire\ndocker builder PR\n\nThis was made after the previous efforts to disable the tests that were failing:\n* https://github.com/pytorch/pytorch/pull/42583\n* https://github.com/pytorch/pytorch/pull/42561\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42603\n\nReviewed By: ezyang\n\nDifferential Revision: D22948743\n\nPulled By: seemethere\n\nfbshipit-source-id: cc8b834e0c8a6a4763f5ba07ce220a9c192ea6eb", "pr_number": "42603", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged", "module: ci", "module: rocm"]}, "b08347fd7b": {"title": "Add CUDA 11 builds for Windows CI (#42420)", "body": "Summary:\nStacked on https://github.com/pytorch/pytorch/pull/42410.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42420\n\nReviewed By: seemethere\n\nDifferential Revision: D22917230\n\nPulled By: malfet\n\nfbshipit-source-id: 6ad394f7f8c430c587e0b0d9c5a5e7b7bcd85bfe", "pr_number": "42420", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/driver_update.bat", ".circleci/scripts/windows_cuda_install.sh", ".circleci/scripts/windows_cudnn_install.sh", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_magma.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "cmake/ProtoBufPatch.cmake", "test/test_sparse.py"], "labels": ["merged", "open source"]}, "dae94ed022": {"title": "Keep manual_kernel_registration only effective in aten codegen. (#42386)", "body": "Summary:\nThis PR removes manual registration in aten/native codebase.\nAnd it separates manual device/catchall kernel registration from manual VariableType kernel registration.\nThe first one remains as manual_kernel_registration in native_functions.yaml.\nThe second one is moved to tools/ codegen.\n\nDifference in generated TypeDefault.cpp: https://gist.github.com/ailzhang/897ef9fdf0c834279cd358febba07734\nNo difference in generated VariableType_X.cpp\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42386\n\nReviewed By: agolynski\n\nDifferential Revision: D22915649\n\nPulled By: ailzhang\n\nfbshipit-source-id: ce93784b9b081234f05f3343e8de3c7a704a5783", "pr_number": "42386", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/README.md", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/TensorProperties.cpp", "aten/src/ATen/native/native_functions.yaml", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/TraceTypeManual.cpp", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "27e8dc78ca": {"title": "[vulkan] VulkanTensor lazy buffer allocation (#42569)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42569\n\nWe do not need to allocate buffers for Vulkan Tensors if they are not the forward input or output.\nRemoving allocate_storage() for outputs of operations by default, their image representation will have the result.\nAllocating buffer only if it was requested for the operations (For some ops like concatenate, transpose) or copy to host.\n\n`VulkanTensor.image()` if buffer was not allocated - just allocates texture skipping copy from buffer to texture.\nAs allocate storage was before for all operations - we are saving buffer allocation and buffer_to_image call.\n\nMobilNetV2 on my Pixel4:\n```\nflame:/data/local/tmp $ ./speed_benchmark_torch  --model=mnfp32-vopt.pt --input_type=float --input_dims=1,3,224,224 --warmup=3 --iter=20 --vulkan=true\nStarting benchmark.\nRunning warmup runs.\nMain runs.\nMain run finished. Microseconds per iter: 305818. Iters per second: 3.26991\nSegmentation fault\n```\n```\n139|flame:/data/local/tmp $ ./speed_benchmark_torch_noas  --model=mnfp32-vopt.pt --input_type=float --input_dims=1,3,224,224 --warmup=3 --iter=20 --vulkan=true\nStarting benchmark.\nRunning warmup runs.\nMain runs.\nMain run finished. Microseconds per iter: 236768. Iters per second: 4.22355\nSegmentation fault\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22946552\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: ac0743bb316847632a22cf9aafb8938e50b2fb7b", "pr_number": "42569", "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.cpp", "aten/src/ATen/native/vulkan/Vulkan.h", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanOps.cpp"], "labels": ["merged"]}, "06d978a9ad": {"title": "[c10/cuda] Reorganize device_count() and robustly surface ASAN warnings (#42249)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42249\n\nMain change is to bring Caffe2's superior error messages for cuda initialization into c10 and use them in all code paths.\n\nBasic logic:\n\n| Case | Call to device_count() | init_cuda, e.g. allocating tensor |\n| -- | -- | -- |\n| all good | non-zero | just works |\n| no gpus | 0, no warning | throw exception with good message |\n| driver issues | 0, produce warning | throw exception with good message |\n| out of memory with ASAN | 0, produce warning| throw exception with ASAN message |\n\nPreviously, the error thrown from init_cuda was very generic and the ASAN warning (if any) was buried in the logs.\n\nOther clean up changes:\n* cache device_count() always in a static variable\n* move all asan macros in c10\n\nTest Plan:\nHard to unittest because of build modes. Verified manually that the behavior from the table above holds by running the following script in different modes (ASAN/no-ASAN, CUDA_VISIBLE_DEVICES=):\n\n```\nprint('before import')\nimport torch\nprint('after import')\nprint('devices: ', torch.cuda.device_count())\nx = torch.tensor([1,2,3])\nprint('tensor creation')\nx = x.cuda()\nprint('moved to cuda')\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D22824329\n\nfbshipit-source-id: 5314007313a3897fc955b02f8b21b661ae35fdf5", "pr_number": "42249", "files_changed": ["aten/src/THC/THCGeneral.cpp", "c10/cuda/CMakeLists.txt", "c10/cuda/CUDAFunctions.cpp", "c10/cuda/CUDAFunctions.h", "c10/macros/Macros.h", "caffe2/core/asan.h", "caffe2/core/common_gpu.cc", "caffe2/core/context_gpu.cu", "caffe2/operators/index_hash_ops.h", "caffe2/python/pybind_state.cc", "torch/csrc/cuda/Module.cpp", "torch/cuda/__init__.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["fb-exported", "merged"]}, "76905527fe": {"title": "Fix illegal memory acess issue for CUDA versionn of SplitByLengths operator.", "body": "Summary:\n1. Fix illegal memory access issue for SplitByLengths operator in the CUDA context.\n2. Add support to scaling lengths vector for SplitByLengths operator.\n3. Add support to test SplitByLengths operator in the CUDA context.\n\nExample for SplitByLengths operator processing scaling lengths vector:\nvalue vector A = [1, 2, 3, 4, 5, 6]\nlength vector B = [1, 2]\nafter execution of SplitByLengths operator,\nthe output should be [1,2] and [3,4,5,6]\n\nTest Plan: buck test mode/dev-nosan caffe2/caffe2/python/operator_test:concat_split_op_test\n\nReviewed By: kennyhorror\n\nDifferential Revision: D22780307\n\nfbshipit-source-id: c5ca60ae16b24032cedfa045a421503b713daa6c", "pr_number": null, "files_changed": ["caffe2/operators/concat_split_op.cc", "caffe2/operators/concat_split_op.h", "caffe2/python/operator_test/concat_split_op_test.py"], "labels": []}, "6d1e43c5a6": {"title": "Release the GIL before invokeOperator (#42341)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41865\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42341\n\nReviewed By: ezyang\n\nDifferential Revision: D22928622\n\nPulled By: wconstab\n\nfbshipit-source-id: 8fa41277c9465f816342db6ec0e6cd4b30095c5c", "pr_number": "42341", "files_changed": ["torch/csrc/jit/python/pybind_utils.h"], "labels": ["merged", "oncall: jit"]}, "5c5d7a9dca": {"title": "Freeze dynamic (re)quantizaiton ops into standard ones (#42591)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42591\n\nWe don't support lowering with 2-input Int8Quantize and 4-input Int8FC. Just do a conversion to absorb the quantization params into the op itself.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/quantization/server:quantize_dnnlowp_op_test\n```\n\nReviewed By: benjibc\n\nDifferential Revision: D22942673\n\nfbshipit-source-id: a392ba2afdfa39c05c5adcb6c4dc5f814c95e449", "pr_number": "42591", "files_changed": ["caffe2/opt/custom/freeze_quantization_params.cc", "caffe2/opt/custom/freeze_quantization_params.h", "caffe2/opt/custom/glow_net_transform.h", "caffe2/quantization/server/pybind.cc", "caffe2/quantization/server/quantize_dnnlowp_op_test.py"], "labels": ["fb-exported", "merged"]}, "7c33225c72": {"title": "Add strict mypy type checking and update code_template.py (#42322)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42322\n\nOur current type checking rules are rather lax, and for\nexample don't force users to make sure they annotate all functions\nwith types.  For code generation code, it would be better to force\n100% typing.  This PR introduces a new mypy configuration\nmypy-strict.ini which applies rules from --strict.  We extend\ntest_type_hints.py to test for this case.  It only covers\ncode_template.py, which I have made strict clean in this PR.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D22846120\n\nPulled By: ezyang\n\nfbshipit-source-id: 8d253829223bfa0d811b6add53b7bc2d3a4356b0", "pr_number": "42322", "files_changed": ["aten/src/ATen/code_template.py", "aten/src/ATen/function_wrapper.py", "mypy-strict.ini", "test/test_type_hints.py"], "labels": ["merged", "module: typing"]}, "18a32b807b": {"title": "Add API to collect output_col_minmax_histogram", "body": "Summary:\nAdd an API to collect output_col_minmax_histogram. This is used to implement input_equalization.\n\nRoll back revised the collect_single_histogram in the new version to make sure it does not affect the product.\nThe newly added one can implement collect the activation histogram and output col max histogram at the same time.\n\nTest Plan:\nAdd a unit test, and pass it.\nhttps://our.intern.facebook.com/intern/testinfra/testrun/2251799847601374\nAfter updating the dump API, it passed the updated unit test\nhttps://our.intern.facebook.com/intern/testinfra/testrun/844425097716401\n\nIntegrated the output_col_minmax_histogram to the collect single histogram, and make it downward compatible\nhttps://our.intern.facebook.com/intern/testinfra/testrun/8162774342207893\n\nI added different cases to tested newly added function. It passed the unit test  https://our.intern.facebook.com/intern/testinfra/testrun/4503599658969000\n\nTested after new revision: https://our.intern.facebook.com/intern/testinfra/testrun/5348024589078557\n\nReviewed By: hx89\n\nDifferential Revision: D22919913\n\nfbshipit-source-id: c9cb05e0cf14af0dfde3d22921abb42f97a61df2", "pr_number": null, "files_changed": ["caffe2/quantization/server/activation_distribution_observer.h", "caffe2/quantization/server/pybind.cc"], "labels": []}, "7221a3d1aa": {"title": "enable torch.optim.swa_utils.SWALR (#42574)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42435\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42574\n\nReviewed By: zou3519\n\nDifferential Revision: D22949369\n\nPulled By: vincentqb\n\nfbshipit-source-id: f2f319ec94a97e0afe4d4327c866504ae632a986", "pr_number": "42574", "files_changed": ["torch/optim/__init__.py", "torch/optim/__init__.pyi"], "labels": ["merged", "module: optimizer"]}, "df7c059428": {"title": "Throw error if `torch.set_deterministic(True)` is called with nondeterministic CuBLAS config (#41377)", "body": "Summary:\nFor CUDA >= 10.2, the `CUBLAS_WORKSPACE_CONFIG` environment variable must be set to either `:4096:8` or `:16:8` to ensure deterministic CUDA stream usage. This PR adds some logic inside `torch.set_deterministic()` to raise an error if this environment variable is not set properly and CUDA >= 10.2.\n\nIssue https://github.com/pytorch/pytorch/issues/15359\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41377\n\nReviewed By: malfet\n\nDifferential Revision: D22758459\n\nPulled By: ezyang\n\nfbshipit-source-id: 4b96f1e9abf85d94ba79140fd927bbd0c05c4522", "pr_number": "41377", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/cuda/detail/CUDAHooks.h", "aten/src/ATen/detail/CUDAHooksInterface.h", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/THC/THCBlas.cu", "test/cpp/api/misc.cpp", "test/test_torch.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged", "open source", "triaged"]}, "24e2a8a171": {"title": "Revert D22780307: Fix illegal memory acess issue for CUDA versionn of SplitByLengths operator.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22780307 (https://github.com/pytorch/pytorch/commit/76905527fe4d1caab195b280f0ec897c6c4251a6)\n\nOriginal commit changeset: c5ca60ae16b2\n\nfbshipit-source-id: f3c99eec5f05121e2bed606fe2ba84a0be0cdf16", "pr_number": null, "files_changed": ["caffe2/operators/concat_split_op.cc", "caffe2/operators/concat_split_op.h", "caffe2/python/operator_test/concat_split_op_test.py"], "labels": []}, "feeb515ad5": {"title": "add Quantizer support to IValue (#42438)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42438\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D22894190\n\nPulled By: bhosmer\n\nfbshipit-source-id: b2d08abd6f582f29daa6cc7ebf05bb1a99f7514b", "pr_number": "42438", "files_changed": ["aten/src/ATen/core/boxing/impl/boxing.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "torch/csrc/jit/frontend/function_schema_parser.cpp", "torch/csrc/jit/frontend/schema_type_parser.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/serialization/unpickler.cpp"], "labels": ["merged", "oncall: jit"]}, "b9c49f0e69": {"title": "[TensorExpr] Support shape inference in TE for aten::cat. (#42387)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42387\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D22879281\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 775e46a4cfd91c63196b378ee587cc4434672c89", "pr_number": "42387", "files_changed": ["test/cpp/tensorexpr/test_kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["merged", "oncall: jit"]}, "ea9053b86d": {"title": "[TensorExpr] Handle constant nodes in shape inference. (#42566)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42566\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D22936176\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 69d0f9907de0e98f1fbd56407df235774cb5b788", "pr_number": "42566", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["merged", "oncall: jit"]}, "ef50694d44": {"title": "[TensorExpr] Apply GenericIntrinsicExpander recursively. (#42567)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42567\n\nBefore this change we didn't expand arguments, and thus in an expr\n`sigmoid(sigmoid(x))` only the outer call was expanded.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D22936177\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 9c05dc96561225bab9a90a407d7bcf9a89b078a1", "pr_number": "42567", "files_changed": ["test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/codegen.cpp", "torch/csrc/jit/tensorexpr/codegen.h"], "labels": ["merged", "oncall: jit"]}, "73351ee91d": {"title": "[TensorExpr] Disallow fallback to JIT interpreter from TensorExprKernel (flip the default). (#42568)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42568\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D22936175\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 62cb505acb77789ed9f483842a8b31eb245697b3", "pr_number": "42568", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["merged", "oncall: jit"]}, "edf6c4bc4d": {"title": "[RPC tests] Merge TensorPipe tests into single entry point (#40816)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40816\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nThis diff does the changes described above for the TensorPipe agent. It fixes its fixture (making it inherit from the generic fixture) and merges all the entry point scripts into a single one, so that it's easier to have a clear overview of all the test suites which we run on TensorPipe (you'll notice that many are missing: the JIT ones, the remote module one, ...).\nghstack-source-id: 109229476\n\nTest Plan: Sandcastle and CircleCI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22283180\n\nfbshipit-source-id: d5e9f9f4e6d4bfd6fbcae7ae56eed63d2567a02f", "pr_number": "40816", "files_changed": ["test/distributed/rpc/tensorpipe/test_ddp_under_dist_autograd.py", "test/distributed/rpc/tensorpipe/test_dist_autograd_spawn.py", "test/distributed/rpc/tensorpipe/test_dist_optimizer_spawn.py", "test/distributed/rpc/tensorpipe/test_rpc_spawn.py", "test/distributed/rpc/test_tensorpipe_agent.py", "test/run_test.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py", "torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py"], "labels": ["merged"]}, "b93c7c54eb": {"title": "[RPC tests] Merge tests for faulty agent into single script (#40817)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40817\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nThis diff does the changes described above for the faulty agent, which is its own strange beast. It merges all the test entry points (i.e., the combinations of agent, suite and fork/spawn) into a single file. It also modifies the test suites that are intended to be run only on the faulty agent, which used to inherit from its fixture, to inherit from the generic fixture, as they will be mixed in with the faulty fixture at the very end, inside the entry point script.\nghstack-source-id: 109229477\n\nTest Plan: Sandcastle and CircleCI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22283178\n\nfbshipit-source-id: 72659efe6652dac8450473642a578933030f2c74", "pr_number": "40817", "files_changed": ["test/distributed/rpc/faulty_agent/test_dist_autograd_spawn.py", "test/distributed/rpc/faulty_agent/test_rpc_spawn.py", "test/distributed/rpc/jit/faulty_agent/test_rpc_spawn.py", "test/distributed/rpc/test_faulty_agent.py", "test/run_test.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged", "oncall: jit"]}, "935fcc9580": {"title": "[RPC tests] Merge process group tests into single entry point (#40818)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40818\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nThis diff does the changes described above for the process group agent. It defines a fixture for it (instead of using the generic fixture in its default behavior) and then merges all the entry points into a single script. Note that after this change there won't be anymore a \"vanilla\" RPC test: all test scripts now specify what agent they are using. This puts all agents on equal standing.\nghstack-source-id: 109229474\n\nTest Plan: Sandcastle and CircleCI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22283182\n\nfbshipit-source-id: 7e3626bbbf37d88b892077a03725f0598576b370", "pr_number": "40818", "files_changed": ["test/distributed/nn/api/__init__.py", "test/distributed/nn/api/test_remote_module_spawn.py", "test/distributed/rpc/jit/test_dist_autograd_spawn.py", "test/distributed/rpc/jit/test_rpc_spawn.py", "test/distributed/rpc/test_dist_autograd_spawn.py", "test/distributed/rpc/test_dist_optimizer_spawn.py", "test/distributed/rpc/test_process_group_agent.py", "test/distributed/rpc/test_rpc_spawn.py", "test/distributed/test_ddp_under_dist_autograd.py", "test/run_test.py", "test/test_determination.py", "torch/testing/_internal/distributed/rpc/process_group_agent_test_fixture.py"], "labels": ["merged", "oncall: jit"]}, "a94039fce5": {"title": "[RPC tests] Avoid decorators to skip tests (#40819)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40819\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nThis diff removes the two decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which were used to skip tests. They were only used to prevent the TensorPipe agent from running tests that were using the process group agent's options. The converse (preventing the PG agent from using the TP options) is achieved by having those tests live in a `TensorPipeAgentRpcTest` class. So here we're doing the same for process group, by moving those tests to a `ProcessGroupAgentRpcTest` class.\nghstack-source-id: 109229473\n\nTest Plan: Sandcastle and CircleCI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22283179\n\nfbshipit-source-id: b9315f9fd67f35e88fe1843faa161fc53a4133c4", "pr_number": "40819", "files_changed": ["test/distributed/rpc/test_process_group_agent.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "2acef69ce3": {"title": "[RPC tests] Make generic fixture an abstract base class (#40820)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40820\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nNow that no one is using the generic fixture anymore (i.e., the fixture that looks up the agent's name in the global TEST_CONFIG) we can make it abstract, i.e., have its methods become no-ops and add decorators that will require all subclasses to provide new implementations of those methods. This is a first step towards removing TEST_CONFIG.\nghstack-source-id: 109229475\n\nTest Plan: Sandcastle and CircleCI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22307777\n\nfbshipit-source-id: e52abd915c37894933545eebdfdca3ecb9559926", "pr_number": "40820", "files_changed": ["test/distributed/rpc/test_faulty_agent.py", "test/distributed/rpc/test_process_group_agent.py", "test/distributed/rpc/test_tensorpipe_agent.py", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py", "torch/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py"], "labels": ["merged"]}, "e7c7eaab82": {"title": "[RPC tests] Move some functions to methods of fixture (#40821)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40821\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nThis change continues the work towards removing TEST_CONFIG, by taking a few functions that were accepting the agent name (as obtained from TEST_CONFIG) and then did a bunch of if/elses on it, and replace them by new abstract methods on the fixtures, so that these functions become \"decentralized\".\nghstack-source-id: 109229472\n\nTest Plan: Sandcastle and CircleCI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22307776\n\nfbshipit-source-id: 9e1f6edca79aacf0bcf9d83d50ce9e0d2beec0dd", "pr_number": "40821", "files_changed": ["torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py", "torch/testing/_internal/distributed/rpc/process_group_agent_test_fixture.py", "torch/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py", "torch/testing/_internal/distributed/rpc/rpc_test.py", "torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py"], "labels": ["merged", "oncall: jit"]}, "2e7b464c43": {"title": "[RPC tests] Remove global TEST_CONFIG (#40822)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40822\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nThis is the last step of removing TEST_CONFIG. As there was no one left using it, there is really not much to it.\nghstack-source-id: 109229471\n\nTest Plan: Sandcastle and CircleCI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22307778\n\nfbshipit-source-id: 0d9498d9367eec671e0a964ce693015f73c5638c", "pr_number": "40822", "files_changed": ["test/distributed/rpc/test_tensorpipe_agent.py", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py"], "labels": ["merged", "oncall: jit"]}, "d7516ccfac": {"title": "[RPC tests] Enroll TensorPipe in missing test suites (#40823)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40823\n\nSummary of the entire stack:\n--\n\nThis diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:\n- Several ways to specify the agent to use: there exists one \"generic\" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.\n- These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.\n- Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a \"trap\" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.\n- Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).\n- There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of \"leaking\" some part of the Thrift agent into OSS.\n- Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.\n- There is no \"master list\" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).\n- All of these tiny \"entry point\" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.\n\nThis refactoring aims to address these problems by:\n- Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be \"mixed in\" to any test suite.\n- Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the \"generic\" test suite) so that they are only picked up by the agent they apply to.\n- Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).\n\nIt provides further advantages:\n- It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.\n- It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...\n\nSummary of this commit\n--\nAs it is now easier to spot that the TensorPipe agent wasn't being run on some test suite, we fix that. We keep this change for last so that if those tests turn out to be flaky and must be reverted this won't affect the rest of the stack.\nghstack-source-id: 109229469\n\nTest Plan: Sandcastle and CircleCI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22309432\n\nfbshipit-source-id: c433a6a49a7b6737e0df4cd953f3dfde290f20b8", "pr_number": "40823", "files_changed": ["test/distributed/rpc/test_tensorpipe_agent.py"], "labels": ["merged"]}, "4da602b004": {"title": "[RPC tests] Generate test classes automatically (#42527)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42527\n\nghstack-source-id: 109229468\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22864698\n\nfbshipit-source-id: 6a55f3201c544f0173493b38699a2c7e95ac1bbc", "pr_number": "42527", "files_changed": ["test/distributed/rpc/test_faulty_agent.py", "test/distributed/rpc/test_process_group_agent.py", "test/distributed/rpc/test_tensorpipe_agent.py", "torch/testing/_internal/distributed/rpc/rpc_test.py", "torch/testing/_internal/distributed/rpc_utils.py"], "labels": ["merged"]}, "2501e2b12d": {"title": "[RPC tests] Run DdpUnderDistAutogradTest and DdpComparisonTest with fork too (#42528)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42528\n\nIt seems it was an oversight that they weren't run. This allows to simplify our auto-generation logic as now all test suites are run in both modes.\nghstack-source-id: 109229969\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22922151\n\nfbshipit-source-id: 0766a6970c927efb04eee4894b73d4bcaf60b97f", "pr_number": "42528", "files_changed": ["torch/testing/_internal/distributed/rpc_utils.py"], "labels": ["merged"]}, "102abb877c": {"title": "Reland D22939119: \"[TensorExpr] Fix a way we were createing np arrays in tests.\" (#42608)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42608\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D22952745\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: fd6a3efbfcaa876a2f4d27b507fe0ccdcb55a002", "pr_number": "42608", "files_changed": ["test/test_tensorexpr.py"], "labels": ["merged"]}, "5023995292": {"title": "fix output size adjustment for onnxifi_op", "body": "Summary: this breaks if we cut the net at certain int8 ops boundary.\n\nTest Plan: with net_runner to lower a single Int8Quantize op. It used to break. Now it works.\n\nReviewed By: yinghai\n\nDifferential Revision: D22912178\n\nfbshipit-source-id: ca306068c9768df84c1cfa8b34226a1330e19912", "pr_number": null, "files_changed": ["caffe2/opt/onnxifi_op.cc"], "labels": []}, "aa4e91a6dc": {"title": "Fix `TestSparse.test_bmm_windows_error` when CUDA is not available (#42626)", "body": "Summary:\nRefactor comnon pattern of (torch.cuda.version and [int(x) for x in torch.cuda.version.split(\".\")] >= [a, b]) into `_get_torch_cuda_version()` function\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42626\n\nReviewed By: seemethere\n\nDifferential Revision: D22956149\n\nPulled By: malfet\n\nfbshipit-source-id: 897c55965e53b477cd20f69e8da15d90489035de", "pr_number": "42626", "files_changed": ["test/test_sparse.py"], "labels": ["merged"]}, "54ffb05eff": {"title": "better error message between C2 and glow (#41603)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41603\n\nPull Request resolved: https://github.com/pytorch/glow/pull/4704\n\nPreviously in the glow onnxifi path, when an error is encountered, we log it to stderr then just return ONNXIFI_STATUS_INTERNAL_ERROR to C2. C2 then does CAFFE2_ENFORCE_EQUAL(return_code, ONNXIFI_STATUS_SUCCESS). The error message that eventually went to the user is something like\n\n   [enforce fail at onnxifi_op.cc:545] eventStatus == ONNXIFI_STATUS_SUCCESS. 1030 vs 0\n\nThis diff adds plumbing to get human readable error message out of glow into C2.\n\nTest Plan:\nRun ads replayer. Overload it with traffic. Now the error message sent back to the client used to be\n\n  E0707 00:57:45.697196 3709559 Caffe2DisaggAcceleratorTask.cpp:493] During running REMOTE_OTHER net: [enforce fail at onnxifi_op.cc:545] eventStatus == ONNXIFI_STATUS_SUCCESS. 1030 vs 0 (Error from operator:....\n\nNow it's\n\n```\nE0707 16:46:48.366263 1532943 Client.cpp:966] Exception when calling caffe2_run_disagg_accelerator on remote predictor for model 190081310_0 : apache::thrift::TApplicationException: c10::Error: [enforce fail at onnxifi_op.cc:556] .\nError code: RUNTIME_REQUEST_REFUSED\nError message: The number of allowed queued requests has been exceeded. queued requests: 100 allowed requests: 100\nError return stack:\nglow/glow/lib/Runtime/HostManager/HostManager.cpp:673\nglow/glow/lib/Onnxifi/HostMana (Error from operator:...\n```\n\nReviewed By: gcatron, yinghai\n\nDifferential Revision: D22416857\n\nfbshipit-source-id: 564bc7644d9666eb660725c2dca5637affae9b73", "pr_number": "41603", "files_changed": ["caffe2/opt/onnxifi_op.cc", "caffe2/opt/onnxifi_op.h", "third_party/foxi"], "labels": ["fb-exported", "merged"]}, "50f0d2b97d": {"title": "quant: add q_batchnorm_1d op (#42491)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42491\n\nHooks up quantized batchnorm_1d to the quantized_bn kernel. Eager mode\nhookup will be in a future PR, and graph mode should work after this PR.\n\nNote: currently the implementation is ~2x slower on the benchmark than q_batch_norm2d\nbecause we convert back to contiguous memory format at the end, since\nchannels_last is only defined for rank >= 4. If further optimization is\nneeded, that can be a separate PR (will need the NHWC folks to see if\nthere is a workaround).  Meanwhile, having this is better than not having anything.\n\nContext: There have been both internal and external requests for various\nquantized BN1d use cases.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizedOps.test_batch_norm_1d_2d_3d\npython test/test_quantization.py TestQuantizedOps.test_batch_norm_1d_2d_3d_relu\npython test/test_quantization.py TestQuantizeJitOps.test_qbatch_norm\n\n// performance:\n// https://gist.github.com/vkuzo/73a07c0f24c05f5804990d9ebfaecf5e\n\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22926254\n\nfbshipit-source-id: 2780e6a81cd13a7455f6ab6e5118c22850a97a12", "pr_number": "42491", "files_changed": ["aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp", "aten/src/ATen/native/quantized/library.cpp", "benchmarks/operator_benchmark/pt/qbatchnorm_test.py", "test/quantization/test_quantize_jit.py", "test/quantization/test_quantized_op.py"], "labels": ["merged"]}, "5d7c3f92b9": {"title": "Issue warning instead of error when parsing Enum while enum support is not enabled (#42623)", "body": "Summary:\nReturnning None rather than error matches previous behavior better.\n\nFixes https://fburl.com/yrrvtes3\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42623\n\nReviewed By: ajaech\n\nDifferential Revision: D22957498\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 61dabc6d23ad44e75bd35d837768bdb6fe71eece", "pr_number": "42623", "files_changed": ["test/jit/test_enum.py", "torch/jit/annotations.py"], "labels": ["merged", "oncall: jit"]}, "9ea9d1b52e": {"title": "[fbs][2/n] Remove .python3 markers", "body": "Test Plan:\n`xbgr '\\.python3'` shows only one (dead) usage of this file:\nhttps://www.internalfb.com/intern/diffusion/FBS/browse/master/fbcode/python/repo_stats/buck.py?commit=9a8dd3243207819325d520c208218f6ab69e4e49&lines=854\n\nReviewed By: lisroach\n\nDifferential Revision: D22955631\n\nfbshipit-source-id: e686d9157c08c347d0ce4acdd05bd7ab29ff7df5", "pr_number": null, "files_changed": [".python3", "caffe2/python/models/seq2seq/.python3"], "labels": []}, "509fb77b70": {"title": "Adjust bound_shape_inferencer to take 4 inputs for FCs (#41934)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41934\n\nThe model exported from online training workflow with int8 quantization contains FCs with 4 inputs. The extra input is the quant_param blob. This diff is to adjust the bound_shape_inferencer and int8 op schema to get shape info for the quant_param input.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/opt:bound_shape_inference_test\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D22683554\n\nfbshipit-source-id: 684d1433212a528120aba1c37d27e26b6a31b403", "pr_number": "41934", "files_changed": ["caffe2/operators/quantized/int8_fc_op.cc", "caffe2/operators/quantized/int8_quantize_op.cc", "caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/quantization/server/int8_gen_quant_params.cc", "caffe2/quantization/server/int8_quant_scheme_blob_fill.cc"], "labels": ["fb-exported", "merged"]}, "9add11ffc1": {"title": "Fix IS_SPMM_AVAILABLE macro definition (#42643)", "body": "Summary:\nThis should fix CUDA-11 on Windows build issue\n\n`defined` is not a function, and so it can not be used in macro substitution.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42643\n\nReviewed By: pbelevich, xw285cornell\n\nDifferential Revision: D22963420\n\nPulled By: malfet\n\nfbshipit-source-id: cccf7db0d03cd62b655beeb154db9e628aa749f0", "pr_number": "42643", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu"], "labels": ["merged"]}, "04d7e1679d": {"title": "[quant] Quantized Average Pool Refactoring (#42009)", "body": "Summary:\n**cc** z-a-f. Refactor `qavg_pool(2,3)d_nhwc_kernel` as mentioned in https://github.com/pytorch/pytorch/issues/40316.\n\n# Benchmarks\n## Python\nBefore | After\n![before_after](https://user-images.githubusercontent.com/37529096/88401550-fea7ba80-ce1d-11ea-81c5-3ae912e81e8f.png)\n## C++\n![before_after_cpp](https://user-images.githubusercontent.com/37529096/88401845-5ba37080-ce1e-11ea-9bf2-3c95ac2b4b49.png)\n## Notes\n- It does seem that for `qint8` and `quint8` there is a noticeable 2x increase in speed at least when the `channels > 64` in the benchmarks.\n## Reproduce\n### Python\n```\nimport time\nimport numpy as np\nimport torch\nfrom termcolor import colored\ndef time_avg_pool2d(X, kernel, stride, padding, ceil_mode, count_include_pad, divisor_override, iterations):\n    X, (scale, zero_point, torch_type) = X\n    qX_nchw = torch.quantize_per_tensor(torch.from_numpy(X), scale=scale,\n                                    zero_point=zero_point, dtype=torch_type)\n    qX_nhwc = qX_nchw.contiguous(memory_format=torch.channels_last)\n    assert(qX_nhwc.stride() != sorted(qX_nhwc.stride()))\n    assert(qX_nchw.is_contiguous(memory_format=torch.contiguous_format))\n    assert(qX_nhwc.is_contiguous(memory_format=torch.channels_last))\n    start = time.time()\n    for _ in range(iterations):\n        X_hat = torch.nn.quantized.functional.avg_pool2d(qX_nchw, kernel_size=kernel, stride=stride, padding=padding, ceil_mode=ceil_mode,\n                count_include_pad=count_include_pad, divisor_override=divisor_override)\n    qnchw_end = time.time() - start\n    start = time.time()\n    for _ in range(iterations):\n        X_hat = torch.nn.quantized.functional.avg_pool2d(qX_nhwc, kernel_size=kernel, stride=stride, padding=padding, ceil_mode=ceil_mode,\n                count_include_pad=count_include_pad, divisor_override=divisor_override)\n    qnhwc_end = time.time() - start\n    return qnchw_end*1000/iterations, qnhwc_end*1000/iterations\n\ndef time_avg_pool3d(X, kernel, stride, padding, ceil_mode, count_include_pad, divisor_override,  iterations):\n    X, (scale, zero_point, torch_type) = X\n    qX_ncdhw = torch.quantize_per_tensor(torch.from_numpy(X), scale=scale,\n                                    zero_point=zero_point, dtype=torch_type)\n    qX_ndhwc = qX_ncdhw.contiguous(memory_format=torch.channels_last_3d)\n    assert(qX_ndhwc.stride() != sorted(qX_ndhwc.stride()))\n    assert(qX_ncdhw.is_contiguous(memory_format=torch.contiguous_format))\n    assert(qX_ndhwc.is_contiguous(memory_format=torch.channels_last_3d))\n    start = time.time()\n    for _ in range(iterations):\n        X_hat = torch.nn.quantized.functional.avg_pool3d(qX_ncdhw, kernel_size=kernel, stride=stride, padding=padding, ceil_mode=ceil_mode,\n                count_include_pad=count_include_pad, divisor_override=divisor_override)\n    qncdhw_end = time.time() - start\n    start = time.time()\n    for _ in range(iterations):\n        X_hat = torch.nn.quantized.functional.avg_pool3d(qX_ndhwc, kernel_size=kernel, stride=stride, padding=padding, ceil_mode=ceil_mode,\n                count_include_pad=count_include_pad, divisor_override=divisor_override)\n    qndhwc_end = time.time() - start\n    return qncdhw_end*1000/iterations, qndhwc_end*1000/iterations\n\niterations = 10000\nprint(\"iterations = {}\".format(iterations))\nprint(\"Benchmark\", \"Time(ms)\", sep=\"\\t\\t\\t\\t\\t\")\nfor torch_type in (torch.qint8, torch.quint8, torch.qint32):\n    for channel in (4,8,64,256):\n        X = np.random.rand(1, channel, 56, 56).astype(np.float32), (0.5, 1, torch_type)\n        ts = time_avg_pool2d(X, 4, None, 0, True, True, None, iterations)\n        print(colored(\"avg_pool2d({}, {}, {})\".format(str(torch_type), channel, \"nchw\"), 'green'), colored(ts[0], 'yellow'), sep=\"\\t\")\n        print(colored(\"avg_pool2d({}, {}, {})\".format(str(torch_type), channel, \"nhwc\"), 'green'), colored(ts[1], 'yellow'), sep=\"\\t\")\nfor torch_type in (torch.qint8, torch.quint8, torch.qint32):\n    for channel in (4,8,64,256):\n        X = np.random.rand(1, channel, 56, 56, 4).astype(np.float32), (0.5, 1, torch_type)\n        ts = time_avg_pool3d(X, 4, None, 0, True, True, None, iterations)\n        print(colored(\"avg_pool3d({}, {}, {})\".format(str(torch_type), channel, \"ncdhw\"), 'green'), colored(ts[0], 'yellow'), sep=\"\\t\")\n        print(colored(\"avg_pool3d({}, {}, {})\".format(str(torch_type), channel, \"ndhwc\"), 'green'), colored(ts[1], 'yellow'), sep=\"\\t\")\n```\n### C++\n1. `git clone https://github.com/google/benchmark.git`\n2. `git clone https://github.com/google/googletest.git benchmark/googletest`\n\n```\n# CMakeLists.txt\ncmake_minimum_required(VERSION 3.10 FATAL_ERROR)\nproject(time_avg_pool VERSION 0.1.0)\n\nfind_package(Torch REQUIRED)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")\nadd_subdirectory(benchmark)\n\nadd_executable(time_average_pool time_average_pool.cpp)\ntarget_link_libraries(time_average_pool ${TORCH_LIBRARIES})\nset_property(TARGET time_average_pool PROPERTY CXX_STANDARD 14)\ntarget_link_libraries(time_average_pool benchmark::benchmark)\n```\n\n```\n// time_average_pool.cpp\n#include <benchmark/benchmark.h>\n#include <torch/torch.h>\n\ntorch::Device device(torch::kCPU);\n\nstatic void BM_TORCH_QAVG_POOL2D_NCHW_SINGLE_THREADED(benchmark::State& state) {\n  torch::init_num_threads();\n  torch::set_num_threads(1);\n  auto x_nchw = torch::rand({1, state.range(0), 56, 56}, device);\n  auto qx_nchw = torch::quantize_per_tensor(x_nchw, 0.5, 1, torch::kQUInt8);\n  torch::Tensor X_hat;\n  for (auto _ : state)\n    X_hat = torch::nn::functional::avg_pool2d(\n        qx_nchw,\n        torch::nn::AvgPool2dOptions({4, 4}).ceil_mode(true).count_include_pad(\n            true));\n}\n\nstatic void BM_TORCH_QAVG_POOL2D_NHWC_SINGLE_THREADED(benchmark::State& state) {\n  torch::init_num_threads();\n  torch::set_num_threads(1);\n  auto x_nchw = torch::rand({1, state.range(0), 56, 56}, device);\n  auto qx_nchw = torch::quantize_per_tensor(x_nchw, 0.5, 1, torch::kQUInt8);\n  auto qx_nhwc = qx_nchw.contiguous(torch::MemoryFormat::ChannelsLast);\n  torch::Tensor X_hat;\n  for (auto _ : state)\n    X_hat = torch::nn::functional::avg_pool2d(\n        qx_nhwc,\n        torch::nn::AvgPool2dOptions({4, 4}).ceil_mode(true).count_include_pad(\n            true));\n}\n\nstatic void BM_TORCH_QAVG_POOL2D_NCHW(benchmark::State& state) {\n  auto x_nchw = torch::rand({1, state.range(0), 56, 56}, device);\n  auto qx_nchw = torch::quantize_per_tensor(x_nchw, 0.5, 1, torch::kQUInt8);\n  torch::Tensor X_hat;\n  for (auto _ : state)\n    X_hat = torch::nn::functional::avg_pool2d(\n        qx_nchw,\n        torch::nn::AvgPool2dOptions({4, 4}).ceil_mode(true).count_include_pad(\n            true));\n}\n\nstatic void BM_TORCH_QAVG_POOL2D_NHWC(benchmark::State& state) {\n  auto x_nchw = torch::rand({1, state.range(0), 56, 56}, device);\n  auto qx_nchw = torch::quantize_per_tensor(x_nchw, 0.5, 1, torch::kQUInt8);\n  auto qx_nhwc = qx_nchw.contiguous(torch::MemoryFormat::ChannelsLast);\n  torch::Tensor X_hat;\n  for (auto _ : state)\n    X_hat = torch::nn::functional::avg_pool2d(\n        qx_nhwc,\n        torch::nn::AvgPool2dOptions({4, 4}).ceil_mode(true).count_include_pad(\n            true));\n}\n\nstatic void BM_TORCH_QAVG_POOL3D_NCDHW_SINGLE_THREADED(\n    benchmark::State& state) {\n  torch::init_num_threads();\n  torch::set_num_threads(1);\n  auto x_ncdhw = torch::rand({1, state.range(0), 56, 56, 4}, device);\n  auto qx_ncdhw = torch::quantize_per_tensor(x_ncdhw, 0.5, 1, torch::kQUInt8);\n  torch::Tensor X_hat;\n  for (auto _ : state)\n    X_hat = torch::nn::functional::avg_pool3d(\n        qx_ncdhw,\n        torch::nn::AvgPool3dOptions({5, 5, 5})\n            .ceil_mode(true)\n            .count_include_pad(true));\n}\n\nstatic void BM_TORCH_QAVG_POOL3D_NDHWC_SINGLE_THREADED(\n    benchmark::State& state) {\n  torch::init_num_threads();\n  torch::set_num_threads(1);\n  auto x_ncdhw = torch::rand({1, state.range(0), 56, 56, 4}, device);\n  auto qx_ncdhw = torch::quantize_per_tensor(x_ncdhw, 0.5, 1, torch::kQUInt8);\n  auto qx_ndhwc = qx_ncdhw.contiguous(torch::MemoryFormat::ChannelsLast3d);\n  torch::Tensor X_hat;\n  for (auto _ : state)\n    X_hat = torch::nn::functional::avg_pool3d(\n        qx_ndhwc,\n        torch::nn::AvgPool3dOptions({5, 5, 5})\n            .ceil_mode(true)\n            .count_include_pad(true));\n}\n\nstatic void BM_TORCH_QAVG_POOL3D_NCDHW(benchmark::State& state) {\n  auto x_ncdhw = torch::rand({1, state.range(0), 56, 56, 4}, device);\n  auto qx_ncdhw = torch::quantize_per_tensor(x_ncdhw, 0.5, 1, torch::kQUInt8);\n  torch::Tensor X_hat;\n  for (auto _ : state)\n    X_hat = torch::nn::functional::avg_pool3d(\n        qx_ncdhw,\n        torch::nn::AvgPool3dOptions({5, 5, 5})\n            .ceil_mode(true)\n            .count_include_pad(true));\n}\n\nstatic void BM_TORCH_QAVG_POOL3D_NDHWC(benchmark::State& state) {\n  auto x_ncdhw = torch::rand({1, state.range(0), 56, 56, 4}, device);\n  auto qx_ncdhw = torch::quantize_per_tensor(x_ncdhw, 0.5, 1, torch::kQUInt8);\n  auto qx_ndhwc = qx_ncdhw.contiguous(torch::MemoryFormat::ChannelsLast3d);\n  torch::Tensor X_hat;\n  for (auto _ : state)\n    X_hat = torch::nn::functional::avg_pool3d(\n        qx_ndhwc,\n        torch::nn::AvgPool3dOptions({5, 5, 5})\n            .ceil_mode(true)\n            .count_include_pad(true));\n}\n\nBENCHMARK(BM_TORCH_QAVG_POOL2D_NCHW)->RangeMultiplier(8)->Range(4, 256);\nBENCHMARK(BM_TORCH_QAVG_POOL2D_NHWC)->RangeMultiplier(8)->Range(4, 256);\nBENCHMARK(BM_TORCH_QAVG_POOL3D_NCDHW)->RangeMultiplier(8)->Range(4, 256);\nBENCHMARK(BM_TORCH_QAVG_POOL3D_NDHWC)->RangeMultiplier(8)->Range(4, 256);\nBENCHMARK(BM_TORCH_QAVG_POOL2D_NCHW_SINGLE_THREADED)\n    ->RangeMultiplier(8)\n    ->Range(4, 256);\nBENCHMARK(BM_TORCH_QAVG_POOL2D_NHWC_SINGLE_THREADED)\n    ->RangeMultiplier(8)\n    ->Range(4, 256);\nBENCHMARK(BM_TORCH_QAVG_POOL3D_NCDHW_SINGLE_THREADED)\n    ->RangeMultiplier(8)\n    ->Range(4, 256);\nBENCHMARK(BM_TORCH_QAVG_POOL3D_NDHWC_SINGLE_THREADED)\n    ->RangeMultiplier(8)\n    ->Range(4, 256);\nBENCHMARK_MAIN();\n```\n\n3. `mkdir build && cd build`\n4. ```cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` .. ```\n5. `cmake --build . --config Release`\n6. `./time_average_pool`\n\n# Further notes\n- I've used `istrideB, istrideD, istrideH, strideW, strideC` to match `_qadaptive_avg_pool_kernel` since there's some code duplication there as mentioned in https://github.com/pytorch/pytorch/issues/40316.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42009\n\nReviewed By: pbelevich\n\nDifferential Revision: D22794441\n\nPulled By: z-a-f\n\nfbshipit-source-id: 16710202811a1fbe1c99ea4d9b45876d6d28a8da", "pr_number": "42009", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "test/quantization/test_quantized_op.py"], "labels": ["merged", "open source", "triaged"]}, "ddb8849ffc": {"title": "Fix method stub used for fixing mypy issue to work with pylint (#42356)", "body": "Summary:\nMake function from method\n\nSince _forward_unimplemented is defined within the nn.Module class,\npylint (correctly) complains about not implementing this method in subclasses.\n\nFixes https://github.com/pytorch/pytorch/issues/42305\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42356\n\nReviewed By: mruberry\n\nDifferential Revision: D22867255\n\nPulled By: ezyang\n\nfbshipit-source-id: ccf3e45e359d927e010791fadf70b2ef231ddb0b", "pr_number": "42356", "files_changed": ["torch/nn/modules/module.py"], "labels": ["merged", "open source"]}, "c14fbc36ed": {"title": "Update docs about CUDA stream priority (#41364)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41364\n\nReviewed By: malfet\n\nDifferential Revision: D22962856\n\nPulled By: ngimel\n\nfbshipit-source-id: 47f65069516cb555579455e8680deb937fc1f544", "pr_number": "41364", "files_changed": ["torch/cuda/streams.py"], "labels": ["merged", "open source", "triaged"]}, "92b7347fd7": {"title": "Enforce counter value to double type in rowwise_counter", "body": "Summary:\nEnforce counter value to double type in rowwise_counter.\n\n**Context:**\nThe existing implementation is using float type for counter value. But due to the precision limit of a floating number [1], we observed that the counter value can't increment beyond 16777216.0 (i.e., the max value is 16777216.0) in our earlier experiments. We decide to enforce double type to avoid this issue.\n\n[1] https://stackoverflow.com/questions/12596695/why-does-a-float-variable-stop-incrementing-at-16777216-in-c\n\nTest Plan:\nop test\n```\nruixliu@devvm1997:~/fbsource/fbcode/caffe2/caffe2/python/operator_test(f0b0b48c)$ buck test :rowwise_counter_test\nTrace available for this run at /tmp/testpilot.20200728-083200.729292.log\nTestPilot test runner for Facebook. See https://fburl.com/testpilot for details.\nTestpilot build revision cd2638f1f47250eac058b8c36561760027d16add fbpkg f88726c8ebde4ba288e1172a348c7f46 at Mon Jul 27 18:11:43 2020 by twsvcscm from /usr/local/fbprojects/packages/testinfra.testpilot/887/t.par\nDiscovering tests\nRunning 1 test\nStarted new test run: https://our.intern.facebook.com/intern/testinfra/testrun/7881299364977047\n      \u2713 caffe2/caffe2/python/operator_test:rowwise_counter_test - test_rowwise_counter (caffe2.caffe2.python.operator_test.rowwise_counter_test.TestRowWiseCounter) 0.265 1/1 (passed)\n      \u2713 caffe2/caffe2/python/operator_test:rowwise_counter_test - main 14.414 (passed)\nFinished test run: https://our.intern.facebook.com/intern/testinfra/testrun/7881299364977047\nSummary (total time 18.51s):\n  PASS: 2\n  FAIL: 0\n  SKIP: 0\n  FATAL: 0\n  TIMEOUT: 0\n  OMIT: 0\n```\n\noptimizer test\n```\nruixliu@devvm1997:~/fbsource/fbcode/caffe2/caffe2/python(7d66fbb9)$ buck test :optimizer_test\nFinished test run: https://our.intern.facebook.com/intern/testinfra/testrun/7036874434841896\nSummary (total time 64.87s):\n  PASS: 48\n  FAIL: 0\n  SKIP: 24\n    caffe2/caffe2/python:optimizer_test - testGPUDense (caffe2.caffe2.python.optimizer_test.TestMomentumSgd)\n    caffe2/caffe2/python:optimizer_test - testGPUDense (caffe2.caffe2.python.optimizer_test.TestGFtrl)\n    caffe2/caffe2/python:optimizer_test - test_caffe2_cpu_vs_numpy (caffe2.caffe2.python.optimizer_test.TestYellowFin)\n    caffe2/caffe2/python:optimizer_test - testGPUDense (caffe2.caffe2.python.optimizer_test.TestSparseRAdam)\n    caffe2/caffe2/python:optimizer_test - testGPUDense (caffe2.caffe2.python.optimizer_test.TestRowWiseAdagradWithCounter)\n    caffe2/caffe2/python:optimizer_test - testGPUDense (caffe2.caffe2.python.optimizer_test.TestAdagrad)\n    caffe2/caffe2/python:optimizer_test - test_caffe2_gpu_vs_numpy (caffe2.caffe2.python.optimizer_test.TestYellowFin)\n    caffe2/caffe2/python:optimizer_test - testDense (caffe2.caffe2.python.optimizer_test.TestRowWiseAdagrad)\n    caffe2/caffe2/python:optimizer_test - testGPUDense (caffe2.caffe2.python.optimizer_test.TestFtrl)\n    caffe2/caffe2/python:optimizer_test - testSparse (caffe2.caffe2.python.optimizer_test.TestRmsProp)\n    ...and 14 more not shown...\n  FATAL: 0\n  TIMEOUT: 0\n  OMIT: 0\n```\n\nparam download test\n```\nruixliu@devvm1997:~/fbsource/fbcode/caffe2/caffe2/fb/net_transforms/tests(7ef20a38)$ sudo buck test :param_download_test\nFinished test run: Finished test run: https://our.intern.facebook.com/intern/testinfra/testrun/6473924481526935\n```\n\ne2e flow:\nf208394929\nf207991149\nf207967273\n\nANP notebook to check the counter value loaded from the flows\nhttps://fburl.com/anp/5fdcbnoi\n\nscreenshot of the loaded counter (note that counter max is larger than 16777216.0)\n\n{F250926501}\n\nReviewed By: ellie-wen\n\nDifferential Revision: D22711514\n\nfbshipit-source-id: 426fed7415270aa3f276dda8141907534734337f", "pr_number": null, "files_changed": ["caffe2/operators/utility_ops.h", "caffe2/python/operator_test/rowwise_counter_test.py", "caffe2/python/optimizer.py", "caffe2/sgd/rowwise_counter.cc", "caffe2/sgd/rowwise_counter.h"], "labels": []}, "3d46e02ea1": {"title": "Add __torch_function__ for methods (#37091)", "body": "Summary:\nAccording to pytorch/rfcs#3\n\nFrom the goals in the RFC:\n\n1. Support subclassing `torch.Tensor` in Python (done here)\n2. Preserve `torch.Tensor` subclasses when calling `torch` functions on them (done here)\n3. Use the PyTorch API with `torch.Tensor`-like objects that are _not_ `torch.Tensor`\n   subclasses (done in https://github.com/pytorch/pytorch/issues/30730)\n4. Preserve `torch.Tensor` subclasses when calling `torch.Tensor` methods. (done here)\n5. Propagating subclass instances correctly also with operators, using\n   views/slices/indexing/etc. (done here)\n6. Preserve subclass attributes when using methods or views/slices/indexing. (done here)\n7. A way to insert code that operates on both functions and methods uniformly\n   (so we can write a single function that overrides all operators). (done here)\n8. The ability to give external libraries a way to also define\n   functions/methods that follow the `__torch_function__` protocol. (will be addressed in a separate PR)\n\nThis PR makes the following changes:\n\n1. Adds the `self` argument to the arg parser.\n2. Dispatches on `self` as well if `self` is not `nullptr`.\n3. Adds a `torch._C.DisableTorchFunction` context manager to disable `__torch_function__`.\n4. Adds a `torch::torch_function_enabled()` and `torch._C._torch_function_enabled()` to check the state of `__torch_function__`.\n5. Dispatches all `torch._C.TensorBase` and `torch.Tensor` methods via `__torch_function__`.\n\nTODO:\n\n- [x] Sequence Methods\n- [x] Docs\n- [x] Tests\n\nCloses https://github.com/pytorch/pytorch/issues/28361\n\nBenchmarks in https://github.com/pytorch/pytorch/pull/37091#issuecomment-633657778\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37091\n\nReviewed By: ngimel\n\nDifferential Revision: D22765678\n\nPulled By: ezyang\n\nfbshipit-source-id: 53f8aa17ddb8b1108c0997f6a7aa13cb5be73de0", "pr_number": "37091", "files_changed": ["benchmarks/overrides_benchmark/common.py", "docs/source/notes/extending.rst", "mypy.ini", "test/test_overrides.py", "test/test_type_hints.py", "tools/autograd/gen_annotated_fn_args.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_nn_functions.cpp", "tools/autograd/templates/python_variable_methods.cpp", "tools/build_variables.bzl", "torch/_lobpcg.py", "torch/_lowrank.py", "torch/_overrides.py", "torch/csrc/Module.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/autograd/utils/python_arg_parsing.h", "torch/csrc/utils/disable_torch_function.cpp", "torch/csrc/utils/disable_torch_function.h", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/functional.py", "torch/nn/functional.py", "torch/nn/parameter.py", "torch/overrides.py", "torch/tensor.py"], "labels": ["open source", "topic: bc-breaking", "triaged"]}, "1848b43c4d": {"title": "[NNC] Add loop unroll transformation (#42465)", "body": "Summary:\nUnroll a loop with constant boundaries, replacing it with multiple\ninstances of the loop body. For example:\n\n```\nfor x in 0..3:\n  A[x] = x*2\n```\n\nbecomes:\n\n```\nA[0] = 0\nA[1] = 2\nA[2] = 4\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42465\n\nTest Plan: `test_tensorexpr` unit tests.\n\nReviewed By: agolynski\n\nDifferential Revision: D22914418\n\nPulled By: asuhan\n\nfbshipit-source-id: 72ca10d7c0b1ac7f9a3688ac872bd94a1c53dc51", "pr_number": "42465", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.h"], "labels": ["merged", "oncall: jit"]}, "eb9ae7c038": {"title": "Implement `gpu_kernel_multiple_outputs` (#37969)", "body": "Summary:\nThis PR introduces a variant of `gpu_kernel` for functions that return multiple values with `thrust::tuple`.\nWith this I simplified `prelu_cuda_backward_share_weights_kernel`.\n\n### Why using `thrust::tuple`?\nBecause `std::tuple` does not support `operator=` on device code which makes the implementation complicated.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37969\n\nReviewed By: paulshaoyuqiao\n\nDifferential Revision: D22868670\n\nPulled By: ngimel\n\nfbshipit-source-id: eda0a29ac0347ad544b24bf60e3d809a7db1a929", "pr_number": "37969", "files_changed": ["aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh"], "labels": ["merged", "open source"]}, "23607441c2": {"title": "Create CuBLAS PointerModeGuard (#42639)", "body": "Summary:\nAdds an RAII guard for `cublasSetPointerMode()`.\nUpdates `dot_cuda` to use the guard, rather than exception catching.\n\nAddresses this comment: https://github.com/pytorch/pytorch/pull/41377#discussion_r465754082\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42639\n\nReviewed By: malfet\n\nDifferential Revision: D22969985\n\nPulled By: ezyang\n\nfbshipit-source-id: b05c35d1884bb890f8767d6a4ef8b4724a329471", "pr_number": "42639", "files_changed": ["aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/native/cuda/LinearAlgebra.cu"], "labels": ["merged", "open source"]}, "644d787cd8": {"title": "find rccl properly (#42072)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42072\n\nReviewed By: malfet\n\nDifferential Revision: D22969778\n\nPulled By: ezyang\n\nfbshipit-source-id: 509178775d4d99460bcb147bcfced29f04cabdc4", "pr_number": "42072", "files_changed": ["cmake/External/rccl.cmake"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "ccfce9d4a9": {"title": "Adds fft namespace (#41911)", "body": "Summary:\nThis PR creates a new namespace, torch.fft (torch::fft) and puts a single function, fft, in it. This function is analogous to is a simplified version of NumPy's [numpy.fft.fft](https://numpy.org/doc/1.18/reference/generated/numpy.fft.fft.html?highlight=fft#numpy.fft.fft) that accepts no optional arguments. It is intended to demonstrate how to add and document functions in the namespace, and is not intended to deprecate the existing torch.fft function.\n\nAdding this namespace was complicated by the existence of the torch.fft function in Python. Creating a torch.fft Python module makes this name ambiguous: does it refer to a function or module? If the JIT didn't exist, a solution to this problem would have been to make torch.fft refer to a callable class that mimicked both the function and module. The JIT, however, cannot understand this pattern. As a workaround it's required to explicitly `import torch.fft` to access the torch.fft.fft function in Python:\n\n```\nimport torch.fft\n\nt = torch.randn(128, dtype=torch.cdouble)\ntorch.fft.fft(t)\n```\n\nSee https://github.com/pytorch/pytorch/issues/42175 for future work. Another possible future PR is to get the JIT to understand torch.fft as a callable class so it need not be imported explicitly to be used.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41911\n\nReviewed By: glaringlee\n\nDifferential Revision: D22941894\n\nPulled By: mruberry\n\nfbshipit-source-id: c8e0b44cbe90d21e998ca3832cf3a533f28dbe8d", "pr_number": "41911", "files_changed": ["BUILD.bazel", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/README.md", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/native_functions.yaml", "caffe2/CMakeLists.txt", "docs/source/fft.rst", "docs/source/index.rst", "test/cpp/api/CMakeLists.txt", "test/cpp/api/fft.cpp", "test/test_spectral_ops.py", "tools/autograd/gen_autograd.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_fft_functions.cpp", "tools/build_variables.bzl", "torch/__init__.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/Module.cpp", "torch/csrc/api/include/torch/all.h", "torch/csrc/api/include/torch/fft.h", "torch/csrc/autograd/python_fft_functions.h", "torch/csrc/autograd/python_nn_functions.h", "torch/fft/__init__.py", "torch/jit/_builtins.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged", "oncall: jit"]}, "a53fdaa23f": {"title": "Remove ProfiledType (#42570)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42570\n\nProfiledType doesn't do anything and is not used atm, removing\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D22938664\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 037c512938028f44258b702bbcde3f8c144f4aa0", "pr_number": "42570", "files_changed": ["BUILD.bazel", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "caffe2/CMakeLists.txt", "test/cpp/jit/test_misc.cpp", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/ProfiledType.cpp", "tools/build_variables.bzl", "torch/csrc/autograd/profiler.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp"], "labels": ["merged", "oncall: jit"]}, "bd458b7d02": {"title": "Don't reference TensorPipe headers in our headers (#42521)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42521\n\nPyTorch's usage of TensorPipe is entirely wrapped within the RPC agent, which means we only need access to TensorPipe within the implementation (the .cpp file) and not in the interface (the .h file). We were however including the TensorPipe headers from the public PyTorch headers, which meant that PyTorch's downstream users had to have the TensorPipe include directories for that to work. By forward-declaring the symbols we need in the PyTorch header, and then including the TensorPipe header in the PyTorch implementation, we avoid \"leaking\" the dependency on TensorPipe, thus effectively keeping it private.\n\nTest Plan: Imported from OSS\n\nReviewed By: beauby\n\nDifferential Revision: D22944238\n\nPulled By: lw\n\nfbshipit-source-id: 2b12d59bd5beeaa439e50f9088a792c9d9bae9e8", "pr_number": "42521", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["merged"]}, "c30bc6d4d7": {"title": "Update TensorPipe submodule (#42522)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42522\n\nMain changes:\n- Consolidated CMake files to have a single entry point, rather than having a specialized one for PyTorch.\n- Changed the way the preprocessor flags are provided, and changed their name.\n\nThere were a few instances in PyTorch's CMake files where we were directly adding TensorPipe's source directory as an include path, which however doesn't contain the auto-generated header we now added. We fix that by adding the `tensorpipe` CMake target as a dependency, so that the include paths defined by TensorPipe are used, which contain that auto-generated header. So instead we link those targets to the tensorpipe target in order for them to pick up the correct include directories.\n\nI'm turning off SHM and CMA for now because they have never been covered by the CI. I'll enable them in a separate PR so that if they turn out to be flaky we can revert that change without reverting this one.\n\nTest Plan: CI\n\nReviewed By: malfet\n\nDifferential Revision: D22959472\n\nfbshipit-source-id: 1959a41c4a66ef78bf0f3bd5e3964969a2a1bf67", "pr_number": "42522", "files_changed": ["caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "setup.py", "test/cpp/rpc/CMakeLists.txt", "third_party/tensorpipe", "third_party/tensorpipe.BUILD", "torch/CMakeLists.txt", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp"], "labels": ["ci/binaries", "merged"]}, "0642d17efc": {"title": "Enable C++ RPC tests (#42636)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42636\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D22967777\n\nPulled By: mrshenli\n\nfbshipit-source-id: 8816c190a4ead7d7f906c140c8a4e76b992f5502", "pr_number": "42636", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "576aab5084": {"title": "Bump up NCCL to 2.7.6 (#42645)", "body": "Summary:\nBecause 2.7.3 has some bug on GA100 which is fixed in 2.7.6\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42645\n\nReviewed By: malfet\n\nDifferential Revision: D22977280\n\nPulled By: mrshenli\n\nfbshipit-source-id: 74779eff90d7d660a988ff33659f3a2237ca7e29", "pr_number": "42645", "files_changed": ["third_party/nccl/nccl"], "labels": ["merged", "open source"]}, "608f99e4ea": {"title": "Fix cudnn version on build_environment of Windows CI (#42615)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42615\n\nReviewed By: mrshenli\n\nDifferential Revision: D22958660\n\nPulled By: malfet\n\nfbshipit-source-id: 97a6a0e769143bd161667d0ee081ea0751995775", "pr_number": "42615", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml"], "labels": ["merged", "open source"]}, "bcab2d6848": {"title": "And type annotations for cpp_extension, utils.data, signal_handling (#42647)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42647\n\nReviewed By: ezyang\n\nDifferential Revision: D22967041\n\nPulled By: malfet\n\nfbshipit-source-id: 35e124da0be56934faef56834a93b2b400decf66", "pr_number": "42647", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in", "torch/utils/cpp_extension.py", "torch/utils/data/_utils/signal_handling.py", "torch/utils/data/dataloader.py"], "labels": ["merged", "module: typing", "open source"]}, "049c1b97be": {"title": "pin numpy version to 1.18.5 (#42670)", "body": "Summary:\nUsing numpy 1.19.x instead of 1.18.x breaks certain unit tests.\nFixes https://github.com/pytorch/pytorch/issues/42561.  Likely also fixes https://github.com/pytorch/pytorch/issues/42583.\n\nCC ezyang xw285cornell sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42670\n\nReviewed By: ezyang\n\nDifferential Revision: D22978369\n\nPulled By: malfet\n\nfbshipit-source-id: ce1f35c7ba620c2b9dd10613f39354cebee8b87d", "pr_number": "42670", "files_changed": [".circleci/docker/common/install_conda.sh", "docker/caffe2/jenkins/common/install_python.sh"], "labels": ["merged", "module: rocm", "open source"]}, "a5af2434fe": {"title": "NVMified NE Eval", "body": "Summary:\nThis diff NVMifies the NE Eval Flow.\n- It defines a `LoadNVM` operator which either\n  - receives a list of nvm blobs, or\n  - extracts the blobs that could be NVMified from the model.\n- dumps NVMified blobs into NVM\n-  and deallocates from DRAM\n- NVMify the Eval net on dper and C2 backend\n\nSpecific NVMOp for SLS is pushed through different diffs.\n\nTest Plan: flow-cli test-locally dper.workflows.evaluation.eval_workflow --parameters-file=/mnt/public/ehsaardestani/temp/small_model.json 2>&1 | tee log\n\nReviewed By: yinghai, amylittleyang\n\nDifferential Revision: D22469973\n\nfbshipit-source-id: ed8379ad404e96d04ac05e580176d3aca984575b", "pr_number": null, "files_changed": ["caffe2/core/plan_executor.cc"], "labels": []}, "65066d779b": {"title": "Add fastrnns benchmark to CI and upload data to scribe (#42030)", "body": "Summary:\nRun fastrnns benchmark using pytest-benchmark infra, then parse its json format and upload to scribe.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42030\n\nReviewed By: malfet\n\nDifferential Revision: D22970270\n\nPulled By: wconstab\n\nfbshipit-source-id: 87da9b7ddf741da14b80d20779771d19123be3c5", "pr_number": "42030", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".gitignore", ".jenkins/pytorch/test.sh", "benchmarks/fastrnns/factory.py", "benchmarks/fastrnns/test_bench.py", "benchmarks/upload_scribe.py"], "labels": ["merged"]}, "a4dbc64800": {"title": "Add documentation for PYTORCH_JIT_TYPE_VERBOSITY (#42241)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42241\n\nthat's it\n\nTest Plan: docs only\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22818705\n\nfbshipit-source-id: 22cdf4f23c3ed0a15c23f116457fc842d7f7b520", "pr_number": "42241", "files_changed": ["torch/csrc/jit/OVERVIEW.md"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "57854e7f08": {"title": "[JIT] Clone runOptimizations and similar functions for profiling executor. (#42656)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42656\n\nThing change will allow us to more freely experiment with pass pipelines\nin the profiling executor without affecting passes in the legacy\nexecutor. Also, it somewhat helps to keep all passes in one place to be\nable to tell what's going on.\n\nCurrently this change should not affect any behavior as I copied the\npasses exactly as they've been invoked before, but we will probably want\nto change these pipelines in a near future.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D22971050\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: f5bb60783a553c7b51c5343eec7f8fe40037ff99", "pr_number": "42656", "files_changed": ["torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["merged", "oncall: jit"]}, "e28a98a904": {"title": "Turn on non ASCII string literals serialization (#40719)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40719\n\nThis is a follow up patch to turn on this feature in order to handle breaking\nforward compatibility.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D22457952\n\nPulled By: bzinodev\n\nfbshipit-source-id: fac0dfed8b8b5fa2d52d342ee8cf06742959b3c5", "pr_number": "40719", "files_changed": ["torch/csrc/jit/serialization/python_print.cpp"], "labels": ["merged", "oncall: jit"]}, "79de9c028a": {"title": "Remove VS2017 workaround for autocasting (#42352)", "body": "Summary:\nBecause VS2017 is no longer supported after https://github.com/pytorch/pytorch/pull/42144\ncc: mcarilli\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42352\n\nReviewed By: malfet\n\nDifferential Revision: D22962809\n\nPulled By: ngimel\n\nfbshipit-source-id: 0346cde87bf5d617dfc0d7b34c92ac6ec5bbf568", "pr_number": "42352", "files_changed": ["aten/src/ATen/autocast_VS2017_helper.h", "aten/src/ATen/autocast_mode.cpp"], "labels": ["merged", "module: cuda", "module: windows", "open source", "triaged"]}, "5ca08b8891": {"title": "Add benchmark for calculate_qparams (#42138)", "body": "Summary:\nAdds a benchmark for `HistogramObserver.calculate_qparams` to the quantized op benchmarks. The next diff in this stack adds a ~15x speedup for this benchmark.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42138\n\nTest Plan:\nWhile in the folder `benchmarks/operator_benchmark`, the benchmark can be run using `python -m benchmark_all_quantized_test --operators HistogramObserverCalculateQparams`.\n\nBenchmark results before speedup:\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: HistogramObserverCalculateQparams\n# Mode: Eager\n# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine\n# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine\nForward Execution Time (us) : 185818.566\n\n# Benchmarking PyTorch: HistogramObserverCalculateQparams\n# Mode: Eager\n# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric\n# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric\nForward Execution Time (us) : 165325.916\n```\n\nBenchmark results after speedup:\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: HistogramObserverCalculateQparams\n# Mode: Eager\n# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine\n# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine\nForward Execution Time (us) : 12242.241\n\n# Benchmarking PyTorch: HistogramObserverCalculateQparams\n# Mode: Eager\n# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric\n# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric\nForward Execution Time (us) : 12655.354\n```\n\nReviewed By: supriyar\n\nDifferential Revision: D22779291\n\nPulled By: durumu\n\nfbshipit-source-id: 1fe17d20eda5dd99e0e2590480142034c3574d4e", "pr_number": "42138", "files_changed": ["benchmarks/operator_benchmark/pt/qobserver_test.py"], "labels": ["merged"]}, "40b6dacb50": {"title": "Delete dead is_named_tensor_only (#42672)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42672\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D22978389\n\nPulled By: ezyang\n\nfbshipit-source-id: ef1302c57fe26a58a46ca1f4a4a7c3e2cdbfdc5d", "pr_number": "42672", "files_changed": ["aten/src/ATen/native_parse.py"], "labels": ["merged"]}, "85a00c4c92": {"title": "Skips spectral tests to prevent ROCm build from timing out (#42667)", "body": "Summary:\nPer title.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42667\n\nReviewed By: ailzhang\n\nDifferential Revision: D22978531\n\nPulled By: mruberry\n\nfbshipit-source-id: 0c3ba116836ed6c433e2c6a0e1a0f2e3c94c7803", "pr_number": "42667", "files_changed": ["test/test_spectral_ops.py"], "labels": ["merged", "module: rocm", "module: tests"]}, "1f689b6ef9": {"title": "suppress all Autograd keys in AutoNonVariableTypeMode (#42610)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42610\n\nFix for https://github.com/pytorch/pytorch/issues/42609: `AutoNonVariableTypeMode` should suppress all autograd dispatch keys, not just `Autograd` (e.g. `XLAPreAutograd`, `PrivateUse<N>_PreAutograd`)\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22963408\n\nPulled By: bhosmer\n\nfbshipit-source-id: 2f3516580ce0c9136aff5e025285d679394f2f18", "pr_number": "42610", "files_changed": ["aten/src/ATen/core/LegacyTypeDispatch.h", "c10/core/impl/LocalDispatchKeySet.cpp", "c10/core/impl/LocalDispatchKeySet.h"], "labels": ["merged"]}, "f22aa601ce": {"title": "All Gather and gather APIs for Python Objects (#42189)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42189\n\nRehash of https://github.com/pytorch/pytorch/pull/28811, which was several months old.\n\nAs part of addressing https://github.com/pytorch/pytorch/issues/23232, this PR adds support for the following APIs:\n\n`allgather_object` and `gather_object` to support gather/allgather of generic, pickable Python objects. This has been a long-requested feature so PyTorch should provide these helpers built-in.\n\nThe methodology is what is proposed in the original issue:\n1) Pickle object to ByteTensor using torch.save\n2) Comm. tensor sizes\n3) Copy local ByteTensor into a tensor of maximal size\n4) Call tensor-based collectives on the result of (3)\n5) Unpickle back into object using torch.load\n\nNote that the API is designed to match other than supporting `async_op`. For now, it is a blocking call. If we see demand to support `async_op`, we will have to make more progress on merging work/future to support this.\n\nIf this is a suitable approach, we can support `scatter`, `broadcast` in follow up PRs.\nghstack-source-id: 109322433\n\nReviewed By: mrshenli\n\nDifferential Revision: D22785387\n\nfbshipit-source-id: a265a44ec0aa3aaffc3c6966023400495904c7d8", "pr_number": "42189", "files_changed": ["test/distributed/test_distributed.py", "torch/distributed/distributed_c10d.py", "torch/testing/_internal/common_distributed.py"], "labels": ["merged"]}, "b44a10c179": {"title": "List[index]::toOptionalStringRef (#42263)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42263\n\nAllow a way to get a reference to the stored string in a `List<optional<string>>` without having to copy the string.\nThis for example improves perf of the map_lookup op by 3x.\nghstack-source-id: 109162026\n\nTest Plan: unit tests\n\nReviewed By: ezyang\n\nDifferential Revision: D22830381\n\nfbshipit-source-id: e6af2bc8cebd6e68794eb18daf183979bc6297ae", "pr_number": "42263", "files_changed": ["aten/src/ATen/core/List.h", "aten/src/ATen/core/List_test.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h"], "labels": ["merged"]}, "cdd7db1ffc": {"title": "Bound shape inferencer: fix int8fc scale and bias", "body": "Summary:\nPrevious when inferring Int8FC, we failed to carry over the scale and zero point properly.\n\nAlso fixed int8 FC weight data type to be int8 instead of uint8 as that's what C2 actually uses.\n\nTest Plan: Use net_runner to lower a single Int8Dequantize op. Previous scale and bias would always be 1 and 0. Now the proper value is set.\n\nReviewed By: yinghai\n\nDifferential Revision: D22912186\n\nfbshipit-source-id: a6620c3493e492bdda91da73775bfc9117db12d1", "pr_number": null, "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc"], "labels": []}, "cc596ac3a8": {"title": "[JIT] Add debug dumps in between passes in graph executor. (#42688)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42688\n\nBoth the profiling executor and the legacy executor have the debug\nloggin now.\n\nIdeally, if we had a pass manager, this could be done as a part of it,\nbut since we have none, I had to insert the debug statements manually.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D22981675\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 22b8789e860aa90d5802fc72a4113b22c6fc4da5", "pr_number": "42688", "files_changed": ["torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["merged", "oncall: jit"]}, "6cb0807f88": {"title": "Fixes ROCm CI (#42701)", "body": "Summary:\nPer title. ROCm CI doesn't have MKL so this adds a couple missing test annotations.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42701\n\nReviewed By: ngimel\n\nDifferential Revision: D22986273\n\nPulled By: mruberry\n\nfbshipit-source-id: efa717e2e3771562e9e82d1f914e251918e96f64", "pr_number": "42701", "files_changed": ["test/test_spectral_ops.py"], "labels": ["merged", "module: rocm", "module: tests"]}, "eaace3e10e": {"title": "Skip CUDA benchmarks on nogpu configs (#42704)", "body": "Summary:\nAvoids timeouts when the benchmark is launched on nogpu configs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42704\n\nReviewed By: mruberry\n\nDifferential Revision: D22987725\n\nPulled By: malfet\n\nfbshipit-source-id: aa9aece16557c0af8e05e612277ae1d9e0173a51", "pr_number": "42704", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "33519e19ab": {"title": "Fix 64-bit indexing in GridSampler (#41923)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41656\n\nFor the CPU version, this is a regression introduced in https://github.com/pytorch/pytorch/issues/10980 which vectorized the `grid_sampler_2d` implementation. It uses the AVX2 gather intrinsic which for `float` requires 32-bit indexing to match the number of floats in the AVX register. There is also an `i64gather_ps` variant but this only utilizes half of the vector width so would be expected to give worse performance in the more likely case where 32-bit indexing is acceptable. So, I've left the optimised AVX version as-is and reinstated the old non-vectorized version as a fallback.\n\nFor the CUDA version, this operation has never supported 32-bit indexing so this isn't a regression. I've templated the kernel on index type and added 64-bit variants. Although I gather in some places a simple `TORCH_CHECK(canUse32BitIndexMath(...))` is used instead. So, there is a decision to be made here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41923\n\nReviewed By: glaringlee\n\nDifferential Revision: D22925931\n\nPulled By: zou3519\n\nfbshipit-source-id: 920816107aae26360c5e7f4e9c729fa9057268bb", "pr_number": "41923", "files_changed": ["aten/src/ATen/cuda/detail/IndexUtils.cu", "aten/src/ATen/cuda/detail/IndexUtils.cuh", "aten/src/ATen/cuda/detail/KernelUtils.h", "aten/src/ATen/native/GridSampler.cpp", "aten/src/ATen/native/IndexingUtils.cpp", "aten/src/ATen/native/IndexingUtils.h", "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "aten/src/ATen/native/cuda/GridSampler.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_nn.py", "tools/autograd/derivatives.yaml", "torch/testing/_internal/common_device_type.py"], "labels": ["merged", "module: cuda", "module: nn", "open source", "triaged"]}, "dab9bbfce7": {"title": "Move jit_profiling tests into test1 on Windows (#42650)", "body": "Summary:\nTest takes 5 min to finish and 5 min to spin up the environment, so it doesn't make much sense to keep it as separate config\nLimit those tests to be run only when `USE_CUDA` environment variable is set to tru\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42650\n\nReviewed By: ailzhang\n\nDifferential Revision: D22967817\n\nPulled By: malfet\n\nfbshipit-source-id: c6c26df140059491e7ff53ee9cbbc93433d2f36f", "pr_number": "42650", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".jenkins/pytorch/win-test-helpers/test_python_jit_profiling.bat", ".jenkins/pytorch/win-test.sh"], "labels": ["merged"]}, "f9a6c14364": {"title": "Fix sequence numbers in profiler output (#42565)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42565\n\nAfter recent changes to the record function we record more\nranges in profiler output and also keep emitting sequence numbers for\nall ranges.\n\nSequence numbers are used by external tools to correlate forward\nand autograd ranges and with many ranges having the same sequence number\nit becomes impossible to do this.\n\nThis PR ensures that we set sequence numbers only for the top-level\nranges and only in case when autograd is enabled.\n\nTest Plan:\nnvprof -fo trace.nvvp --profile-from-start off python test_script.py\ntest_script\nhttps://gist.github.com/ilia-cher/2baffdd98951ee2a5f2da56a04fe15d0\nthen examining ranges in nvvp\n\nReviewed By: ngimel\n\nDifferential Revision: D22938828\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 9a5a076706a6043dfa669375da916a1708d12c19", "pr_number": "42565", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/grad_mode.cpp", "test/test_autograd.py", "torch/autograd/profiler.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/profiler.h"], "labels": ["merged"]}, "40ac95dd3c": {"title": "[ONNX] Update ONNX export of torch.where to support ByteTensor as input. (#42264)", "body": "Summary:\n`torch.where` supports `ByteTensor` and `BoolTensor` types for the first input argument (`condition` predicate). Currently, ONNX exporter assumes that the first argument is `BoolTensor`. This PR updates the export for `torch.where` to correctly support export when first argument is a `ByteTensor`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42264\n\nReviewed By: houseroad\n\nDifferential Revision: D22968473\n\nPulled By: bzinodev\n\nfbshipit-source-id: 7306388c8446ef3faeb86dc89d72d1f72c1c2314", "pr_number": "42264", "files_changed": ["test/onnx/expect/TestOperators.test_expand.expect", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "4959981cff": {"title": "[ONNX] Export tensor (#41872)", "body": "Summary:\nAdding tensor symbolic for opset 9\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41872\n\nReviewed By: houseroad\n\nDifferential Revision: D22968426\n\nPulled By: bzinodev\n\nfbshipit-source-id: 70e1afc7397e38039e2030e550fd72f09bac7c7c", "pr_number": "41872", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "9152f2f73a": {"title": "Optimization of Backward Implementation for Learnable Fake Quantize Per Tensor Kernels (CPU and GPU) (#42384)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42384\n\nIn this diff, the original backward pass implementation is sped up by merging the 3 iterations computing dX, dScale, and dZeroPoint separately. In this case, a native loop is directly used on a byte-wise level (referenced by `strides`).\n\nIn the benchmark test on the operators, for an input of shape `3x3x256x256`, we have observed the following improvement in performance:\n- original python operator: 1021037 microseconds\n- original learnable kernel: 407576 microseconds\n- optimized learnable kernel: 102584 microseconds\n- original non-backprop kernel: 139806 microseconds\n\n**Speedup from python operator**: ~10x\n**Speedup from original learnable kernel**: ~4x\n**Speedup from non-backprop kernel**: ~1.2x\n\nTest Plan:\nTo assert correctness of the new kernel, on a devvm, enter the command\n\n`buck test //caffe2/test:quantization -- learnable_backward_per_tensor`\n\nTo benchmark the operators, on a devvm, enter the command\n1. Set the kernel size to 3x3x256x256 or a reasonable input size.\n2. Run `buck test //caffe2/benchmarks/operator_benchmark/pt:quantization_test`\n3. The relevant outputs are as follows:\n\n(CPU)\n```\n# Benchmarking PyTorch: FakeQuantizePerTensorOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerTensorOpBenchmark_N3_C3_H256_W256_nbits4_cpu_op_typepy_module\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: py_module\nBackward Execution Time (us) : 1021036.957\n\n# Benchmarking PyTorch: FakeQuantizePerTensorOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerTensorOpBenchmark_N3_C3_H256_W256_nbits4_cpu_op_typelearnable_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: learnable_kernel\nBackward Execution Time (us) : 102583.693\n\n# Benchmarking PyTorch: FakeQuantizePerTensorOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerTensorOpBenchmark_N3_C3_H256_W256_nbits4_cpu_op_typeoriginal_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: original_kernel\nBackward Execution Time (us) : 139806.086\n```\n\n(GPU)\n```\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typepy_module\n# Input: N: 3, C: 3, H: 256, W: 256, device: cuda, op_type: py_module\nBackward Execution Time (us) : 6548.350\n\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typelearnable_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cuda, op_type: learnable_kernel\nBackward Execution Time (us) : 1340.724\n\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typeoriginal_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cuda, op_type: original_kernel\nBackward Execution Time (us) : 656.863\n```\n\nReviewed By: vkuzo\n\nDifferential Revision: D22875998\n\nfbshipit-source-id: cfcd62c327bb622270a783d2cbe97f00508c4a16", "pr_number": "42384", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu", "aten/src/ATen/native/quantized/fake_quant_affine.h", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp"], "labels": ["fb-exported", "merged"]}, "a6c8730045": {"title": "[ONNX] Add preprocess pass for onnx export (#41832)", "body": "Summary:\nin `_jit_pass_onnx`, symbolic functions are called for each node for conversion. However, there are nodes that cannot be converted without additional context. For example, the number of outputs from split (and whether it is static or dynamic) is unknown until the point where it is unpacked by listUnpack node. This pass does a preprocess, and prepares the nodes such that enough context can be received by the symbolic function.\n* After preprocessing, `_jit_pass_onnx` should have enough context to produce valid ONNX nodes, instead of half baked nodes that replies on fixes from later postpasses.\n* `_jit_pass_onnx_peephole` should be a pass that does ONNX specific optimizations instead of ONNX specific fixes.\n* Producing more valid ONNX nodes in `_jit_pass_onnx` enables better utilization of the ONNX shape inference https://github.com/pytorch/pytorch/issues/40628.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41832\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22968334\n\nPulled By: bzinodev\n\nfbshipit-source-id: 8226f03c5b29968e8197d242ca8e620c6e1d42a5", "pr_number": "41832", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "test/onnx/test_pytorch_onnx_onnxruntime.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/onnx/helper.cpp", "torch/csrc/jit/passes/onnx/helper.h", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp", "torch/csrc/jit/passes/onnx/preprocess_for_onnx.h", "torch/csrc/jit/python/init.cpp", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py"], "labels": ["merged", "module: onnx", "oncall: jit", "open source", "triaged"]}, "952526804c": {"title": "Print TE CUDA kernel (#42692)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42692\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D22986112\n\nPulled By: bertmaher\n\nfbshipit-source-id: 52ec3389535c8b276858bef8c470a59aeba4946f", "pr_number": "42692", "files_changed": ["torch/csrc/jit/tensorexpr/cuda_codegen.cpp"], "labels": ["merged", "oncall: jit"]}, "9597af01ca": {"title": "Support iterating through an Enum class (#42661)", "body": "Summary:\n[5/N] Implement Enum JIT support\n\nImplement Enum class iteration\nAdd aten.ne for EnumType\n\nSupported:\nEnum-typed function arguments\nusing Enum type and comparing them\nSupport getting name/value attrs of enums\nUsing Enum value as constant\nSupport Enum-typed return values\nSupport iterating through Enum class (enum value list)\n\nTODO:\nSupport serialization and deserialization\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42661\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22977364\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 1a0216f91d296119e34cc292791f9aef1095b5a8", "pr_number": "42661", "files_changed": ["test/jit/test_enum.py", "torch/csrc/jit/ir/node_hashing.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["merged", "oncall: jit"]}, "cb1ac94069": {"title": "[blob reorder] Seperate user embeddings and ad embeddings in large model loading script", "body": "Summary: Put user embedding before ads embedding in blobReorder, for flash verification reason.\n\nTest Plan:\n```\nbuck run mode/opt-clang -c python.package_style=inplace sigrid/predictor/scripts:enable_large_model_loading -- --model_path_src=\"/home/$USER/models/\" --model_path_dst=\"/home/$USER/models_modified/\" --model_file_name=\"182560549_0.predictor\"\n```\nhttps://www.internalfb.com/intern/anp/view/?id=320921 to check blobsOrder\n\nReviewed By: yinghai\n\nDifferential Revision: D22964332\n\nfbshipit-source-id: 78b4861476a3c889a5ff62492939f717c307a8d2", "pr_number": null, "files_changed": ["caffe2/proto/metanet.proto"], "labels": []}, "73642d9425": {"title": "Updates alias pattern (and torch.absolute to use it) (#42586)", "body": "Summary:\nThis PR canonicalizes our (current) pattern for adding aliases to PyTorch. That pattern is:\n\n- Copy the original functions native_functions.yaml entry, but replace the original function's name with their own.\n- Implement the corresponding functions and have them redispatch to the original function.\n- Add docstrings to the new functions that reference the original function.\n- Update the alias_map in torch/csrc/jit/passes/normalize_ops.cpp.\n- Update the op_alias_mappings in torch/testing/_internal/jit_utils.py.\n- Add a test validating the alias's behavior is the same as the original function's.\n\nAn alternative pattern would be to use Python and C++ language features to alias ops directly. For example in Python:\n\n```\ntorch.absolute = torch.abs\n```\n\nLet the pattern in this PR be the \"native function\" pattern, and the alternative pattern be the \"language pattern.\" There are pros/cons to both approaches:\n\n**Pros of the \"Language Pattern\"**\n- torch.absolute is torch.abs.\n- no (or very little) overhead for calling the alias.\n- no native_functions.yaml redundancy or possibility of \"drift\" between the original function's entries and the alias's.\n\n**Cons of the \"Language Pattern\"**\n- requires manually adding doc entries\n- requires updating Python alias and C++ alias lists\n- requires hand writing alias methods on Tensor (technically this should require a C++ test to validate)\n- no single list of all PyTorch ops -- have to check native_functions.yaml and one of the separate alias lists\n\n**Pros of the \"Native Function\" pattern**\n\n- alias declarations stay in native_functions.yaml\n- doc entries are written as normal\n\n**Cons of the \"Native Function\" pattern**\n\n- aliases redispatch to the original functions\n- torch.absolute is not torch.abs (requires writing test to validate behavior)\n- possibility of drift between original's and alias's native_functions.yaml entries\n\nWhile either approach is reasonable, I suggest the \"native function\" pattern since it preserves \"native_functions.yaml\" as a source of truth and minimizes the number of alias lists that need to be maintained. In the future, entries in native_functions.yaml may support an \"alias\" argument and replace whatever pattern we choose now.\n\nOps that are likely to use aliasing are:\n\n- div (divide, true_divide)\n- mul (multiply)\n- bucketize (digitize)\n- cat (concatenate)\n- clamp (clip)\n- conj (conjugate)\n- rad2deg (degrees)\n- trunc (fix)\n- neg (negative)\n- deg2rad (radians)\n- round (rint)\n- acos (arccos)\n- acosh (arcosh)\n- asin (arcsin)\n- asinh (arcsinh)\n- atan (arctan)\n- atan2 (arctan2)\n- atanh (arctanh)\n- bartlett_window (bartlett)\n- hamming_window (hamming)\n- hann_window (hanning)\n- bitwise_not (invert)\n- gt (greater)\n- ge (greater_equal)\n- lt (less)\n- le (less_equal)\n- ne (not_equal)\n- ger (outer)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42586\n\nReviewed By: ngimel\n\nDifferential Revision: D22991086\n\nPulled By: mruberry\n\nfbshipit-source-id: d6ac96512d095b261ed2f304d7dddd38cf45e7b0", "pr_number": "42586", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "fb8aa0046c": {"title": "Add use_glow_aot, and include ONNX again as a backend for onnxifiGlow (#4787)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/glow/pull/4787\n\nResurrect ONNX as a backend through onnxifiGlow (was killed as part of D16215878). Then look for the `use_glow_aot` argument in the Onnxifi op. If it's there and true, then we override whatever `backend_id` is set and use the ONNX backend.\n\nReviewed By: yinghai, rdzhabarov\n\nDifferential Revision: D22762123\n\nfbshipit-source-id: abb4c3458261f8b7eeae3016dda5359fa85672f0", "pr_number": null, "files_changed": ["caffe2/opt/onnxifi_op.h"], "labels": []}, "4eb02add51": {"title": "Blacklist to Blocklist in onnxifi_transformer (#42590)", "body": "Summary:\nFixes issues in https://github.com/pytorch/pytorch/issues/41704 and https://github.com/pytorch/pytorch/issues/41705\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42590\n\nReviewed By: ailzhang\n\nDifferential Revision: D22977357\n\nPulled By: malfet\n\nfbshipit-source-id: ab61b964cfdf8bd2b469f4ff8f6486a76bc697de", "pr_number": "42590", "files_changed": ["caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/onnxifi_transformer.h"], "labels": ["merged", "open source"]}, "3c66a3795a": {"title": "[vulkan] Ops registration to TORCH_LIBRARY_IMPL (#42194)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42194\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22803036\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 2f402541aecf887d78f650bf05d758a0e403bc4d", "pr_number": "42194", "files_changed": ["BUILD.bazel", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/AdaptiveAveragePooling.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanAten.h", "aten/src/ATen/native/vulkan/VulkanConvolution.cpp", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/stub/VulkanAtenStub.cpp", "aten/src/ATen/test/vulkan_test.cpp", "aten/src/ATen/vulkan/Context.cpp", "aten/src/ATen/vulkan/Context.h"], "labels": ["merged"]}, "31ed468905": {"title": "Fix cmake warning (#42707)", "body": "Summary:\nIf argumenets in set_target_properties are not separated by whitespace, cmake raises a warning:\n```\nCMake Warning (dev) at cmake/public/cuda.cmake:269:\n  Syntax Warning in cmake code at column 54\n\n  Argument not separated from preceding token by whitespace.\n```\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42707\n\nReviewed By: ailzhang\n\nDifferential Revision: D22988055\n\nPulled By: malfet\n\nfbshipit-source-id: c3744f23b383d603788cd36f89a8286a46b6c00f", "pr_number": "42707", "files_changed": ["cmake/public/cuda.cmake"], "labels": ["merged"]}, "c9346ad3b8": {"title": "[CPU] Added torch.bmm for complex tensors (#42383)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42383\n\nTest Plan - Updated existing tests to run for complex dtypes as well.\n\nAlso added tests for `torch.addmm`, `torch.badmm`\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22960339\n\nPulled By: anjali411\n\nfbshipit-source-id: 0805f21caaa40f6e671cefb65cef83a980328b7d", "pr_number": "42383", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/mkl/LinearAlgebra.cpp", "test/test_torch.py"], "labels": ["merged"]}, "9c8021c0b1": {"title": "Adds torch.linalg namespace (#42664)", "body": "Summary:\nThis PR adds the `torch.linalg` namespace as part of our continued effort to be more compatible with NumPy. The namespace is tested by adding a single function, `torch.linalg.outer`, and testing it in a new test suite, test_linalg.py. It follows the same pattern that https://github.com/pytorch/pytorch/pull/41911, which added the `torch.fft` namespace, did.\n\nFuture PRs will likely:\n\n- add more functions to torch.linalg\n- expand the testing done in test_linalg.py, including legacy functions, like torch.ger\n- deprecate existing linalg functions outside of `torch.linalg` in preference to the new namespace\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42664\n\nReviewed By: ngimel\n\nDifferential Revision: D22991019\n\nPulled By: mruberry\n\nfbshipit-source-id: 39258d9b116a916817b3588f160b141f956e5d0b", "pr_number": "42664", "files_changed": ["BUILD.bazel", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/README.md", "aten/src/ATen/native/native_functions.yaml", "caffe2/CMakeLists.txt", "docs/source/index.rst", "docs/source/linalg.rst", "test/test_linalg.py", "tools/autograd/gen_autograd.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_linalg_functions.cpp", "tools/build_variables.bzl", "torch/__init__.py", "torch/csrc/Module.cpp", "torch/csrc/api/include/torch/all.h", "torch/csrc/api/include/torch/linalg.h", "torch/csrc/autograd/python_linalg_functions.h", "torch/jit/_builtins.py", "torch/linalg/__init__.py"], "labels": ["merged", "module: numpy", "oncall: jit"]}, "dcee8933fb": {"title": "Fix some linking rules to allow path with whitespaces (#42718)", "body": "Summary:\nEssentially, replace `-Wl,--whole-archive,$<TARGET_FILE:FOO>` with `-Wl,--whole-archive,\\\"$<TARGET_FILE:FOO>\\\"` as TARGET_FILE might return path containing whitespaces\n\nFixes https://github.com/pytorch/pytorch/issues/42657\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42718\n\nReviewed By: ezyang\n\nDifferential Revision: D22993568\n\nPulled By: malfet\n\nfbshipit-source-id: de878b17d20e35b51dd350f20d079c8b879f70b5", "pr_number": "42718", "files_changed": ["cmake/public/utils.cmake"], "labels": ["merged"]}, "2971bc23a6": {"title": "Handle fused scale and bias in fake fp16 layernorm", "body": "Summary: Allow passing scale and bias to fake fp16 layernorm.\n\nTest Plan: net_runner. Now matches glow's fused layernorm.\n\nReviewed By: hyuen\n\nDifferential Revision: D22952646\n\nfbshipit-source-id: cf9ad055b14f9d0167016a18a6b6e26449cb4de8", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h", "caffe2/opt/custom/fakefp16_transform.cc", "caffe2/opt/custom/fakefp16_transform.h"], "labels": []}, "944ac133d0": {"title": "[NNC] Remove VarBinding and go back to Let stmts (#42634)", "body": "Summary:\nAwhile back when commonizing the Let and LetStmt nodes, I ended up removing both and adding a separate VarBinding section the Block. At the time I couldn't find a counter example, but I found it today: Local Vars and Allocations dependencies may go in either direction and so we need to support interleaving of those statements.\n\nSo, I've removed all the VarBinding logic and reimplemented Let statements. ZolotukhinM I think you get to say \"I told you so\". No new tests, existing tests should cover this.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42634\n\nReviewed By: mruberry\n\nDifferential Revision: D22969771\n\nPulled By: nickgg\n\nfbshipit-source-id: a46c5193357902d0f59bf30ab103fe123b1503f1", "pr_number": "42634", "files_changed": ["test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.h", "torch/csrc/jit/tensorexpr/eval.h", "torch/csrc/jit/tensorexpr/hash_provider.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.h", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/ir_printer.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.h", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["merged", "oncall: jit"]}, "586399c03f": {"title": "Remove duplicate definitions of CppTypeToScalarType (#42640)", "body": "Summary:\nI noticed that `TensorIteratorDynamicCasting.h` defines a helper meta-function `CPPTypeToScalarType` which does exactly the same thing as the `c10::CppTypeToScalarType` meta-function I added in gh-40927. No need for two identical definitions.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42640\n\nReviewed By: malfet\n\nDifferential Revision: D22969708\n\nPulled By: ezyang\n\nfbshipit-source-id: 8303c7f4a75ae248f393a4811ae9d2bcacab44ff", "pr_number": "42640", "files_changed": ["aten/src/ATen/native/TensorIteratorDynamicCasting.h"], "labels": ["merged", "open source"]}, "04c62d4a06": {"title": "[vulkan] Fix warnings: static_cast, remove unused (#42195)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42195\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22803035\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: d7bf256437eccb5c421a7fd0aa8ec23a8fec0470", "pr_number": "42195", "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.cpp", "aten/src/ATen/native/vulkan/Vulkan.h", "aten/src/ATen/native/vulkan/VulkanCommon.h", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "9f88bcb5a2": {"title": "Minor typo fix (#42731)", "body": "Summary:\nJust fixed a typo in test/test_sparse.py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42731\n\nReviewed By: ezyang\n\nDifferential Revision: D22999930\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1b5b21d7cb274bd172fb541b2761f727ba06302c", "pr_number": "42731", "files_changed": ["test/test_sparse.py"], "labels": ["merged", "open source"]}, "eba35025e0": {"title": "[JIT] Exclude staticmethods from TS class compilation (#42611)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42611\n\n**Summary**\nThis commit modifies the Python frontend to ignore static functions on\nTorchscript classes when compiling them. They are currently included\nalong with methods, which causes the first argument of the\nstaticfunction to be unconditionally inferred to be of the type of the\nclass it belongs to (regardless of how it is annotated or whether it is\nannotated at all). This can lead to compilation errors depending on\nhow that argument is used in the body of the function.\n\nStatic functions are instead imported and scripted as if they were\nstandalone functions.\n\n**Test Plan**\nThis commit augments the unit test for static methods in `test_class_types.py`\nto test that static functions can call each other and the class\nconstructor.\n\n**Fixes**\nThis commit fixes #39308.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22958163\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 45c3c372792299e6e5288e1dbb727291e977a2af", "pr_number": "42611", "files_changed": ["test/jit/test_class_type.py", "torch/_jit_internal.py", "torch/jit/frontend.py"], "labels": ["merged", "oncall: jit"]}, "98de150381": {"title": "C++ API TransformerEncoderLayer (#42633)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42633\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22994332\n\nPulled By: glaringlee\n\nfbshipit-source-id: 873abdf887d135fb05bde560d695e2e8c992c946", "pr_number": "42633", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/api/CMakeLists.txt", "test/cpp/api/transformer.cpp", "tools/build_variables.bzl", "torch/csrc/api/include/torch/enum.h", "torch/csrc/api/include/torch/nn/modules.h", "torch/csrc/api/include/torch/nn/modules/transformer.h", "torch/csrc/api/include/torch/nn/options.h", "torch/csrc/api/include/torch/nn/options/transformer.h", "torch/csrc/api/src/enum.cpp", "torch/csrc/api/src/nn/modules/transformer.cpp", "torch/csrc/api/src/nn/options/transformer.cpp"], "labels": ["merged"]}, "7332c21f7a": {"title": "Speed up HistogramObserver by vectorizing critical path (#41041)", "body": "Summary:\n22x speedup over the code this replaces. Tested on ResNet18 on a devvm using CPU only, using default parameters for HistogramObserver (i.e. 2048 bins).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41041\n\nTest Plan:\nTo run the test against the reference (old) implementation, you can use `python test/test_quantization.py TestRecordHistogramObserver.test_histogram_observer_against_reference`.\n\nTo run the benchmark, while in the folder `benchmarks/operator_benchmark`, you can use `python -m benchmark_all_quantized_test --operators HistogramObserverCalculateQparams`.\n\nBenchmark results before speedup:\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: HistogramObserverCalculateQparams\n# Mode: Eager\n# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine\n# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine\nForward Execution Time (us) : 185818.566\n\n# Benchmarking PyTorch: HistogramObserverCalculateQparams\n# Mode: Eager\n# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric\n# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric\nForward Execution Time (us) : 165325.916\n```\n\nBenchmark results after speedup:\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: HistogramObserverCalculateQparams\n# Mode: Eager\n# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine\n# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine\nForward Execution Time (us) : 12242.241\n\n# Benchmarking PyTorch: HistogramObserverCalculateQparams\n# Mode: Eager\n# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric\n# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric\nForward Execution Time (us) : 12655.354\n```\n\nReviewed By: raghuramank100\n\nDifferential Revision: D22400755\n\nPulled By: durumu\n\nfbshipit-source-id: 639ac796a554710a33c8a930c1feae95a1148718", "pr_number": "41041", "files_changed": ["test/quantization/test_workflow_module.py", "torch/quantization/observer.py"], "labels": ["merged"]}, "6ebc0504ca": {"title": "BAND, BOR and BXOR for NCCL (all_)reduce should throw runtime errors (#42669)", "body": "Summary:\ncc rohan-varma\nFixes https://github.com/pytorch/pytorch/issues/41362 #39708\n\n# Description\nNCCL doesn't support `BAND, BOR, BXOR`. Since the [current mapping](https://github.com/pytorch/pytorch/blob/0642d17efc73041e5209e3be265d9a39892e8908/torch/lib/c10d/ProcessGroupNCCL.cpp#L39) doesn't contain any of the mentioned bitwise operator, a default value of `ncclSum` is used instead.\n\nThis PR should provide the expected behaviour where a runtime exception is thrown.\n\n# Notes\n- The way I'm throwing exceptions is derived from [ProcessGroupGloo.cpp](https://github.com/pytorch/pytorch/blob/0642d17efc73041e5209e3be265d9a39892e8908/torch/lib/c10d/ProcessGroupGloo.cpp#L101)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42669\n\nReviewed By: ezyang\n\nDifferential Revision: D22996295\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 83a9fedf11050d2890f9f05ebcedf53be0fc3516", "pr_number": "42669", "files_changed": ["test/distributed/test_c10d.py", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged", "open source", "triaged"]}, "02f58bdbd7": {"title": "[caffe2] add type annotations for caffe2.distributed.python", "body": "Summary: Add Python type annotations for the `caffe2.distributed.python` module.\n\nTest Plan: Will check sandcastle results.\n\nReviewed By: jeffdunn\n\nDifferential Revision: D22994012\n\nfbshipit-source-id: 30565cc41dd05b5fbc639ae994dfe2ddd9e56cb1", "pr_number": null, "files_changed": ["caffe2/distributed/python.pyi"], "labels": []}, "faca3c43e6": {"title": "fix celu in quantized benchmark (#42756)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42756\n\nSimilar to ELU, CELU was also broken in the quantized benchmark, fixing.\n\nTest Plan:\n```\ncd benchmarks/operator_benchmark\npython -m pt.qactivation_test\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23010863\n\nfbshipit-source-id: 203e63f9cff760af6809f6f345b0d222dc1e9e1b", "pr_number": "42756", "files_changed": ["benchmarks/operator_benchmark/pt/qactivation_test.py"], "labels": ["merged"]}, "95f4f67552": {"title": "Restrict conversion to SmallVector (#42694)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42694\n\nThe old implementation allowed calling SmallVector constructor and operator= for any type without restrictions,\nbut then failed with a compiler error when the type wasn't a collection.\n\nInstead, we should only use it if Container follows a container concept and just not match the constructor otherwise.\n\nThis fixes an issue kimishpatel was running into.\nghstack-source-id: 109370513\n\nTest Plan: unit tests\n\nReviewed By: kimishpatel, ezyang\n\nDifferential Revision: D22983020\n\nfbshipit-source-id: c31264f5c393762d822f3d64dd2a8e3279d8da44", "pr_number": "42694", "files_changed": ["c10/util/SmallVector.h"], "labels": ["merged"]}, "55b1706775": {"title": "Skips some complex tests on ROCm (#42759)", "body": "Summary:\nFixes ROCm build on OSS master.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42759\n\nReviewed By: ngimel\n\nDifferential Revision: D23011560\n\nPulled By: mruberry\n\nfbshipit-source-id: 3339ecbd5a0ca47aede6f7c3f84739af1ac820d5", "pr_number": "42759", "files_changed": ["test/test_torch.py"], "labels": ["merged", "module: rocm", "module: tests"]}, "2b04712205": {"title": "Exposing Percentile Caffe2 Operator in PyTorch", "body": "Summary: As titled.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/python/operator_test:torch_integration_test -- test_percentile\n```\n\nReviewed By: yf225\n\nDifferential Revision: D22999896\n\nfbshipit-source-id: 2e3686cb893dff1518d533cb3d78c92eb2a6efa5", "pr_number": null, "files_changed": ["caffe2/operators/percentile_op.cc", "caffe2/operators/percentile_op.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": []}, "48e978ba18": {"title": "Add fake quantize operator that works in backward pass (#40532)", "body": "Summary:\nThis diff adds FakeQuantizeWithBackward. This works the same way as the regular FakeQuantize module, allowing QAT to occur in the forward pass, except it has an additional quantize_backward parameter. When quantize_backward is enabled, the gradients are fake quantized as well (dynamically, using hard-coded values). This allows the user to see whether there would be a significant loss of accuracy if the gradients were quantized in their model.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40532\n\nTest Plan: The relevant test for this can be run using `python test/test_quantization.py TestQATBackward.test_forward_and_backward`\n\nReviewed By: supriyar\n\nDifferential Revision: D22217029\n\nPulled By: durumu\n\nfbshipit-source-id: 7055a2cdafcf022f1ea11c3442721ae146d2b3f2", "pr_number": "40532", "files_changed": ["test/quantization/test_qat_backward.py", "test/test_quantization.py", "torch/quantization/fake_quantize_backward.py", "torch/quantization/qconfig.py"], "labels": ["merged", "open source"]}, "13bc542829": {"title": "Fix lite trainer unit test submodule registration (#42714)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42714\n\nChange two unit tests for the lite trainer to register two instances/objects of the same submodule type instead of the same submodule object twice.\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22990736\n\nPulled By: ann-ss\n\nfbshipit-source-id: 2bf56b5cc438b5a5fc3db90d3f30c5c431d3ae77", "pr_number": "42714", "files_changed": ["test/cpp/jit/test_lite_trainer.cpp"], "labels": ["merged", "oncall: jit"]}, "3fa0581cf2": {"title": "[fbgemm] use new more general depthwise 3d conv interface (#42697)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42697\n\nPull Request resolved: https://github.com/pytorch/FBGEMM/pull/401\n\nAs title\n\nTest Plan: CI\n\nReviewed By: dskhudia\n\nDifferential Revision: D22972233\n\nfbshipit-source-id: a2c8e989dee84b2c0587faccb4f8e3bcb05c797c", "pr_number": "42697", "files_changed": ["caffe2/quantization/server/conv_dnnlowp_op.cc"], "labels": ["fb-exported", "merged"]}, "d4a4c62df3": {"title": "[caffe2] Fix the timeout (stuck) issues of dedup SparseAdagrad C2 kernel", "body": "Summary:\nBackout D22800959 (https://github.com/pytorch/pytorch/commit/f30ac66e7970dae2833e390bc623ea29141c8ff9). This one is causing the timeout (machine stuck) issues for dedup kernels. Reverting it make the unit test pass. Still need to investigate why this is the culprit...\n\nOriginal commit changeset: 641d52a51070\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient \\(caffe2\\.caffe2\\.fb\\.net_transforms\\.tests\\.fuse_sparse_ops_test\\.TestFuseSparseOps\\)' --print-passing-details\n```\n\nReviewed By: jspark1105\n\nDifferential Revision: D23008389\n\nfbshipit-source-id: 4f1b9a41c78eaa5541d57b9d8aa12401e1d495f2", "pr_number": null, "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cu"], "labels": []}, "0a804be47d": {"title": "[NCCL] DDP communication hook: getFuture() without cudaStreamAddCallback (#42335)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42335\n\n**Main goal:** For DDP communication hook, provide an API called \"get_future\" to retrieve a future associated with the completion of c10d.ProcessGroupNCCL.work. Enable NCCL support for this API in this diff.\n\nWe add an API `c10::intrusive_ptr<c10::ivalue::Future> getFuture()` to `c10d::ProcessGroup::Work`. This API will only be supported by NCCL in the first version, the default implementation will throw UnsupportedOperation.\n\nWe no longer consider a design that involves cudaStreamAddCallback which potentially was causing performance regression in [#41596](https://github.com/pytorch/pytorch/pull/41596).\n\nghstack-source-id: 109461507\n\nTest Plan:\n```(pytorch) [sinannasir@devgpu017.ash6 ~/local/pytorch] python test/distributed/test_c10d.py\nCouldn't download test skip set, leaving all tests enabled...\n..............................s.....................................................s................................\n----------------------------------------------------------------------\nRan 117 tests in 298.042s\n\nOK (skipped=2)\n```\n### Facebook Internal:\n2\\. HPC PT trainer run to validate no regression. Check the QPS number:\n**Master:** QPS after 1000 iters: around ~34100\n```\nhpc_dist_trainer --fb-data=none --mtml-fusion-level=1 --target-model=ifr_video --max-ind-range=1000000 --embedding-partition=row-wise mast --domain $USER\"testvideo_master\" --trainers 16 --trainer-version 1c53912\n```\n```\n[0] I0806 142048.682 metrics_publishers.py:50] Finished iter 999, Local  window NE: [0.963963 0.950479 0.953704], lifetime NE: [0.963963 0.950479 0.953704], loss: [0.243456 0.235225 0.248375], QPS: 34199\n```\n[detailed logs](https://www.internalfb.com/intern/tupperware/details/task/?handle=priv3_global%2Fmast_hpc%2Fhpc.sinannasirtestvideo_mastwarm.trainer.trainer%2F0&ta_tab=logs)\n\n**getFuture/new design:** QPS after 1000 iters: around ~34030\n```\nhpc_dist_trainer --fb-data=none --mtml-fusion-level=1 --target-model=ifr_video --max-ind-range=1000000 --embedding-partition=row-wise mast --domain $USER\"testvideo_getFutureCyclicFix\" --trainers 16 --trainer-version 8553aee\n```\n```\n[0] I0806 160149.197 metrics_publishers.py:50] Finished iter 999, Local  window NE: [0.963959 0.950477 0.953704], lifetime NE: [0.963959 0.950477 0.953704], loss: [0.243456 0.235225 0.248375], QPS: 34018\n```\n[detailed logs](https://www.internalfb.com/intern/tupperware/details/task/?handle=priv3_global%2Fmast_hpc%2Fhpc.sinannasirtestvideo_getFutureCyclicFix.trainer.trainer%2F0&ta_tab=logs)\n**getFuture/new design Run 2:** QPS after 1000 iters: around ~34200\n```\nhpc_dist_trainer --fb-data=none --mtml-fusion-level=1 --target-model=ifr_video --max-ind-range=1000000 --embedding-partition=row-wise mast --domain $USER\"test2video_getFutureCyclicFix\" --trainers 16 --trainer-version 8553aee\n```\n```\n[0] I0806 160444.650 metrics_publishers.py:50] Finished iter 999, Local  window NE: [0.963963 0.950482 0.953706], lifetime NE: [0.963963 0.950482 0.953706], loss: [0.243456 0.235225 0.248375], QPS: 34201\n```\n[detailed logs](https://www.internalfb.com/intern/tupperware/details/task/?handle=priv3_global%2Fmast_hpc%2Fhpc.sinannasirtest2video_getFutureCyclicFix.trainer.trainer%2F0&ta_tab=logs)\n**getFuture/old design (Regression):** QPS after 1000 iters: around ~31150\n```\nhpc_dist_trainer --fb-data=none --mtml-fusion-level=1 --target-model=ifr_video --max-ind-range=1000000 --embedding-partition=row-wise mast --domain $USER\u201dtestvideo_OLDgetFutureD22583690 (https://github.com/pytorch/pytorch/commit/d904ea597277673eefbb3661430d3f905e8760d5)\" --trainers 16 --trainer-version 1cb5cbb\n```\n```\npriv3_global/mast_hpc/hpc.sinannasirtestvideo_OLDgetFutureD22583690 (https://github.com/pytorch/pytorch/commit/d904ea597277673eefbb3661430d3f905e8760d5).trainer.trainer/0 [0] I0805 101320.407 metrics_publishers.py:50] Finished iter 999, Local  window NE: [0.963964 0.950482 0.953703], lifetime NE: [0.963964 0.950482 0.953703], loss: [0.243456 0.235225 0.248375], QPS: 31159\n```\n3\\. `flow-cli` tests; roberta_base; world_size=4:\n**Master:** f210039922\n```\ntotal:\n  32 GPUs -- 32 GPUs: p25:  0.908    35/s  p50:  1.002    31/s  p75:  1.035    30/s  p90:  1.051    30/s  p95:  1.063    30/s\nforward:\n  32 GPUs -- 32 GPUs: p25:  0.071   452/s  p50:  0.071   449/s  p75:  0.072   446/s  p90:  0.072   445/s  p95:  0.072   444/s\nbackward:\n  32 GPUs -- 32 GPUs: p25:  0.821    38/s  p50:  0.915    34/s  p75:  0.948    33/s  p90:  0.964    33/s  p95:  0.976    32/s\noptimizer:\n  32 GPUs -- 32 GPUs: p25:  0.016  2037/s  p50:  0.016  2035/s  p75:  0.016  2027/s  p90:  0.016  2019/s  p95:  0.016  2017/s\n```\n**getFuture new design:** f210285797\n```\ntotal:\n  32 GPUs -- 32 GPUs: p25:  0.952    33/s  p50:  1.031    31/s  p75:  1.046    30/s  p90:  1.055    30/s  p95:  1.070    29/s\nforward:\n  32 GPUs -- 32 GPUs: p25:  0.071   449/s  p50:  0.072   446/s  p75:  0.072   445/s  p90:  0.072   444/s  p95:  0.072   443/s\nbackward:\n  32 GPUs -- 32 GPUs: p25:  0.865    37/s  p50:  0.943    33/s  p75:  0.958    33/s  p90:  0.968    33/s  p95:  0.982    32/s\noptimizer:\n  32 GPUs -- 32 GPUs: p25:  0.016  2037/s  p50:  0.016  2033/s  p75:  0.016  2022/s  p90:  0.016  2018/s  p95:  0.016  2017/s\n\n```\n\nReviewed By: ezyang\n\nDifferential Revision: D22833298\n\nfbshipit-source-id: 1bb268d3b00335b42ee235c112f93ebe2f25b208", "pr_number": "42335", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/comm.cpp", "torch/csrc/distributed/c10d/comm.h", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/nn/parallel/distributed.py"], "labels": ["merged"]}, "e95fbaaba3": {"title": "Adding Peter's Swish Op ULP analysis. (#42573)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42573\n\n* Generate the ULP png files for different ranges.\n\nTest Plan: test_op_ulp_error.py\n\nReviewed By: hyuen\n\nDifferential Revision: D22938572\n\nfbshipit-source-id: 6374bef6d44c38e1141030d44029dee99112cd18", "pr_number": "42573", "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/python/fakelowp/test_utils.py"], "labels": ["fb-exported", "merged"]}, "6755e49cad": {"title": "Set proper return type (#42454)", "body": "Summary:\nThis function was always expecting to return a `size_t` value\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42454\n\nReviewed By: ezyang\n\nDifferential Revision: D22993168\n\nPulled By: ailzhang\n\nfbshipit-source-id: 044df8ce17983f04681bda8c30cd742920ef7b1e", "pr_number": "42454", "files_changed": ["c10/util/intrusive_ptr.h"], "labels": ["merged", "open source"]}, "5dd230d6a2": {"title": "[vulkan] inplace add_, relu_ (#41380)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41380\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22754939\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 19b0bbfc5e1f149f9996b5043b77675421ecb2ed", "pr_number": "41380", "files_changed": ["aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "c889de7e25": {"title": "update DispatchKey::toString() (#42619)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42619\n\nAdded missing entries to `DispatchKey::toString()` and reordered to match declaration order in `DispatchKey.h`\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22963407\n\nPulled By: bhosmer\n\nfbshipit-source-id: 34a012135599f497c308ba90ea6e8117e85c74ac", "pr_number": "42619", "files_changed": ["c10/core/DispatchKey.cpp"], "labels": ["merged"]}, "18ca999e1a": {"title": "integrate int8 swish with net transformer", "body": "Summary:\nadd a fuse path for deq->swish->quant\nupdate swish fake op interface to take arguments accordingly\n\nTest Plan:\nnet_runner passes\nunit tests need to be updated\n\nReviewed By: venkatacrc\n\nDifferential Revision: D22962064\n\nfbshipit-source-id: cef79768db3c8af926fca58193d459d671321f80", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/int8_swish_op_nnpi.h", "caffe2/contrib/fakelowp/test/test_deq_swish_quant_nnpi.py", "caffe2/opt/custom/fakefp16_transform.cc"], "labels": []}, "b7a9bc0802": {"title": "Revert D22217029: Add fake quantize operator that works in backward pass", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22217029 (https://github.com/pytorch/pytorch/commit/48e978ba181b6c2b54a371733f86b600f6983d37)\n\nOriginal commit changeset: 7055a2cdafcf\n\nfbshipit-source-id: f57a27be412c6fbfd5a5b07a26f758ac36be3b67", "pr_number": null, "files_changed": ["test/quantization/test_qat_backward.py", "test/test_quantization.py", "torch/quantization/fake_quantize_backward.py", "torch/quantization/qconfig.py"], "labels": []}, "5cd0f5e8ec": {"title": "[PyFI] Update hypothesis and switch from tp2 (#41645)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41645\n\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1405\n\nTest Plan: buck test\n\nReviewed By: thatch\n\nDifferential Revision: D20323893\n\nfbshipit-source-id: 54665d589568c4198e96a27f0ed8e5b41df7b86b", "pr_number": "41645", "files_changed": ["caffe2/contrib/fakelowp/test/test_batchmatmul_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_batchnorm_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_fc_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py", "caffe2/contrib/fakelowp/test/test_layernorm_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_4bit_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp16.py", "caffe2/contrib/gloo/gloo_test.py", "caffe2/contrib/nnpack/nnpack_ops_test.py", "caffe2/python/allcompare_test.py", "caffe2/python/core_gradients_test.py", "caffe2/python/crf_viterbi_test.py", "caffe2/python/data_parallel_model_test.py", "caffe2/python/hypothesis_test.py", "caffe2/python/hypothesis_test_util.py", "caffe2/python/ideep/LRN_op_test.py", "caffe2/python/ideep/channel_shuffle_op_test.py", "caffe2/python/ideep/concat_split_op_test.py", "caffe2/python/ideep/conv_op_test.py", "caffe2/python/ideep/dropout_op_test.py", "caffe2/python/ideep/fc_op_test.py", "caffe2/python/ideep/leaky_relu_op_test.py", "caffe2/python/ideep/order_switch_op_test.py", "caffe2/python/ideep/pool_op_test.py", "caffe2/python/ideep/relu_op_test.py", "caffe2/python/ideep/shape_op_test.py", "caffe2/python/ideep/sigmoid_op_test.py", "caffe2/python/ideep/spatial_bn_op_test.py", "caffe2/python/ideep/transpose_op_test.py", "caffe2/python/layers/build_index.py", "caffe2/python/layers/last_n_window_collector.py", "caffe2/python/layers/reservoir_sampling.py", "caffe2/python/layers_test.py", "caffe2/python/lazy_dyndep_test.py", "caffe2/python/memonger_test.py", "caffe2/python/mkl/mkl_pool_op_test.py", "caffe2/python/models/resnet_test.py", "caffe2/python/models/shufflenet_test.py", "caffe2/python/observer_test.py", "caffe2/python/operator_test/activation_ops_test.py", "caffe2/python/operator_test/adadelta_test.py", "caffe2/python/operator_test/adagrad_test.py", "caffe2/python/operator_test/affine_channel_op_test.py", "caffe2/python/operator_test/arg_ops_test.py", "caffe2/python/operator_test/assert_test.py", "caffe2/python/operator_test/batch_box_cox_test.py", "caffe2/python/operator_test/batch_moments_op_test.py", "caffe2/python/operator_test/batch_sparse_to_dense_op_test.py", "caffe2/python/operator_test/bbox_transform_test.py", "caffe2/python/operator_test/boolean_mask_test.py", "caffe2/python/operator_test/box_with_nms_limit_op_test.py", "caffe2/python/operator_test/ceil_op_test.py", "caffe2/python/operator_test/channel_backprop_stats_op_test.py", "caffe2/python/operator_test/channel_stats_op_test.py", "caffe2/python/operator_test/clip_op_test.py", "caffe2/python/operator_test/clip_tensor_op_test.py", "caffe2/python/operator_test/collect_and_distribute_fpn_rpn_proposals_op_test.py", "caffe2/python/operator_test/concat_split_op_test.py", "caffe2/python/operator_test/conv_test.py", "caffe2/python/operator_test/conv_transpose_test.py", "caffe2/python/operator_test/copy_rows_to_tensor_op_test.py", "caffe2/python/operator_test/crf_test.py", "caffe2/python/operator_test/cross_entropy_ops_test.py", "caffe2/python/operator_test/ctc_beam_search_decoder_op_test.py", "caffe2/python/operator_test/ctc_greedy_decoder_op_test.py", "caffe2/python/operator_test/depthwise_3x3_conv_test.py", "caffe2/python/operator_test/dropout_op_test.py", "caffe2/python/operator_test/elementwise_logical_ops_test.py", "caffe2/python/operator_test/elementwise_op_broadcast_test.py", "caffe2/python/operator_test/elementwise_ops_test.py", "caffe2/python/operator_test/enforce_finite_op_test.py", "caffe2/python/operator_test/erf_op_test.py", "caffe2/python/operator_test/expand_op_test.py", "caffe2/python/operator_test/filler_ops_test.py", "caffe2/python/operator_test/find_op_test.py", "caffe2/python/operator_test/flexible_top_k_test.py", "caffe2/python/operator_test/floor_op_test.py", "caffe2/python/operator_test/fused_nbit_rowwise_conversion_ops_test.py", "caffe2/python/operator_test/gather_ops_test.py", "caffe2/python/operator_test/gather_ranges_op_test.py", "caffe2/python/operator_test/glu_op_test.py", "caffe2/python/operator_test/group_conv_test.py", "caffe2/python/operator_test/group_norm_op_test.py", "caffe2/python/operator_test/gru_test.py", "caffe2/python/operator_test/histogram_test.py", "caffe2/python/operator_test/hsm_test.py", "caffe2/python/operator_test/im2col_col2im_test.py", "caffe2/python/operator_test/image_input_op_test.py", "caffe2/python/operator_test/index_hash_ops_test.py", "caffe2/python/operator_test/instance_norm_test.py", "caffe2/python/operator_test/integral_image_ops_test.py", "caffe2/python/operator_test/layer_norm_op_test.py", "caffe2/python/operator_test/learning_rate_adaption_op_test.py", "caffe2/python/operator_test/learning_rate_op_test.py", "caffe2/python/operator_test/length_split_op_test.py", "caffe2/python/operator_test/locally_connected_op_test.py", "caffe2/python/operator_test/lpnorm_op_test.py", "caffe2/python/operator_test/margin_ranking_criterion_op_test.py", "caffe2/python/operator_test/math_ops_test.py", "caffe2/python/operator_test/matmul_op_test.py", "caffe2/python/operator_test/mkl_conv_op_test.py", "caffe2/python/operator_test/momentum_sgd_test.py", "caffe2/python/operator_test/negate_gradient_op_test.py", "caffe2/python/operator_test/normalize_op_test.py", "caffe2/python/operator_test/numpy_tile_op_test.py", "caffe2/python/operator_test/one_hot_ops_test.py", "caffe2/python/operator_test/onnx_while_test.py", "caffe2/python/operator_test/order_switch_test.py", "caffe2/python/operator_test/pack_ops_test.py", "caffe2/python/operator_test/piecewise_linear_transform_test.py", "caffe2/python/operator_test/pooling_test.py", "caffe2/python/operator_test/python_op_test.py", "caffe2/python/operator_test/rand_quantization_op_speed_test.py", "caffe2/python/operator_test/rebatching_queue_test.py", "caffe2/python/operator_test/recurrent_net_executor_test.py", "caffe2/python/operator_test/recurrent_network_test.py", "caffe2/python/operator_test/reduce_ops_test.py", "caffe2/python/operator_test/reduction_ops_test.py", "caffe2/python/operator_test/resize_op_test.py", "caffe2/python/operator_test/rmac_regions_op_test.py", "caffe2/python/operator_test/rnn_cell_test.py", "caffe2/python/operator_test/segment_ops_test.py", "caffe2/python/operator_test/self_binning_histogram_test.py", "caffe2/python/operator_test/selu_op_test.py", "caffe2/python/operator_test/sequence_ops_test.py", "caffe2/python/operator_test/sinusoid_position_encoding_op_test.py", "caffe2/python/operator_test/softmax_ops_test.py", "caffe2/python/operator_test/softplus_op_test.py", "caffe2/python/operator_test/sparse_gradient_checker_test.py", "caffe2/python/operator_test/sparse_ops_test.py", "caffe2/python/operator_test/sparse_to_dense_mask_op_test.py", "caffe2/python/operator_test/spatial_bn_op_test.py", "caffe2/python/operator_test/specialized_segment_ops_test.py", "caffe2/python/operator_test/square_root_divide_op_test.py", "caffe2/python/operator_test/string_ops_test.py", "caffe2/python/operator_test/thresholded_relu_op_test.py", "caffe2/python/operator_test/tile_op_test.py", "caffe2/python/operator_test/top_k_test.py", "caffe2/python/operator_test/transpose_op_test.py", "caffe2/python/operator_test/trigonometric_op_test.py", "caffe2/python/operator_test/unique_ops_test.py", "caffe2/python/operator_test/upsample_op_test.py", "caffe2/python/operator_test/utility_ops_test.py", "caffe2/python/operator_test/weighted_sum_test.py", "caffe2/python/operator_test/wngrad_test.py", "caffe2/python/parallelize_bmuf_distributed_test.py", "caffe2/python/python_op_test.py", "caffe2/python/regularizer_test.py", "caffe2/python/serialized_test/serialized_test_util.py", "caffe2/python/test/executor_test_util.py", "caffe2/python/test/inference_lstm_op_test.py", "caffe2/python/workspace_test.py", "caffe2/quantization/server/batch_matmul_dnnlowp_op_test.py", "caffe2/quantization/server/batch_permutation_dnnlowp_op_test.py", "caffe2/quantization/server/channel_shuffle_dnnlowp_op_test.py", "caffe2/quantization/server/conv_depthwise_dnnlowp_op_test.py", "caffe2/quantization/server/conv_dnnlowp_acc16_op_test.py", "caffe2/quantization/server/conv_dnnlowp_op_test.py", "caffe2/quantization/server/conv_groupwise_dnnlowp_acc16_op_test.py", "caffe2/quantization/server/conv_groupwise_dnnlowp_op_test.py", "caffe2/quantization/server/elementwise_mul_dnnlowp_op_test.py", "caffe2/quantization/server/int8_gen_quant_params_test.py", "caffe2/quantization/server/lstm_unit_dnnlowp_op_test.py", "caffe2/quantization/server/pool_dnnlowp_op_test.py", "caffe2/quantization/server/quantize_dnnlowp_op_test.py", "caffe2/quantization/server/resize_nearest_3d_dnnlowp_op_test.py", "caffe2/quantization/server/resize_nearest_dnnlowp_op_test.py", "caffe2/quantization/server/tanh_dnnlowp_op_test.py", "modules/detectron/upsample_nearest_op_test.py", "test/quantization/test_workflow_module.py"], "labels": ["fb-exported", "merged"]}, "d8801f590c": {"title": "fix asan failure for module freezing in conv bn folding (#42739)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42739\n\nThis is a test case which fails with ASAN on at the module freezing\nstep.\n\nTest Plan:\n```\nUSE_ASAN=1 USE_CUDA=0 python setup.py develop\nLD_PRELOAD=/usr/lib64/libasan.so.4 python test/test_mobile_optimizer.py TestOptimizer.test_optimize_for_mobile_asan\n\n// output tail: https://gist.github.com/vkuzo/7a0018b9e10ffe64dab0ac7381479f23\n```\n\nImported from OSS\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23005962\n\nfbshipit-source-id: b7d4492e989af7c2e22197c16150812bd2dda7cc", "pr_number": "42739", "files_changed": ["test/test_mobile_optimizer.py", "torch/csrc/jit/passes/fold_conv_bn.cpp", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h"], "labels": ["merged", "oncall: jit"]}, "79b8328aaf": {"title": "optimize_for_mobile: bring packed params to root module (#42740)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42740\n\nAdds a pass to hoist conv packed params to root module.\nThe benefit is that if there is nothing else in the conv module,\nsubsequent passes will delete it, which will reduce module size.\n\nFor context, freezing does not handle this because conv packed\nparams is a custom object.\n\nTest Plan:\n```\nPYTORCH_JIT_LOG_LEVEL=\">hoist_conv_packed_params.cpp\" python test/test_mobile_optimizer.py TestOptimizer.test_hoist_conv_packed_params\n```\n\nImported from OSS\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23005961\n\nfbshipit-source-id: 31ab1f5c42a627cb74629566483cdc91f3770a94", "pr_number": "42740", "files_changed": ["docs/source/mobile_optimizer.rst", "test/test_mobile_optimizer.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/hoist_conv_packed_params.cpp", "torch/csrc/jit/passes/hoist_conv_packed_params.h", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.h", "torch/csrc/jit/python/init.cpp"], "labels": ["merged", "oncall: jit"]}, "b6810c1064": {"title": "Include/ExcludeDispatchKeySetGuard API (#42658)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42658\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D22971426\n\nPulled By: bhosmer\n\nfbshipit-source-id: 4d63e0cb31745e7b662685176ae0126ff04cdece", "pr_number": "42658", "files_changed": ["aten/src/ATen/core/LegacyTypeDispatch.h", "c10/core/impl/LocalDispatchKeySet.cpp", "c10/core/impl/LocalDispatchKeySet.h"], "labels": ["merged", "oncall: jit"]}, "87970b70a7": {"title": "Adds 'clip' alias for clamp (#42770)", "body": "Summary:\nPer title. Also updates our guidance for adding aliases to clarify interned_string and method_test requirements. The alias is tested by extending test_clamp to also test clip.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42770\n\nReviewed By: ngimel\n\nDifferential Revision: D23020655\n\nPulled By: mruberry\n\nfbshipit-source-id: f1d8e751de9ac5f21a4f95d241b193730f07b5dc", "pr_number": "42770", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/jit/test_op_normalization.py", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/jit_utils.py"], "labels": ["merged", "oncall: jit"]}, "162972e980": {"title": "Fix op benchmark (#42757)", "body": "Summary:\nA benchmark relies on abs_ having a functional variant.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42757\n\nReviewed By: ngimel\n\nDifferential Revision: D23011037\n\nPulled By: mruberry\n\nfbshipit-source-id: c04866015fa259e4c544e5cf0c33ca1e11091d92", "pr_number": "42757", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["merged"]}, "55ac240589": {"title": "[ONNX] Fix scalar type cast for comparison ops (#37787)", "body": "Summary:\nAlways promote type casts for comparison operators, regardless if the input is tensor or scalar. Unlike arithmetic operators, where scalars are implicitly cast to the same type as tensors.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37787\n\nReviewed By: hl475\n\nDifferential Revision: D21440585\n\nPulled By: houseroad\n\nfbshipit-source-id: fb5c78933760f1d1388b921e14d73a2cb982b92f", "pr_number": "37787", "files_changed": [".jenkins/caffe2/test.sh", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp"], "labels": ["merged", "oncall: jit", "open source", "triaged"]}, "05f00532f5": {"title": "Fix TensorPipe submodule (#42789)", "body": "Summary:\nNot sure what happened, but possibly I landed a PR on PyTorch which updated the TensorPipe submodule to a commit hash of a *PR* of TensorPipe. Now that the latter PR has been merged though that same commit has a different hash. The commit referenced by PyTorch, therefore, has become orphaned. This is causing some issues.\n\nHence here I am updating the commit, which however does not change a single line of code.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42789\n\nReviewed By: houseroad\n\nDifferential Revision: D23023238\n\nPulled By: lw\n\nfbshipit-source-id: ca2dcf6b7e07ab64fb37e280a3dd7478479f87fd", "pr_number": "42789", "files_changed": ["third_party/tensorpipe"], "labels": ["merged"]}, "bc779667d6": {"title": "generalize circleci docker build.sh and add centos support (#41255)", "body": "Summary:\nAdd centos Dockerfile and support to circleci docker builds, and allow generic image names to be parsed by build.sh, so both hardcoded images and custom images can be built.\n\nCurrently only adds a ROCm centos Dockerfile.\n\nCC ezyang xw285cornell sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41255\n\nReviewed By: mrshenli\n\nDifferential Revision: D23003218\n\nPulled By: malfet\n\nfbshipit-source-id: 562c53533e7fb9637dc2e81edb06b2242afff477", "pr_number": "41255", "files_changed": [".circleci/docker/build.sh", ".circleci/docker/centos-rocm/Dockerfile", ".circleci/docker/common/install_base.sh", ".circleci/docker/common/install_conda.sh", ".circleci/docker/common/install_db.sh", ".circleci/docker/common/install_devtoolset.sh", ".circleci/docker/common/install_glibc.sh", ".circleci/docker/common/install_protobuf.sh", ".circleci/docker/common/install_rocm.sh", ".circleci/docker/common/install_vision.sh"], "labels": ["merged", "module: ci", "module: rocm", "open source", "triaged"]}, "e5adf45dde": {"title": "Add python unittest target to `caffe2/test/TARGETS` (#42766)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42766\n\n**Summary**\nSome python tests are missing in `caffe2/test/TARGETS`, add them to be more comprehension.\n\nAccording to [run_test.py](https://github.com/pytorch/pytorch/blob/master/test/run_test.py#L125), some tests are slower. Slow tests are added as independent targets and others are put together into one `others` target. The reason is because we want to reduce overhead, especially for code covarge collection.  Tests in one target can be run as a bundle, and then coverage can be collected together. Typically coverage collection procedure is time-expensive, so this helps us save time.\n\nTest Plan:\nRun all the new test targets locally in dev server and record the time they cost.\n**Statistics**\n\n```\n# jit target\nreal    33m7.694s\nuser    653m1.181s\nsys     58m14.160s\n\n--------- Compare to Initial Jit Target runtime: ----------------\n\nreal    32m13.057s\nuser    613m52.843s\nsys     54m58.678s\n\n```\n\n```\n# others target\nreal    9m2.920s\nuser    164m21.927s\nsys     12m54.840s\n```\n\n```\n# serialization target\nreal    4m21.090s\nuser    23m33.501s\nsys     1m53.308s\n\n```\n\n```\n# tensorexpr\nreal    11m28.187s\nuser    33m36.420s\nsys     1m15.925s\n```\n\n```\n# type target\nreal    3m36.197s\nuser    51m47.912s\nsys     4m14.149s\n```\n\nReviewed By: malfet\n\nDifferential Revision: D22979219\n\nfbshipit-source-id: 12a30839bb76a64871359bc024e4bff670c5ca8b", "pr_number": "42766", "files_changed": ["test/te_utils.py", "test/test_jit_fuser_te.py", "test/test_tensorexpr.py", "torch/testing/_internal/te_utils.py"], "labels": ["fb-exported", "merged"]}, "e7b5a23607": {"title": "include missing settings import", "body": "Summary: from hypothesis import given, settings\n\nTest Plan: test_op_nnpi_fp16.py\n\nDifferential Revision: D23031038\n\nfbshipit-source-id: 751547e6a6e992d8816d4cc2c5a699ba19a97796", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py"], "labels": []}, "d83cc92948": {"title": "[ONNX] Add support for scalar src in torch.scatter ONNX export. (#42765)", "body": "Summary:\n`torch.scatter` supports two overloads \u2013 one where `src` input tensor is same size as the `index` tensor input, and second, where `src` is a scalar. Currrently, ONNX exporter only supports the first overload. This PR adds export support for the second overload of `torch.scatter`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42765\n\nReviewed By: hl475\n\nDifferential Revision: D23025189\n\nPulled By: houseroad\n\nfbshipit-source-id: 5c2a3f3ce3b2d69661a227df8a8e0ed7c1858dbf", "pr_number": "42765", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "d7aaa3327b": {"title": ".circleci: Only do comparisons when available (#42816)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42816\n\nComparisons were being done on branches where the '<<\npipeline.git.base_revision >>' didn't exist before so let's just move it\nso that comparison / code branch is only run when that variable is\navailable\n\nExample: https://app.circleci.com/pipelines/github/pytorch/pytorch/198611/workflows/8a316eef-d864-4bb0-863f-1454696b1e8a/jobs/6610393\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23032900\n\nPulled By: seemethere\n\nfbshipit-source-id: 98a49c78b174d6fde9c6b5bd3d86a6058d0658bd", "pr_number": "42816", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/docker_jobs.yml"], "labels": ["merged", "module: ci"]}, "752f433a24": {"title": "DDP communication hook: skip dividing grads by world_size if hook registered. (#42400)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42400\n\nmcarilli spotted that in the original DDP communication hook design described in [39272](https://github.com/pytorch/pytorch/issues/39272), the hooks receive grads that are already predivided by world size.\n\nIt makes sense to skip the divide completely if hook registered. The hook is meant for the user to completely override DDP communication. For example, if the user would like to implement something like GossipGrad, always dividing by the world_size would not be a good idea.\n\nWe also included a warning in the register_comm_hook API as:\n> GradBucket bucket's tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce.\nghstack-source-id: 109548696\n\n**Update:** We discovered and fixed a bug with the sparse tensors case. See new unit test called `test_ddp_comm_hook_sparse_gradients` and changes in `reducer.cpp`.\n\nTest Plan: python test/distributed/test_c10d.py and perf benchmark tests.\n\nReviewed By: ezyang\n\nDifferential Revision: D22883905\n\nfbshipit-source-id: 3277323fe9bd7eb6e638b7ef0535cab1fc72f89e", "pr_number": "42400", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/nn/parallel/distributed.py"], "labels": ["merged"]}, "e06b4be5ae": {"title": "change pt_defs.bzl to python file (#42725)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42725\n\nThis diff changes pt_defs.bzl to pt_defs.py, so that it can be included as python source file.\n\nThe reason is if we remove base ops, pt_defs.bzl becomes too big (8k lines) and we cannot pass its content to gen_oplist (python library). The easy solution is to change it to a python source file so that it can be used in gen_oplist.\n\nTest Plan: sandcastle\n\nReviewed By: ljk53, iseeyuan\n\nDifferential Revision: D22968258\n\nfbshipit-source-id: d720fe2e684d9a2bf5bd6115b6e6f9b812473f12", "pr_number": "42725", "files_changed": ["pt_deps.py"], "labels": ["fb-exported", "merged"]}, "3cf2551f2f": {"title": "Fix `torch.nn.functional.grid_sample` crashes if `grid` has NaNs (#42703)", "body": "Summary:\nIn `clip_coordinates` replace `minimum(maximum(in))` composition with `clamp_max(clamp_min(in))`\nSwap order of `clamp_min` operands to clamp NaNs in grid to 0\n\nFixes https://github.com/pytorch/pytorch/issues/42616\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42703\n\nReviewed By: ezyang\n\nDifferential Revision: D22987447\n\nPulled By: malfet\n\nfbshipit-source-id: a8a2d6de8043d6b77c8707326c5412d0250efae6", "pr_number": "42703", "files_changed": ["aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "test/test_nn.py", "torch/nn/functional.py"], "labels": ["merged", "module: nn", "topic: NaNs and Infs", "triaged"]}, "8718524571": {"title": "[vulkan] cat op (concatenate) (#41434)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41434\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22754941\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: cd03577e1c2f639b2592d4b7393da4657422e23c", "pr_number": "41434", "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.cpp", "aten/src/ATen/native/vulkan/Vulkan.h", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/VulkanOps.h", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "64a7939ee5": {"title": "test_cpp_rpc: Build test_e2e_process_group.cpp only if USE_GLOO is true (#42836)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42776\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42836\n\nReviewed By: seemethere\n\nDifferential Revision: D23041274\n\nPulled By: malfet\n\nfbshipit-source-id: 8605332701271bea6d9b3a52023f548c11d8916f", "pr_number": "42836", "files_changed": ["test/cpp/rpc/CMakeLists.txt"], "labels": ["merged"]}, "8f67c7a624": {"title": "BatchedTensor fallback: extended to support ops with multiple Tensor returns (#42628)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42628\n\nThis PR extends the BatchedTensor fallback to support operators with\nmultiple Tensor returns. If an operator has multiple returns, we stack\nshards of each return to create the full outputs.\n\nTest Plan:\n- `pytest test/test_vmap.py -v`. Added a new test for an operator with\nmultiple returns (torch.var_mean).\n\nReviewed By: izdeby\n\nDifferential Revision: D22957095\n\nPulled By: zou3519\n\nfbshipit-source-id: 5c0ec3bf51283cc4493b432bcfed1acf5509e662", "pr_number": "42628", "files_changed": ["aten/src/ATen/BatchedFallback.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "a2559652ab": {"title": "Rename some BatchedTensorImpl APIs (#42700)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42700\n\nI was about to use `isBatched` somewhere not in the files used to\nimplement vmap but then realized how silly that sounds due to\nambiguity. This PR renames some of the BatchedTensor APIs to make a bit\nmore sense to onlookers.\n\n- isBatched(Tensor) -> isBatchedTensor(Tensor)\n- unsafeGetBatched(Tensor) -> unsafeGetBatchedImpl(Tensor)\n- maybeGetBatched(Tensor) -> maybeGetBatchedImpl(Tensor)\n\nTest Plan: - build Pytorch, run tests.\n\nReviewed By: ezyang\n\nDifferential Revision: D22985868\n\nPulled By: zou3519\n\nfbshipit-source-id: b8ed9925aabffe98085bcf5c81d22cd1da026f46", "pr_number": "42700", "files_changed": ["aten/src/ATen/BatchedFallback.cpp", "aten/src/ATen/BatchedTensorImpl.cpp", "aten/src/ATen/BatchedTensorImpl.h", "aten/src/ATen/VmapTransforms.cpp", "aten/src/ATen/VmapTransforms.h", "aten/src/ATen/native/Batching.cpp", "aten/src/ATen/test/vmap_test.cpp"], "labels": ["merged"]}, "a414bd69de": {"title": "Skip test_c10d.ProcessGroupNCCLTest under TSAN (#42750)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42750\n\nAll of these tests fail under TSAN since we fork in a multithreaded\nenvironment.\nghstack-source-id: 109566396\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23007746\n\nfbshipit-source-id: 65571607522b790280363882d61bfac8a52007a1", "pr_number": "42750", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["merged"]}, "c9e825640a": {"title": "[c10d] Template computeLengthsAndOffsets() (#42706)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42706\n\nDifferent backends accept different type of length to, like MPI_Alltoallv, nccSend/Recv(), gloo::alltoallv(). So to make computeLengthsAndOffsets() template\n\nTest Plan:\nSandcastle\nCI\nHPC: ./trainer_cmd.sh -p 16 -n 8 -d nccl\n\nReviewed By: osalpekar\n\nDifferential Revision: D22961459\n\nfbshipit-source-id: 45ec271f8271b96f2dba76cd9dce3e678bcfb625", "pr_number": "42706", "files_changed": ["torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/Utils.hpp"], "labels": ["fb-exported", "merged"]}, "c14a7f6808": {"title": "adaptive_avg_pool[23]d: check output_size.size() (#42831)", "body": "Summary:\nReturn an error if output_size is unexpected\n\nFixes https://github.com/pytorch/pytorch/issues/42578\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42831\n\nReviewed By: ezyang\n\nDifferential Revision: D23039295\n\nPulled By: malfet\n\nfbshipit-source-id: d14a5e6dccdf785756635caee2c87151c9634872", "pr_number": "42831", "files_changed": ["aten/src/ATen/native/AdaptiveAveragePooling.cpp", "aten/src/ATen/native/AdaptiveAveragePooling3d.cpp", "aten/src/ATen/native/AdaptiveMaxPooling2d.cpp"], "labels": ["merged"]}, "103887892c": {"title": "Fix \"non-negative integer\" error messages (#42734)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42662\n\nUse \"positive integer\" error message for consistency with: https://github.com/pytorch/pytorch/blob/17f76f9a7896eccdfdba5fd22fd3a24002b0d917/torch/optim/lr_scheduler.py#L958-L959\nhttps://github.com/pytorch/pytorch/blob/ad7133d3c11a35a7aedf9786ccf8d7a52939b753/torch/utils/data/sampler.py#L102-L104\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42734\n\nReviewed By: zdevito\n\nDifferential Revision: D23039575\n\nPulled By: smessmer\n\nfbshipit-source-id: 1be1e0caa868891540ecdbe6f471a6cd51c40ede", "pr_number": "42734", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged", "open source"]}, "a4b763bc2c": {"title": "add net transforms for fusion (#42763)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42763\n\nadd the fp16 fusions as net transforms:\n-layernorm fused with mul+add\n-swish int8\n\nTest Plan: added unit test, ran flows\n\nReviewed By: yinghai\n\nDifferential Revision: D23002043\n\nfbshipit-source-id: f0b13d51d68c240b05d2a237a7fb8273e996328b", "pr_number": "42763", "files_changed": ["caffe2/python/fakefp16_transform_lib.py", "caffe2/python/pybind_state.cc", "caffe2/python/test/fakefp16_transform_test.py"], "labels": ["fb-exported", "merged"]}, "dedcc30c84": {"title": "Fix ROCm CI by increasing test timeout (#42827)", "body": "Summary:\nROCm is failing to run this test in the allotted time. See, for example, https://app.circleci.com/pipelines/github/pytorch/pytorch/198759/workflows/f6066acf-b289-46c5-aad0-6f4f663ce820/jobs/6618625.\n\ncc jeffdaily\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42827\n\nReviewed By: pbelevich\n\nDifferential Revision: D23042220\n\nPulled By: mruberry\n\nfbshipit-source-id: 52b426b0733b7b52ac3b311466d5000334864a82", "pr_number": "42827", "files_changed": ["caffe2/python/operator_test/activation_ops_test.py"], "labels": ["merged", "module: rocm", "module: tests"]}, "59b10f7929": {"title": "[quant] Sorting the list of dispathes (#42758)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42758\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23011764\n\nPulled By: z-a-f\n\nfbshipit-source-id: df87acdcf77ae8961a109eaba20521bc4f27ad0e", "pr_number": "42758", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h"], "labels": ["merged"]}, "ddcf3ded3e": {"title": "Revert D23002043: add net transforms for fusion", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23002043 (https://github.com/pytorch/pytorch/commit/a4b763bc2c24381bd9ad98fb70f0dc721b101c31)\n\nOriginal commit changeset: f0b13d51d68c\n\nfbshipit-source-id: d43602743af35db825e951358992e979283a26f6", "pr_number": null, "files_changed": ["caffe2/python/fakefp16_transform_lib.py", "caffe2/python/pybind_state.cc", "caffe2/python/test/fakefp16_transform_test.py"], "labels": []}, "ffc3da35f4": {"title": "Don't materialize output grads (#41821)", "body": "Summary:\nAdded a new option in AutogradContext to tell autograd to not materialize output grad tensors, that is, don't expand undefined/None tensors into tensors full of zeros before passing them as input to the backward function.\n\nThis PR is the second part that closes https://github.com/pytorch/pytorch/issues/41359. The first PR is https://github.com/pytorch/pytorch/pull/41490.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41821\n\nReviewed By: albanD\n\nDifferential Revision: D22693163\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: a8d060405a17ab1280a8506a06a2bbd85cb86461", "pr_number": "41821", "files_changed": ["docs/source/notes/extending.rst", "test/cpp/api/autograd.cpp", "test/test_autograd.py", "torch/autograd/function.py", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/custom_function.h", "torch/csrc/autograd/python_function.cpp", "torch/csrc/autograd/python_function.h"], "labels": ["merged"]}, "e8f4b04d9a": {"title": "vmap: temporarily disable support for random functions (#42617)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42617\n\nWhile we figure out the random plan, I want to initially disable\nsupport for random operations. This is because there is an ambiguity in\nwhat randomness means. For example,\n\n```\ntensor = torch.zeros(B0, 1)\nvmap(lambda t: t.normal_())(tensor)\n```\n\nin the above example, should tensor[0] and tensor[1] be equal (i.e.,\nuse the same random seed), or should they be different?\n\nThe mechanism for disabling random support is as follows:\n- We add a new dispatch key called VmapMode\n- Whenever we're inside vmap, we enable VmapMode for all tensors.\nThis is done via at::VmapMode::increment_nesting and\nat::VmapMode::decrement_nesting.\n- DispatchKey::VmapMode's fallback kernel is the fallthrough kernel.\n- We register kernels that raise errors for all random functions on\nDispatchKey::VmapMode. This way, whenever someone calls a random\nfunction on any tensor (not just BatchedTensors) inside of a vmap block,\nan error gets thrown.\n\nTest Plan: - pytest test/test_vmap.py -v -k \"Operators\"\n\nReviewed By: ezyang\n\nDifferential Revision: D22954840\n\nPulled By: zou3519\n\nfbshipit-source-id: cb8d71062d4087e10cbf408f74b1a9dff81a226d", "pr_number": "42617", "files_changed": ["aten/src/ATen/VmapMode.cpp", "aten/src/ATen/VmapMode.h", "aten/src/ATen/VmapModeRegistrations.cpp", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "test/test_vmap.py", "torch/_C/__init__.pyi.in", "torch/_vmap_internals.py", "torch/csrc/Module.cpp"], "labels": ["merged"]}, "d396d135db": {"title": "Added torch::cuda::manual_seed(_all) to mirror torch.cuda.manual_seed(_all) (#42638)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42638\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D23030317\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: b0d7bdf0bc592a913ae5b1ffc14c3a5067478ce3", "pr_number": "42638", "files_changed": ["torch/csrc/api/include/torch/cuda.h", "torch/csrc/api/src/cuda.cpp"], "labels": ["merged"]}, "42b4a7132e": {"title": "Raise error if `at::native::embedding` is given 0-D weight (#42550)", "body": "Summary:\nPreviously, `at::native::embedding` implicitly assumed that the `weight` argument would be 1-D or greater. Given a 0-D tensor, it would segfault. This change makes it throw a RuntimeError instead.\n\nFixes https://github.com/pytorch/pytorch/issues/41780\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42550\n\nReviewed By: smessmer\n\nDifferential Revision: D23040744\n\nPulled By: albanD\n\nfbshipit-source-id: d3d315850a5ee2d2b6fcc0bdb30db2b76ffffb01", "pr_number": "42550", "files_changed": ["aten/src/ATen/native/Embedding.cpp", "test/test_nn.py", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "d28639a080": {"title": "Optimization with Backward Implementation of Learnable Fake Quantize Per Channel Kernel (CPU and GPU) (#42810)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42810\n\nIn this diff, the original backward pass implementation is sped up by merging the 3 iterations computing dX, dScale, and dZeroPoint separately. In this case, a native loop is directly used on a byte-wise level (referenced by `strides`). In addition, vectorization is used such that scale and zero point are expanded to share the same shape and the element-wise corresponding values to X along the channel axis.\n\nIn the benchmark test on the operators, for an input of shape `3x3x256x256`, we have observed the following improvement in performance:\n**Speedup from python operator**: ~10x\n**Speedup from original learnable kernel**: ~5.4x\n**Speedup from non-backprop kernel**: ~1.8x\n\nTest Plan:\nTo assert correctness of the new kernel, on a devvm, enter the command\n\n`buck test //caffe2/test:quantization -- learnable_backward_per_channel`\n\nTo benchmark the operators, on a devvm, enter the command\n1. Set the kernel size to 3x3x256x256 or a reasonable input size.\n2. Run `buck test //caffe2/benchmarks/operator_benchmark/pt:quantization_test`\n3. The relevant outputs for CPU are as follows:\n\n```\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cpu_op_typepy_module\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: py_module\nBackward Execution Time (us) : 989024.686\n\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cpu_op_typelearnable_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: learnable_kernel\nBackward Execution Time (us) : 95654.079\n\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cpu_op_typeoriginal_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: original_kernel\nBackward Execution Time (us) : 176948.970\n```\n4. The relevant outputs for GPU are as follows:\nThe relevant outputs are as follows\n\n**Pre-optimization**:\n\n```\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typepy_module\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: py_module\nBackward Execution Time (us) : 6795.173\n\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typelearnable_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: learnable_kernel\nBackward Execution Time (us) : 4321.351\n\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typeoriginal_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: original_kernel\nBackward Execution Time (us) : 1052.066\n```\n\n**Post-optimization**:\n```\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typepy_module\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: py_module\nBackward Execution Time (us) : 6737.106\n\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typelearnable_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: learnable_kernel\nBackward Execution Time (us) : 2112.484\n\n# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark\n# Mode: Eager\n# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typeoriginal_kernel\n# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: original_kernel\nBackward Execution Time (us) : 1078.79\n\nReviewed By: vkuzo\n\nDifferential Revision: D22946853\n\nfbshipit-source-id: 1a01284641480282b3f57907cc7908d68c68decd", "pr_number": "42810", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu", "aten/src/ATen/native/quantized/fake_quant_affine.h", "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp", "benchmarks/operator_benchmark/pt/quantization_test.py", "test/quantization/test_workflow_module.py"], "labels": ["fb-exported", "merged"]}, "916235284c": {"title": "[JIT] Fix typing.Final for python 3.8 (#39568)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/39566\n\n`typing.Final` is a thing since python 3.8, and on python 3.8, `typing_extensions.Final` is an alias of `typing.Final`, therefore, `ann.__module__ == 'typing_extensions'` will become False when using 3.8 and `typing_extensions` is installed.\n\n~~I don't know why the test is skipped, seems like due to historical reason when python 2.7 was still a thing?~~ Edit: I know now, the `Final` for `<3.7` don't have `__origin__`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39568\n\nReviewed By: smessmer\n\nDifferential Revision: D23043388\n\nPulled By: malfet\n\nfbshipit-source-id: cc87a9e4e38090d784e9cea630e1c543897a1697", "pr_number": "39568", "files_changed": ["test/jit/test_recursive_script.py", "torch/_jit_internal.py"], "labels": ["merged", "oncall: jit", "open source", "triaged"]}, "1041bdebb0": {"title": "Fix a typo in EmbeddingBag.cu (#42742)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42742\n\nReviewed By: smessmer\n\nDifferential Revision: D23011029\n\nPulled By: mrshenli\n\nfbshipit-source-id: 615f8b876ef1881660af71b6e145fb4ca97d2ebb", "pr_number": "42742", "files_changed": ["aten/src/ATen/native/cuda/EmbeddingBag.cu"], "labels": ["merged", "open source"]}, "42114a0154": {"title": "Update the documentation for scatter to include streams parameter. (#42814)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41827\n\n![Screenshot from 2020-08-10 13-41-20](https://user-images.githubusercontent.com/46765601/89813181-41041380-db0f-11ea-88c2-a97d7b994ac5.png)\n\nCurrent:\nhttps://pytorch.org/docs/stable/cuda.html#communication-collectives\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42814\n\nReviewed By: smessmer\n\nDifferential Revision: D23033544\n\nPulled By: mrshenli\n\nfbshipit-source-id: 88747fbb06e88ef9630c042ea9af07dafd422296", "pr_number": "42814", "files_changed": ["torch/nn/parallel/comm.py"], "labels": ["merged", "open source"]}, "7524699d58": {"title": "Modify clang code coverage to CMakeList.txt (for MacOS) (#42837)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42837\n\nOriginally we use\n```\nlist(APPEND CMAKE_C_FLAGS  -fprofile-instr-generate -fcoverage-mapping)\nlist(APPEND CMAKE_CXX_FLAGS  -fprofile-instr-generate -fcoverage-mapping)\n```\nBut when compile project on mac with Coverage On, it has the error:\n`clang: error: no input files\n/bin/sh: -fprofile-instr-generate: command not found\n/bin/sh: -fcoverage-mapping: command not found`\n\nThe reason behind it, is `list(APPEND CMAKE_CXX_FLAGS` will add an additional `;` to the variable. This means, if we do `list(APPEND foo a)` and then `list(APPEND foo b)`, then `foo` will be `a;b` -- with the additional `;`. Since we have `CMAKE_CXX_FLAGS` defined before in the `CMakeList.txt`, we can only use `set(...)` here\nAfter changing it to\n```\nset(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fprofile-instr-generate -fcoverage-mapping\")\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fprofile-instr-generate -fcoverage-mapping\")\n```\nTest successufully in local mac machine.\n\nTest Plan: Test locally on mac machine\n\nReviewed By: malfet\n\nDifferential Revision: D23043057\n\nfbshipit-source-id: ff6f4891b35b7f005861ee2f8e4c550c997fe961", "pr_number": "42837", "files_changed": ["CMakeLists.txt"], "labels": ["fb-exported", "merged"]}, "575e7497f6": {"title": "Introduce experimental FX library (#42741)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42741\n\nTest Plan: Imported from OSS\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23006383\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 6cb6d921981fcae47a07df581ffcf900fb8a7fe8", "pr_number": "42741", "files_changed": ["test/run_test.py", "test/test_fx.py", "torch/fx/__init__.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/node.py", "torch/fx/symbolic_trace.py"], "labels": ["merged"]}, "2c8cbd78bd": {"title": "Fix orgqr input size conditions (#42825)", "body": "Summary:\n* Adds support for `n > k`\n* Throw error if `m >= n >= k` is not true\n* Updates existing error messages to match argument names shown in public docs\n* Adds error tests\n\nFixes https://github.com/pytorch/pytorch/issues/41776\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42825\n\nReviewed By: smessmer\n\nDifferential Revision: D23038916\n\nPulled By: albanD\n\nfbshipit-source-id: e9bec7b11557505e10e0568599d0a6cb7e12ab46", "pr_number": "42825", "files_changed": ["aten/src/TH/generic/THTensorLapack.cpp", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "a7bdf575cb": {"title": "align qconv benchmark to conv benchmark (#42761)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42761\n\nMakes the qconv benchmark follow the conv benchmark exactly. This way\nit will be easy to compare q vs fp with the same settings.\n\nTest Plan:\n```\ncd benchmarks/operator_benchmark\npython -m pt.qconv_test\npython -m pt.conv_test\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23012533\n\nfbshipit-source-id: af30ee585389395569a6322f5210828432963077", "pr_number": "42761", "files_changed": ["benchmarks/operator_benchmark/pt/configs.py", "benchmarks/operator_benchmark/pt/conv_test.py", "benchmarks/operator_benchmark/pt/qconv_test.py"], "labels": ["merged"]}, "57b056b5f2": {"title": "align qlinear benchmark to linear benchmark (#42767)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42767\n\nSame as previous PR, forcing the qlinear benchmark to follow the fp one\n\nTest Plan:\n```\ncd benchmarks/operator_benchmark\npython -m pt.linear_test\npython -m pt.qlinear_test\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23013937\n\nfbshipit-source-id: fffaa7cfbfb63cea41883fd4d70cd3f08120aaf8", "pr_number": "42767", "files_changed": ["benchmarks/operator_benchmark/pt/configs.py", "benchmarks/operator_benchmark/pt/linear_test.py", "benchmarks/operator_benchmark/pt/qconv_test.py", "benchmarks/operator_benchmark/pt/qlinear_test.py"], "labels": ["merged"]}, "aabdef51f9": {"title": "[NNC] Registerizer for GPU [1/x] (#42606)", "body": "Summary:\nAdds a new optimization pass, the Registerizer, which looks for common Stores and Loads to a single item in a buffer and replaces them with a local temporary scalar which is cheaper to write.\n\nFor example it can replace:\n```\nA[0] = 0;\nfor (int x = 0; x < 10; x++) {\n  A[0] = (A[0]) + x;\n}\n```\n\nwith:\n```\nint A_ = 0;\nfor (int x = 0; x < 10; x++) {\n  A_ = x + A_;\n}\nA[0] = A_;\n```\n\nThis is particularly useful on GPUs when parallelizing, since after replacing loops with metavars we have a lot of accesses like this. Early tests of simple reductions on a V100 indicates this can speed them up by ~5x.\n\nThis diff got a bit unwieldy with the integration code so that will come in a follow up.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42606\n\nReviewed By: bertmaher\n\nDifferential Revision: D22970969\n\nPulled By: nickgg\n\nfbshipit-source-id: 831fd213f486968624b9a4899a331ea9aeb40180", "pr_number": "42606", "files_changed": ["test/cpp/tensorexpr/test_registerizer.cpp", "test/cpp/tensorexpr/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/analysis.h", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/ir.cpp", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.h", "torch/csrc/jit/tensorexpr/registerizer.cpp", "torch/csrc/jit/tensorexpr/registerizer.h", "torch/csrc/jit/tensorexpr/stmt.h", "torch/csrc/jit/tensorexpr/var_substitutor.h"], "labels": ["merged", "oncall: jit"]}, "4bafca1a69": {"title": "Adds list of operator-related information for testing (#41662)", "body": "Summary:\nThis PR adds:\n\n- an \"OpInfo\" class in common_method_invocations that can contain useful information about an operator, like what dtypes it supports\n- a more specialized \"UnaryUfuncInfo\" class designed to help test the unary ufuncs\n- the `ops` decorator, which can generate test variants from lists of OpInfos\n- test_unary_ufuncs.py, a new test suite stub that shows how the `ops` decorator and operator information can be used to improve the thoroughness of our testing\n\nThe single test in test_unary_ufuncs.py simply ensures that the dtypes associated with a unary ufunc operator in its OpInfo entry are correct. Writing a test like this previously, however, would have required manually constructing test-specific operator information and writing a custom test generator. The `ops` decorator and a common place to put operator information make writing tests like this easier and allows what would have been test-specific information to be reused.\n\nThe `ops` decorator extends and composes with the existing device generic test framework, allowing its decorators to be reused. For example, the `onlyOnCPUAndCUDA` decorator works with the new `ops` decorator. This should keep the tests readable and consistent.\n\nFuture PRs will likely:\n\n- continue refactoring the too large test_torch.py into more verticals (unary ufuncs, binary ufuncs, reductions...)\n- add more operator information to common_method_invocations.py\n- refactor tests for unary ufuncs into test_unary_ufunc\n\nExamples of possible future extensions are [here](https://github.com/pytorch/pytorch/pull/41662/commits/616747e50dbb5a3338deedf41ff44957b162ab51), where an example unary ufunc test is added, and [here](https://github.com/pytorch/pytorch/pull/41662/commits/d0b624f110d470b9a37ad02b389d2f4258c3d632), where example autograd tests are added. Both tests leverage the operator info in common_method_invocations to simplify testing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41662\n\nReviewed By: ngimel\n\nDifferential Revision: D23048416\n\nPulled By: mruberry\n\nfbshipit-source-id: ecce279ac8767f742150d45854404921a6855f2c", "pr_number": "41662", "files_changed": ["test/run_test.py", "test/test_unary_ufuncs.py", "torch/testing/__init__.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["merged", "module: tests"]}, "6471b5dc66": {"title": "Correct the type of some floating point literals in calc_digamma (#42846)", "body": "Summary:\nThey are double, but they are supposed to be of accscalar_t or a faster type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42846\n\nReviewed By: zou3519\n\nDifferential Revision: D23049405\n\nPulled By: mruberry\n\nfbshipit-source-id: 29bb5d5419dc7556b02768f0ff96dfc28676f257", "pr_number": "42846", "files_changed": ["aten/src/ATen/native/cuda/Math.cuh"], "labels": ["merged", "open source"]}, "c660d2a9ae": {"title": "Initial quantile operator implementation (#42755)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42755\n\nAttempting to land quantile again after being landed here https://github.com/pytorch/pytorch/pull/39417 and reverted here https://github.com/pytorch/pytorch/pull/41616.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23030338\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 124a86eea3aee1fdaa0aad718b04863935be26c7", "pr_number": "42755", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["merged"]}, "9c8f5cb61d": {"title": "Ensure IDEEP transpose operator works correctly", "body": "Summary: I found out that without exporting to public format IDEEP transpose operator in the middle of convolution net produces incorrect results (probably reading some out-of-bound memory). Exporting to public format might not be the most efficient solution, but at least it ensures correct behavior.\n\nTest Plan: Running ConvFusion followed by transpose should give identical results on CPU and IDEEP\n\nReviewed By: bwasti\n\nDifferential Revision: D22970872\n\nfbshipit-source-id: 1ddca16233e3d7d35a367c93e72d70632d28e1ef", "pr_number": null, "files_changed": ["caffe2/ideep/operators/transpose_op.cc"], "labels": []}, "4afbf39737": {"title": "Add nn.functional.adaptive_avg_pool size empty tests (#42857)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42857\n\nReviewed By: seemethere\n\nDifferential Revision: D23053677\n\nPulled By: malfet\n\nfbshipit-source-id: b3d0d517cddc96796461332150e74ae94aac8090", "pr_number": "42857", "files_changed": ["test/test_nn.py"], "labels": ["merged"]}, "71dbfc79b3": {"title": "Export BatchBucketOneHot Caffe2 Operator to PyTorch", "body": "Summary: As titled.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/python/operator_test:torch_integration_test -- test_batch_bucket_one_hot_op\n```\n\nReviewed By: yf225\n\nDifferential Revision: D23005981\n\nfbshipit-source-id: 1daa8d3e7d6ad75e97e94964db95ccfb58541672", "pr_number": null, "files_changed": ["caffe2/operators/one_hot_ops.cc", "caffe2/operators/one_hot_ops.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": []}, "43613b4236": {"title": "Fix incorrect aten::sorted.str return type (#42853)", "body": "Summary:\naten::sorted.str output type was incorrectly set to bool[] due to a copy-paste error. This PR fixes it.\n\nFixes https://fburl.com/0rv8amz7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42853\n\nReviewed By: yf225\n\nDifferential Revision: D23054907\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: a62968c90f0301d4a5546e6262cb9315401a9729", "pr_number": "42853", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/test_jit.py", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["merged", "oncall: jit"]}, "0ff0fea42b": {"title": "[FX] fix lint (#42866)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42866\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23056813\n\nPulled By: jamesr66a\n\nfbshipit-source-id: d30cdffe6f0465223354dec00f15658eb0b08363", "pr_number": "42866", "files_changed": ["torch/fx/__init__.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/node.py", "torch/fx/symbolic_trace.py"], "labels": ["merged"]}, "3bf2978497": {"title": "remove deadline enforcement for hypothesis (#42871)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42871\n\nold version of hypothesis.testing was not enforcing deadlines\nafter the library got updated, default deadline=200ms, but even with 1s or\nmore, tests are flaky. Changing deadline to non-enforced which is the same\nbehavior as the old version\n\nTest Plan: tested fakelowp/tests\n\nReviewed By: hl475\n\nDifferential Revision: D23059033\n\nfbshipit-source-id: 79b6aec39a2714ca5d62420c15ca9c2c1e7a8883", "pr_number": "42871", "files_changed": ["caffe2/contrib/fakelowp/test/test_batchmatmul_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_batchnorm_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_fc_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py", "caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_4bit_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp32.py"], "labels": ["fb-exported", "merged"]}, "eeb43ffab9": {"title": "format for readability (#42851)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42851\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D23048382\n\nPulled By: bhosmer\n\nfbshipit-source-id: 55d84d5f9c69be089056bf3e3734c1b1581dc127", "pr_number": "42851", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h"], "labels": ["merged"]}, "7a9ae52550": {"title": "[hypothesis] Deadline followup (#42842)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42842\n\nTest Plan: `buck test`\n\nReviewed By: thatch\n\nDifferential Revision: D23045269\n\nfbshipit-source-id: 8a3f4981869287a0f5fb3f0009e13548b7478086", "pr_number": "42842", "files_changed": ["caffe2/python/hypothesis_test.py", "caffe2/python/layers_test.py", "caffe2/python/memonger_test.py", "caffe2/python/operator_test/activation_ops_test.py"], "labels": ["fb-exported", "merged"]}, "b0b8340065": {"title": "Collect more data in collect_env (#42887)", "body": "Summary:\nCollect Python runtime bitness (32 vs 64 bit)\nCollect Mac/Linux OS machine time (x86_64, arm, Power, etc)\nCollect Clang version\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42887\n\nReviewed By: seemethere\n\nDifferential Revision: D23064788\n\nPulled By: malfet\n\nfbshipit-source-id: df361bdbb79364dc521b8e1ecbed1b4bd08f9742", "pr_number": "42887", "files_changed": ["torch/utils/collect_env.py"], "labels": ["merged"]}, "5edd9aa95a": {"title": "Fix manual seed to unpack unsigned long (#42206)", "body": "Summary:\n`torch.manual_seed` was unpacking its argument as an `int64_t`. This fix changes it to a `uint64_t`.\n\nFixes https://github.com/pytorch/pytorch/issues/33546\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42206\n\nReviewed By: ezyang\n\nDifferential Revision: D22822098\n\nPulled By: albanD\n\nfbshipit-source-id: 97c978139c5cb2d5b62cc2c963550c758ee994f7", "pr_number": "42206", "files_changed": ["test/test_torch.py", "torch/_torch_docs.py", "torch/csrc/Generator.cpp", "torch/csrc/utils/python_numbers.h", "torch/random.py"], "labels": ["merged", "open source"]}, "a846ed5ce7": {"title": "[quant] Reduce number of variants of add/mul (#42769)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42769\n\nSome of the quantized add and mul can have the same name\n\nTest Plan: Imported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23054822\n\nfbshipit-source-id: c1300f3f0f046eaf0cf767d03b957835e22cfb4b", "pr_number": "42769", "files_changed": ["aten/src/ATen/native/quantized/cpu/qadd.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py"], "labels": ["merged"]}, "e845b0ab51": {"title": "[Resending] [ONNX] Add eliminate_unused_items pass (#42743)", "body": "Summary:\nThis PR:\n\n- Adds eliminate_unused_items pass that removes unused inputs and initializers.\n- Fixes run_embed_params function so it doesn't export unnecessary parameters.\n- Removes test_modifying_params in test_verify since it's no longer needed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42743\n\nReviewed By: hl475\n\nDifferential Revision: D23058954\n\nPulled By: houseroad\n\nfbshipit-source-id: cd1e81463285a0bf4e60766c8c87fc9a350d9c7e", "pr_number": "42743", "files_changed": ["test/onnx/debug_embed_params.py", "test/onnx/test_utility_funs.py", "test/onnx/test_verify.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/onnx/constant_fold.cpp", "torch/csrc/jit/passes/onnx/eliminate_unused_items.cpp", "torch/csrc/jit/passes/onnx/eliminate_unused_items.h", "torch/csrc/jit/passes/onnx/eval_peephole.cpp", "torch/csrc/jit/passes/onnx/helper.cpp", "torch/csrc/jit/passes/onnx/helper.h", "torch/csrc/jit/python/init.cpp", "torch/onnx/utils.py"], "labels": ["merged", "oncall: jit", "open source", "triaged"]}, "ac93d45906": {"title": "[quant] Attach qconfig to all modules (#42576)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42576\n\nPreviously we have qconfig propagate list and we only attach qconfig for modules\nin the list, this works when everything is quantized in the form of module.\nbut now we are expanding quantization for functional/torch ops, we'll need to attach qconfig\nto all modules\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22939453\n\nfbshipit-source-id: 7d6a1f73ff9bfe461b3afc75aa266fcc8f7db517", "pr_number": "42576", "files_changed": ["test/quantization/test_workflow_module.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "cd756ee3d4": {"title": "Support boolean key in dictionary (#42833)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41449 .\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42833\n\nTest Plan: `python test/test_jit.py TestDict`\n\nReviewed By: zou3519\n\nDifferential Revision: D23056250\n\nPulled By: asuhan\n\nfbshipit-source-id: 90dabe1490c99d3e57a742140a4a2b805f325c12", "pr_number": "42833", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/jit/test_list_dict.py", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["merged", "oncall: jit"]}, "bee174dc3f": {"title": "Adds linalg.det alias, fixes outer alias, updates alias testing (#42802)", "body": "Summary:\nThis PR:\n\n- updates test_op_normalization.py, which verifies that aliases are correctly translated in the JIT\n- adds torch.linalg.det as an alias for torch.det\n- moves the torch.linalg.outer alias to torch.outer (to be consistent with NumPy)\n\nThe torch.linalg.outer alias was put the linalg namespace erroneously as a placeholder since it's a \"linear algebra op\" according to NumPy but is actually still in the main NumPy namespace.\n\nThe updates to test_op_normalization are necessary. Previously it was using method_tests to generate tests, and method_tests assumes test suites using it also use the device generic framework, which test_op_normalization did not. For example, some ops require decorators like `skipCPUIfNoLapack`, which only works in device generic test classes. Moving test_op_normalization to the device generic framework also lets these tests run on CPU and CUDA.\n\nContinued reliance on method_tests() is excessive since the test suite is only interested in testing aliasing, and a simpler and more readable `AliasInfo` class is used for the required information. An example impedance mismatch between method_tests and the new tests, for example, was how to handle ops in namespaces like torch.linalg.det. In the future this information will likely be folded into a common 'OpInfo' registry in the test suite.\n\nThe actual tests performed are similar to what they were previously: a scripted and traced version of the op is run and the test verifies that both graphs do not contain the alias name and do contain the aliased name.\n\nThe guidance for adding an alias has been updated accordingly.\n\ncc mattip\n\nNote:\n\nngimel suggests:\n- deprecating and then removing the `torch.ger` name\n- reviewing the implementation of `torch.outer`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42802\n\nReviewed By: zou3519\n\nDifferential Revision: D23059883\n\nPulled By: mruberry\n\nfbshipit-source-id: 11321c2a7fb283a6e7c0d8899849ad7476be42d1", "pr_number": "42802", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/linalg.rst", "docs/source/tensors.rst", "docs/source/torch.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/jit/test_op_normalization.py", "test/run_test.py", "test/test_jit.py", "test/test_linalg.py", "test/test_op_normalization.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/api/include/torch/linalg.h", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/linalg/__init__.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/jit_utils.py"], "labels": ["merged", "module: numpy", "module: tests", "oncall: jit", "topic: linear algebra"]}, "38c7b9a168": {"title": "avoid redundant isCustomClassRegistered() checks (#42852)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42852\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D23048381\n\nPulled By: bhosmer\n\nfbshipit-source-id: 40b71670a84cb6f7e5a03279f58ce227d676aa03", "pr_number": "42852", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/ivalue_inl.h", "torch/custom_class.h"], "labels": ["merged"]}, "ab0a04dc9c": {"title": "Add `torch.nansum` (#38628)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/38349\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38628\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D22860549\n\nPulled By: mruberry\n\nfbshipit-source-id: 87fcbfd096d83fc14b3b5622f2301073729ce710", "pr_number": "38628", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOps.h", "aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceSumProdKernel.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_autograd.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["merged", "module: numpy", "open source", "triaged"]}, "a346e90c49": {"title": "Update to NNP-I v1.0.0.5 (#4770)", "body": "Summary:\nAlign code to NNP-I v1.0.0.5 (glow tracing changes).\n\nPull Request resolved: https://github.com/pytorch/glow/pull/4770\n\nReviewed By: arunm-git\n\nDifferential Revision: D22927904\n\nPulled By: hl475\n\nfbshipit-source-id: 3746a6b07f3fcffc662d80a95513427cfccac7a5", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp32.py"], "labels": []}, "ecb9e790ed": {"title": "Remove excessive logging in plan_executor (#42888)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42888\n\nas title\n\nTest Plan: flow-cli test-locally dper.workflows.evaluation.eval_workflow --parameters-file /mnt/public/ehsanardestani/temp/quant_eval_inputs_all.json\n\nReviewed By: amylittleyang\n\nDifferential Revision: D23066529\n\nfbshipit-source-id: f925afd1734e617e412b0f171e16c781d13272d9", "pr_number": "42888", "files_changed": ["caffe2/core/plan_executor.cc"], "labels": ["fb-exported", "merged"]}, "4665f3fc8d": {"title": "Fix freeze_module pass for sharedtype (#42457)", "body": "Summary:\nDuring cleanup phase, calling recordReferencedAttrs would record\nthe attributes which are referenced and hence kept.\nHowever, if you have two instances of the same type which are preserved\nthrough freezing process, as the added testcase shows, then during\nrecording the attributes which are referenced, we iterate through the\ntype INSTANCES that we have seen so far and record those ones.\nThus if we have another instance of the same type, we will just look at\nthe first instance in the list, and record that instances.\nThis PR fixes that by traversing the getattr chains and getting the\nactual instance of the getattr output.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42457\n\nTest Plan:\npython test/test_jit.py TestFreezing\nFixes #{issue number}\n\nReviewed By: zou3519\n\nDifferential Revision: D22898051\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 8b1d80f0eb40ab99244f931d4a1fdb28290a4683", "pr_number": "42457", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/freeze_module.cpp"], "labels": ["merged", "oncall: jit"]}, "77bd4d3426": {"title": "MAINT: speed up istft by using col2im (the original python code used \u2026 (#42826)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42213\n\nThe [original python code](https://github.com/pytorch/audio/blob/v0.5.0/torchaudio/functional.py#L178) from `torchaudio` was converted to a native function, but used `eye` to  allocate a Tensor and was much slower.\nUsing `at::col2im` (which is the equivalent of `torch.nn.functional.fold`) solved the slowdown.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42826\n\nReviewed By: smessmer\n\nDifferential Revision: D23043673\n\nPulled By: mthrok\n\nfbshipit-source-id: 3f5d0779a87379b002340ea19c9ae5042a43e94e", "pr_number": "42826", "files_changed": ["aten/src/ATen/native/SpectralOps.cpp"], "labels": ["merged", "open source", "triaged"]}, "2f1baf6c25": {"title": "Fix coding style and safety issues in CuBLAS nondeterministic unit test (#42627)", "body": "Summary:\nAddresses some comments that were left unaddressed after PR https://github.com/pytorch/pytorch/issues/41377 was merged:\n\n* Use `check_output` instead of `Popen` to run each subprocess sequentially\n* Use f-strings rather than old python format string style\n* Provide environment variables to subprocess through the `env` kwarg\n* Check for correct error behavior inside the subprocess, and raise another error if incorrect. Then the main process fails the test if any error is raised\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42627\n\nReviewed By: malfet\n\nDifferential Revision: D22969231\n\nPulled By: ezyang\n\nfbshipit-source-id: 38d5f3f0d641c1590a93541a5e14d90c2e20acec", "pr_number": "42627", "files_changed": ["test/test_torch.py"], "labels": ["merged", "open source"]}, "2878efb35d": {"title": "Use `C10_API_ENUM` to fix invalid attribute warnings (#42464)", "body": "Summary:\nUsing the macro added in https://github.com/pytorch/pytorch/issues/38988 to fix more attribute warnings.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42464\n\nReviewed By: malfet\n\nDifferential Revision: D22916943\n\nPulled By: ezyang\n\nfbshipit-source-id: ab9ca8755cd8b89aaf7f8718b4107b4b94d95005", "pr_number": "42464", "files_changed": ["aten/src/ATen/record_function.h", "torch/csrc/autograd/profiler.h", "torch/csrc/jit/tensorexpr/bounds_inference.h"], "labels": ["merged", "oncall: jit", "open source"]}, "75a15d3d01": {"title": "Follow-up for pytorch/pytorch#37091. (#42806)", "body": "Summary:\nThis is a follow-up PR for https://github.com/pytorch/pytorch/issues/37091, fixing some of the quirks of that PR as that one was landed early to avoid merge conflicts.\n\nThis PR addresses the following action items:\n\n- [x] Use error-handling macros instead of a `try`-`catch`.\n- [x] Renamed and added comments to clarify the use of `HANDLED_FUNCTIONS_WRAPPERS` in tests. `HANDLED_FUNCTIONS_NAMESPACES` was already removed in the last PR as we had a way to test for methods.\n\nThis PR does NOT address the following action item, as it proved to be difficult:\n\n- [ ] Define `__module__`  for whole API.\n\nSingle-line repro-er for why this is hard:\n\n```python\n>>> torch.Tensor.grad.__get__.__module__ = \"torch.Tensor.grad\"\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'method-wrapper' object has no attribute '__module__'\n```\n\nExplanation: Methods  defined in C/properties don't always have a `__dict__` attribute or a mutable `__module__` slot for us to modify.\n\nThe documentation action items were addressed in the following commit, with the additional future task of adding the rendered RFCs to the documentation: https://github.com/pytorch/rfcs/pull/3/commits/552ba37c0500f70b8738522591b276c82cb7ca2a\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42806\n\nReviewed By: smessmer\n\nDifferential Revision: D23031501\n\nPulled By: ezyang\n\nfbshipit-source-id: b781c97f7840b8838ede50a0017b4327f96bc98a", "pr_number": "42806", "files_changed": ["test/test_overrides.py", "tools/autograd/templates/python_variable_methods.cpp", "torch/csrc/autograd/python_variable.cpp"], "labels": ["merged", "open source"]}, "686705c98b": {"title": "Optimize LayerNorm performance on CPU both forward and backward (#35750)", "body": "Summary:\nThis PR aims at improving `LayerNorm` performance on CPU for both forward and backward.\n\nResults on Xeon 6248:\n1. single socket inference **1.14x** improvement\n2. single core inference **1.77x** improvement\n3. single socket training **6.27x** improvement\n\nThe fine tuning of GPT2 on WikiTest2 dataset time per iteration on dual socket reduced from **4.69s/it** to **3.16s/it**, **1.48x** improvement.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/35750\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D20810026\n\nPulled By: glaringlee\n\nfbshipit-source-id: c5801bd76eb944f2e46c2fe4991d9ad4f40495c3", "pr_number": "35750", "files_changed": ["aten/src/ATen/cpu/vec256/functional.h", "aten/src/ATen/native/cpu/layer_norm_kernel.cpp", "aten/src/ATen/native/layer_norm.cpp"], "labels": ["merged", "module: cpu", "open source", "topic: performance", "triaged"]}, "5157afcf59": {"title": "fix int8 FC (#42691)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42691\n\nfix quantization of FC bias to match nnpi\nquantize biases to fp16\n\nTest Plan: improved the unit test to have input tensors in fp32\n\nReviewed By: tracelogfb\n\nDifferential Revision: D22941521\n\nfbshipit-source-id: 00afb70610f8a149110344d52595c39e3fc988ab", "pr_number": "42691", "files_changed": ["caffe2/contrib/fakelowp/common.h", "caffe2/contrib/fakelowp/layernorm_fp16_fake_op.cc", "caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py", "caffe2/quantization/server/fbgemm_pack_op.cc", "caffe2/quantization/server/fbgemm_pack_op.h"], "labels": ["fb-exported", "merged"]}, "5c39146c34": {"title": "Fix get_writable_path (#42895)", "body": "Summary:\nAs name suggests, this function should always return a writable path\nCall `mkdtemp` to create temp folder if path is not writable\n\nThis fixes `TestNN.test_conv_backcompat` if PyTorch is installed in non-writable location\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42895\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23070320\n\nPulled By: malfet\n\nfbshipit-source-id: ed6a681d46346696a0de7e71f0b21cba852a964e", "pr_number": "42895", "files_changed": ["torch/_utils_internal.py"], "labels": ["merged"]}, "bda0007620": {"title": "Improve calling backward() and grad() inside vmap error messages (#42876)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42876\n\nPreviously, the error messages were pretty bad. This PR adds nice\nerror messages for the following cases:\n- user attempts to call .backward() inside vmap for any reason\nwhatsoever\n- user attempts to call autograd.grad(outputs, inputs, grad_outputs),\nwhere outputs or inputs is being vmapped over (so they are\nBatchedTensors).\n\nThe case we do support is calling autograd.grad(outputs, inputs,\ngrad_outputs) where `grad_outputs` is being vmapped over. This is the\ncase for batched gradient support (e.g., user passes in a batched\ngrad_output).\n\nTest Plan: - new tests: `pytest test/test_vmap.py -v`\n\nReviewed By: ezyang\n\nDifferential Revision: D23059836\n\nPulled By: zou3519\n\nfbshipit-source-id: 2fd4e3fd93f558e67e2f0941b18f0d00d8ab439f", "pr_number": "42876", "files_changed": ["test/test_vmap.py", "torch/csrc/autograd/python_engine.cpp"], "labels": ["merged"]}, "3d3752d716": {"title": "Revert D22898051: [pytorch][PR] Fix freeze_module pass for sharedtype", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22898051 (https://github.com/pytorch/pytorch/commit/4665f3fc8d8ab797d916b312ca27d7abe6cde7b2)\n\nOriginal commit changeset: 8b1d80f0eb40\n\nfbshipit-source-id: 4dc0ba274282a157509db16df13269eed6cd5be9", "pr_number": null, "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/freeze_module.cpp"], "labels": []}, "ea65a56854": {"title": "Use `string(APPEND FOO \" bar\")` instead of `set(FOO \"${FOO} bar\") (#42844)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42844\n\nReviewed By: scintiller\n\nDifferential Revision: D23067577\n\nPulled By: malfet\n\nfbshipit-source-id: e4380ce02fd6aca37c955a7bc24435222c5d8b19", "pr_number": "42844", "files_changed": ["CMakeLists.txt", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/cpu/tbb/CMakeLists.txt"], "labels": ["merged"]}, "59f8692350": {"title": "[pytorch] BUCK build for Vulkan backend", "body": "Summary:\nIntroducing `//xplat/caffe2:aten_vulkan` target which contains pytorch Vulkan backend and its ops.\n\n `//xplat/caffe2:aten_vulkan` depends on ` //xplat/caffe2:aten_cpu`\n\nJust inclusion it to linking registers Vulkan Backend and its ops.\n\n**Code generation:**\n1. `VulkanType.h`, `VulkanType.cpp`\nTensor Types for Vulkan backend are generated by `//xplat/caffe2:gen_aten_vulkan` which runs aten code generation (`aten/src/ATen/gen.py`) with `--vulkan` argument.\n\n2. Shaders compilation\n`//xplat/caffe2:gen_aten_vulkan_spv`  genrule runs `//xplat/caffe2:gen_aten_vulkan_spv_bin` which is a wrapper on `aten/src/ATen/native/vulkan/gen_spv.py`\n\nGLSL files are listed in `aten/src/ATen/native/vulkan/glsl/*` and to compile them `glslc` (glsl compiler) is required.\n\n`glslc` is in opensource https://github.com/google/shaderc , that also has a few dependencies  on other libraries, that porting this build to BUCK will take significant amount of time.\n\nTo use `glslc` in BUCK introducing\n\ndotslash `xplat/caffe2/fb/vulkan/dotslash/glslc` which is stored on manifold the latest prebuilt binaries of `glslc` from ANDROID_NDK for linux, macos and windows.\n\nNot using it from ANDROID_NDK directly allows to update it without dependency on ndk.\n\nTest Plan:\nBuilding aten_vulkan target:\n```\nbuck build //xplat/caffe2:aten_vulkan\n```\n\nBuilding vulkan_test that contains vulkan unittests for android:\n```\nbuck build //xplat/caffe2:pt_vulkan_test_binAndroid#android-armv7\n```\nAnd running it on the device with vulkan support.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D22770299\n\nfbshipit-source-id: 843af8df226d4b5395b8e480eb47b233d57201df", "pr_number": null, "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.cpp"], "labels": []}, "ada8404f2d": {"title": "[jit] Scaffold a static runtime (#42753)", "body": "Summary:\nThe premise of this approach is that a small subset of neural networks are well represented by a data flow graph.  The README contains more information.\n\nThe name is subject to change, but I thought it was a cute reference to fire.\n\nsuo let me know if you'd prefer this in a different spot.  Since it lowers a JIT'd module directly I assumed the JIT folder would be appropriate.  There is no exposed Python interface yet (but is mocked up in `test_accelerant.py`)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42753\n\nReviewed By: zou3519\n\nDifferential Revision: D23043771\n\nPulled By: bwasti\n\nfbshipit-source-id: 5353731e3aae31c08b5b49820815da98113eb551", "pr_number": "42753", "files_changed": ["test/test_static_runtime.py", "tools/build_variables.bzl", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/runtime/static/README.md", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/init.cpp", "torch/csrc/jit/runtime/static/init.h"], "labels": ["merged", "oncall: jit"]}, "7f3f5020e6": {"title": "CUDA reduction: allow outputs to have different strides (#42649)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42364\n\nBenchmark:\nhttps://github.com/zasdfgbnm/things/blob/master/2020Q3/min-benchmark.ipynb\n```python\nimport torch\n\nprint(torch.__version__)\nprint()\n\nfor i in range(100):\n    torch.randn(1000, device='cuda')\n\nfor e in range(7, 15):\n    N = 2 ** e\n    input_ = torch.randn(N, N, device='cuda')\n    torch.cuda.synchronize()\n    %timeit input_.min(dim=0); torch.cuda.synchronize()\n    input_ = torch.randn(N, N, device='cuda').t()\n    torch.cuda.synchronize()\n    %timeit input_.min(dim=0); torch.cuda.synchronize()\n    print()\n```\nBefore\n```\n1.7.0a0+5d7c3f9\n\n21.7 \u00b5s \u00b1 1.67 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n20.6 \u00b5s \u00b1 773 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n22.5 \u00b5s \u00b1 294 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n20.2 \u00b5s \u00b1 250 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n26.4 \u00b5s \u00b1 67 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n20.9 \u00b5s \u00b1 316 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n33 \u00b5s \u00b1 474 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n21.1 \u00b5s \u00b1 218 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n84.2 \u00b5s \u00b1 691 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n50.3 \u00b5s \u00b1 105 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n181 \u00b5s \u00b1 2.36 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n145 \u00b5s \u00b1 149 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n542 \u00b5s \u00b1 753 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n528 \u00b5s \u00b1 10.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n2.04 ms \u00b1 9.74 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n2.01 ms \u00b1 22.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\nAfter\n```\n1.7.0a0+9911817\n\n21.4 \u00b5s \u00b1 695 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n20.6 \u00b5s \u00b1 989 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n22.4 \u00b5s \u00b1 153 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n20.5 \u00b5s \u00b1 58.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n26.6 \u00b5s \u00b1 147 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n20.9 \u00b5s \u00b1 675 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n35.4 \u00b5s \u00b1 560 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n21.7 \u00b5s \u00b1 1.17 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n86.5 \u00b5s \u00b1 1.99 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n52.2 \u00b5s \u00b1 1.57 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n195 \u00b5s \u00b1 2.97 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n153 \u00b5s \u00b1 4.46 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n550 \u00b5s \u00b1 7.72 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n527 \u00b5s \u00b1 3.04 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n2.05 ms \u00b1 7.87 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n2 ms \u00b1 4.93 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42649\n\nReviewed By: ezyang\n\nDifferential Revision: D22994446\n\nPulled By: ngimel\n\nfbshipit-source-id: cc60beebad2e04c26ebf3ca702a6cb05846522c9", "pr_number": "42649", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/cuda/Reduce.cu", "aten/src/ATen/native/cuda/Reduce.cuh", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "62bd2ddec7": {"title": "Implemented non-named version of unflatten (#42563)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42563\n\nMoved logic for non-named unflatten from python nn module to aten/native to be reused by the nn module later. Fixed some inconsistencies with doc and code logic.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23030301\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 7c804ed0baa5fca960a990211b8994b3efa7c415", "pr_number": "42563", "files_changed": ["aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensor_view.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_namedtensor.py", "test/test_torch.py", "torch/nn/modules/flatten.py", "torch/tensor.py"], "labels": ["merged"]}, "92885ebe16": {"title": "Implement hypot (#42291)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/38349\nCloses https://github.com/pytorch/pytorch/issues/22764\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42291\n\nReviewed By: malfet\n\nDifferential Revision: D22951859\n\nPulled By: mruberry\n\nfbshipit-source-id: d0118f2b6437e5c3f775f699ec46e946a8da50f0", "pr_number": "42291", "files_changed": ["aten/src/ATen/core/NamedRegistrations.cpp", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_bfloat16.h", "aten/src/ATen/cpu/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_complex_float.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vec256_float_neon.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "c10/util/math_compat.h", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["merged", "module: numpy", "open source", "triaged"]}, "0134deda0f": {"title": "[FX] Add interface to reject nodes (#42865)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42865\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23056584\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 02db08165ab41be5f3c4b5ff253cbb444eb9a7b8", "pr_number": "42865", "files_changed": ["test/test_fx.py", "torch/fx/graph.py", "torch/fx/node.py", "torch/fx/symbolic_trace.py"], "labels": ["merged"]}, "86841f5f61": {"title": "Update cuda init docstring to improve clarity (#42923)", "body": "Summary:\nA small clarity improvement to the cuda init docstring\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42923\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23080693\n\nPulled By: mrshenli\n\nfbshipit-source-id: aad5ed9276af3b872c1def76c6175ee30104ccb2", "pr_number": "42923", "files_changed": ["torch/cuda/__init__.py"], "labels": ["merged", "open source"]}, "f373cda021": {"title": "Revert D22994446: [pytorch][PR] CUDA reduction: allow outputs to have different strides", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22994446 (https://github.com/pytorch/pytorch/commit/7f3f5020e6d930792c3db6b6604de74bb58aa291)\n\nOriginal commit changeset: cc60beebad2e\n\nfbshipit-source-id: f4635deac386db0c161f910760cace09f15a1ff9", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/cuda/Reduce.cu", "aten/src/ATen/native/cuda/Reduce.cuh", "test/test_torch.py"], "labels": []}, "1adeed2720": {"title": "Speed up CUDA kernel launch when block/thread extents are statically known (#42899)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42899\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23078708\n\nPulled By: bertmaher\n\nfbshipit-source-id: 237404b47a31672d7145d70996868a3b9b97924e", "pr_number": "42899", "files_changed": ["torch/csrc/jit/tensorexpr/cuda_codegen.cpp"], "labels": ["merged", "oncall: jit"]}, "33d209b5f4": {"title": "Fix TE microbenchmark harness to use appropriate fuser/executor (#42900)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42900\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23079715\n\nPulled By: bertmaher\n\nfbshipit-source-id: 6aa2b08a550835b7737e355960a16a7ca83878ea", "pr_number": "42900", "files_changed": ["benchmarks/tensorexpr/__main__.py"], "labels": ["merged"]}, "b8ae563ce6": {"title": "Add a microbenchmark for LSTM elementwise portion (#42901)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42901\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23079714\n\nPulled By: bertmaher\n\nfbshipit-source-id: 28f8c3b5019ee898e82e64a0a674da1b4736d252", "pr_number": "42901", "files_changed": ["benchmarks/tensorexpr/__main__.py", "benchmarks/tensorexpr/rnn_eltwise.py"], "labels": ["merged"]}, "5d2e9b6ed9": {"title": "Add missing type annotation for Tensor.ndim (#42909)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42908\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42909\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23090364\n\nPulled By: malfet\n\nfbshipit-source-id: 44457fddc86f6abde635aa671e7611b405780ab9", "pr_number": "42909", "files_changed": ["torch/_C/__init__.pyi.in"], "labels": ["merged", "module: typing", "open source", "triaged"]}, "20e0e54dbe": {"title": "Allow Tensor& in the unboxing logic (#42712)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42712\n\nPreviously, operators taking Tensor& as arguments or returning it couldn't be c10-full because the unboxing logic didn't support it.\nThis adds temporary support for that. We're planning to remove this again later, but for now we need it to make those ops c10-full.\nSee https://docs.google.com/document/d/19thMVO10yMZA_dQRoB7H9nTPw_ldLjUADGjpvDmH0TQ for the full plan.\n\nThis PR also makes some ops c10-full that now can be.\nghstack-source-id: 109693706\n\nTest Plan: unit tests\n\nReviewed By: bhosmer\n\nDifferential Revision: D22989242\n\nfbshipit-source-id: 1bd97e5fa2b90b0860784da4eb772660ca2db5a3", "pr_number": "42712", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/native/native_functions.yaml", "torch/csrc/autograd/TraceTypeManual.cpp", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "7a7424bf91": {"title": "Remove impl_unboxedOnlyKernel (#42841)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42841\n\nThere is nothing using those APIs anymore. While we still have ops that require an unboxedOnly implementation (i.e. that aren't c10-full yet), those are all already migrated to the new op registration API and use `.impl_UNBOXED()`.\nghstack-source-id: 109693705\n\nTest Plan: waitforsandcastle\n\nReviewed By: bhosmer\n\nDifferential Revision: D23045335\n\nfbshipit-source-id: d8e15cea1888262135e0d1d94c515d8a01bddc45", "pr_number": "42841", "files_changed": ["aten/src/ATen/core/boxing/impl/kernel_function_test.cpp", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp"], "labels": ["merged"]}, "8cb42fce17": {"title": "[quant][fix] Remove activation_post_process in qat modules (#42343)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42343\n\nCurrently activation_post_process are inserted by default in qat modules, which is not\nfriendly to automatic quantization tools, this PR removes them.\n\nTest Plan: Imported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D22856816\n\nfbshipit-source-id: 988a43bce46a992b38fd0d469929f89e5b046131", "pr_number": "42343", "files_changed": ["test/quantization/test_quantize.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/intrinsic/qat/modules/linear_relu.py", "torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "c9dcc833bc": {"title": "[quant][pyper] Make offsets an optional paramter in the qembedding_bag op (#42924)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42924\n\noffsets is an optional paramter in the python module currently. So we update the operator to follow suit\nin order to avoid bad optional access\n\nTest Plan:\npython test/test_quantization.py TestQuantizeDynamicJitOps.test_embedding_bag\n\nImported from OSS\n\nReviewed By: radkris-git\n\nDifferential Revision: D23081152\n\nfbshipit-source-id: 847b58f826f5a18e8d4978fc4afc6f3a96dc4230", "pr_number": "42924", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "aten/src/ATen/native/quantized/library.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": ["merged", "oncall: jit"]}, "c88d3a5e76": {"title": "Remove Python dependency from TensorPipe RPC agent (#42678)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42678\n\nghstack-source-id: 109544679\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D22978716\n\nfbshipit-source-id: 31f91d35e9538375b047184cf4a735e4b8809a15", "pr_number": "42678", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["merged"]}, "8493b0d5d6": {"title": "Enroll TensorPipe agent in C++-only E2E test (#42680)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42680\n\nghstack-source-id: 109544678\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D22978714\n\nfbshipit-source-id: 04d6d190c240c6ead9bd9f3b7f3a5f964d7451e8", "pr_number": "42680", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/rpc/CMakeLists.txt", "test/cpp/rpc/e2e_test_base.cpp", "test/cpp/rpc/e2e_test_base.h", "test/cpp/rpc/test_e2e_tensorpipe.cpp"], "labels": ["merged"]}, "607e49cc83": {"title": "Revert D22856816: [quant][fix] Remove activation_post_process in qat modules", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD22856816 (https://github.com/pytorch/pytorch/commit/8cb42fce17675142742d5fea78a067de4491d2cc)\n\nOriginal commit changeset: 988a43bce46a\n\nfbshipit-source-id: eff5b9abdfc15b21c02c61eefbda38d349173436", "pr_number": null, "files_changed": ["test/quantization/test_quantize.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/intrinsic/qat/modules/linear_relu.py", "torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "ba9025bc1a": {"title": "[tensorexpr] Autograd for testing (#42548)", "body": "Summary:\nA simple differentiable abstraction to allow testing of full training graphs.\n\nIncluded in this 1st PR is an example of trivial differentiation.\n\nIf approved, I can add a full MLP and demonstrate convergence using purely NNC (for performance testing) in the next PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42548\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23057920\n\nPulled By: bwasti\n\nfbshipit-source-id: 4a239852c5479bf6bd20094c6c35f066a81a832e", "pr_number": "42548", "files_changed": ["test/cpp/tensorexpr/test_train.cpp", "test/cpp/tensorexpr/test_train.h", "test/cpp/tensorexpr/test_train_impl.cpp", "test/cpp/tensorexpr/tests.h"], "labels": ["merged"]}, "f03f9ad621": {"title": "update clone doc (#42931)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42931\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23083000\n\nPulled By: albanD\n\nfbshipit-source-id: d76d90476ca294763f204c185a62ff6484381c67", "pr_number": "42931", "files_changed": ["torch/_tensor_docs.py"], "labels": ["merged"]}, "6fb5ce5569": {"title": "[NNC] Fix some bugs in Round+Mod simplification (#42934)", "body": "Summary:\nWhen working on the Cuda Codegen, I found that running the IRSimplifier before generating code lead to test fails. This was due to a bug in Round+Mod simplification (e.g. (x / y * y) + (x % y) => x) to do with the order in which the terms appeared. After fixing it and writing a few tests around those cases, I found another bug in simplification of the same pattern and have fixed it (with some more test coverage).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42934\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23085548\n\nPulled By: nickgg\n\nfbshipit-source-id: e780967dcaa7a5fda9f6d7d19a6b7e7b4e94374b", "pr_number": "42934", "files_changed": ["test/cpp/tensorexpr/test_simplify.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp"], "labels": ["merged", "oncall: jit"]}, "ebc7ebc74e": {"title": "Do not ignore `torch/__init__.pyi` (#42958)", "body": "Summary:\nDelete abovementioned from .gitignore as the file is gone since https://github.com/pytorch/pytorch/issues/42908 and no longer should be autogenerated.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42958\n\nReviewed By: seemethere\n\nDifferential Revision: D23094391\n\nPulled By: malfet\n\nfbshipit-source-id: af303477301ae89d6f283e34d7aeddeda7a9260f", "pr_number": "42958", "files_changed": [".gitignore"], "labels": ["merged"]}, "0ff51accd8": {"title": "collect_env.py: Print CPU architecture after Linux OS name (#42961)", "body": "Summary:\nMissed this case in https://github.com/pytorch/pytorch/pull/42887\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42961\n\nReviewed By: zou3519\n\nDifferential Revision: D23095264\n\nPulled By: malfet\n\nfbshipit-source-id: ff1fb0eba9ecd29bfa3d8f5e4c3dcbcb11deefcb", "pr_number": "42961", "files_changed": ["torch/utils/collect_env.py"], "labels": ["merged"]}, "6f8446840e": {"title": "[quant] Create PerRowQuantizer for floating point scale and zero_point (#42612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42612\n\nAdd a new Quantizer that supports an input zero point (bias) that can be float.\nThe quantization equation in this case is\n\nXq = (Xf - bias) * inv_scale, where bias is float zero_point value\nWe start with per-row implementation and can extend to per-tensor in the future, if necessary\n\nTest Plan:\npython test/test_quantization.py TestQuantizedTensor\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22960142\n\nfbshipit-source-id: ca9ab6c5b45115d3dcb1c4358897093594313706", "pr_number": "42612", "files_changed": ["aten/src/ATen/core/Formatting.cpp", "aten/src/ATen/native/quantized/QTensor.cpp", "aten/src/ATen/native/quantized/TensorFactories.cpp", "aten/src/ATen/native/quantized/affine_quantizer.cpp", "aten/src/ATen/native/quantized/affine_quantizer.h", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "aten/src/ATen/quantized/Quantizer.h", "c10/core/QScheme.h", "test/quantization/test_quantized_tensor.py", "torch/_tensor_str.py"], "labels": ["merged"]}, "816d37b1d8": {"title": "[quant] Make PerChannel Observer work with float qparams (#42690)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42690\n\nAdd implementation for new qscheme per_channel_affine_float_qparams in observer\n\nTest Plan:\npython test/test_quantization.py TestObserver.test_per_channel_observers\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23070633\n\nfbshipit-source-id: 84d348b0ad91e9214770131a72f7adfd3970349c", "pr_number": "42690", "files_changed": ["test/quantization/test_workflow_module.py", "torch/quantization/observer.py"], "labels": ["merged"]}, "fd5ed4b6d6": {"title": "Update ort-nightly version to dev202008122 (#43019)", "body": "Summary:\nFixes caffe2_onnx_ort1_py3_6_clang7_ubuntu16_04 test failures\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43019\n\nReviewed By: gchanan\n\nDifferential Revision: D23108767\n\nPulled By: malfet\n\nfbshipit-source-id: 0131cf4ac0bf93d3d93cb0c97a888f1524e87472", "pr_number": "43019", "files_changed": [".jenkins/caffe2/test.sh"], "labels": ["merged", "triaged"]}, "eb47940c0a": {"title": "Add executor and fuser options to the fastrnn test fixture (#42946)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42946\n\nThere are 3 options for the executor and fuser and some of them aren't\nsuper interesting so I've combined the options into a single parameter, but\nmade it fairly easy to expand the set if there are other configs we might care\nabout.\n\nTest Plan:\nBenchmark it\n\nImported from OSS\n\nReviewed By: zheng-xq\n\nDifferential Revision: D23090177\n\nfbshipit-source-id: bd93a93c3fc64e5a4a847d1ce7f42ce0600a586e", "pr_number": "42946", "files_changed": ["benchmarks/fastrnns/test_bench.py", "benchmarks/upload_scribe.py"], "labels": ["merged"]}, "6753157c5a": {"title": "Enable torch.utils typechecks (#42960)", "body": "Summary:\nFix typos in torch.utils/_benchmark/README.md\nAdd empty __init__.py to examples folder to make example invocations from README.md correct\nFixed uniform distribution logic generation when mixval and maxval are None\n\nFixes https://github.com/pytorch/pytorch/issues/42984\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42960\n\nReviewed By: seemethere\n\nDifferential Revision: D23095399\n\nPulled By: malfet\n\nfbshipit-source-id: 0546ce7299b157d9a1f8634340024b10c4b7e7de", "pr_number": "42960", "files_changed": ["mypy.ini", "torch/utils/_benchmark/README.md", "torch/utils/_benchmark/examples/__init__.py", "torch/utils/_benchmark/examples/end_to_end.py", "torch/utils/_benchmark/examples/op_benchmark.py", "torch/utils/_benchmark/utils/fuzzer.py", "torch/utils/checkpoint.py", "torch/utils/cpp_extension.py"], "labels": ["merged", "module: typing", "triaged"]}, "8b5642a786": {"title": "Fix to Learnable Fake Quantization Op Benchmarking (#43018)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43018\n\nIn this diff, a fix is added where the original non-learnable fake quantize is provided with trainable scale and zero point, whereas the requires_grad for both parameters should be completely disabled.\n\nTest Plan:\nUse the following command to execute the benchmark test:\n\n`buck test mode/dev-nosan pt:quantization_test`\n\nReviewed By: vkuzo\n\nDifferential Revision: D23107846\n\nfbshipit-source-id: d2213983295f69121e9e6ae37c84d1f37d78ef39", "pr_number": "43018", "files_changed": ["benchmarks/operator_benchmark/pt/quantization_test.py"], "labels": ["fb-exported", "merged"]}, "3544f60f76": {"title": "make deadline=None for all numerics tests (#43014)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43014\n\nchanging this behavior mimics the behavior of the hold hypothesis\ntesting library\n\nTest Plan: ran all tests on devserver\n\nReviewed By: hl475\n\nDifferential Revision: D23085949\n\nfbshipit-source-id: 433fdfbb04b6a609b738eb7c319365049a49579b", "pr_number": "43014", "files_changed": ["caffe2/contrib/fakelowp/test/test_batchmatmul_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp32.py"], "labels": ["fb-exported", "merged"]}, "a6b69fdd33": {"title": "Add DDP+RPC tutorial to RPC docs page. (#42828)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42828\n\nghstack-source-id: 109855425\n\nTest Plan: waitforbuildbot\n\nReviewed By: jlin27\n\nDifferential Revision: D23037016\n\nfbshipit-source-id: 250f322b652b86257839943309b8f0b8ce1bb25b", "pr_number": "42828", "files_changed": ["docs/source/rpc.rst"], "labels": ["merged"]}, "21823aa680": {"title": "Nightly checkout tool (#42635)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/40829\n\nThis is cross-platform but I have only tried it on linux, personally. Also, I am not fully certain of the usage pattern, so if there are any additional features / adjustments / tests that you want me to add, please just let me know!\n\nCC ezyang rgommers\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42635\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23078663\n\nPulled By: ezyang\n\nfbshipit-source-id: 5c8c8abebd1d462409c22dc4301afcd8080922bb", "pr_number": "42635", "files_changed": ["CONTRIBUTING.md", "tools/nightly_checkout.py"], "labels": ["merged", "open source", "triaged"]}, "89b0b3bc8c": {"title": "Allow RPC to be initialized again after shutdown. (#42723)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42723\n\nThis PR is addressing https://github.com/pytorch/pytorch/issues/39340\nand allows users to initialize RPC again after shutdown. Major changes in the\nPR include:\n\n1. Change to DistAutogradContainer to support this.\n2. Ensure PythonRpcHandler is reinitialized appropriately.\n3. Use PrefixStore in RPC initialization to ensure each new `init_rpc` uses a\ndifferent prefix.\nghstack-source-id: 109805368\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D22993909\n\nfbshipit-source-id: 9f1c1e0a58b58b97125f41090601e967f96f70c6", "pr_number": "42723", "files_changed": ["torch/csrc/distributed/autograd/context/container.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/distributed/rpc/__init__.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "523b2ce9c6": {"title": "[jit][static runtime] Simplify the graph and add operator whitelist (#43024)", "body": "Summary:\nThis PR whitelists and simplifies graphs to help with development later on.  Key to note in this PR is the use of both a pattern substitution and the registration of custom operators.  This will likely be one of the main optimization types done in this folder.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43024\n\nReviewed By: hlu1\n\nDifferential Revision: D23114262\n\nPulled By: bwasti\n\nfbshipit-source-id: e25aa3564dcc8a2b48cfd1561b3ee2a4780ae462", "pr_number": "43024", "files_changed": ["test/test_static_runtime.py", "torch/csrc/jit/runtime/static/impl.cpp"], "labels": ["merged", "oncall: jit"]}, "85752b989d": {"title": "[quant][doc] Print more info for fake quantize module (#43031)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43031\n\nfixes: https://github.com/pytorch/pytorch/issues/43023\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23116200\n\nfbshipit-source-id: faa90ce8711da0785d635aacd0362c45717cfacc", "pr_number": "43031", "files_changed": ["torch/quantization/fake_quantize.py"], "labels": ["merged"]}, "830423b80b": {"title": "Python/C++ API Parity: TransformerDecoderLayer (#42717)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/37756\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42717\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23095841\n\nPulled By: glaringlee\n\nfbshipit-source-id: 327a5a23c9a3cca05e422666a6d7d802a7e8c468", "pr_number": "42717", "files_changed": ["test/cpp/api/transformer.cpp", "torch/csrc/api/include/torch/nn/modules/transformer.h", "torch/csrc/api/include/torch/nn/options/transformer.h", "torch/csrc/api/src/nn/modules/transformer.cpp", "torch/csrc/api/src/nn/options/transformer.cpp"], "labels": ["merged", "open source", "triaged"]}, "8cf01c5c35": {"title": "Back out \"change pt_defs.bzl to python file\"", "body": "Summary: Original commit changeset: d720fe2e684d\n\nTest Plan: CIs\n\nReviewed By: linbinyu\n\nDifferential Revision: D23114839\n\nfbshipit-source-id: fda570b5e989a51936a6c5bc68f0e60c6f6b4b82", "pr_number": null, "files_changed": ["pt_deps.py"], "labels": []}, "a55b7e2a6d": {"title": "[reland][quant][fix] Remove activation_post_process in qat modules (#42343) (#43015)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43015\n\nCurrently activation_post_process are inserted by default in qat modules, which is not\nfriendly to automatic quantization tools, this PR removes them.\n\nTest Plan:\nImported from OSS\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23105059\n\nfbshipit-source-id: 3439ac39e718ffb0390468163bcbffd384802b57", "pr_number": "43015", "files_changed": ["test/quantization/test_quantize.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/intrinsic/qat/modules/linear_relu.py", "torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "b992a927a9": {"title": "Clearer Semantics and Naming for Customized Quantization Range Initialization in Observer (#42602)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42602\n\nIn this diff, clearer semantics and namings for are introduced by splitting the original `init_dynamic_qrange` into 2 separate `Optional[int]` types `qmin` and `qmax` to avoid the confusion of the parameters with dynamic quantization.\n\nThe `qmin` and `qmax` parameters allow customers to specify their own customary quantization range and enables specific use cases for lower bit quantization.\n\nTest Plan:\nTo assert the correctness and compatibility of the changes with existing observers, on a devvm, execute the following command to run the unit tests:\n\n`buck test //caffe2/test:quantization -- observer`\n\nReviewed By: vkuzo, raghuramank100\n\nDifferential Revision: D22948334\n\nfbshipit-source-id: 275bc8c9b5db4ba76fc2e79ed938376ea4f5a37c", "pr_number": "42602", "files_changed": ["torch/quantization/observer.py"], "labels": ["fb-exported", "merged"]}, "3dc845319f": {"title": "Add more verbose error message about PackedSequence lengths argument (#42891)", "body": "Summary:\nAdd given tensor dimentionality, device and dtype to the error message\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42891\n\nReviewed By: ezyang\n\nDifferential Revision: D23068769\n\nPulled By: malfet\n\nfbshipit-source-id: e49d0a5d0c10918795c1770b4f4e02494d799c51", "pr_number": "42891", "files_changed": ["aten/src/ATen/native/PackedSequence.cpp"], "labels": ["merged"]}, "02c8ad70f2": {"title": "Reconstruct scopes (#41615)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41615\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22611331\n\nPulled By: taivu1998\n\nfbshipit-source-id: d4ed4cf6360bc1f72ac9fa24bb4fcf6b7d9e7576", "pr_number": "41615", "files_changed": ["test/test_jit.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/reconstruct_scopes.cpp", "torch/csrc/jit/passes/reconstruct_scopes.h", "torch/csrc/jit/python/init.cpp"], "labels": ["merged", "oncall: jit"]}, "48c183af3d": {"title": "[TensorExpr] Wrap fuser in a class. (#42936)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42936\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23084407\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: f622874efbcbf8d4e49c8fa519a066161ebe4877", "pr_number": "42936", "files_changed": ["torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["merged", "oncall: jit"]}, "fc304bec9f": {"title": "[TensorExpr] Remove redundant checks from canHandle in TE fuser. (#42937)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42937\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23084408\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 8e562e25ecc73b4e7b01e30f8b282945b96b4871", "pr_number": "42937", "files_changed": ["torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["merged", "oncall: jit"]}, "b9a105bcc0": {"title": "[TensorExpr] Cleanup logic in the TensorExpr fuser pass. (#42938)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42938\n\n1. Structure the logic in a more straight-forward way: instead of magic\n   tricks with node iterators in a block we now have a function that\n   tries to create a fusion group starting from a given node (and pull\n   everything it can into it).\n2. The order in which we're pulling nodes into a fusion group is now\n   more apparent.\n3. The new pass structure automatically allows us to support fusion\n   groups of size=1.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23084409\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: d59fc00c06af39a8e1345a4aed8d829494db084c", "pr_number": "42938", "files_changed": ["torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["merged", "oncall: jit"]}, "e4373083a2": {"title": "torch.complex and torch.polar (#39617)", "body": "Summary:\nFor https://github.com/pytorch/pytorch/issues/35312 and https://github.com/pytorch/pytorch/issues/38458#issuecomment-636066256.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39617\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23083926\n\nPulled By: anjali411\n\nfbshipit-source-id: 1874378001efe2ff286096eaf1e92afe91c55b29", "pr_number": "39617", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorFactories.h", "aten/src/ATen/native/cpu/ComplexKernel.cpp", "aten/src/ATen/native/cuda/ComplexKernel.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["merged", "open source", "triaged"]}, "b8102b1550": {"title": "Implement torch.nextafter (#42580)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/38349.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42580\n\nReviewed By: smessmer\n\nDifferential Revision: D23012260\n\nPulled By: mruberry\n\nfbshipit-source-id: ce82a63c4ad407ec6ffea795f575ca7c58cd6137", "pr_number": "42580", "files_changed": ["aten/src/ATen/core/NamedRegistrations.cpp", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_complex_float.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vec256_float_neon.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "c10/util/math_compat.h", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["merged", "module: numpy", "open source", "triaged"]}, "e182ec97b3": {"title": "Fix illegal memory acess issue for CUDA versionn of SplitByLengths operator.", "body": "Summary:\n1. Fix illegal memory access issue for SplitByLengths operator in the CUDA context.\n2. Add support to scaling lengths vector for SplitByLengths operator.\n3. Add support to test SplitByLengths operator in the CUDA context.\n\nExample for SplitByLengths operator processing scaling lengths vector:\nvalue vector A = [1, 2, 3, 4, 5, 6]\nlength vector B = [1, 2]\nafter execution of SplitByLengths operator,\nthe output should be [1,2] and [3,4,5,6]\n\nTest Plan: buck test mode/dev-nosan caffe2/caffe2/python/operator_test:concat_split_op_test\n\nReviewed By: kennyhorror\n\nDifferential Revision: D23079841\n\nfbshipit-source-id: 3700e7f2ee0a5a2791850071fdc16e5b054f8400", "pr_number": null, "files_changed": ["caffe2/operators/concat_split_op.cc", "caffe2/operators/concat_split_op.h", "caffe2/python/operator_test/concat_split_op_test.py"], "labels": []}, "ccd9f3244b": {"title": "Get, save, and load module information for each operator (#42133)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42133\n\nTest Plan:\nWe save a module with module debugging information as follows.\n```\nimport torch\nm = torch.jit.load('./detect.pt')\n# Save module without debug info\nm._save_for_lite_interpreter('./detect.bc')\n# Save module with debug info\nm._save_for_lite_interpreter('./detect.bc', _save_debug_info_in_bytecode=True)\n```\nSize of the file without module debugging information: 4.508 MB\nSize of the file with module debugging information: 4.512 MB\n\nReviewed By: kimishpatel\n\nDifferential Revision: D22803740\n\nPulled By: taivu1998\n\nfbshipit-source-id: c82ea62498fde36a1cfc5b073e2cea510d3b7edb", "pr_number": "42133", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "test/mobile/test_lite_script_module.py", "torch/csrc/jit/api/module.h", "torch/csrc/jit/api/module_save.cpp", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/passes/reconstruct_scopes.cpp", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/serialization/export.h", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/import_export_constants.h"], "labels": ["merged", "oncall: jit"]}, "ed242cbec5": {"title": "Guard TensorPipe agent by USE_TENSORPIPE (#42682)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42682\n\nghstack-source-id: 109834351\n\nTest Plan: CI\n\nReviewed By: malfet\n\nDifferential Revision: D22978717\n\nfbshipit-source-id: 18b7cbdb532e78ff9259e82f0f92ad279124419d", "pr_number": "42682", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/rpc/CMakeLists.txt", "test/cpp/rpc/test_e2e_tensorpipe.cpp", "torch/CMakeLists.txt", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["merged"]}, "c7d2774d20": {"title": "Fix typo in collect_env.py (#43050)", "body": "Summary:\nMinor typo fix introduced in yesterdays PR: https://github.com/pytorch/pytorch/pull/42961\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43050\n\nReviewed By: ezyang, malfet\n\nDifferential Revision: D23130936\n\nPulled By: zou3519\n\nfbshipit-source-id: e8fa2bf155ab6a5988c74e8345278d8d70855894", "pr_number": "43050", "files_changed": ["torch/utils/collect_env.py"], "labels": ["merged", "open source"]}, "a2b86d95d1": {"title": "Make Mish support large inputs. (#43037)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43037\n\nIn the previous version of mish_op.cc, the output would be 'nan' for large inputs. We re-write mish_op.cc to solve this problem.\n\nTest Plan:\nUnit test\nbuck test //dper3/dper3/modules/tests:core_modules_test -- test_linear_compress_embedding_with_attention_with_activation_mish\n{F284052906}\n\nbuck test mode/opt //dper3/dper3_models/ads_ranking/tests:model_paradigm_e2e_tests -- test_sparse_nn_with_mish\n{F284224158}\n\n## Workflow\nf212113434\n\n{F285281318}\n\nDifferential Revision: D23102644\n\nfbshipit-source-id: 98f1ea82f8c8e05b655047b4520c600fc1a826f4", "pr_number": "43037", "files_changed": ["caffe2/operators/mish_op.cc"], "labels": ["fb-exported", "merged"]}, "31788ae151": {"title": "Trim trailing whitespace", "body": "Test Plan: CI\n\nReviewed By: linbinyu\n\nDifferential Revision: D23108919\n\nfbshipit-source-id: 913c982351a94080944f350641d7966c6c2cc508", "pr_number": null, "files_changed": ["torch/nn/functional.py"], "labels": []}, "2f9fd8ad29": {"title": "Build test_e2e_tensorpipe only if Gloo is enabled (#43041)", "body": "Summary:\ntest_e2e_tensorpipe depends on ProcessGroupGloo, therefore it could not be tested with Gloo disabled\nOtherwise, it re-introduces  https://github.com/pytorch/pytorch/issues/42776\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43041\n\nReviewed By: lw\n\nDifferential Revision: D23122101\n\nPulled By: malfet\n\nfbshipit-source-id: a8a088b6522a3bc888238ede5c2d589b83c6ea94", "pr_number": "43041", "files_changed": ["test/cpp/rpc/CMakeLists.txt"], "labels": ["merged", "module: build", "triaged"]}, "ff6a2b0b7a": {"title": "Add inplace option for torch.nn.Hardsigmoid and torch.nn.Hardswish layers (#42346)", "body": "Summary:\n**`torch.nn.Hardsigmoid`** and **`torch.nn.Hardswish`** classes currently do not support `inplace` operations as it uses `torch.nn.functional.hardsigmoid` and `torch.nn.functional.hardswish` functions with their default inplace argument which is `False`.\n\nSo, I added `inplace` argument for `torch.nn.Hardsigmoid` and `torch.nn.Hardswish` classes so that forward operation can be done inplace as well while using these layers.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42346\n\nReviewed By: izdeby\n\nDifferential Revision: D23108487\n\nPulled By: albanD\n\nfbshipit-source-id: 0767334fa10e5ecc06fada2d6469f3ee1cacd957", "pr_number": "42346", "files_changed": ["torch/nn/modules/activation.py"], "labels": ["merged", "module: nn", "open source", "triaged"]}, "c3fb152274": {"title": "Test the type promotion between every two dtypes thoroughly (#42585)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41842\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42585\n\nReviewed By: izdeby\n\nDifferential Revision: D23126759\n\nPulled By: mruberry\n\nfbshipit-source-id: 8337e02f23a4136c2ba28c368f8bdbd28400de44", "pr_number": "42585", "files_changed": ["test/test_type_promotion.py"], "labels": ["merged", "open source", "triaged"]}, "1c616c5ab7": {"title": "Add complex tensor dtypes for the __cuda_array_interface__ spec (#42918)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42860\n\nThe `__cuda_array_interface__` tensor specification is missing the appropriate datatypes for the newly merged complex64 and complex128 tensors. This PR addresses this issue by casting:\n\n* `torch.complex64` to 'c8'\n* `torch.complex128` to 'c16'\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42918\n\nReviewed By: izdeby\n\nDifferential Revision: D23130219\n\nPulled By: anjali411\n\nfbshipit-source-id: 5f8ee8446a71cad2f28811afdeae3a263a31ad11", "pr_number": "42918", "files_changed": ["test/test_numba_integration.py", "torch/tensor.py"], "labels": ["merged", "module: complex", "open source", "triaged"]}, "75dfa5a459": {"title": "Remove `itruediv` because it's already defined in torch/tensor.py (#42962)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42955\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42962\n\nReviewed By: mruberry\n\nDifferential Revision: D23111523\n\nPulled By: malfet\n\nfbshipit-source-id: ecab7a4aae1fe556753b8d6528cae1ae201beff3", "pr_number": "42962", "files_changed": ["tools/pyi/gen_pyi.py"], "labels": ["merged", "module: typing", "open source", "triaged"]}, "71bbd5f1d4": {"title": "Add back Tensor.nonzero type annotation (#43053)", "body": "Summary:\nCloses gh-42998\n\nThe issue is marked for 1.6.1, if there's anything I need to do for a backport please tell me what that is.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43053\n\nReviewed By: izdeby\n\nDifferential Revision: D23131708\n\nPulled By: malfet\n\nfbshipit-source-id: 2744bacce6bdf6ae463c17411b672f09707e0887", "pr_number": "43053", "files_changed": ["tools/pyi/gen_pyi.py"], "labels": ["merged", "module: typing", "open source"]}, "059aa34b12": {"title": "Clip Binomial results for different endpoints in curand_uniform (#42702)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42153\n\nAs [documented](https://docs.nvidia.com/cuda/curand/device-api-overview.html) (search for `curand_uniform` on the page), `curand_uniform` returns \"from 0.0 to 1.0, where 1.0 is included and 0.0 is excluded.\" These endpoints are different than the CPU equivalent, and makes the calculation in the PR fail when the value is 1.0.\n\nThe test from the issue is added, it failed for me consistently before the PR even though I cut the number of samples by 10.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42702\n\nReviewed By: gchanan\n\nDifferential Revision: D23107451\n\nPulled By: ngimel\n\nfbshipit-source-id: 3575d5b8cd5668e74b5edbecd95154b51aa485a1", "pr_number": "42702", "files_changed": ["aten/src/ATen/native/cuda/Distributions.cu", "test/test_distributions.py"], "labels": ["merged", "open source", "triaged"]}, "1f6d0985d7": {"title": "fix searchsorted output type (#42933)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41389\nMake sure searchsorted that returns integer type does not make them require gradients.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42933\n\nReviewed By: gchanan\n\nDifferential Revision: D23109583\n\nPulled By: albanD\n\nfbshipit-source-id: 5af300b2f7f3c140d39fd7f7d87799f7b93a79c1", "pr_number": "42933", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_variable_type.py"], "labels": ["merged"]}, "64a7684219": {"title": "Enable typechecking of `collect_env.py` during CI (#43062)", "body": "Summary:\nNo type annotations can be added to the script, as it still have to be Python-2 compliant.\n Make changes to avoid variable type redefinition.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43062\n\nReviewed By: zou3519\n\nDifferential Revision: D23132991\n\nPulled By: malfet\n\nfbshipit-source-id: 360c02e564398f555273e5889a99f834a5467059", "pr_number": "43062", "files_changed": ["mypy.ini", "torch/utils/collect_env.py"], "labels": ["merged", "module: typing", "triaged"]}, "fcc10d75e1": {"title": "[JIT] Add property support to TorchScript classes (#42389)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42389\n\n**Summary**\nThis commit adds support for properties to TorchScript classes,\nspecifically for getters and setters. They are implemented essentially\nas pointers to the methods that the corresponding decorators decorate,\nwhich are treated like regular class methods. Deleters for properties\nare considered to be out of scope (and probably useless for TorchScript\nanyway).\n\n**Test Plan**\nThis commit adds a unit test for a class with a property that has both\ngetter and setter and one that has only a getter.\n\n`python test/test_jit.py TestClassType.test_properties`\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison, ppwwyyxx\n\nDifferential Revision: D22880232\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 4828640f4234cb3b0d4f3da4872a75fbf519e5b0", "pr_number": "42389", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/jit/test_class_type.py", "torch/csrc/jit/api/compilation_unit.h", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/lexer.h", "torch/csrc/jit/frontend/script_type_parser.cpp", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/tree_views.h", "torch/csrc/jit/python/python_tree_views.cpp", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/serialization/import_source.cpp", "torch/jit/frontend.py"], "labels": ["merged", "oncall: jit"]}, "1c6ace87d1": {"title": "Embed torch.nn typing annotations (#43044)", "body": "Summary:\nDelete several .pyi files and embed annotations from those files in respective .py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43044\n\nReviewed By: ezyang\n\nDifferential Revision: D23123234\n\nPulled By: malfet\n\nfbshipit-source-id: 4ba361cc84402352090523924b0035e100ba48b1", "pr_number": "43044", "files_changed": [".gitignore", "torch/nn/__init__.pyi", "torch/nn/utils/__init__.pyi", "torch/nn/utils/clip_grad.py", "torch/nn/utils/clip_grad.pyi", "torch/nn/utils/convert_parameters.py", "torch/nn/utils/convert_parameters.pyi", "torch/nn/utils/spectral_norm.py", "torch/nn/utils/spectral_norm.pyi", "torch/nn/utils/weight_norm.py", "torch/nn/utils/weight_norm.pyi"], "labels": ["merged", "module: typing", "triaged"]}, "c8e789e06e": {"title": "add fake fp16 fusions to net transforms (#42927)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42927\n\nadded fp16 fusion to net transforms\nrefactored the transforms as well as glow_transform to get out of opt/custom so that the OSS builds passed\n\nTest Plan: added net runner tests for this\n\nReviewed By: yinghai\n\nDifferential Revision: D23080881\n\nfbshipit-source-id: ee6451811fedfd07c6560c178229854bca29301f", "pr_number": "42927", "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc", "caffe2/opt/custom/fakefp16_transform.h", "caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/custom/glow_net_transform.h", "caffe2/opt/fakefp16_transform.cc", "caffe2/opt/fakefp16_transform.h", "caffe2/opt/glow_net_transform.cc", "caffe2/opt/glow_net_transform.h", "caffe2/python/fakefp16_transform_lib.py", "caffe2/python/pybind_state.cc", "caffe2/python/test/fakefp16_transform_test.py", "caffe2/quantization/server/pybind.cc"], "labels": ["fb-exported", "merged"]}, "5014cf4a4d": {"title": "Export MergeIdLists Caffe2 Operator to PyTorch", "body": "Summary: As titled.\n\nTest Plan: buck test //caffe2/caffe2/python/operator_test:torch_integration_test -- test_merge_id_lists\n\nReviewed By: yf225\n\nDifferential Revision: D23076951\n\nfbshipit-source-id: c37dfd93003590eed70b0d46e0151397a402dde6", "pr_number": null, "files_changed": ["caffe2/operators/merge_id_lists_op.cc", "caffe2/operators/merge_id_lists_op.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": []}, "33c5fe3c1d": {"title": "Enable test_logit FakeLowP test. (#43073)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43073\n\nEnable test_logit FakeLowP test.\n\nTest Plan: test_op_nnpi_fp16.py\n\nReviewed By: hyuen\n\nDifferential Revision: D23141375\n\nfbshipit-source-id: cb7e7879487e33908b14ef401e1ab05fda193d28", "pr_number": "43073", "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py"], "labels": ["fb-exported", "merged"]}, "3d8c144400": {"title": "Implemented torch::nn::Unflatten in libtorch (#42613)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42613\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D23030302\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 954f1cdfcbd3a62a7f0e887fcf5995ef27222a87", "pr_number": "42613", "files_changed": ["test/cpp/api/modules.cpp", "torch/csrc/api/include/torch/nn/modules/linear.h", "torch/csrc/api/include/torch/nn/options/linear.h", "torch/csrc/api/src/nn/modules/linear.cpp", "torch/csrc/api/src/nn/options/linear.cpp"], "labels": ["merged"]}, "450315198a": {"title": "Fix a casting warning (#42451)", "body": "Summary:\nFix an annoying casting warning\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42451\n\nReviewed By: yf225\n\nDifferential Revision: D22993194\n\nPulled By: ailzhang\n\nfbshipit-source-id: f317a212d4e768d49d24f50aeff9c003be2fd30a", "pr_number": "42451", "files_changed": ["torch/csrc/api/include/torch/nn/parallel/data_parallel.h"], "labels": ["merged", "open source"]}, "7632a9b090": {"title": "[quant] Add embeddingbag_prepack function that works on quantized tensor. (#42762)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42762\n\nUse a prepack function that accepts qtensor as an input. The output is a byte tensor with packed data.\nThis is currently implemented only for 8-bit. In the future once we add 4-bit support this function will be extended to support that too.\n\nNote -In the following change I will add TorchBind support for this to support serialization of packed weights.\n\nTest Plan:\npython test/test_quantization.py TestQuantizedEmbeddingBag\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23070632\n\nfbshipit-source-id: 502aa1302dffec1298cdf52832c9e2e5b69e44a8", "pr_number": "42762", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py"], "labels": ["merged"]}, "66b3382c5b": {"title": "[quant] Add torchbind support for embedding_bag packed weights (#42881)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42881\n\nThis enables serialization/de-serialization of embedding packed params using getstate/setstate calls.\nAdded version number to deal with changes to serialization formats in future.\n\nThis can be extended in the future to support 4-bit/2-bit once we add support for that.\n\nTest Plan:\npython test/test_quantization.py TestQuantizedEmbeddingBag\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23070634\n\nfbshipit-source-id: 2ca322ab998184c728be6836f9fd12cec98b2660", "pr_number": "42881", "files_changed": ["aten/src/ATen/native/quantized/cpu/embedding_packed_params.h", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.h", "aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py"], "labels": ["merged"]}, "a1a6e1bc91": {"title": "Fix warning: dynamic initialization in unreachable code. (#43065)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43065\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D23136883\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 878f6af13ff8df63fef5f34228f7667ee452dd95", "pr_number": "43065", "files_changed": ["aten/src/ATen/core/builtin_function.h"], "labels": ["merged"]}, "4011685a8b": {"title": "[fx] split Node into Node/Proxy (#42991)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42991\n\nHave Node both be a record of the operator in the graph, and the\nway we _build_ the graph made it difficult to keep the IR datastructure\nseparate from the proxying logic in the build.\n\nAmong other issues this means that typos when using nodes would add\nthings to the graph:\n```\n    for node in graph.nodes:\n        node.grph # does not error, returns an node.Attribute object!\n```\n\nThis separates the builder into a Proxy object. Graph/Node no longer\nneed to understand `delegate` objects since they are now just pure IR.\nThis separates the `symbolic_trace` (proxy.py/symbolic_trace.py) from\nthe IR (node.py, graph.py).\n\nThis also allows us to add `create_arg` to the delegate object,\nallowing the customization of how aggregate arguments are handled\nwhen converting to a graph.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23099786\n\nPulled By: zdevito\n\nfbshipit-source-id: 6f207a8c237e5eb2f326b63b0d702c3ebcb254e4", "pr_number": "42991", "files_changed": ["test/test_fx.py", "torch/fx/__init__.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/node.py", "torch/fx/proxy.py", "torch/fx/symbolic_trace.py"], "labels": ["merged"]}, "91b090ceaf": {"title": "Add polygamma where n >= 2 (#42499)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/40980\n\nI have a few questions during implementing Polygamma function...\nso, I made PR prior to complete it.\n\n1. some code blocks brought from cephes library(and I did too)\n```\n/*\n * The following function comes with the following copyright notice.\n * It has been released under the BSD license.\n *\n * Cephes Math Library Release 2.8:  June, 2000\n * Copyright 1984, 1987, 1992, 2000 by Stephen L. Moshier\n */\n```\nis it okay for me to use cephes code with this same copyright notice(already in the Pytorch codebases)\n\n2. There is no linting in internal Aten library. (as far as I know, I read https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md)\nHow do I'm sure my code will follow appropriate guidelines of this library..?\n\n3. Actually, there's a digamma, trigamma function already\ndigamma is needed, however, trigamma function becomes redundant if  polygamma function is added.\nit is okay for trigamma to be there or should be removed?\n\nbtw, CPU version works fine with 3-rd order polygamma(it's what we need to play with variational inference with beta/gamma distribution) now and I'm going to finish GPU version soon.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42499\n\nReviewed By: gchanan\n\nDifferential Revision: D23110016\n\nPulled By: albanD\n\nfbshipit-source-id: 246f4c2b755a99d9e18a15fcd1a24e3df5e0b53e", "pr_number": "42499", "files_changed": ["aten/src/ATen/native/Math.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/UnaryGammaKernels.cu", "test/test_torch.py", "torch/_torch_docs.py"], "labels": ["merged", "open source", "triaged"]}, "0cf4a5bccb": {"title": "Add GCC codecoverage flags (#43066)", "body": "Summary:\nRename `CLANG_CODE_COVERAGE` option to `CODE_COVERAGE` and add compiler specific flags for GCC and Clang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43066\n\nReviewed By: scintiller\n\nDifferential Revision: D23137488\n\nPulled By: malfet\n\nfbshipit-source-id: a89570469692f878d84f7da6f9d5dc01df423e80", "pr_number": "43066", "files_changed": ["CMakeLists.txt", "cmake/Summary.cmake"], "labels": ["merged"]}, "bcf54f9438": {"title": "Stop treating ASAN as special case (#43048)", "body": "Summary:\nAdd \"asan\" node to a `CONFIG_TREE_DATA` rather than hardcoded that non-xla clang-5 is ASAN\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43048\n\nReviewed By: houseroad\n\nDifferential Revision: D23126296\n\nPulled By: malfet\n\nfbshipit-source-id: 22f02067bb2f5435a0e963a6c722b9c115ccfea4", "pr_number": "43048", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py"], "labels": ["merged"]}, "c84f78470b": {"title": "Fix type annotations for a number of torch.utils submodules (#42711)", "body": "Summary:\nRelated issue on `torch.utils` type annotation hiccups: gh-41794\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42711\n\nReviewed By: mrshenli\n\nDifferential Revision: D23005434\n\nPulled By: malfet\n\nfbshipit-source-id: 151554b1e7582743f032476aeccdfdad7a252095", "pr_number": "42711", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in", "torch/types.py", "torch/utils/__init__.pyi", "torch/utils/bottleneck/__main__.py", "torch/utils/tensorboard/_onnx_graph.py", "torch/utils/tensorboard/_pytorch_graph.py", "torch/utils/tensorboard/summary.py", "torch/utils/tensorboard/writer.py"], "labels": ["merged", "module: typing", "open source", "triaged"]}, "06aaf8c20d": {"title": "Add set_device_map to TensorPipeOptions to support GPU args (#42637)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42637\n\nThis commit enables sending non-CPU tensors through RPC using\nTensorPipe backend. Users can configure device mappings by calling\nset_map_location on `TensorPipeRpcBackendOptions`. Internally,\nthe `init_rpc` API verifies the correctness of device mappings. It\nwill shutdown RPC if the check failed, or proceed and pass global\nmappings to `TensorPipeAgent` if the check was successful. For serde,\nwe added a device indices field to TensorPipe read and write buffers,\nwhich should be either empty (all tensors must be on CPU) or match\nthe tensors in order and number in the RPC message. This commit\ndoes not yet avoid zero-copy, the tensor is always moved to CPU\non the sender and then moved to the specified device on the receiver.\n\nTest Plan: Imported from OSS\n\nReviewed By: izdeby\n\nDifferential Revision: D23011572\n\nPulled By: mrshenli\n\nfbshipit-source-id: 62b617eed91237d4e9926bc8551db78b822a1187", "pr_number": "42637", "files_changed": ["test/cpp/rpc/test_tensorpipe_serialization.cpp", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py", "torch/distributed/rpc/backend_registry.py", "torch/distributed/rpc/options.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "19902f6c0e": {"title": "Document unavailable reduction ops with NCCL backend (#42822)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42822\n\nThese ops arent supported with NCCL backend and used to silently error.\nWe disabled them as part of addressing https://github.com/pytorch/pytorch/issues/41362, so\ndocument that here.\nghstack-source-id: 109957761\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D23023046\n\nfbshipit-source-id: 45d69028012e0b6590c827d54b35c66cd17e7270", "pr_number": "42822", "files_changed": ["torch/csrc/distributed/c10d/init.cpp"], "labels": ["merged"]}, "91f3114fc1": {"title": "[JIT] Represent profiled types as a node attribute (#43035)", "body": "Summary:\nThis changes profiled types from being represented as:\n`%23 : Float(4:256, 256:1, requires_grad=0, device=cpu) = prim::profile(%0)`\n->\n`%23 : Tensor = prim::profile[profiled_type=Float(4:256, 256:1, requires_grad=0, device=cpu)](%0)`\n\nPreviously, by representing the profiled type in the IR directly it was very easy for optimizations to accidentally use profiled types without inserting the proper guards that would ensure that the specialized type would be seen.\n\nIt would be a nice follow up to extend this to prim::Guard as well, however we have short term plans to get rid of prim::Guard.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43035\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23120226\n\nPulled By: eellison\n\nfbshipit-source-id: c78d7904edf314dd65d1a343f2c3a947cb721b32", "pr_number": "43035", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/cpp/jit/test_misc.cpp", "torch/csrc/jit/passes/insert_guards.cpp", "torch/csrc/jit/runtime/profiling_record.cpp"], "labels": ["merged", "oncall: jit"]}, "8864148823": {"title": "[jit] DeepAndWide benchmark (#43096)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43096\n\nAdd benchmark script for deep and wide model.\n\nReviewed By: bwasti, yinghai\n\nDifferential Revision: D23099925\n\nfbshipit-source-id: aef09d8606eba1eccc0ed674dfea59b890d3648b", "pr_number": "43096", "files_changed": ["benchmarks/static_runtime/deep_wide_pt.h", "benchmarks/static_runtime/deep_wide_pt_bench.cc", "test/test_static_runtime.py", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/init.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "5bcf9b017a": {"title": "Implement hstack, vstack, dstack (#42799)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/38349\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42799\n\nReviewed By: izdeby\n\nDifferential Revision: D23140704\n\nPulled By: mruberry\n\nfbshipit-source-id: 6a36363562c50d0abce87021b84b194bb32825fb", "pr_number": "42799", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/torch.rst", "test/test_autograd.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["merged", "open source", "triaged"]}, "d4c5f561ec": {"title": "Updates torch.clone documentation to be consistent with other functions (#43098)", "body": "Summary:\n`torch.clone` exists but was undocumented, and the method incorrectly listed `memory_format` as a positional argument. This:\n\n- documents `torch.clone`\n- lists `memory_format` as a keyword-only argument\n- wordsmiths the documentation\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43098\n\nReviewed By: ngimel\n\nDifferential Revision: D23153397\n\nPulled By: mruberry\n\nfbshipit-source-id: c2ea781cdcb8b5ad3f04987c2b3a2f1fe0eaf18b", "pr_number": "43098", "files_changed": ["docs/source/torch.rst", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "4ae832e106": {"title": "Optimize SiLU (Swish) op in PyTorch (#42976)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42976\n\nOptimize SiLU (Swish) op in PyTorch.\n\nSome benchmark result\n\ninput = torch.rand(1024, 32768, dtype=torch.float, device=\"cpu\")\nforward: 221ms -> 133ms\nbackward: 600ms -> 170ms\n\ninput = torch.rand(1024, 32768, dtype=torch.double, device=\"cpu\")\nforward: 479ms -> 297ms\nbackward: 1438ms -> 387ms\n\ninput = torch.rand(8192, 32768, dtype=torch.float, device=\"cuda\")\nforward: 24.34ms -> 9.83ms\nbackward: 97.05ms -> 29.03ms\n\ninput = torch.rand(4096, 32768, dtype=torch.double, device=\"cuda\")\nforward: 44.24ms -> 30.15ms\nbackward: 126.21ms -> 49.68ms\n\nTest Plan: buck test mode/dev-nosan //caffe2/test:nn -- \"SiLU\"\n\nReviewed By: houseroad\n\nDifferential Revision: D23093593\n\nfbshipit-source-id: 1ba7b95d5926c4527216ed211a5ff1cefa3d3bfd", "pr_number": "42976", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "docs/source/scripts/build_activation_images.py", "test/cpp_api_parity/parity-tracker.md", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp", "torch/csrc/api/include/torch/enum.h", "torch/csrc/api/include/torch/nn/functional/activation.h", "torch/csrc/api/include/torch/nn/modules/activation.h", "torch/csrc/api/src/nn/modules/activation.cpp", "torch/testing/_internal/common_nn.py"], "labels": ["fb-exported", "merged"]}, "e2eb0cb1a9": {"title": "Adds arccosh alias for acosh and adds an alias consistency test (#43107)", "body": "Summary:\nThis adds the torch.arccosh alias and updates alias testing to validate the consistency of the aliased and original operations. The alias testing is also updated to run on CPU and CUDA, which revealed a memory leak when tracing (see https://github.com/pytorch/pytorch/issues/43119).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43107\n\nReviewed By: ngimel\n\nDifferential Revision: D23156472\n\nPulled By: mruberry\n\nfbshipit-source-id: 6155fac7954fcc49b95e7c72ed917c85e0eabfcd", "pr_number": "43107", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/run_test.py", "test/test_op_aliases.py", "test/test_op_normalization.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py"], "labels": ["merged", "module: numpy", "oncall: jit"]}, "248b6a30f4": {"title": "add training mode to mobile::Module (#42880)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42880\n\nEnable switching between and checking for training and eval mode for torch::jit::mobile::Module using train(), eval(), and is_training(), like exists for torch::jit::Module.\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23063006\n\nPulled By: ann-ss\n\nfbshipit-source-id: b79002148c46146b6e961cbef8aaf738bbd53cb2", "pr_number": "42880", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h"], "labels": ["merged", "oncall: jit"]}, "269fdb5bb2": {"title": "prepare to split transformer header file (#43069)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43069\n\nThe transformer c++ impl need to put TransformerEncoderLayer/DecoderLayer and TransformerEncoder/TransformerDecoder in different header since TransformerEncoder/Decoder's options class need TransformerEncoderLayer/DecoderLayer as input parameter. Split header files to avoid cycle includsion.\n\nTest Plan: Imported from OSS\n\nReviewed By: yf225\n\nDifferential Revision: D23139437\n\nPulled By: glaringlee\n\nfbshipit-source-id: 3c752ed7702ba18a9742e4d47d049e62d2813de0", "pr_number": "43069", "files_changed": ["test/cpp/api/transformer.cpp", "torch/csrc/api/include/torch/nn/modules.h", "torch/csrc/api/include/torch/nn/modules/transformer.h", "torch/csrc/api/include/torch/nn/modules/transformerlayer.h", "torch/csrc/api/include/torch/nn/options.h", "torch/csrc/api/include/torch/nn/options/transformer.h", "torch/csrc/api/include/torch/nn/options/transformerlayer.h", "torch/csrc/api/src/nn/modules/transformer.cpp", "torch/csrc/api/src/nn/options/transformer.cpp"], "labels": ["merged"]}, "472f291375": {"title": "Fix freeze_module pass for sharedtype (#42457)", "body": "Summary:\nDuring cleanup phase, calling recordReferencedAttrs would record\nthe attributes which are referenced and hence kept.\nHowever, if you have two instances of the same type which are preserved\nthrough freezing process, as the added testcase shows, then during\nrecording the attributes which are referenced, we iterate through the\ntype INSTANCES that we have seen so far and record those ones.\nThus if we have another instance of the same type, we will just look at\nthe first instance in the list, and record that instances.\nThis PR fixes that by traversing the getattr chains and getting the\nactual instance of the getattr output.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42457\n\nTest Plan:\npython test/test_jit.py TestFreezing\nFixes #{issue number}\n\nReviewed By: gchanan\n\nDifferential Revision: D23106921\n\nPulled By: kimishpatel\n\nfbshipit-source-id: ffff52876938f8a1fedc69b8b24a3872ea66103b", "pr_number": "42457", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/freeze_module.cpp"], "labels": ["merged", "oncall: jit"]}, "aab66602c4": {"title": "Add torch.dot for complex tensors (#42745)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42745\n\nTest Plan: Imported from OSS\n\nReviewed By: izdeby\n\nDifferential Revision: D23056382\n\nPulled By: anjali411\n\nfbshipit-source-id: c97f15e057095f78069844dbe0299c14104d2fce", "pr_number": "42745", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/BlasKernel.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/TH/generic/THBlas.cpp", "errors.txt", "test/test_torch.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["merged", "module: complex"]}, "034e6727e7": {"title": "Set default ATen threading backend to native if USE_OPENMP is false (#43067)", "body": "Summary:\nSince OpenMP is not available on some platforms, or might be disabled by user, set default `ATEN_THREADING` based on USE_OPENMP and USE_TBB options\n\nFixes https://github.com/pytorch/pytorch/issues/43036\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43067\n\nReviewed By: houseroad\n\nDifferential Revision: D23138856\n\nPulled By: malfet\n\nfbshipit-source-id: cc8f9ee59a5559baeb3f19bf461abbc08043b71c", "pr_number": "43067", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["merged"]}, "7cb8d68ae1": {"title": "Rename XLAPreAutograd to AutogradXLA. (#43047)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43047\n\nReviewed By: ezyang\n\nDifferential Revision: D23134326\n\nPulled By: ailzhang\n\nfbshipit-source-id: 5fcbc23755daa8a28f9b03af6aeb3ea0603b5c9a", "pr_number": "43047", "files_changed": ["aten/src/ATen/core/LegacyTypeDispatch.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "c10/core/Backend.h", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.h", "c10/core/TensorOptions.h", "torch/library.h"], "labels": ["merged"]}, "9c3f579528": {"title": ".circleci: Copy LLVM from pre-built image (#43038)", "body": "Summary:\nLLVM builds took a large amount of time and bogged down docker builds in\ngeneral. Since we build it the same for everything let's just copy it\nfrom a pre-built image instead of building it from source every time.\n\nBuilds are defined in https://github.com/pytorch/builder/pull/491\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43038\n\nReviewed By: malfet\n\nDifferential Revision: D23119513\n\nPulled By: seemethere\n\nfbshipit-source-id: f44324439d45d97065246caad07c848e261a1ab6", "pr_number": "43038", "files_changed": [".circleci/docker/common/install_llvm.sh", ".circleci/docker/ubuntu-cuda/Dockerfile", ".circleci/docker/ubuntu/Dockerfile"], "labels": ["merged", "module: ci"]}, "768c2a8c25": {"title": "vmap: fixed to work with functools.partial (#43028)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43028\n\nThere was a bug where we always tried to grab the `__name__` attribute of\nthe function passed in by the user. Not all Callables have the\n`__name__` attribute, an example being a Callable produced by\nfunctools.partial.\n\nThis PR modifies the error-checking code to use `repr` if `__name__` is\nnot available. Furthermore, it moves the \"get the name of this function\"\nfunctionality to the actual error sites as an optimization so we don't\nspend time trying to compute `__repr__` for the Callable if there is no\nerror.\n\nTest Plan: - `pytest test/test_vmap.py -v`, added new tests.\n\nReviewed By: yf225\n\nDifferential Revision: D23130235\n\nPulled By: zou3519\n\nfbshipit-source-id: 937f3640cc4d759bf6fa38b600161f5387a54dcf", "pr_number": "43028", "files_changed": ["test/test_vmap.py", "torch/_vmap_internals.py"], "labels": ["merged"]}, "37252e8f00": {"title": "Implement batching rules for some unary ops (#43059)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43059\n\nThis PR implements batching rules for some unary ops. In particular, it\nimplements the batching rules for the unary ops that take a single\ntensor as input (and nothing else).\n\nThe batching rule for a unary op is:\n(1) grab the physical tensor straight out of the BatchedTensor\n(2) call the unary op\n(3) rewrap the physical tensor in a BatchedTensor\n\nTest Plan: - new tests `pytest test/test_vmap.py -v -k \"Operators\"`\n\nReviewed By: ezyang\n\nDifferential Revision: D23132277\n\nPulled By: zou3519\n\nfbshipit-source-id: 24b9d7535338207531d767155cdefd2c373ada77", "pr_number": "43059", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "6db0b8785d": {"title": "Adds movedim method, fixes movedim docs, fixes view doc links (#43122)", "body": "Summary:\nThis PR:\n\n- Adds a method variant to movedim\n- Fixes the movedim docs so it will actually appear in the documentation\n- Fixes three view doc links which were broken\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43122\n\nReviewed By: ngimel\n\nDifferential Revision: D23166222\n\nPulled By: mruberry\n\nfbshipit-source-id: 14971585072bbc04b5366d4cc146574839e79cdb", "pr_number": "43122", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "docs/source/tensor_view.rst", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "864f0cfb2d": {"title": "Fix type annotations for torch.sparse, enable in CI (#43108)", "body": "Summary:\nCloses gh-42982\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43108\n\nReviewed By: malfet\n\nDifferential Revision: D23167560\n\nPulled By: ezyang\n\nfbshipit-source-id: 0d660ca686ada2347bf440c6349551d1539f99ef", "pr_number": "43108", "files_changed": ["mypy.ini", "torch/sparse/__init__.py"], "labels": ["merged", "module: typing", "open source"]}, "825ec18eed": {"title": "[jit] better error message (#43093)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43093\n\nwithout this it's hard to tell which module is going wrong\n\nTest Plan:\n```\n> TypeError:\n> 'numpy.int64' object in attribute 'Linear.in_features' is not a valid constant.\n> Valid constants are:\n> 1. a nn.ModuleList\n> 2. a value of type {bool, float, int, str, NoneType, torch.device, torch.layout, torch.dtype}\n> 3. a list or tuple of (2)\n```\n\nReviewed By: eellison\n\nDifferential Revision: D23148516\n\nfbshipit-source-id: b86296cdeb7b47c9fd69b5cfa479914c58ef02e6", "pr_number": "43093", "files_changed": ["test/test_jit.py", "torch/jit/_recursive.py"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "133e9f96e1": {"title": "Use c10 threadpool for GPU to CPU distributed autograd continuations. (#42511)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42511\n\nDistEngine currently only has a single thread to execute GPU to CPU\ncontinuations as part of the backward pass. This would be a significant\nperformance bottleneck in cases where we have such continuations and would like\nto execute these using all CPU cores.\n\nTo alleviate this in this PR, we have the single thread in DistEngine only\ndequeue work from the global queue, but then hand off execution of that work to\nthe c10 threadpool where we call \"execute_graph_task_until_ready_queue_empty\".\n\nFor more context please see:\nhttps://github.com/pytorch/pytorch/issues/40255#issuecomment-663298062.\nghstack-source-id: 109997718\n\nTest Plan: waitforbuildbot\n\nReviewed By: albanD\n\nDifferential Revision: D22917579\n\nfbshipit-source-id: c634b6c97f3051f071fd7b994333e6ecb8c54155", "pr_number": "42511", "files_changed": ["torch/csrc/autograd/engine.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h"], "labels": ["merged"]}, "1f6e6a1166": {"title": "Remove unused variable vecVecStartIdx (#42257)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42257\n\nReviewed By: gchanan\n\nDifferential Revision: D23109328\n\nPulled By: ezyang\n\nfbshipit-source-id: dacd438395fedd1050ad3ffb81327bbb746c776c", "pr_number": "42257", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp"], "labels": ["merge-this-please", "merged", "open source"]}, "5aa61afbfb": {"title": "quant bench: update observer configs (#42956)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42956\n\nIn preparation for observer perf improvement, cleans up the\nmicro benchmarks:\n* disable CUDA for histogram observers (it's too slow)\n* add larger shapes for better representation of real workloads\n\nTest Plan:\n```\ncd benchmarks/operator_benchmark\npython -m pt.qobserver_test\n```\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23093996\n\nfbshipit-source-id: 5dc477c9bd5490d79d85ff8537270cd25aca221a", "pr_number": "42956", "files_changed": ["benchmarks/operator_benchmark/pt/qobserver_test.py"], "labels": ["merged"]}, "a5dfba0a6e": {"title": "observers: make eps a buffer (#43149)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43149\n\nThis value doesn't change, making it a buffer to only pay\nthe cost of creating a tensor once.\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23170428\n\nfbshipit-source-id: 6b963951a573efcc5b5a57649c814590b448dd72", "pr_number": "43149", "files_changed": ["torch/quantization/observer.py"], "labels": ["merged"]}, "3264ba065c": {"title": "observers: use clamp instead of min/max in calculate_qparams (#43150)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43150\n\nThe current logic was expensive because it created tensors on CUDA.\nSwitching to clamp since it can work without needing to create tensors.\n\nTest Plan:\nbenchmarks\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23170427\n\nfbshipit-source-id: 6fe3a728e737aca9f6c2c4d518c6376738577e21", "pr_number": "43150", "files_changed": ["torch/quantization/observer.py"], "labels": ["merged"]}, "57af1ec145": {"title": "observers: use torch.all to check for valid min and max values (#43151)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43151\n\nUsing `torch.all` instead of `torch.sum` and length check.\nIt's unclear whether the increase in perf (~5% for small inputs) is\nreal, but should be a net benefit, especially for larger channel inputs.\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23170426\n\nfbshipit-source-id: ee5c25eb93cee1430661128ac9458a9c525df8e5", "pr_number": "43151", "files_changed": ["torch/quantization/observer.py"], "labels": ["merged"]}, "cd96dfd44b": {"title": "Delete accidentally committed file errors.txt. (#43164)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43164\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23175392\n\nPulled By: gchanan\n\nfbshipit-source-id: 0d2d918fdf4a94361cdc3344bf1bc89dd0286ace", "pr_number": "43164", "files_changed": ["errors.txt"], "labels": ["merged"]}, "3c5e3966f4": {"title": "[ONNX] Squeeze operator should give an error when trying to apply to a dimension with shape > 1 (#38476)", "body": "Summary:\nThe ONNX spec for the Squeeze operator:\n\n> Remove single-dimensional entries from the shape of a tensor. Takes a parameter axes with a list of axes to squeeze. If axes is not provided, all the single dimensions will be removed from the shape. If an axis is selected with shape entry not equal to one, an error is raised.\n\nCurrently, as explained in issue https://github.com/pytorch/pytorch/issues/36796, it is possible to export such a model to ONNX, and this results in an exception from ONNX runtime.\n\nFixes https://github.com/pytorch/pytorch/issues/36796.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38476\n\nReviewed By: hl475\n\nDifferential Revision: D22158024\n\nPulled By: houseroad\n\nfbshipit-source-id: bed625f3c626eabcbfb2ea83ec2f992963defa19", "pr_number": "38476", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/helper.cpp", "torch/csrc/jit/passes/onnx/helper.h", "torch/csrc/jit/python/python_ir.cpp", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py"], "labels": ["merged", "open source", "triaged"]}, "aef2890a75": {"title": "Improve zero sized input for addmv (#41824)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/41340\n\nUnfortunately, I still can not get a K80 to verify the fix, but it should be working.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41824\n\nReviewed By: mruberry\n\nDifferential Revision: D23172775\n\nPulled By: ngimel\n\nfbshipit-source-id: aa6af96fe74e3bb07982c006cb35ecc7f18181bc", "pr_number": "41824", "files_changed": ["aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/BlasKernel.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/cuda/Blas.cu", "aten/src/ATen/native/cuda/LinearAlgebra.cu"], "labels": ["merged", "module: operators", "open source", "triaged"]}, "e8db0425b5": {"title": "remove dot from TH (#43148)", "body": "Summary:\nsmall cleanup of dead code\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43148\n\nReviewed By: mruberry\n\nDifferential Revision: D23175571\n\nPulled By: ngimel\n\nfbshipit-source-id: b1b0ae9864d373c75666b95c589d090a9ca791b2", "pr_number": "43148", "files_changed": ["aten/src/TH/generic/THBlas.cpp"], "labels": ["merged"]}, "c44b1de54e": {"title": "Pin VC++ version to 14.26 (#43184)", "body": "Summary:\nVC++14.27 fails to compile mkl-dnn, see oneapi-src/oneDNN#812\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43184\n\nReviewed By: glaringlee\n\nDifferential Revision: D23181803\n\nPulled By: malfet\n\nfbshipit-source-id: 9861c6243673c775374d77d2f51b45a42791b475", "pr_number": "43184", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/windows_vs_14_26_install.ps1", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged", "module: ci", "module: windows", "open source", "triaged"]}, "b3bda94393": {"title": "[NVFuser] Enable E2E BCast-PWise-Reduction fusions (#43129)", "body": "Summary:\nHad a bunch of merged commits that shouldn't have been there, reverted them to prevent conflicts. Lots of new features, highlights listed below.\n\n**Overall:**\n\n- Enables pointwise fusion, single (but N-D) broadcast -- pointwise fusion, single (but N-D) broadcast -- pointwise -- single (but N-D) reduction fusion.\n\n**Integration:**\n\n- Separate \"magic scheduler\" logic that takes a fusion and generates code generator schedule\n- Reduction fusion scheduling with heuristics closely matching eagermode (unrolling supported, but no vectorize support)\n- 2-Stage caching mechanism, one on contiguity, device, type, and operations, the other one is input size->reduction heuristic\n\n**Code Generation:**\n\n- More generic support in code generation for computeAt\n- Full rework of loop nest generation and Indexing to more generically handle broadcast operations\n- Code generator has automatic kernel launch configuration (including automatic allocation of grid reduction buffers)\n- Symbolic (runtime) tilling on grid/block dimensions is supported\n- Simplified index generation based on user-defined input contiguity\n- Automatic broadcast support (similar to numpy/pytorch semantics)\n- Support for compile time constant shared memory buffers\n- Parallelized broadcast support (i.e. block reduction -> block broadcast support)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43129\n\nReviewed By: mrshenli\n\nDifferential Revision: D23162207\n\nPulled By: soumith\n\nfbshipit-source-id: 16deee4074c64de877eed7c271d6a359927111b2", "pr_number": "43129", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_gpu.cpp", "test/cpp/jit/tests.h", "test/test_jit_cuda_fuser.py", "tools/build_variables.bzl", "torch/csrc/jit/codegen/cuda/arith.cpp", "torch/csrc/jit/codegen/cuda/arith.h", "torch/csrc/jit/codegen/cuda/compute_at.cpp", "torch/csrc/jit/codegen/cuda/compute_at.h", "torch/csrc/jit/codegen/cuda/dispatch.cpp", "torch/csrc/jit/codegen/cuda/dispatch.h", "torch/csrc/jit/codegen/cuda/executor.cpp", "torch/csrc/jit/codegen/cuda/executor.h", "torch/csrc/jit/codegen/cuda/executor_kernel_arg.cpp", "torch/csrc/jit/codegen/cuda/executor_kernel_arg.h", "torch/csrc/jit/codegen/cuda/executor_launch_params.cpp", "torch/csrc/jit/codegen/cuda/executor_launch_params.h", "torch/csrc/jit/codegen/cuda/executor_utils.cpp", "torch/csrc/jit/codegen/cuda/executor_utils.h", "torch/csrc/jit/codegen/cuda/expr_evaluator.cpp", "torch/csrc/jit/codegen/cuda/expr_evaluator.h", "torch/csrc/jit/codegen/cuda/fusion.cpp", "torch/csrc/jit/codegen/cuda/fusion.h", "torch/csrc/jit/codegen/cuda/graph_fuser.cpp", "torch/csrc/jit/codegen/cuda/index_compute.cpp", "torch/csrc/jit/codegen/cuda/index_compute.h", "torch/csrc/jit/codegen/cuda/ir_all_nodes.h", "torch/csrc/jit/codegen/cuda/ir_base_nodes.cpp", "torch/csrc/jit/codegen/cuda/ir_base_nodes.h", "torch/csrc/jit/codegen/cuda/ir_cloner.cpp", "torch/csrc/jit/codegen/cuda/ir_cloner.h", "torch/csrc/jit/codegen/cuda/ir_graphviz.cpp", "torch/csrc/jit/codegen/cuda/ir_graphviz.h", "torch/csrc/jit/codegen/cuda/ir_interface_nodes.h", "torch/csrc/jit/codegen/cuda/ir_internal_nodes.h", "torch/csrc/jit/codegen/cuda/ir_iostream.cpp", "torch/csrc/jit/codegen/cuda/ir_iostream.h", "torch/csrc/jit/codegen/cuda/ir_nodes.cpp", "torch/csrc/jit/codegen/cuda/ir_utils.h", "torch/csrc/jit/codegen/cuda/iter_visitor.cpp", "torch/csrc/jit/codegen/cuda/iter_visitor.h", "torch/csrc/jit/codegen/cuda/kernel.cpp", "torch/csrc/jit/codegen/cuda/kernel.h", "torch/csrc/jit/codegen/cuda/kernel_arg.h", "torch/csrc/jit/codegen/cuda/kernel_cache.cpp", "torch/csrc/jit/codegen/cuda/kernel_cache.h", "torch/csrc/jit/codegen/cuda/kernel_ir.cpp", "torch/csrc/jit/codegen/cuda/kernel_ir.h", "torch/csrc/jit/codegen/cuda/kernel_resource_strings.h", "torch/csrc/jit/codegen/cuda/lower2device.cpp", "torch/csrc/jit/codegen/cuda/lower2device.h", "torch/csrc/jit/codegen/cuda/lower_index.cpp", "torch/csrc/jit/codegen/cuda/lower_index.h", "torch/csrc/jit/codegen/cuda/lower_loops.cpp", "torch/csrc/jit/codegen/cuda/lower_loops.h", "torch/csrc/jit/codegen/cuda/lower_thread_predicate.cpp", "torch/csrc/jit/codegen/cuda/lower_thread_predicate.h", "torch/csrc/jit/codegen/cuda/lower_unroll.cpp", "torch/csrc/jit/codegen/cuda/lower_unroll.h", "torch/csrc/jit/codegen/cuda/lower_utils.cpp", "torch/csrc/jit/codegen/cuda/lower_utils.h", "torch/csrc/jit/codegen/cuda/lower_validation.cpp", "torch/csrc/jit/codegen/cuda/lower_validation.h", "torch/csrc/jit/codegen/cuda/manager.cpp", "torch/csrc/jit/codegen/cuda/mutator.cpp", "torch/csrc/jit/codegen/cuda/mutator.h", "torch/csrc/jit/codegen/cuda/parser.cpp", "torch/csrc/jit/codegen/cuda/parser.h", "torch/csrc/jit/codegen/cuda/partition.cpp", "torch/csrc/jit/codegen/cuda/predicate_compute.cpp", "torch/csrc/jit/codegen/cuda/predicate_compute.h", "torch/csrc/jit/codegen/cuda/scheduler.cpp", "torch/csrc/jit/codegen/cuda/scheduler.h", "torch/csrc/jit/codegen/cuda/shape_inference.cpp", "torch/csrc/jit/codegen/cuda/shape_inference.h", "torch/csrc/jit/codegen/cuda/tensor_view.cpp", "torch/csrc/jit/codegen/cuda/transform_iter.cpp", "torch/csrc/jit/codegen/cuda/transform_iter.h", "torch/csrc/jit/codegen/cuda/transform_replay.cpp", "torch/csrc/jit/codegen/cuda/transform_rfactor.cpp", "torch/csrc/jit/codegen/cuda/type.cpp", "torch/csrc/jit/codegen/cuda/type.h", "torch/csrc/jit/codegen/cuda/utils.cpp", "torch/csrc/jit/codegen/cuda/utils.h"], "labels": ["merged", "oncall: jit", "open source"]}, "b92b556a12": {"title": "Add shape inference to SparseLengthsSumSparse ops (#43181)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43181\n\natt\n\nTest Plan:\n```\nbuck test caffe2/caffe2/opt:bound_shape_inference_test\n```\n\nReviewed By: ChunliF\n\nDifferential Revision: D23097145\n\nfbshipit-source-id: 3e4506308446f28fbeb01dcac97dce70c0443975", "pr_number": "43181", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/split_slss_test.cc"], "labels": ["fb-exported", "merged"]}, "ee74c2e5be": {"title": "Compress fatbin to fit into 32bit indexing (#43074)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/39968\n\ntested with `TORCH_CUDA_ARCH_LIST='3.5 5.2 6.0 6.1 7.0 7.5 8.0+PTX'`, before this PR, it was failing, and with this  PR, the build succeed.\n\nWith `TORCH_CUDA_ARCH_LIST='7.0 7.5 8.0+PTX'`, `libtorch_cuda.so` with symbols changes from 2.9GB -> 2.2GB\n\ncc: ptrblck mcarilli jjsjann123\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43074\n\nReviewed By: mrshenli\n\nDifferential Revision: D23176095\n\nPulled By: malfet\n\nfbshipit-source-id: 7b3e6d049fc080e519f21e80df05ef68e7bea57e", "pr_number": "43074", "files_changed": ["CMakeLists.txt"], "labels": ["merged", "open source"]}, "53bbf5a48b": {"title": "Update README.md (#43100)", "body": "Summary:\nThe changes are minor.\n1. Add back the external links so that readers can find out more about external tools on how to accelerate PyTorch.\n2. Fix typo\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43100\n\nReviewed By: colesbury\n\nDifferential Revision: D23192251\n\nPulled By: mrshenli\n\nfbshipit-source-id: dde54b7942ebff5bbe3d58ad95744c6d95fe60fe", "pr_number": "43100", "files_changed": ["README.md"], "labels": ["merged", "open source"]}, "5d608d45cf": {"title": "Added Encoder Layer constructor with default parameters (#43130)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/37756\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43130\n\nReviewed By: colesbury\n\nDifferential Revision: D23189803\n\nPulled By: mrshenli\n\nfbshipit-source-id: 53f3fca838828ddd728d8b44c36745bab5acee1f", "pr_number": "43130", "files_changed": ["torch/csrc/api/include/torch/nn/modules/transformerlayer.h"], "labels": ["merged", "open source"]}, "e39b43fd76": {"title": "Issue 43057 (#43063)", "body": "Summary:\nA small change that adds a docstring that can be found with\n`getattr(nn.Module, nn.Module.forward.__name__, None).__doc__`\n\nFixes https://github.com/pytorch/pytorch/issues/43057\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43063\n\nReviewed By: mrshenli\n\nDifferential Revision: D23161782\n\nPulled By: ezyang\n\nfbshipit-source-id: 95456f858e2b6a0e41ae551ea4ec2e78dd35ee3f", "pr_number": "43063", "files_changed": ["torch/nn/modules/module.py"], "labels": ["merged", "open source"]}, "0a9c35aba3": {"title": "maybe minor fix to dispatch/backend_fallback_test.cpp? (#42990)", "body": "Summary:\nI think you want to push rewrapped `rets`, not `args`, back to the stack.\n\nDoesn't matter for test purposes because tests only check if/when fallbacks were called, they don't check outputs for correctness.  But it avoids reader confusion.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42990\n\nReviewed By: mrshenli\n\nDifferential Revision: D23168277\n\nPulled By: ezyang\n\nfbshipit-source-id: 2559f0707acdca2e3deac09006bc66ce3c788ea3", "pr_number": "42990", "files_changed": ["aten/src/ATen/core/dispatch/backend_fallback_test.cpp"], "labels": ["merged", "open source", "triaged"]}, "fbf274f5a7": {"title": "Autocast support for cudnn RNNs (#42385)", "body": "Summary:\nShould close https://github.com/pytorch/pytorch/issues/36428.\n\nThe cudnn RNN API expects weights to occupy a flat buffer in memory with a particular layout.  This PR implements a \"speed of light\" fix:  [`_cudnn_rnn_cast_reflatten`](https://github.com/pytorch/pytorch/pull/42385/files#diff-9ef93b6a4fb5a06a37c562b83737ac6aR327) (the autocast wrapper assigned to `_cudnn_rnn`) copies weights to the right slices of a flat FP16 buffer with a single read/write per weight (as opposed to casting them to FP16 individually then reflattening the individual FP16 weights, which would require 2 read/writes per weight).\n\nIt isn't pretty but IMO it doesn't make rnn bindings much more tortuous than they already are.\n\nThe [test](https://github.com/pytorch/pytorch/pull/42385/files#diff-e68a7bc6ba14f212e5e7eb3727394b40R2683) tries a forward under autocast and a backward for the full cross product of RNN options and input/weight/hidden dtypes.  As for all FP16list autocast tests, forward output and backward grads are checked against a control where inputs (including RNN module weights in this case) are precasted to FP16 on the python side.\n\nNot sure who to ask for review, tagging ezyang and ngimel because Ed wrote this file (almost 2 years ago) and Natalia did the most recent major [surgery](https://github.com/pytorch/pytorch/pull/12600).\n\nSide quests discovered:\n- Should we update [persistent RNN heuristics](https://github.com/pytorch/pytorch/blob/dbdd28207c5cf6c4a35ceb1de0811c4812e8882c/aten/src/ATen/native/cudnn/RNN.cpp#L584) to include compute capability 8.0?  Could be another PR but seems easy enough to include.\n- Many (maybe all?!) the raw cudnn API calls in [RNN.cpp](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cudnn/RNN.cpp) are deprecated in cudnn 8.  I don't mind taking the AI to update them since my mental cache is full of rnn stuff, but that would be a substantial separate PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42385\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23077782\n\nPulled By: ezyang\n\nfbshipit-source-id: a2afb1bdab33ba0442879a703df13dc87f03ec2e", "pr_number": "42385", "files_changed": ["BUILD.bazel", "aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/autocast_mode.h", "aten/src/ATen/cudnn/AutocastRNN.cpp", "aten/src/ATen/cudnn/Types.cpp", "aten/src/ATen/cudnn/Types.h", "aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/RNN.h", "aten/src/ATen/native/cudnn/RNN.cpp", "aten/src/ATen/native/cudnn/RNNUtils.h", "aten/src/ATen/native/miopen/RNN_miopen.cpp", "test/test_cuda.py"], "labels": ["merged", "open source", "triaged"]}, "0744dd6166": {"title": "Fix shapes in the MarginRankingLoss docs (#43131)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42884\n\nI did some additional research and considering the first few lines of the docs (`Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch Tensors, and a label 1D mini-batch tensor y (containing 1 or -1`) and the provided tests, this loss should be used primarily with 1-D tensors. More advanced users (that may use this loss in non-standard ways) can easily check the source and see that the definition accepts inputs/targets of arbitrary dimension as long as they match in shape or are broadcastable.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43131\n\nReviewed By: colesbury\n\nDifferential Revision: D23192011\n\nPulled By: mrshenli\n\nfbshipit-source-id: c412c28daf9845c0142ea33b35d4287e5b65fbb9", "pr_number": "43131", "files_changed": ["torch/nn/modules/loss.py"], "labels": ["merged", "open source"]}, "493b3c2c7c": {"title": "Replace all AT_ASSERTM under ATen CPU kernels. (#41876)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41876\n\nTest Plan: Imported from OSS\n\nReviewed By: colesbury\n\nDifferential Revision: D23190010\n\nPulled By: ezyang\n\nfbshipit-source-id: 238f1cd8db283805d6e892de7549763d0aa13316", "pr_number": "41876", "files_changed": ["aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp"], "labels": ["merged", "open source"]}, "dfdd797723": {"title": "Replace all AT_ASSERTM under ATen CUDA kernels. (#42989)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42989\n\nTest Plan: Imported from OSS\n\nReviewed By: colesbury\n\nDifferential Revision: D23190011\n\nPulled By: ezyang\n\nfbshipit-source-id: 7489598d7d920f32334943c1bf12bba74208a96c", "pr_number": "42989", "files_changed": ["aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/RNN.cu", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/native/cuda/SummaryOps.cu"], "labels": ["merge-this-please", "merged", "open source"]}, "888ae1b3d8": {"title": "Introducing Matrix exponential (#40161)", "body": "Summary:\nImplements (batched) matrix exponential. Fixes [https://github.com/pytorch/pytorch/issues/9983](https://github.com/pytorch/pytorch/issues/9983).\n\nThe algorithm follows:\n```\n Bader, P.; Blanes, S.; Casas, F.\n Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation.\n Mathematics 2019, 7, 1174.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40161\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D22951372\n\nPulled By: ezyang\n\nfbshipit-source-id: aa068cb76d5cf71696b333d3e72cee287b3089e3", "pr_number": "40161", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/FunctionOfAMatrixUtils.cpp", "aten/src/ATen/native/FunctionOfAMatrixUtils.h", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp", "aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "triaged"]}, "2e6e295ecc": {"title": "refactor _save_parameters to _save_data (#43162)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43162\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23175286\n\nPulled By: ann-ss\n\nfbshipit-source-id: 6f930b98c367242fd4efbf51cb1d09995f7c4b40", "pr_number": "43162", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_lite_trainer.cpp", "tools/build_variables.bzl", "torch/csrc/jit/mobile/export.cpp", "torch/csrc/jit/mobile/export.h", "torch/csrc/jit/mobile/export_data.cpp", "torch/csrc/jit/mobile/export_data.h", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/import_data.h"], "labels": ["merged", "oncall: jit"]}, "dd194c1612": {"title": "add _save_parameters to serialize map (#43163)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43163\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23175287\n\nPulled By: ann-ss\n\nfbshipit-source-id: ddfd734513c07e8bdbec108f26d1ca1770d098a6", "pr_number": "43163", "files_changed": ["test/cpp/jit/test_lite_trainer.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/export_data.cpp", "torch/csrc/jit/mobile/export_data.h", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/import_data.h"], "labels": ["merged", "oncall: jit"]}, "3951457ca5": {"title": "[FX] Add in resnet + quantization tests (#43157)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43157\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23173327\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 724d0f5399d389cdaa53917861b2113c33b9b5f9", "pr_number": "43157", "files_changed": ["test/fx/quantization.py", "test/test_fx.py"], "labels": ["merged"]}, "da5df7e2d2": {"title": "Remove use of term \"blacklist\" from tools/autograd/gen_python_functions.py (#42047)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41720\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42047\n\nReviewed By: colesbury\n\nDifferential Revision: D23197785\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 8ef38518f479e5e96b6a51bc420b0df5b35b447c", "pr_number": "42047", "files_changed": ["tools/autograd/gen_python_functions.py"], "labels": ["merged", "open source", "triaged"]}, "6c99d5611d": {"title": "[tensorexpr] Fix promotion of booleans (#43097)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43097\n\nBoolean arguments weren't promoted, so if you tried to write a comparison with\ntypes such as `Tensor(Bool) == Int` you'd fail typechecking inside the TE\nengine.\n\nTest Plan: Imported from OSS\n\nReviewed By: protonu, zheng-xq\n\nDifferential Revision: D23167926\n\nPulled By: bertmaher\n\nfbshipit-source-id: 47091a815d5ae521637142a5c390e8a51a776906", "pr_number": "43097", "files_changed": ["test/test_tensorexpr.py", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["merged", "oncall: jit"]}, "d5bc2a8058": {"title": "Remove std::complex from c10::Half (#39833)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39833\n\nReviewed By: mrshenli\n\nDifferential Revision: D22644987\n\nPulled By: anjali411\n\nfbshipit-source-id: 5ae5db10b12d410560eca43234efa04b711a639c", "pr_number": "39833", "files_changed": ["c10/util/Half.h"], "labels": ["merged", "module: complex", "open source", "triaged"]}, "d06f1818ad": {"title": "Fix `codegen/cuda` gcc-5.4 compilation issues (#43223)", "body": "Summary:\nMost of the fixes is the same old enum-is-not-hasheable error\nIn manager.cpp use std::unordered_map::emplace rather than `insert` to avoid error triggered by missed copy elision\nThis regression was introduced by https://github.com/pytorch/pytorch/pull/43129\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43223\n\nReviewed By: albanD, seemethere\n\nDifferential Revision: D23198330\n\nPulled By: malfet\n\nfbshipit-source-id: 576082f7a4454dd29182892c9c4e0b51a967d456", "pr_number": "43223", "files_changed": ["torch/csrc/jit/codegen/cuda/lower_thread_predicate.h", "torch/csrc/jit/codegen/cuda/lower_utils.cpp", "torch/csrc/jit/codegen/cuda/lower_utils.h", "torch/csrc/jit/codegen/cuda/manager.cpp"], "labels": ["merged", "oncall: jit", "triaged"]}, "e41ca2d9fa": {"title": "In copy_weights_to_flat_buf_views() explicitly construct tuple (#43244)", "body": "Summary:\nIn some versions of GCC, tuple constructor from initializer list  is marked as explicit, which results in the following compilation error:\n```\n/var/lib/jenkins/workspace/aten/src/ATen/native/cudnn/RNN.cpp: In function 'std::tuple<at::Tensor, std::vector<at::Tensor, std::allocator<at::Tensor> > > at::native::cudnn_rnn::copy_weights_to_flat_buf_views(at::TensorList, int64_t, int64_t, int64_t, int64_t, int64_t, bool, bool, cudnnDataType_t, const c10::TensorOptions&, bool, bool, bool)':\n/var/lib/jenkins/workspace/aten/src/ATen/native/cudnn/RNN.cpp:687:35: error: converting to 'std::tuple<at::Tensor, std::vector<at::Tensor, std::allocator<at::Tensor> > >' from initializer list would use explicit constructor 'constexpr std::tuple<_T1, _T2>::tuple(_U1&&, _U2&&) [with _U1 = at::Tensor&; _U2 = std::vector<at::Tensor>&; <template-parameter-2-3> = void; _T1 = at::Tensor; _T2 = std::vector<at::Tensor>]'\n     return {weight_buf, params_arr};\n```\nThis regression was introduced by https://github.com/pytorch/pytorch/pull/42385\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43244\n\nReviewed By: pbelevich\n\nDifferential Revision: D23205656\n\nPulled By: malfet\n\nfbshipit-source-id: 51470386ad95290c7c99d733fc1fe655aa27d009", "pr_number": "43244", "files_changed": ["aten/src/ATen/native/cudnn/RNN.cpp"], "labels": ["merged", "module: cuda", "triaged"]}, "7c923a1025": {"title": "Optimize linux CI build/test matrix (#43240)", "body": "Summary:\nMake CUDA-10.1 configs build-only, as CUDA-10.1 and CUDA-10.2 test matrix is almost identical, and now, since CUDA-11 is out perhaps it's time to stop testing CUDA-10.1.\nMake CUDA-9.2+GCC_5.4 an important (i.e. running on PR) build only config, because of the big overlap between  CUDA-9.2-GCC7 and CUDA-9.2-GCC5.4 test coverage.\nMake CUDA-11 libtorch tests important rather that CUDA-10.2.\n\nAs result of the change, every PR will be built against CUDA-9.2, CUDA-10.2 and CUDA-11 and tested against CUDA-10.2\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43240\n\nReviewed By: ezyang\n\nDifferential Revision: D23205129\n\nPulled By: malfet\n\nfbshipit-source-id: 70932e8b2167cce9fd621115c8bf24b1c81ed621", "pr_number": "43240", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml"], "labels": ["merged"]}, "8094228f26": {"title": "update path in CI script to access ninja (#43236)", "body": "Summary:\nThis relaxes the assumption that test.sh will be run in the CI environment by the CI user.\n\nCC ezyang xw285cornell sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43236\n\nReviewed By: colesbury\n\nDifferential Revision: D23205981\n\nPulled By: ezyang\n\nfbshipit-source-id: 302743cb03c9e9c6bfcdd478a6cd920b536dc29b", "pr_number": "43236", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged", "open source"]}, "27ec91b0c9": {"title": "remove thunk fix now that ROCm CI images are >= ROCm 3.5 (#43226)", "body": "Summary:\nAlso, relax BUILD_ENVIRONMENT exact match to rocm when installing pip packages for tests.\n\nCC ezyang xw285cornell sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43226\n\nReviewed By: colesbury\n\nDifferential Revision: D23200460\n\nPulled By: xw285cornell\n\nfbshipit-source-id: 11cd889cc320d0249d7ebea4da261bfe779e82ac", "pr_number": "43226", "files_changed": [".jenkins/caffe2/test.sh", ".jenkins/pytorch/test.sh"], "labels": ["merged", "module: rocm", "open source"]}, "ab366d0f5f": {"title": "Fix some mistakes in native_functions.yaml (#43156)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43156\n\n- supports_named_tensor no longer does anything, so I have removed\n  it.  I'm guessing these were cargo culted from some old occurrences\n  of it in native_functions.yaml\n\n- comma, not period, in variants\n\nIn my upcoming codegen rewrite, there will be strict error checking\nfor these cases (indeed, that is how I found these problems), so\nI do not add error testing here.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23183977\n\nPulled By: ezyang\n\nfbshipit-source-id: a47d342152badfb8aea248a819ad94fd93dd6ab2", "pr_number": "43156", "files_changed": ["aten/src/ATen/core/NamedRegistrations.cpp", "aten/src/ATen/native/native_functions.yaml"], "labels": ["merged"]}, "fa6b34b54c": {"title": "2 Bit Embedding Conversion Operator support. (#43077)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43077\n\n2 Bit Embedding weight conversion operation is quite similar to\n4 bit embedding weight conversion.\n\nThe diff contains both the\n1. 2bit packing op `embedding_bag_2bit_prepack`.\n2. 2bit unpacking op `embedding_bag_2bit_unpack`.\n\nComments about the op are inline with the op definition.\n\nTest Plan: buck test caffe2/test:quantization -- test_embedding_bag_2bit_unpack\n\nReviewed By: supriyar\n\nDifferential Revision: D23143262\n\nfbshipit-source-id: fd8877f049ac1f7eb4bc580e588dc95f8b1edef0", "pr_number": "43077", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py"], "labels": ["fb-exported", "merged"]}, "06d43dc69a": {"title": "default ice-ref to c-step (#4812)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/glow/pull/4812\n\nif no compilation options are passed, default to c-step\n\nfixed the FC and batchmatmul implementations to match C-step\nfixed the fakelowp map calling to make sure we use the fp32 substitution of operators\nupdated the accumulator test to make it pass with fp32\n\nTest Plan:\nfakelowp tests\nglow/test/numerics\nnet_runner\n\nReviewed By: jfix71\n\nDifferential Revision: D23086534\n\nfbshipit-source-id: 3fbb8c4055bb190becb39ce8cdff6671f8558734", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/fp16_gemm_utils.cc", "caffe2/contrib/fakelowp/test/test_batchmatmul_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_fc_nnpi_fp16.py"], "labels": []}, "bc0e1e8ed2": {"title": "Add dataclasses to base Docker images. (#43217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43217\n\nDataclasses is part of standard library in Python 3.7 and there\nis a backport for it in Python 3.6.  Our code generation will\nstart using it, so add it to the default library set.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D23214028\n\nPulled By: ezyang\n\nfbshipit-source-id: a2ae20b9fa8f0b22966ae48506d4ddea203e7459", "pr_number": "43217", "files_changed": [".circleci/docker/common/install_conda.sh", ".circleci/docker/common/install_travis_python.sh"], "labels": ["merged"]}, "1e248caba8": {"title": "[CircleCI] Use `canary` images until VC++ 14.27 issue is resolved (#43220)", "body": "Summary:\nShould fix binary build issue on Windows, and promptly error out if images are updated to a different version of VC++\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43220\n\nReviewed By: ezyang\n\nDifferential Revision: D23198530\n\nPulled By: malfet\n\nfbshipit-source-id: 0c80361ad7dcfb7aaffccc306b7d741671bedc11", "pr_number": "43220", "files_changed": [".circleci/config.yml", ".circleci/scripts/windows_vs_14_26_install.ps1", ".circleci/verbatim-sources/header-section.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged"]}, "7d10298067": {"title": "Implement Tensor.to batching rule (#43206)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43206\n\nThe batching rule is the same as the unary pointwise batching rules:\ngiven a BatchedTensor, we unwrap it, call Tensor.to, and then re-wrap\nit.\n\nTest Plan: - `pytest test/test_vmap.py -v -k`\n\nReviewed By: ezyang\n\nDifferential Revision: D23189053\n\nPulled By: zou3519\n\nfbshipit-source-id: 51b4e41b1cd34bd082082ec4fff3c643002edbaf", "pr_number": "43206", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "d467ac8ff0": {"title": "[GLOO] handle empty split size (#43256)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43256\n\n* Handle empty split size by moving to call computeLengthsAndOffsets()\n* Enable GLOO alltoall python tests\nghstack-source-id: 109292763\n\nTest Plan:\nbuck build mode/dev-nosan caffe2/torch/lib/c10d:ProcessGroupGlooTest\n\n./trainer_cmd.sh -p 16 -n 8 -d gloo (modify ./trainer_cmd.sh a bit)\n\nReviewed By: mingzhe09088\n\nDifferential Revision: D22961600\n\nfbshipit-source-id: b9e90dadf7b45323b8af2e6cab2e156043b7743b", "pr_number": "43256", "files_changed": ["test/distributed/test_distributed.py", "torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["merged"]}, "eb7fc2e98f": {"title": ".circleci: Simplify binary upload process (#43159)", "body": "Summary:\nBinary uploads were gated into 3 separate scripts making it difficult to\nactually contribute changes, this simplifies that by consolidating all 3\nscripts into one single script and then further consolidates it by\nputting them all into the same job.\n\nThis also further simplifies things by separating upload jobs into their\nown function under binary_build_definitions.py, since following the\nconditional logic tree under the generic function was too difficult.\n\nTesting this change here: https://github.com/pytorch/pytorch/pull/43161\n\nProof of success:\n* [libtorch](https://app.circleci.com/pipelines/github/pytorch/pytorch/201868/workflows/54ce962f-f35b-4d97-93a7-bee186b14ead/jobs/6791347)\n* [conda](https://app.circleci.com/pipelines/github/pytorch/pytorch/201868/workflows/54ce962f-f35b-4d97-93a7-bee186b14ead/jobs/6794359)\n* [manywheel](https://app.circleci.com/pipelines/github/pytorch/pytorch/201868/workflows/54ce962f-f35b-4d97-93a7-bee186b14ead/jobs/6794253)\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43159\n\nReviewed By: malfet\n\nDifferential Revision: D23175174\n\nPulled By: seemethere\n\nfbshipit-source-id: a2de64c033df99b03a124d3a0a2c92560af62c37", "pr_number": "43159", "files_changed": [".circleci/cimodel/data/binary_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/binary_linux_upload.sh", ".circleci/scripts/binary_macos_upload.sh", ".circleci/scripts/binary_upload.sh", ".circleci/scripts/binary_windows_upload.sh", ".circleci/verbatim-sources/commands.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml"], "labels": ["merged", "module: ci"]}, "410d5b95b2": {"title": "[jit] fix str -> Device implicit conversions (#43213)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43213\n\nA reversed isSubtypeOf caused erroreous conversions to be inserted.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23192787\n\nPulled By: zdevito\n\nfbshipit-source-id: 4a90b19d99a4fc889e55568ced850f08dadbc3fe", "pr_number": "43213", "files_changed": ["test/test_jit_py3.py", "torch/csrc/jit/frontend/schema_matching.cpp"], "labels": ["merged", "oncall: jit"]}, "aad1ff9f18": {"title": "[quant][cleanup]test_qlinear_legacy should be under TestDynamicQuantizedLinear. (#40084)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40084\n\nThis is just a nit diff (got merge conflict) while writing some unit-tests.\nThis move was nit as part of D21628596 (https://github.com/pytorch/pytorch/commit/655f1ea176c25dda88e96ffb05e4d84df7d0257d).\n\nTest Plan: buck test test:quantization -- test_qlinear_legacy\n\nReviewed By: supriyar\n\nDifferential Revision: D22065463\n\nfbshipit-source-id: 96ceaa53355349af7157f38b3a6366c550eeec6f", "pr_number": "40084", "files_changed": ["test/quantization/test_quantized_op.py"], "labels": ["fb-exported", "merged"]}, "0617156f0e": {"title": "[vulkan] fix invalid memory op and tests (#43312)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43312\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D23232809\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 11b070b6e082bac72e21dd4c25c9c675bbc8c4a3", "pr_number": "43312", "files_changed": ["aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/test/vulkan_test.cpp"], "labels": ["merged"]}, "97d62bcd19": {"title": "Modify Circle CI script to upload test report for analysis. (#43180)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43180\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D23195934\n\nPulled By: walterddr\n\nfbshipit-source-id: 5b9b411c3ea769951b5b1a456b5f7696b8ba0a92", "pr_number": "43180", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "test/print_test_stats.py"], "labels": ["merged"]}, "6e1127ea3f": {"title": "[NCCL] Changed FutureNCCL's then callback logic for better efficiency. (#42869)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42869\n\nWe realized that when we invoke a simple callback that divides the tensors by `world_size` after `allreduce`, the performance was almost 50% lower in terms of QPS compared to the case where a simple `allreduce` hook is used with no `then` callback.\n\nThe main problem was as we call `work.wait()` before invoking `then` callback, we were synchronizing `work`'s stream with the default PyTorch stream inside [`runHook`](https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp#L609) and stalling the backward computation.\n\nIn that PR, we ensure that FutureNCCL's `then` callback is not stalling the backward computation. Assuming single-process single-device, `FutureNCCL` gets a new stream from device's pool using `at::cuda::getStreamFromPool` to run `callback` and before invoking the `callback` inline it synchronizes `WorkNCCL`'s stream by callback's stream not the default stream.\n\nghstack-source-id: 110208431\n\nTest Plan: Run performance benchmark tests to validate performance issue is resolved. Also, `python test/distributed/test_c10d.py` to avoid any odd issues.\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23055807\n\nfbshipit-source-id: 60e50993f1ed97497514eac5cb1018579ed2a4c5", "pr_number": "42869", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/nn/parallel/distributed.py"], "labels": ["merged", "oncall: jit"]}, "3eb31325fc": {"title": "refactor torch/cuda/nccl.h to remove direct dependency on NCCL in libtorch_python (#42687)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42687\n\nReviewed By: malfet\n\nDifferential Revision: D23145834\n\nPulled By: walterddr\n\nfbshipit-source-id: c703a953a54a638852f6e5a1479ca95ae6a10529", "pr_number": "42687", "files_changed": ["torch/CMakeLists.txt", "torch/csrc/cuda/nccl.cpp", "torch/csrc/cuda/nccl.h", "torch/csrc/cuda/python_nccl.cpp"], "labels": ["merged"]}, "4e964f3b97": {"title": "Make Windows CUDA-11 tests master only (#43234)", "body": "Summary:\nAccording to the correlation analysis, CUDA-10.1 vs CUDA-11 test failures are quite dependent on each other\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43234\n\nReviewed By: ezyang, seemethere\n\nDifferential Revision: D23204289\n\nPulled By: malfet\n\nfbshipit-source-id: c53c5f87e55f2dabbb6735a0566c314c204ebc69", "pr_number": "43234", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml"], "labels": ["merged", "module: build", "triaged"]}, "60b524f271": {"title": "Update torch.Tensor.is_set_to documentation (#43052)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30350\n\nPreview:\n\n![image](https://user-images.githubusercontent.com/5676233/90250018-69d72200-de09-11ea-8984-7401cfd6c719.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43052\n\nReviewed By: mrshenli\n\nDifferential Revision: D23173066\n\nPulled By: suraj813\n\nfbshipit-source-id: d90a11490739068ea448d975548a71e07180bd77", "pr_number": "43052", "files_changed": ["torch/_tensor_docs.py"], "labels": ["merged"]}, "6a09df99e1": {"title": "Fix ASAN error in QNNPACK's integration of qlinear_dynamic. (#41967)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41967\n\nTest Plan: `buck test fbandroid/mode/asan xplat/assistant/oacr/nlu/tests:nlu_testsAndroid` no longer reports an error.\n\nReviewed By: kimishpatel, xuwenfang\n\nDifferential Revision: D22715307\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: bec7296b345125ec5243ee6e6c484246ecfca3b7", "pr_number": "41967", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack_utils.h", "aten/src/ATen/native/utils/Factory.cpp", "aten/src/ATen/native/utils/Factory.h", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Factory.cpp", "aten/src/ATen/native/xnnpack/Factory.h", "aten/src/ATen/native/xnnpack/Linear.cpp", "aten/src/ATen/native/xnnpack/MaxPooling.cpp"], "labels": ["merged"]}, "a2ae2d3203": {"title": "Nightly Pull (#43294)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/40829\n\nThis addresses remaining issues/improvements in https://github.com/pytorch/pytorch/issues/40829 that were brought up prior to https://github.com/pytorch/pytorch/issues/42635 being merged.  Namely, this changes the name of the script and adds separate `checkout` and `pull` subcommands. I have tested it locally and everything appears to work.  Please let me know if you encounter any issues. I hope that this supports a more natural workflow.\n\nCC ezyang rgommers\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43294\n\nReviewed By: pbelevich\n\nDifferential Revision: D23241849\n\nPulled By: ezyang\n\nfbshipit-source-id: c24556024d7e5d14b9a5006e927819d4ad370dd7", "pr_number": "43294", "files_changed": ["CONTRIBUTING.md", "tools/nightly.py", "tools/nightly_checkout.py"], "labels": ["merged", "open source"]}, "f9a766bb39": {"title": "Increase deadline time for load_save tests (#43205)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43205\n\nA number of tests that forward to `TestLoadSaveBase.load_save` are all marked as flaky due to them regularly taking much longer to start up than hypothesis' default timeout of 200ms. This diff fixes the problem by removing the timeout for `load_save`. This is alright as these tests aren't meant to be testing the performance of these operators.\n\nI would set the deadline to 60s if I could however it appears the that caffe2 github CI uses a different version of hypothesis that doesn't allow using `dateutil.timedelta` so instead of trying to figure out an approach that works on both I've just removed the deadline time.\n\nI've also tagged all existing tasks WRT these failures.\n\nDifferential Revision: D23175752\n\nfbshipit-source-id: 324f9ff034df1ac4874797f04f50067149a6ba48", "pr_number": "43205", "files_changed": ["caffe2/python/operator_test/load_save_test.py"], "labels": ["fb-exported", "merged"]}, "397325a109": {"title": "Make _compute_linear_combination.out a true out function (#43272)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43272\n\nWas missing kwarg-onlyness.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D23215506\n\nPulled By: ezyang\n\nfbshipit-source-id: 2c282c9a534fa8ea1825c31a24cb2441f0d6b234", "pr_number": "43272", "files_changed": ["aten/src/ATen/native/FunctionOfAMatrixUtils.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "66a79bf114": {"title": ".circleci: Don't quote glob for conda upload (#43297)", "body": "Summary:\nGlobs don't get expanded if you quote them in a bash script...\napparently.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43297\n\nReviewed By: malfet\n\nDifferential Revision: D23227626\n\nPulled By: seemethere\n\nfbshipit-source-id: d124025cfcaacbfb68167a062ca487c08f7f6bc9", "pr_number": "43297", "files_changed": [".circleci/scripts/binary_upload.sh"], "labels": ["merged", "module: ci", "releng"]}, "ca9d4401d4": {"title": ".circleci: Remove manual docker installation (#43277)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43277\n\nDocker added native support for GPUs with the release of 19.03 and\nCircleCI's infrastructure is all on Docker 19.03 as of now.\n\nThis also removes all references to `nvidia-docker` in the `.circleci` fodler.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23217570\n\nPulled By: seemethere\n\nfbshipit-source-id: af297c7e82bf264252f8ead10d1a154354b24689", "pr_number": "43277", "files_changed": [".circleci/README.md", ".circleci/config.yml", ".circleci/scripts/binary_run_in_docker.sh", ".circleci/scripts/setup_ci_environment.sh", ".circleci/verbatim-sources/commands.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs/binary_update_htmls.yml", ".circleci/verbatim-sources/job-specs/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs/docker_jobs.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged", "module: ci"]}, "c8bc298d6c": {"title": "streamline stride propagation logic in TensorIterator (#42922)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41314 among other things.\nThis PR streamlines layout propagation logic in TensorIterator and removes almost all cases of channels-last hardcoding. The new rules and changes are as follows:\n1) behavior of undefined `output` and defined output of the wrong (e.g. 0) size is always the same (before this PR the behavior was divergent)\n2) in obvious cases (unary operation on memory-dense tensors, binary operations on memory-dense tensors with the same layout) strides are propagated (before propagation was inconsistent) (see footnote)\n3) in other cases the output permutation is obtained as inverse permutation of sorting inputs by strides. Sorting is done with comparator obeying the following rules: strides of broadcasted dimensions are set to 0, and 0 compares equal to anything. Strides of not-broadcasted dimensions (including dimensions of size `1`) participate in sorting. Precedence is given to the first input, in case of a tie in the first input, first the corresponding dimensions are considered, and if that does not indicate that swap is needed, strides of the same dimension in subsequent inputs are considered. See changes in `reorder_dimensions` and `compute_strides`. Note that first inspecting dimensions of the first input allows us to better recover it's permutation (and we select this behavior because it more reliably propagates channels-last strides) but in some rare cases could result in worse traversal order for the second tensor.\n\nThese rules are enough to recover previously hard-coded behavior related to channels last, so all existing tests are passing.\nIn general, these rules will produce intuitive results, and in most cases permutation of the full size input (in case of broadcasted operation) will be recovered, or permutation of the first input (in case of same sized inputs) will be recovered, including cases with trivial (1) dimensions. As an example of the latter, the following tensor\n```\nx=torch.randn(2,1,3).permute(1,0,2)\n```\nwill produce output with the same stride (3,3,1) in binary operations with 1d tensor. Another example is a tensor of size N1H1 that has strides `H,H,1,1` when contiguous and `H, 1, 1, 1` when channels-last. The output retains these strides in binary operations when another 1d tensor is broadcasted on this one.\n\nFootnote: for ambiguous cases where all inputs are memory dense and have the same physical layout that nevertheless can correspond to different permutations, such as e.g. NC11-sized physically contiguous tensors, regular contiguous tensor is returned, and thus permutation information of the input is lost (so for NC11 channels-last input had the strides `C, 1, C, C`, but output will have the strides `C, 1, 1, 1`). This behavior is unchanged from before and consistent with numpy, but it still makes sense to change it. The blocker for doing it currently is performance of `empty_strided`. Once we make it on par with `empty` we should be able to propagate layouts in these cases. For now, to not slow down common contiguous case, we default to contiguous.\nThe table below shows how in some cases current behavior loses permutation/stride information, whereas new behavior propagates permutation.\n| code                                                                                                                                                                                           | old                                                   | new                                                  |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|------------------------------------------------------|\n| #strided tensors<br>a=torch.randn(2,3,8)[:,:,::2].permute(2,0,1)<br>print(a.stride())<br>print(a.exp().stride())<br>print((a+a).stride())<br>out = torch.empty(0)<br>torch.add(a,a,out=out)<br>print(out.stride()) | (2, 24, 8) <br>(6, 3, 1) <br>(1, 12, 4) <br>(6, 3, 1) | (2, 24, 8)<br>(1, 12, 4)<br>(1, 12, 4)<br>(1, 12, 4) |\n| #memory dense tensors<br>a=torch.randn(3,1,1).as_strided((3,1,1), (1,3,3))<br>print(a.stride(), (a+torch.randn(1)).stride())<br>a=torch.randn(2,3,4).permute(2,0,1)<br>print(a.stride())<br>print(a.exp().stride())<br>print((a+a).stride())<br>out = torch.empty(0)<br>torch.add(a,a,out=out)<br>print(out.stride())                                                                                                                                                                                               |  (1, 3, 3) (1, 1, 1)<br>(1, 12, 4)<br>(6, 3, 1)<br>(1, 12, 4)<br>(6, 3, 1)                                                       |  (1, 3, 3) (1, 3, 3)<br>(1, 12, 4)<br>(1, 12, 4)<br>(1, 12, 4)<br>(1, 12, 4) |\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42922\n\nReviewed By: ezyang\n\nDifferential Revision: D23148204\n\nPulled By: ngimel\n\nfbshipit-source-id: 670fb6188c7288e506e5ee488a0e11efc8442d1f", "pr_number": "42922", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "test/test_torch.py"], "labels": ["merged", "topic: bc-breaking"]}, "4fc9e958c4": {"title": "[quant] Add benchmakrs for embedding_bag coversion ops (#43291)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43291\n\nTest Float2Fused and Fused2Float conversion operators for embedding_bag byte and 4-bit ops\n\nTest Plan:\n```\npython -m pt.qembedding_pack_tes\n```\n\nImported from OSS\n\nReviewed By: radkris-git\n\nDifferential Revision: D23231641\n\nfbshipit-source-id: a2afe51bba52980d2e96dfd7dbc183327e9349fd", "pr_number": "43291", "files_changed": ["benchmarks/operator_benchmark/benchmark_all_quantized_test.py", "benchmarks/operator_benchmark/pt/qembedding_pack_test.py"], "labels": ["merged"]}, "0dc41ff465": {"title": "[pytorch] add flag for autograd ops to mobile builds (#43154)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43154\n\nAdds the build flag `BUILD_MOBILE_AUTOGRAD` which toggles whether autograd files should be included for a PyTorch mobile build (default off).\nghstack-source-id: 110369406\n\nTest Plan: CI\n\nReviewed By: ljk53\n\nDifferential Revision: D23061913\n\nfbshipit-source-id: bc3d6683ab17f158990d83e4fae0a011d5adeca1", "pr_number": "43154", "files_changed": ["CMakeLists.txt", "cmake/Summary.cmake"], "labels": ["merged"]}, "c66ca7a48d": {"title": "vmap: Fix bug with x * 0.1 (#43218)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43218\n\nPreviously, `vmap(lambda x: x * 0.1)(torch.ones(3))` would return a\nfloat64 tensor(!!). This is because there is a subtle bug in the\nbatching rule: the batching rule receives:\n- A batched tensor for x\n- a scalar tensor: tensor(0.1, dtype=torch.float64).\nThe batching rule decides to expand the scalar tensor to be the same\nsize as x and then multiplies the two tensors, promoting the output to\nbe a float64 tensor. However, this isn't correct: we should treat the\nscalar tensor like a scalar tensor. When adding a FloatTensor to a\nDouble scalar tensor, we don't promote the type usually.\n\nAnother example of a bug this PR fixes is the following:\n`vmap(torch.mul)(torch.ones(3), torch.ones(3, dtype=torch.float64))`\nMultiplying a scalar float tensor with a scalar double tensor produces a\nfloat tensor, but the above produced a float64 before this PR due to\nmistakingly type-promoting the tensors.\n\nTest Plan:\n- new test: `pytest test/test_vmap.py -v`\n- I refactored some tests a bit.\n\nReviewed By: cpuhrsch\n\nDifferential Revision: D23195418\n\nPulled By: zou3519\n\nfbshipit-source-id: 33b7da841e55b47352405839f1f9445c4e0bc721", "pr_number": "43218", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "d0a6819b0e": {"title": "[ROCm] skip test_rpc in .jenkins/pytorch/test.sh (#43305)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/42636 added test_rpc, but this test binary is not built for ROCm.  Skip this test for ROCm builds.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43305\n\nReviewed By: pbelevich\n\nDifferential Revision: D23233087\n\nPulled By: mrshenli\n\nfbshipit-source-id: 29cd81e88a543c922a988e09d5f789becf4b74e4", "pr_number": "43305", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged", "module: rocm", "open source"]}, "5006d24302": {"title": "Make TensorPipe the default backend for RPC (#43246)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43246\n\nTest Plan: Imported from OSS\n\nReviewed By: osalpekar\n\nDifferential Revision: D23206042\n\nPulled By: osalpekar\n\nfbshipit-source-id: 258481ea9e753cd36c2787183827ca3b81d678e3", "pr_number": "43246", "files_changed": ["docs/source/rpc.rst", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/backend_registry.py"], "labels": ["merged"]}, "a12fe1a242": {"title": "Minor RPC doc fixes (#43337)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43337\n\nTest Plan: Imported from OSS\n\nReviewed By: osalpekar\n\nDifferential Revision: D23242698\n\nPulled By: osalpekar\n\nfbshipit-source-id: 7757fc43824423e3a6efd4da44c69995f64a6015", "pr_number": "43337", "files_changed": ["docs/source/rpc.rst", "torch/csrc/distributed/autograd/init.cpp", "torch/distributed/rpc/options.py"], "labels": ["merged"]}, "c89d2c6bf2": {"title": "Replace black_list with block_list (#42088)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41735\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42088\n\nReviewed By: pbelevich\n\nDifferential Revision: D22794582\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: e256353befefa2630b99f9bcf0b79df3a7a8dcbd", "pr_number": "42088", "files_changed": ["caffe2/contrib/fakelowp/test/test_int8_quant.py", "caffe2/python/onnx/onnxifi.py"], "labels": ["merged", "open source", "triaged"]}, "dae2973fae": {"title": "[quant][graphmode][fx] Add graph mode quantization on fx (#43175)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43175\n\nThis PR added graph mode quantization on fx: https://github.com/pytorch/pytorch/pull/42741\nCurrently it matches eager mode quantization for torchvision with static/dynamic/qat\nddp/synbn test is still wip\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFx\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23178602\n\nfbshipit-source-id: 8e7e0322846fbda2cfa79ad188abd7235326f879", "pr_number": "43175", "files_changed": ["mypy.ini", "test/quantization/test_quantize_fx.py", "test/test_quantization.py", "torch/quantization/_quantize_fx.py", "torch/quantization/fx/__init__.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/pattern_utils.py", "torch/quantization/fx/quantize.py", "torch/quantization/fx/utils.py"], "labels": ["merged"]}, "51bab0877d": {"title": "Fix torch.hub for new zipfile format. (#42333)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42239\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42333\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D23215210\n\nPulled By: ailzhang\n\nfbshipit-source-id: 161ead8b457c11655dd2cab5eecfd0edf7ae5c2b", "pr_number": "42333", "files_changed": ["test/test_utils.py", "torch/hub.py"], "labels": ["merged"]}, "97d594b9f7": {"title": "Make grad point to bucket buffer in DDP to save memory usage (#41954)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41954\nMake both variable.grad() and grad in distautograd context point to bucket buffer in DDP to save memory usage.\nIn this case, grad will be view of bucket buffer tensors, in order to make it compatiable with optimizer.zero_grad(), we\nmade changes in https://github.com/pytorch/pytorch/pull/41283.\n\nAlso be noted that we can not make variable.grad() pointing to bucket buffer during construction time, because we want to\nkeep grad undefined for unused parameters.\nghstack-source-id: 110260297\n\nTest Plan:\nunit tests,\n\nFor roberta_base model with ~1GB parameters, peak memory dropped ~1GB (8250MB-7183MB).  Per iteration latency (0.982s ->0.909s), 8% speed up\nhttps://www.internalfb.com/intern/fblearner/details/211713882?tab=operator_details\nhttps://www.internalfb.com/intern/fblearner/details/211772923?tab=operator_details\n\nFor resnet model with ~97M parameters, peak memory dropped ~100MB (3089MB -> 2988MB). Per iteration latency has no change (0.122s -> 0.123s)\nhttps://www.internalfb.com/intern/fblearner/details/211713577?tab=operator_details\nhttps://www.internalfb.com/intern/fblearner/details/211712582?tab=operator_details\n\naccuracy benchmark is expected as well\nhttps://www.internalfb.com/intern/fblearner/details/213237067?tab=Outputs\n\nReviewed By: mrshenli\n\nDifferential Revision: D22707857\n\nfbshipit-source-id: b5e767cfb34ccb3d067db2735482a86d59aea7a4", "pr_number": "41954", "files_changed": ["test/distributed/test_distributed.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/functions/accumulate_grad.h", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/nn/parallel/distributed.py"], "labels": ["merged"]}, "2b7108a96f": {"title": "Update hardcoded pytorch_android_gradle_custom_build_single hash (#43340)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43340\n\nThis doesn't fix https://github.com/pytorch/pytorch/issues/43338 but\nit gets us a little more up to date.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: seemethere\n\nDifferential Revision: D23243933\n\nPulled By: ezyang\n\nfbshipit-source-id: ce2773c55864d1a6f6628ba60bb9ad6aee4aba14", "pr_number": "43340", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["merged"]}, "b0ec336477": {"title": "[quant][graphmode][fx][test] Add per op test for graph mode quant on fx (#43229)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43229\n\nTest Plan: Imported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23201692\n\nfbshipit-source-id: 37fa54dcf0a9d5029f1101e11bfd4ca45b422641", "pr_number": "43229", "files_changed": ["test/quantization/test_quantize_fx.py", "test/test_quantization.py", "torch/quantization/_quantize_fx.py", "torch/quantization/fx/__init__.py", "torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "e10aa47615": {"title": "Fix `at::native::view_as_real()` for ComplexHalf Tensors (#43279)", "body": "Summary:\nAdd ComplexHalf case to toValueType, which fixes the logic how view_as_real and view_as_complex slices complex tensor to the floating point one, as it is used to generate tensor of random complex values, see:\nhttps://github.com/pytorch/pytorch/blob/018b4d7abb894c3de21f9f10b0678c80a7b0a701/aten/src/ATen/native/DistributionTemplates.h#L200\nAlso add ability to convert python complex object to `c10::complex<at::Half>`\n\nAdd `torch.half` and `torch.complex32` to the list of `test_randn` dtypes\n\nFixes https://github.com/pytorch/pytorch/issues/43143\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43279\n\nReviewed By: mrshenli\n\nDifferential Revision: D23230296\n\nPulled By: malfet\n\nfbshipit-source-id: b4bb66c4c81dd867e72ab7c4563d73f6a4d80a44", "pr_number": "43279", "files_changed": ["aten/src/ATen/native/ComplexHelper.h", "c10/core/ScalarType.h", "c10/util/Half.h", "test/test_torch.py", "torch/csrc/utils/python_scalars.h"], "labels": ["merged", "module: complex", "module: half", "topic: crash", "triaged"]}, "9a1f2b3617": {"title": ".circleci: Use dynamic docker image for android (#43356)", "body": "Summary:\nWe recently upgraded to a dynamic docker image and this android build\njob was missed during that transition\n\nFixes https://github.com/pytorch/pytorch/issues/43338\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43356\n\nReviewed By: pbelevich\n\nDifferential Revision: D23253175\n\nPulled By: seemethere\n\nfbshipit-source-id: 4831d4fe554a126e202e788444a63516d34b3d72", "pr_number": "43356", "files_changed": [".circleci/cimodel/data/simple/android_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["merged", "module: ci"]}, "5cf8592663": {"title": "Fix backward compatibility test (#43371)", "body": "Summary:\nDrop `.out` suffix from allow_list pattern added by https://github.com/pytorch/pytorch/issues/43272\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43371\n\nReviewed By: pbelevich\n\nDifferential Revision: D23256914\n\nPulled By: malfet\n\nfbshipit-source-id: 10168b55b98c24c84ac2676963049d1eca5c182d", "pr_number": "43371", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "ad8294d35b": {"title": "[vulkan][ci] Vulkan tests running on linux build via swiftshader (added to docker) (#42614)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42614\n\nVulkan backend linux build (USE_VULKAN=1) and running Vulkan tests using software Vulkan implementation via [swiftshader](https://github.com/google/swiftshader)\n\nVulkan linux build needs VulkanSdk and running tests needs Swiftshader.\nswiftshader needs to be compiled using clang toolchain, added them to bionic-clang-9 docker image.\n\nVulkanSdk will be downloaded from aws;\nSwiftshader is cloned from github, as it has many submodules , commit hash is fixed in install_swiftshader script.\n\nTo pass all the tests:\nDisabled adaptive_avg_pool2d_2 as it needs at::view which will be landed in https://github.com/pytorch/pytorch/pull/42676 and after that can be enabled\n\nChange strides, padding, dilation params in tests to vector\n\nDocker image rebuild:\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/200251/workflows/465f911f-f170-47e1-954e-b9605d91abd8/jobs/6700311\nVulkan Linux Build:\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/200251/workflows/465f911f-f170-47e1-954e-b9605d91abd8/jobs/6701604\nVulkan Linux Test:\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/200251/workflows/465f911f-f170-47e1-954e-b9605d91abd8/jobs/6703026\n\nTest Plan: Imported from OSS\n\nReviewed By: seemethere\n\nDifferential Revision: D23174038\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 431c72e31743ca0c0b82a497420f6330a311b35b", "pr_number": "42614", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_swiftshader.sh", ".circleci/docker/common/install_vulkan_sdk.sh", ".circleci/docker/ubuntu/Dockerfile", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "e32d014f46": {"title": "remove empty override pretty_print (#43341)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43341\n\nThis is to remove the empty pretty_print() since it overrides the impl within Module base which is not as designed here.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D23244616\n\nPulled By: glaringlee\n\nfbshipit-source-id: 94b8dfd3697dfc450f53b3b4eee6e9c13cafba7b", "pr_number": "43341", "files_changed": ["test/cpp/api/transformer.cpp", "torch/csrc/api/include/torch/nn/modules/transformerlayer.h", "torch/csrc/api/src/nn/modules/transformer.cpp"], "labels": ["merged"]}, "7c50c2f79e": {"title": "Reimplement per-operator selective build (#39401)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39401\n\nThis uses the technique proposed by smessmer in D16451848 to selectively\nregister operators without codegen.  See the Note inside for more\ndetails.\n\nThis PR has feature parity with the old selective build apparatus:\nit can whitelist schema def()s, impl()s, and on a per dispatch key\nbasis.  It has expanded dispatch key whitelisting, whereas previously\nmanually written registrations were not whitelisted at all.  (This\nmeans we may be dropping dispatch keys where we weren't previously!)\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D21905593\n\nPulled By: ezyang\n\nfbshipit-source-id: d4870f800c66be5ce57ec173c9b6e14a52c4a48b", "pr_number": "39401", "files_changed": ["aten/src/ATen/core/op_registration/op_whitelist.h", "aten/src/ATen/core/op_registration/op_whitelist_test.cpp", "aten/src/ATen/core/operator_name.h", "tools/code_analyzer/op_deps_pass.cpp", "torch/library.h"], "labels": ["merged"]}, "9984d33542": {"title": "[quant][graphmode][fx] Add support for conv module (#43285)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43285\n\nPorting op tests from test_quantize_jit.py\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23221733\n\nfbshipit-source-id: c1f0f7ae0c82379143aa33fc1af7284d8303174b", "pr_number": "43285", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "844d469ae7": {"title": "Remove proprietary notices", "body": "Summary:\nThese were added accidentally (probably by an IDE) during a refactor.\nThese files have always been Open Source.\n\nTest Plan: CI\n\nReviewed By: xcheng16\n\nDifferential Revision: D23250761\n\nfbshipit-source-id: 4974430c0e28dd3269424d38edb36f4f71508157", "pr_number": null, "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/INativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/LiteModuleLoader.java", "android/pytorch_android/src/main/java/org/pytorch/LiteNativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/NativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/PyTorchCodegenLoader.java"], "labels": []}, "217ddea93a": {"title": "[quant] Make OP_LIST_TO_FUSER_METHOD public (#43286)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43286\n\nWe need to use this in graph mode quantization on fx\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23221734\n\nfbshipit-source-id: 7c3c3840ce5bdc185b962e081aff1618f4c58e85", "pr_number": "43286", "files_changed": ["torch/quantization/fuse_modules.py"], "labels": ["merged"]}, "e8139624f2": {"title": "Search on system path for Vulkan headers and libraries as a last resort. (#43301)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43301\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252338\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 8eefe98eedf9dbeb570565bfb13ab61b1d6bca0e", "pr_number": "43301", "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.cpp", "cmake/VulkanCodegen.cmake", "cmake/VulkanDependencies.cmake"], "labels": ["merged"]}, "665da61d2b": {"title": "Replace Conv1d with Conv2d (#42867)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42867\n\nTest Plan: Imported from OSS\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23177916\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 68cc40cf42d03e5b8432dc08f9933a4409c76e25", "pr_number": "42867", "files_changed": ["test/test_xnnpack_integration.py", "torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.h", "torch/csrc/jit/python/init.cpp"], "labels": ["merged", "oncall: jit"]}, "17f9edda42": {"title": "Bias Correction Implementation (#41845)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41845\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22661503\n\nPulled By: edmundw314\n\nfbshipit-source-id: a88c349c6cc15b1c66aa6dee7593ef3df588eb85", "pr_number": "41845", "files_changed": ["test/quantization/test_bias_correction.py", "test/test_quantization.py", "torch/quantization/_correct_bias.py"], "labels": ["merged"]}, "e31cd46278": {"title": "Add alias torch.fix for torch.trunc to be compatible with NumPy. (#43326)", "body": "Summary:\nxref https://github.com/pytorch/pytorch/issues/42515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43326\n\nReviewed By: pbelevich\n\nDifferential Revision: D23249089\n\nPulled By: mruberry\n\nfbshipit-source-id: 6afa9eb20493983d084e0676022c6245e7463e05", "pr_number": "43326", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_op_aliases.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py"], "labels": ["merged", "oncall: jit", "open source"]}, "c64594f5cc": {"title": "Extends test_unary_ufunc.py with numerics, contiguity, domain tests (#42965)", "body": "Summary:\nThis PR:\n\n- ports the tests in TestTorchMathOps to test_unary_ufuncs.py\n- removes duplicative tests for the tested unary ufuncs from test_torch.py\n- adds a new test, test_reference_numerics, that validates the behavior of our unary ufuncs vs. reference implementations on empty, scalar, 1D, and 2D tensors that are contiguous, discontiguous, and that contain extremal values, for every dtype the unary ufunc supports\n- adds support for skipping tests by regex, this behavior is used to make the test suite pass on Windows, MacOS, and ROCm builds, which have a variety of issues, and on Linux builds (see https://github.com/pytorch/pytorch/issues/42952)\n- adds a new OpInfo helper, `supports_dtype`, to facilitate test writing\n- extends unary ufunc op info to include reference, domain, and extremal value handling information\n- adds OpInfos for `torch.acos` and `torch.sin`\n\nThese improvements reveal that our testing has been incomplete on several systems, especially with larger float values and complex values, and several TODOs have been added for follow-up investigations. Luckily when writing tests that cover many ops we can afford to spend additional time crafting the tests and ensuring coverage.\n\nFollow-up PRs will:\n\n- refactor TestTorchMathOps into test_unary_ufuncs.py\n- continue porting tests from test_torch.py to test_unary_ufuncs.py (where appropriate)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42965\n\nReviewed By: pbelevich\n\nDifferential Revision: D23238083\n\nPulled By: mruberry\n\nfbshipit-source-id: c6be317551453aaebae9d144f4ef472f0b3d08eb", "pr_number": "42965", "files_changed": ["test/test_torch.py", "test/test_unary_ufuncs.py", "torch/testing/__init__.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["merged"]}, "da70976e66": {"title": "[ONNX] Add support for operator `add` between tensor list (#41888)", "body": "Summary:\nE.g.\n```python\nouts = []\nouts += [torch.randn(3,4)]\nouts = outs + [torch.randn(4,5), torch.randn(5,6)]\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41888\n\nReviewed By: houseroad\n\nDifferential Revision: D23172880\n\nPulled By: bzinodev\n\nfbshipit-source-id: 93865106e3de5908a993e0cfa82f626ba94dab7e", "pr_number": "41888", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "da036250cd": {"title": "Add benchmark for performance comparison (#43221)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43221\n\nTest Plan: Example: https://www.internalfb.com/intern/paste/P139226521/\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23197567\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 7d0f8e653c62f0bee5795618e712d07effbd460a", "pr_number": "43221", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/benchmarks/stateful_conv1d.cpp"], "labels": ["merged"]}, "0cb52cb458": {"title": "Autograd better error (#43308)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/5025\n\nThanks for the conversation in the issue thread. Hopefully this must fix it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43308\n\nReviewed By: ezyang\n\nDifferential Revision: D23241918\n\nPulled By: suraj813\n\nfbshipit-source-id: e1efac13f5ce590196f227149f011c973c2bbdde", "pr_number": "43308", "files_changed": ["torch/csrc/autograd/python_function.cpp"], "labels": ["merged", "open source"]}, "c2511bdfa4": {"title": "Implement first draft of autograd benchmark. (#40586)", "body": "Summary:\nIt is quite a lot of code because I pulled some code from torchaudio and torchvision to remove issues I had to get latest version with pytorch built from source while I can't build there libs from source (dependency missing for torchaudio).\n\nThe compare script generates table as follows:\n| model | task | speedup | mean (before) | var (before) | mean (after) | var (after) |\n| -- | -- | -- | -- | -- | -- | -- |\n| resnet18 | vjp | 1.021151844124464 | 1.5627719163894653 | 0.005164200905710459 | 1.5304011106491089 | 0.003979875706136227 |\n| resnet18 | vhp | 0.9919114430761606 | 6.8089728355407715 | 0.019538333639502525 | 6.86449670791626 | 0.014775685034692287 |\n| resnet18 | jvp | 0.9715963084255123 | 5.720699310302734 | 0.08197150379419327 | 5.887938499450684 | 0.018408503383398056 |\n| ppl_simple_reg | vjp | 0.9529183269165618 | 0.000362396240234375 | 7.526952949810095e-10 | 0.00038030146970413625 | 7.726220357939795e-11 |\n| ppl_simple_reg | vhp | 0.9317708619586977 | 0.00048058031825348735 | 5.035701855504726e-10 | 0.0005157709238119423 | 3.250243477137538e-11 |\n| ppl_simple_reg | jvp | 0.8609755877018406 | 0.00045447348384186625 | 9.646707044286273e-11 | 0.0005278587341308594 | 1.4493808930815533e-10 |\n| ppl_simple_reg | hvp | 0.9764100147808232 | 0.0005881547695025802 | 7.618464747949361e-10 | 0.0006023645401000977 | 6.370915461850757e-10 |\n| ppl_simple_reg | jacobian | 1.0019173715134297 | 0.0003612995205912739 | 2.2979899233499523e-11 | 0.0003606081008911133 | 1.2609764794835332e-11 |\n| ppl_simple_reg | hessian | 1.0358429970264393 | 0.00206911563873291 | 2.590938796842579e-09 | 0.0019975185859948397 | 2.8916853356264482e-09 |\n| ppl_robust_reg | vjp | 1.0669910916521521 | 0.0017304659122601151 | 3.1047047155396967e-09 | 0.0016218185191974044 | 4.926861585374809e-09 |\n| ppl_robust_reg | vhp | 1.0181130455462972 | 0.0029563189018517733 | 2.6359153082466946e-08 | 0.0029037236236035824 | 1.020585038702393e-08 |\n| ppl_robust_reg | jvp | 0.9818360373406179 | 0.0026934861671179533 | 6.981357714153091e-09 | 0.00274331565015018 | 3.589908459389335e-08 |\n| ppl_robust_reg | hvp | 1.0270848910527002 | 0.005576515104621649 | 3.2798087801211295e-08 | 0.005429458804428577 | 6.438724398094564e-08 |\n| ppl_robust_reg | jacobian | 1.0543611284155785 | 0.00167675013653934 | 2.3236829349571053e-08 | 0.001590299652889371 | 1.2011492245278532e-08 |\n| ppl_robust_reg | hessian | 1.0535378727082656 | 0.01643357239663601 | 1.8450685956850066e-06 | 0.015598463825881481 | 2.1876705602608126e-07 |\n| wav2letter | vjp | 1.0060408105086573 | 0.3516994118690491 | 1.4463969819189515e-05 | 0.349587619304657 | 9.897866402752697e-05 |\n| wav2letter | vhp | 0.9873655295086051 | 1.1196287870407104 | 0.00474404776468873 | 1.133955717086792 | 0.009759620763361454 |\n| wav2letter | jvp | 0.9741820317882822 | 0.7888165712356567 | 0.0017476462526246905 | 0.8097219467163086 | 0.0018235758179798722 |\n| transfo | vjp | 0.9883954031921641 | 2.8865864276885986 | 0.008410997688770294 | 2.9204773902893066 | 0.006901870481669903 |\n| transfo | vhp | 1.0111290842971339 | 8.374398231506348 | 0.014904373325407505 | 8.282224655151367 | 0.04449500888586044 |\n| transfo | jvp | 1.0080534543381963 | 6.293097972869873 | 0.03796082362532616 | 6.24282169342041 | 0.010179692879319191 |\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40586\n\nReviewed By: pbelevich\n\nDifferential Revision: D23242101\n\nPulled By: albanD\n\nfbshipit-source-id: a2b92d5a4341fe1472711a685ca425ec257d6384", "pr_number": "40586", "files_changed": ["benchmarks/functional_autograd_benchmark/README.md", "benchmarks/functional_autograd_benchmark/audio_text_models.py", "benchmarks/functional_autograd_benchmark/compare.py", "benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py", "benchmarks/functional_autograd_benchmark/ppl_models.py", "benchmarks/functional_autograd_benchmark/torchaudio_models.py", "benchmarks/functional_autograd_benchmark/torchvision_models.py", "benchmarks/functional_autograd_benchmark/utils.py", "benchmarks/functional_autograd_benchmark/vision_models.py", "test/run_test.py", "test/test_functional_autograd_benchmark.py"], "labels": ["merged"]}, "7b520297dc": {"title": "Remove erroneous trailing backslashes (#43318)", "body": "Summary:\nThey were likely copied from some macro definition, but they do not\nbelong to macro definitions here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43318\n\nReviewed By: pbelevich\n\nDifferential Revision: D23241526\n\nPulled By: mrshenli\n\nfbshipit-source-id: e0b5eddfde2c882bb67f56d84ee79281cc5fc941", "pr_number": "43318", "files_changed": ["c10/core/ScalarType.h"], "labels": ["merge-this-please", "merged", "open source"]}, "f4b6ef9c56": {"title": "Do not define the macro \"isnan\" (#43242)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43242\n\nThis causes \"std::isnan\" to produce confusing error messages (std::std has not been declared).\nInstead, simply let isnan be exposed in the global namespace.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D23214374\n\nPulled By: ezyang\n\nfbshipit-source-id: 9615116a980340e36376a20f2e546e4d36839d4b", "pr_number": "43242", "files_changed": ["aten/src/ATen/native/Distributions.h"], "labels": ["merged", "open source"]}, "0bd35de30e": {"title": "Add Enum convert back to Python object support (#43121)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43121\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23222628\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 6850c56ced5b52943a47f627b2d1963cc9239408", "pr_number": "43121", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue_inl.h", "test/jit/test_enum.py", "torch/csrc/jit/python/pybind_utils.h", "torch/jit/annotations.py"], "labels": ["merged", "oncall: jit"]}, "478fb925e6": {"title": "[jit] PyTorchStreamReader::getAllRecord should omit archive name prefix (#43317)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43317\n\nPrevious version was returning the path with a prefix so subsequent `getRecord` would fail.\n\nThere's only one place in PyTorch codebase that uses this function (introduced in https://github.com/pytorch/pytorch/pull/29339 ) and it's unlikely that anyone else is using it - it's not a public API anyway.\n\nTest Plan: unittest\n\nReviewed By: houseroad\n\nDifferential Revision: D23235241\n\nfbshipit-source-id: 6f7363e6981623aa96320f5e39c54e65d716240b", "pr_number": "43317", "files_changed": ["caffe2/serialize/inline_container.cc", "torch/serialization.py"], "labels": ["fb-exported", "merged"]}, "3aec1185e0": {"title": "Enables bfloat16 x [float16, complex64, complex128] type promotion (#43324)", "body": "Summary:\nImplements bfloat16 type promotion consistent with JAX (see https://jax.readthedocs.io/en/latest/type_promotion.html), addressing issue https://github.com/pytorch/pytorch/issues/43049.\n\n- bfloat16 x float16 -> float32\n- bfloat16 x complex64 -> complex64\n- bfloat16 x complex128 -> complex128\n\nExisting tests, after updates, are sufficient to validate the new behavior.\n\ncc xuhdev\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43324\n\nReviewed By: albanD\n\nDifferential Revision: D23259823\n\nPulled By: mruberry\n\nfbshipit-source-id: ca9c2c7d0325faced1f884f3c37edf8fa8c8b089", "pr_number": "43324", "files_changed": ["c10/core/ScalarType.h", "test/test_torch.py", "test/test_type_promotion.py"], "labels": ["merged"]}, "e57b89c8dc": {"title": "Adds arccos, arcsin, arctan aliases (#43319)", "body": "Summary:\nThese aliases are consistent with NumPy (see, for example, https://numpy.org/doc/stable/reference/generated/numpy.arccos.html?highlight=acos).\n\nNote that PyTorch's existing names are consistent with Python (see https://docs.python.org/3.10/library/math.html?highlight=acos#math.acos) and C++ (see, for example, https://en.cppreference.com/w/cpp/numeric/math/acos).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43319\n\nReviewed By: pbelevich\n\nDifferential Revision: D23260426\n\nPulled By: mruberry\n\nfbshipit-source-id: 98a6c97f69d1f718a396c2182e938a7a260c0889", "pr_number": "43319", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_op_aliases.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py"], "labels": ["merged", "module: numpy", "oncall: jit"]}, "743cff4a1a": {"title": "Fix PackedGemmMatrixFP16 repacking (#43320)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43320\n\nPrevious impl seem to be buggy although I don't why. New impl is copied from https://fburl.com/diffusion/cing6mxv\n\nReviewed By: jianyuh\n\nDifferential Revision: D23235964\n\nfbshipit-source-id: 780b6e388ef895232e3ba34b125c2492b1cee60c", "pr_number": "43320", "files_changed": ["caffe2/quantization/server/fb_fc_packed_op.h"], "labels": ["fb-exported", "merged"]}, "f20a04fa2d": {"title": "[TensorExpr] Simplify conditional select (#43350)", "body": "Summary:\nFold conditional select when both sides are constant.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43350\n\nTest Plan: test_tensorexpr --gtest_filter=TensorExprTest.ConditionalSelectFold*\n\nReviewed By: pbelevich\n\nDifferential Revision: D23256602\n\nPulled By: asuhan\n\nfbshipit-source-id: ec04b1e4ae64f59fa574047f2d7af55a717a5262", "pr_number": "43350", "files_changed": ["test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.h"], "labels": ["merged", "oncall: jit"]}, "4db8ca1129": {"title": "[quant] Create nn.quantized.dynamic.EmbeddingBag (#43088)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43088\n\nCreate quantized module that the user can use to perform embedding bag quantization\nThe module uses the EmbeddingPackedParams to store the weights which can be serialized /deserialized\nusing TorchBind custom classes (C++ get/setstate code)\nFollowing PR will add support for `from_float` to convert from float to quantized module\n\nTest Plan:\npython test/test_quantization.py TestDynamicQuantizedModule.test_embedding_bag_api\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23167519\n\nfbshipit-source-id: 029d7bb44debf78c4ef08bfebf267580ed94d033", "pr_number": "43088", "files_changed": ["test/quantization/test_quantized_module.py", "torch/_utils.py", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/unpickler.cpp", "torch/nn/quantized/dynamic/modules/__init__.py", "torch/nn/quantized/dynamic/modules/embeddingbag.py", "torch/nn/quantized/modules/linear.py", "torch/nn/quantized/modules/utils.py", "torch/tensor.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged", "oncall: jit"]}, "b354b422ee": {"title": "[quant] Make offsets an optional argument (#43090)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43090\n\nTo match the floating point module\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23167518\n\nfbshipit-source-id: 29db596e10731be4cfed7efd18f33a0b3dbd0ca7", "pr_number": "43090", "files_changed": ["aten/src/ATen/native/quantized/cpu/embedding_packed_params.h", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.h", "aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "aten/src/ATen/native/quantized/library.cpp", "torch/nn/quantized/dynamic/modules/embeddingbag.py"], "labels": ["merged"]}, "3293fdfa80": {"title": "[quant] Enable from_float for quantized Embedding_Bag (#43176)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43176\n\nConvert floating point nn.EmbeddingBag module to\nnn.quantized.dynamic.EmbeddingBag module\n\nTest Plan:\npython test/test_quantization.py TestDynamicQuantizedModule.test_embedding_bag_api\npython test/test_quantization.py TestPostTrainingDynamic.test_embedding_quantization\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23200196\n\nfbshipit-source-id: 090f47dbf7aceab9c719cbf282fad20fe3e5a983", "pr_number": "43176", "files_changed": ["test/quantization/test_quantize.py", "test/quantization/test_quantized_module.py", "torch/nn/quantized/dynamic/modules/embeddingbag.py", "torch/quantization/default_mappings.py", "torch/quantization/observer.py", "torch/quantization/qconfig.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "650590da0d": {"title": "[quant][graphmode][fx] Add support for conv module + relu (#43287)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43287\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23221735\n\nfbshipit-source-id: 2513892a1928f92c09d7e9a24b2ea12b00de218d", "pr_number": "43287", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/nn/quantized/modules/conv.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "74781ab5b8": {"title": "Revert D23242101: [pytorch][PR] Implement first draft of autograd benchmark.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23242101 (https://github.com/pytorch/pytorch/commit/c2511bdfa4bef89e6c328a161d3a4d3466261ff7)\n\nOriginal commit changeset: a2b92d5a4341\n\nfbshipit-source-id: bda562d15565f074b448022d180ec8f959c6ecc9", "pr_number": null, "files_changed": ["benchmarks/functional_autograd_benchmark/README.md", "benchmarks/functional_autograd_benchmark/audio_text_models.py", "benchmarks/functional_autograd_benchmark/compare.py", "benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py", "benchmarks/functional_autograd_benchmark/ppl_models.py", "benchmarks/functional_autograd_benchmark/torchaudio_models.py", "benchmarks/functional_autograd_benchmark/torchvision_models.py", "benchmarks/functional_autograd_benchmark/utils.py", "benchmarks/functional_autograd_benchmark/vision_models.py", "test/run_test.py", "test/test_functional_autograd_benchmark.py"], "labels": []}, "100649d6a9": {"title": "Normalize loops with non-zero start. (#43179)", "body": "Summary:\nThis diff normalizes for-loops that have non 0 loop starts to always start from 0. Given a for-loop, this normalization changes the loop start to be 0 and adjusts the loop end and all accesses to the index variable within the loop body appropriately.\n\nThis diff also adds tests for several cases of normalization and also tests normalization in conjunction with `splitwithTail` transformation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43179\n\nReviewed By: nickgg\n\nDifferential Revision: D23220534\n\nPulled By: navahgar\n\nfbshipit-source-id: 64be0c72e4dbc76906084f7089dea81ae07d6020", "pr_number": "43179", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.h"], "labels": ["merged", "oncall: jit"]}, "6e48c88e09": {"title": ".circleci: Prefer using env-file for docker run (#43293)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43293\n\n'docker run' has the capability to use a file for environment variables,\nwe should prefer to use that instead of having it be sourced per command\nin the docker container.\n\nAlso opens the door for cutting down on the total number of commands we\nneed to echo into a script to then execute as a 'docker exec' command.\n\nPlus side of this approach is that the BASH_ENV is persisted through all\nof the steps so there's no need to do any exports / worry about\nenvironment variables not persisting through jobs.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23227059\n\nPulled By: seemethere\n\nfbshipit-source-id: be425aa21b420b9c6e96df8b2177f508ee641a20", "pr_number": "43293", "files_changed": [".circleci/config.yml", ".circleci/scripts/setup_ci_environment.sh", ".circleci/verbatim-sources/commands.yml", ".circleci/verbatim-sources/job-specs/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["merged", "module: ci"]}, "452a473729": {"title": "[quant][graphmode][fx] Add support for add (#43331)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43331\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23243561\n\nfbshipit-source-id: 5a6399d25cc881728cf298c77570ce2aaf3ca22e", "pr_number": "43331", "files_changed": ["test/quantization/test_quantize_fx.py"], "labels": ["merged"]}, "26be4dcfa1": {"title": "[quant][graphmode][fx] Add support for add relu (#43332)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43332\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23243564\n\nfbshipit-source-id: 3cd1786c6356aaa234d31b50f12ad6ddc38d5664", "pr_number": "43332", "files_changed": ["test/quantization/test_quantize_fx.py"], "labels": ["merged"]}, "3d76f7065e": {"title": "[quant][graphmode][fx] Add support for cat (#43333)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43333\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23243562\n\nfbshipit-source-id: 5c8eab2af592a9ea4afa713fb884e34e0ffd82b1", "pr_number": "43333", "files_changed": ["test/quantization/test_quantize_fx.py"], "labels": ["merged"]}, "054073c60d": {"title": "Refactor Vulkan context into its own files. Use RAII. (#42273)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42273\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252335\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 43144446f2f3530e6cb2a85706a9afc60771347d", "pr_number": "42273", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/vulkan/api/Common.cpp", "aten/src/ATen/native/vulkan/api/Common.h", "aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/api.h", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/vulkan_api_test.cpp"], "labels": ["merged"]}, "9e87a8ddf4": {"title": "[quant][graphmode][fx] Add support for batchnorm (#43334)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43334\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23243560\n\nfbshipit-source-id: 0a7bc331293bbc3db85616bf43a995d3b112beb6", "pr_number": "43334", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py"], "labels": ["merged"]}, "109ea59afc": {"title": "[quant][graphmode][fx] Add support for batchnorm relu (#43335)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43335\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23243563\n\nfbshipit-source-id: 3c562f519b90e0157761a00c89eca63af8b909f2", "pr_number": "43335", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/nn/quantized/modules/batchnorm.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/quantize.py"], "labels": ["merged"]}, "ff454cc429": {"title": "[quant][grapphmode][fx][test][refactor] Refactor quantized add test (#43372)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43372\n\nSo that adding more binary op tests are easier\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23257046\n\nfbshipit-source-id: 661acd4c38abdc892c9db8493b569226b13e0d0d", "pr_number": "43372", "files_changed": ["test/quantization/test_quantize_fx.py"], "labels": ["merged"]}, "8eb3de76ba": {"title": "Fix enum constant printing and add FileCheck to all Enum tests (#42874)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42874\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23222894\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 86495a350d388c82276933d24a2ca3c0f59af8da", "pr_number": "42874", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue_inl.h", "test/jit/test_enum.py"], "labels": ["merged", "oncall: jit"]}, "6c772515ed": {"title": "Revert D23252335: Refactor Vulkan context into its own files. Use RAII.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23252335 (https://github.com/pytorch/pytorch/commit/054073c60d066a7f2e6d2eca69fba52760c43544)\n\nOriginal commit changeset: 43144446f2f3\n\nfbshipit-source-id: 442b914f47a82efee18cfd84aab893e22d1defdd", "pr_number": null, "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/vulkan/api/Common.cpp", "aten/src/ATen/native/vulkan/api/Common.h", "aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/api.h", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/vulkan_api_test.cpp"], "labels": []}, "e96871ea46": {"title": "[quant][graphmode][fx] Add support for mul and mul relu (#43373)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43373\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23257047\n\nfbshipit-source-id: b7f9fcef965d6368018e05cff09260f0eb6f3b50", "pr_number": "43373", "files_changed": ["test/quantization/test_quantize_fx.py"], "labels": ["merged"]}, "93f1b5c8da": {"title": "Mobile backward compatibility (#42413)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42413\n\nWhen a default argument is added, it does not break backward compatibility (BC) for full-jit, but does break BC for mobile bytecode. For example, https://github.com/pytorch/pytorch/pull/40737. To make bytecode BC in this case, we\n\n1. Introduce kMinSupportedBytecodeVersion. The loaded model version should be between kMinSupportedBytecodeVersion and kProducedBytecodeVersion.\n2. If an operator is updated, and we can handle BC, bump the kProducedBytecodeVersion (for example, from 3 to 4).\n3. If model version is at the older version of the operator, add an adapter function at loading. For the added default arg, we push this default arg to stack before calling the actual operator function.\n\nTest Plan: Imported from OSS\n\nReviewed By: xcheng16\n\nDifferential Revision: D22898314\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 90d339f8e1365f4bb178db8db7c147390173372b", "pr_number": "42413", "files_changed": ["caffe2/serialize/inline_container.h", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp"], "labels": ["merged", "oncall: jit"]}, "5a02c6b158": {"title": "[quant][graphmode][fx] Add support for hardswish (#43374)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43374\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23257044\n\nfbshipit-source-id: 2cdf12e104db6e51ffa0324eb602e68132a646ef", "pr_number": "43374", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/default_mappings.py", "torch/quantization/fx/quantize.py"], "labels": ["merged"]}, "089bb1a8e4": {"title": "[quant][graphmode][fx] Add support for elu (#43375)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43375\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23257043\n\nfbshipit-source-id: 22360610d87ef98d25871daff3fdc3dbb3ec5bdb", "pr_number": "43375", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/default_mappings.py", "torch/quantization/fx/quantize.py"], "labels": ["merged"]}, "aec917a408": {"title": "[quant][graphmode][fx] Add support for layer_norm (#43376)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43376\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23257048\n\nfbshipit-source-id: 47a04a5221bcaf930d574f879d515e3dff2d1f6d", "pr_number": "43376", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/default_mappings.py", "torch/quantization/fx/quantize.py"], "labels": ["merged"]}, "aa53b2d427": {"title": "Workaround bugs in user side embedding meta info and better msgs (#43355)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43355\n\nThere seem to be some bugs where we cannot guarantees that blobs in `PARAMETERS_BLOB_TYPE_FULLY_REMOTE_REQUEST_ONLY` and `PARAMETERS_BLOB_TYPE_DISAGG_ACC_REMOTE_OTHER` are disjoint. Hence we need to walk around this.\n\nAlso make the msg more informative.\n\nTest Plan:\n```\nflow-cli test-locally --mode opt dper.workflows.evaluation.eval_workflow --parameters-file=/mnt/shared/yinghai/v0_ctr_mbl_feed_1120_onnx.json\n```\n\nReviewed By: ehsanardestani\n\nDifferential Revision: D23141538\n\nfbshipit-source-id: 8e311f8fc0e40eff6eb2c778213f78592e6bf079", "pr_number": "43355", "files_changed": ["caffe2/opt/onnxifi_op.cc"], "labels": ["fb-exported", "merged"]}, "f269fb83c1": {"title": "Add Enum TorchScript serialization and deserialization support (#42963)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42963\n\n* Adds code printing for enum type\n* Enhance enum type to include all contained enum names and values\n* Adds code parsing for enum type in deserialization\n* Enabled serialization/deserialization test in most TestCases. (With a few dangling issues to be addressed in later PRs to avoid this PR grows too large)\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23223281\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 716d1866b7770dfb7bd8515548cfe7dc4c4585f7", "pr_number": "42963", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/test/ivalue_test.cpp", "test/jit/test_enum.py", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/python/python_ir.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/python_print.cpp", "torch/jit/annotations.py"], "labels": ["merged", "oncall: jit"]}, "a5a6a3e633": {"title": "add support for optional int list with scalar fill (#43262)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43262\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23212049\n\nPulled By: bhosmer\n\nfbshipit-source-id: c7ceb2318645c07d36c3f932c981c9ee3c414f82", "pr_number": "43262", "files_changed": ["aten/src/ATen/core/function_schema.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "test/test_native_functions.py", "tools/autograd/gen_python_functions.py", "torch/csrc/jit/frontend/function_schema_parser.cpp"], "labels": ["merged", "oncall: jit"]}, "490d41aaa6": {"title": "[quant][graphmode][fx] Add support for instance_norm (#43377)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43377\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23257045\n\nfbshipit-source-id: 7f4ad5d81f21bf0b8b9d960b054b20dc889e6c3b", "pr_number": "43377", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/default_mappings.py", "torch/quantization/fx/quantize.py"], "labels": ["merged"]}, "abe878ce96": {"title": "Allow Freezing of Module containing interface attribute (#41860)", "body": "Summary:\nThis patch allows to freeze model that utilizes interfaces. Freezing works\nunder the user assumption that the interfase module dones not aliases with\nany value used in the model.\n\nTo enable freezing of such modules, added an extra pramater:\n\ntorch._C._freeze_module(module, ignoreInterfaces = True)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41860\n\nReviewed By: eellison\n\nDifferential Revision: D22670566\n\nPulled By: bzinodev\n\nfbshipit-source-id: 41197a724bc2dca2e8495a0924c224dc569f62a4", "pr_number": "41860", "files_changed": ["test/jit/test_freezing.py", "test/jit/test_module_interface.py", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/passes/freeze_module.cpp", "torch/csrc/jit/passes/freeze_module.h", "torch/csrc/jit/python/init.cpp"], "labels": ["merged", "oncall: jit"]}, "2a08566b8f": {"title": "Simple caching allocator for CPU. (#42006)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42006\n\nThis PR introduces a simple CPU caching allocator. This is specifically\nintended for mobile use cases and for inference. There is nothing\nspecific to the implementation that can prevent it from other use cases,\nhowever its simplicity may not be suitable everywhere.\nIt simply tracks allocation by sizes and relies on deterministic\nrepeatable behavior where allocation of same sizes are made on every\ninference.\nThus after the first allocation when the pointer is returned, instead of\nreturning it to system, allocator caches it for subsequent use.\nMemory is freed automatically at the end of the process, or it can be\nexplicitly freed.\nThis is enabled at the moment in DefaultMobileCPUAllocator only.\n\nTest Plan:\nandroid test: cpu_caching_allocator_test\n\nImported from OSS\n\nReviewed By: dreiss\n\nDifferential Revision: D22726976\n\nfbshipit-source-id: 9a38b1ce34059d5653040a1c3d035bfc97609e6c", "pr_number": "42006", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/cpu_caching_allocator_test.cpp", "binaries/speed_benchmark_torch.cc", "c10/core/CPUAllocator.cpp", "c10/core/CPUCachingAllocator.cpp", "c10/core/CPUCachingAllocator.h", "torch/csrc/jit/mobile/observer.h"], "labels": ["merged", "oncall: jit"]}, "04aa42a073": {"title": "Refactor qconv to reduce allocations. (#42007)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42007\n\nzero buffer and indirection pointers are allocatoed on every iterations.\nWith this refactor we create op once for qnnpackconv struct and keep\nrepopulating indirection pointer as necessary.\n\nFor deconv moved much of op creation outside so that we can avoid creating and\ndestroying ops every time.\n\nTest Plan:\nCI quantization tests.\ndeconvolution-test\n\nImported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22726972\n\nfbshipit-source-id: 07c03a4e90b397c36aae537ef7c0b7d81d4adc1a", "pr_number": "42007", "files_changed": ["aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/include/conv_utils.h", "aten/src/ATen/native/quantized/cpu/qnnpack/include/qnnpack_func.h", "aten/src/ATen/native/quantized/cpu/qnnpack/src/conv-run.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/src/deconv-run.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/params.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/convolution-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/deconvolution-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack_utils.h"], "labels": ["merged"]}, "fb12992b5d": {"title": "Call qnnpack's conv setup only if input pointer has changed. (#42008)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42008\n\nWith caching allocator we have increased the likelihood of getting the\nsame input pointer. With that we can cache qnnpack operator and input\npointer and check if the input pointer is the same. If so we can skip\nsetup step.\n\nTest Plan:\nRan one of the quantized models to observe\n1. No pagefaults due to indirection buffer reallocation.\n2. Much less time spent in indirection buffer population.\n\nImported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22726973\n\nfbshipit-source-id: 2dd2a6a6ecf1b5cfa7dde65e384b36a6eab052d7", "pr_number": "42008", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/src/conv-run.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/src/deconv-run.cc", "aten/src/ATen/native/quantized/cpu/qnnpack_utils.h"], "labels": ["merged"]}, "5e04bb2c1c": {"title": "caffe2: expose CPUContext RandSeed for backwards compatibility with external RNG (#43239)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43239\n\nThis is an incremental step as part of the process to migrate caffe2 random number generator off of std::mt19937 and to instead use at::mt19937+at::CPUGeneratorImpl. The ATen variants are much more performant (10x faster).\n\nThis adds a way to get the CPUContext RandSeed for tail use cases that require a std::mt19937 and borrow the CPUContext one.\n\nTest Plan: This isn't used anywhere within the caffe2 codebase. Compile should be sufficient.\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23203280\n\nfbshipit-source-id: 595c1cb447290604ee3ef61d5b5fc079b61a4e14", "pr_number": "43239", "files_changed": ["caffe2/core/context.h"], "labels": ["fb-exported", "merged"]}, "98307a2821": {"title": "Fix bfloat16 erfinv get incorrect value problem for cpu path (#43399)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/43344\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43399\n\nReviewed By: albanD\n\nDifferential Revision: D23264789\n\nPulled By: pbelevich\n\nfbshipit-source-id: 8b77c0f6ca44346e44599844fb1e172fdbd9df6c", "pr_number": "43399", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_bfloat16.h", "test/test_torch.py"], "labels": ["merged", "open source"]}, "40c77f926c": {"title": "Add prim::TypeCheck operation (#43026)", "body": "Summary:\nTypeCheck is a new operation to check the shape of tensors against\n expectd shapes. TypeCheck is a variadic operation. An example,\n\n %t0 : Tensor = ...\n %t1 : Tensor = ...\n %2 : FLOAT(20, 20), %3 : FLOAT(30, 30), %1 : bool =\n prim::TypeCheck(%t1, %t2)\n prim::If(%1)\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43026\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23115830\n\nPulled By: bzinodev\n\nfbshipit-source-id: fbf142126002173d2d865cf4b932dea3864466b4", "pr_number": "43026", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/cpp/jit/test_interpreter.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/runtime/instruction.h", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["merged", "oncall: jit"]}, "192c4b0050": {"title": "[quant][graphmode][fx] Add support for clamp (#43437)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43437\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23278584\n\nfbshipit-source-id: 266dc68c9ca30d9160a1dacf28dc7781b3d472c2", "pr_number": "43437", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "88b564ce39": {"title": "[quant][graphmode][fx] Add support for general shape ops (#43438)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43438\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23278583\n\nfbshipit-source-id: 34b73390d47c7ce60528444da77c4096432ea2cb", "pr_number": "43438", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py"], "labels": ["merged"]}, "915fd1c8fc": {"title": "centralize autograd dispatch key set (#43387)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43387\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D23258687\n\nPulled By: bhosmer\n\nfbshipit-source-id: 3718f74fc7324db027f87eda0b90893a960aa56e", "pr_number": "43387", "files_changed": ["aten/src/ATen/core/LegacyTypeDispatch.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h"], "labels": ["merged"]}, "d94b10a832": {"title": "Revert D23223281: Add Enum TorchScript serialization and deserialization support", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23223281 (https://github.com/pytorch/pytorch/commit/f269fb83c104cb8e4106171fd70aac6cb0f930cf)\n\nOriginal commit changeset: 716d1866b777\n\nfbshipit-source-id: da1ad8387b7d7aad9ff69e1ebeb5cd0b9394c2df", "pr_number": null, "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/test/ivalue_test.cpp", "test/jit/test_enum.py", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/python/python_ir.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/python_print.cpp", "torch/jit/annotations.py"], "labels": []}, "47e1b7a8f1": {"title": "Set CONSTEXPR_EXCEPT_WIN_CUDA as const while it is not constexpr (#43380)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42467\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43380\n\nReviewed By: albanD\n\nDifferential Revision: D23278930\n\nPulled By: pbelevich\n\nfbshipit-source-id: 6ce0bc9fd73cd0ead46c414fdea5f6fb7e9fec3e", "pr_number": "43380", "files_changed": ["c10/macros/Macros.h"], "labels": ["csprng", "merged", "open source"]}, "ec9e6e07bc": {"title": "[quant][graphmode][fx] Add support for general value ops (#43439)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43439\n\nPorting op tests from test_quantize_jit.py\n\nTest Plan:\nTestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23278585\n\nfbshipit-source-id: ad29f39482cf4909068ce29555470ef430ea17f6", "pr_number": "43439", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py"], "labels": ["merged"]}, "8efa898349": {"title": "[ONNX] Export split_to_sequence as slice when output number is static (#42744)", "body": "Summary:\nOptimize exported graph to export slice nodes for aten::split when the number of split outputs are fixed. Previously under some cases these are exported as onnx::SplitToSequence, which is dynamic in tensor output count.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42744\n\nReviewed By: houseroad\n\nDifferential Revision: D23172465\n\nPulled By: bzinodev\n\nfbshipit-source-id: 11e432b4ac1351f17e48356c16dc46f877fdf7da", "pr_number": "42744", "files_changed": ["test/onnx/test_utility_funs.py", "torch/onnx/symbolic_opset11.py"], "labels": ["merged", "open source", "triaged"]}, "b1d31428e7": {"title": "Reduce number of `prim::profile` (#43147)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43147\n\nReviewed By: colesbury\n\nDifferential Revision: D23190137\n\nPulled By: Krovatkin\n\nfbshipit-source-id: bf5f29a76e5ebfb5b9d3b6adee424e213c25891b", "pr_number": "43147", "files_changed": ["torch/csrc/jit/runtime/profiling_record.cpp", "torch/csrc/jit/runtime/profiling_record.h"], "labels": ["merged", "oncall: jit"]}, "b52e6d00f9": {"title": "Change quantizer to account for input tensor's memory format. (#42178)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42178\n\nThis otherwise introduces unnecessary calls to contiguous in the rest of\nthe network, where certain ops want channels last format.\n\nTest Plan:\nQuantization tests.\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D22796479\n\nfbshipit-source-id: f1ada1c2eeed84991b9b195120699b943ef6e421", "pr_number": "42178", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "test/quantization/test_quantized_tensor.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "b003f2cc28": {"title": "Enable input pointer caching in XNNPACK integration. (#42840)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42840\n\nBy caching input/output pointers and input parameters we enable the use\nof caching allocator and check if we get the same input/output pointers.\nIf so we skip setup steps.\n\nTest Plan:\npython test/test_xnnpack_integration.py\n\nImported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D23044585\n\nfbshipit-source-id: ac676cff77f264d8ccfd792d1a540c76816d5359", "pr_number": "42840", "files_changed": ["aten/src/ATen/native/xnnpack/Common.h", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Convolution.h"], "labels": ["merged"]}, "e4af45f3aa": {"title": "Fix bugs in vec256_float_neon.h (#43321)", "body": "Summary:\nfixing neon vector conversion problems.\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43321\n\nReviewed By: pbelevich\n\nDifferential Revision: D23241536\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 37a4e10989c9342ae5e8c78f6875b7aad785dd76", "pr_number": "43321", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_float_neon.h"], "labels": ["merged", "open source"]}, "35351ff409": {"title": "Fix ToC Link (#43427)", "body": "Summary:\nCC ezyang - no code here\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43427\n\nReviewed By: albanD\n\nDifferential Revision: D23273866\n\nPulled By: mrshenli\n\nfbshipit-source-id: ca07d286410f367cc78549828e517510a86d63ec", "pr_number": "43427", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged", "open source"]}, "4dc8f3be8c": {"title": "Creates test_tensor_creation_ops.py test suite (#43104)", "body": "Summary:\nAs part of our continued refactoring of test_torch.py, this takes tests for tensor creation ops like torch.eye, torch.randint, and torch.ones_like and puts them in test_tensor_creation_ops.py. There hare three test classes in the new test suite: TestTensorCreation, TestRandomTensorCreation, TestLikeTensorCreation. TestViewOps and tests for construction of tensors from NumPy arrays have been left in test_torch.py. These might be refactored separately into test_view_ops.py and test_numpy_interop.py in the future.\n\nMost of the tests ported from test_torch.py were left as is or received a signature change to make them nominally \"device generic.\" Future work will need to review test coverage and update the tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43104\n\nReviewed By: ngimel\n\nDifferential Revision: D23280358\n\nPulled By: mruberry\n\nfbshipit-source-id: 469325dd1a734509dd478cc7fe0413e276ffb192", "pr_number": "43104", "files_changed": ["test/run_test.py", "test/test_tensor_creation_ops.py", "test/test_torch.py"], "labels": ["merged", "module: tests"]}, "d70b263e3a": {"title": "[DPER3] Separate user embeddings and ad embeddings in blob reorder", "body": "Summary:\nSeparate user embeddings and ad embeddings in blobsOrder. New order:\n1. meta_net_def\n2. preload_blobs\n3. user_embeddings (embeddings in remote request only net)\n4. ad_embeddings (embeddings in remote other net)\n\nAdd a field requestOnlyEmbeddings in meta_net_def to record user_embeddings.\n\nThis is for flash verification.\n\nTest Plan:\nbuck test dper3/dper3_backend/delivery/tests:blob_reorder_test\n\nRun a flow with canary package f211282476\nCheck the net: n326826, request_only_embeddings are recorded as expected\n\nReviewed By: ipiszy\n\nDifferential Revision: D23008305\n\nfbshipit-source-id: 9360ba3d078f205832821005e8f151b8314f0cf2", "pr_number": null, "files_changed": ["caffe2/python/predictor/predictor_py_utils.py"], "labels": []}, "a97ca93c0e": {"title": "remove prim::profile and special-casing (#43160)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43160\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23284421\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 35e97aad299509a682ae7e95d7cef53301625309", "pr_number": "43160", "files_changed": ["test/cpp/jit/test_misc.cpp", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/passes/insert_guards.cpp", "torch/csrc/jit/passes/specialize_autogradzero.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp", "torch/csrc/jit/runtime/profiling_record.cpp", "torch/csrc/jit/runtime/profiling_record.h"], "labels": ["merged", "oncall: jit"]}, "b349f58c21": {"title": "[fx] enabling typechecking of fx files (#43082)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43082\n\nFixes all present errors in mypy. Does not try to add annotations everywhere.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23145854\n\nPulled By: zdevito\n\nfbshipit-source-id: 18e483ed605e89ed8125971e84da1a83128765b7", "pr_number": "43082", "files_changed": ["test/test_fx.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/node.py", "torch/fx/proxy.py", "torch/fx/symbolic_trace.py"], "labels": ["merged"]}, "1f0cfbaaad": {"title": "[fx] add type annotations (#43083)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43083\n\nThis adds type annotations to all classes, arguments, and returns\nfor fx. This should make it easier to understand the code, and\nencourage users of the library to also write typed code.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23145853\n\nPulled By: zdevito\n\nfbshipit-source-id: 648d91df3f9620578c1c51408003cd5152e34514", "pr_number": "43083", "files_changed": ["torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/node.py", "torch/fx/proxy.py", "torch/fx/symbolic_trace.py"], "labels": ["merged"]}, "c4e841654d": {"title": "Add alias torch.negative to torch.neg. (#43400)", "body": "Summary:\nxref https://github.com/pytorch/pytorch/issues/42515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43400\n\nReviewed By: albanD\n\nDifferential Revision: D23266011\n\nPulled By: mruberry\n\nfbshipit-source-id: ca20b30d99206a255cf26438b09c3ca1f99445c6", "pr_number": "43400", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_op_aliases.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py"], "labels": ["merged", "oncall: jit", "open source"]}, "db78c07ced": {"title": "Enable torch.cuda.nvtx typechecking (#43443)", "body": "Summary:\nAdd pyi file covering torch._C.nvtx submodule\n\nFixes https://github.com/pytorch/pytorch/issues/43436\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43443\n\nReviewed By: ezyang\n\nDifferential Revision: D23280188\n\nPulled By: malfet\n\nfbshipit-source-id: 882860cce9feb0b5307c8b7c887f4a2f2c1548a2", "pr_number": "43443", "files_changed": ["mypy.ini", "torch/_C/_nvtx.pyi", "torch/cuda/nvtx.py"], "labels": ["merged", "module: typing", "triaged"]}, "c972e6232a": {"title": "Implement batching rules for basic arithmetic ops (#43362)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43362\n\nBatching rules implemented for: addition subtraction division\nmultiplication.\n\nI refactored the original `mul_batching_rule` into a templated function\nso that one can insert arbitrary binary operations into it.\n\nadd, sub, rsub, mul, and div all work the same way. However, other\nbinary operations work slightly differently (I'm still figuring out the\ndifferences and why they're different) so those may need a different\nimplementation.\n\nTest Plan: - \"pytest test/test_vmap.py -v\": new tests\n\nReviewed By: ezyang\n\nDifferential Revision: D23252317\n\nPulled By: zou3519\n\nfbshipit-source-id: 6d36cd837a006a2fd31474469323463c1bd797fc", "pr_number": "43362", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "7cc1efec13": {"title": "Add lite SequentialSampler to torch mobile (#43299)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43299\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23228415\n\nPulled By: ann-ss\n\nfbshipit-source-id: eebe54353a128783f039c7dac0e2dd765a61940d", "pr_number": "43299", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_lite_trainer.cpp", "test/cpp/jit/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/mobile/sequential.cpp", "torch/csrc/jit/mobile/sequential.h"], "labels": ["merged", "oncall: jit"]}, "7024ce8a2c": {"title": "[quant] Add benchmarks for quantized embeddingbag module (#43296)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43296\n\nUse common config for float and quantized embedding_bag modules\n\nTest Plan:\n```\npython -m pt.qembeddingbag_test\n\n Benchmarking PyTorch: qEmbeddingBag\n Mode: Eager\n Name: qEmbeddingBag_embeddingbags10_dim4_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu\n Input: embeddingbags: 10, dim: 4, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu\nForward Execution Time (us) : 35.738\n\n Benchmarking PyTorch: qEmbeddingBag\n Mode: Eager\n Name: qEmbeddingBag_embeddingbags10_dim4_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu\n Input: embeddingbags: 10, dim: 4, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu\nForward Execution Time (us) : 62.708\n\npython -m pt.embeddingbag_test\n\n Benchmarking PyTorch: embeddingbag\n Mode: Eager\n Name: embeddingbag_embeddingbags10_dim4_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu\n Input: embeddingbags: 10, dim: 4, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu\nForward Execution Time (us) : 46.878\n\n Benchmarking PyTorch: embeddingbag\n Mode: Eager\n Name: embeddingbag_embeddingbags10_dim4_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu\n Input: embeddingbags: 10, dim: 4, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu\nForward Execution Time (us) : 103.904\n\n```\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23245531\n\nfbshipit-source-id: 81b44fde522238d3eef469434e93dd7f94b528a8", "pr_number": "43296", "files_changed": ["benchmarks/operator_benchmark/pt/configs.py", "benchmarks/operator_benchmark/pt/embeddingbag_test.py", "benchmarks/operator_benchmark/pt/qembeddingbag_test.py"], "labels": ["merged"]}, "0fa99d50bc": {"title": "Enable torch.cuda.memory typechecking (#43444)", "body": "Summary:\nAdd number of function prototypes defined in torch/csrs/cuda/Module.cpp to `__init__.pyi.in`\n\nFixes https://github.com/pytorch/pytorch/issues/43442\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43444\n\nReviewed By: ezyang\n\nDifferential Revision: D23280221\n\nPulled By: malfet\n\nfbshipit-source-id: 7d67dff7b24c8d7b7e72c919e6e7b847f242ef83", "pr_number": "43444", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in", "torch/cuda/memory.py"], "labels": ["merged", "module: typing"]}, "35a36c1280": {"title": "Implement JIT Enum type serialization and deserialization (#43460)", "body": "Summary:\n[Re-review tips: nothing changed other than a type in python_ir.cpp to fix a windows build failure]\n\nAdds code printing for enum type\nEnhance enum type to include all contained enum names and values\nAdds code parsing for enum type in deserialization\nEnabled serialization/deserialization test in most TestCases. (With a few dangling issues to be addressed in later PRs to avoid this PR grows too large)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43460\n\nReviewed By: albanD\n\nDifferential Revision: D23284929\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: e3e81d6106f18b7337ac3ff5cd1eeaff854904f3", "pr_number": "43460", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/test/ivalue_test.cpp", "test/jit/test_enum.py", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/python/python_ir.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/python_print.cpp", "torch/jit/annotations.py"], "labels": ["merged", "oncall: jit"]}, "4cfac34075": {"title": "[ROCm] allow .jenkins/pytorch/test.sh to run on centos (#42197)", "body": "Summary:\nThis doesn't fix any reported issue.  We validate ROCm PyTorch on ubuntu and centos.  For centos, we must modify the test.sh script to let it run on centos.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42197\n\nReviewed By: ezyang, ngimel\n\nDifferential Revision: D23175669\n\nPulled By: malfet\n\nfbshipit-source-id: 0da435de6fb17d2ca48e924bec90ef61ebbb5042", "pr_number": "42197", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "e08e93f946": {"title": "Reland of benchmark code (#43428)", "body": "Summary:\nReland of the benchmark code that broke the slow tests because the GPU were running out of memory\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43428\n\nReviewed By: ngimel\n\nDifferential Revision: D23296136\n\nPulled By: albanD\n\nfbshipit-source-id: 0002ae23dc82f401604e33d0905d6b9eedebc851", "pr_number": "43428", "files_changed": ["benchmarks/functional_autograd_benchmark/README.md", "benchmarks/functional_autograd_benchmark/audio_text_models.py", "benchmarks/functional_autograd_benchmark/compare.py", "benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py", "benchmarks/functional_autograd_benchmark/ppl_models.py", "benchmarks/functional_autograd_benchmark/torchaudio_models.py", "benchmarks/functional_autograd_benchmark/torchvision_models.py", "benchmarks/functional_autograd_benchmark/utils.py", "benchmarks/functional_autograd_benchmark/vision_models.py", "test/run_test.py", "test/test_functional_autograd_benchmark.py"], "labels": ["merged"]}, "ed8b08a3ba": {"title": "Update quantize_jit to handle new upsample overloads (#43407)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43407\n\nghstack-source-id: 110404846\n\nTest Plan:\ntest_general_value_ops passes with D21209991 applied.\n(Without this diff D21209991 breaks that test.)\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23256503\n\nfbshipit-source-id: 0f75e50a9f7fccb5b4325604319a5f76b42dfe5e", "pr_number": "43407", "files_changed": ["torch/csrc/jit/passes/quantization/quantization_patterns.h"], "labels": ["merged", "oncall: jit"]}, "e37f871e87": {"title": "[2/3][lite interpreter] add metadata when saving and loading models for mobile", "body": "Summary:\n1. add `metadata.pkl` to `.bc` file which includes the model info that we are interested in\n2. load `metadata.pkl` as a attribute `unordered_map<string, string>` in the module\n\nTest Plan:\n- CI\n```buck build //xplat/caffe2:jit_module_saving\n```\n```buck build //xplat/caffe2:torch_mobile_core\n```\n\nReviewed By: xcheng16\n\nDifferential Revision: D23047144\n\nfbshipit-source-id: f368d00f7baef2d3d15f89473cdb146467aa1e0b", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/serialization/export.h", "torch/csrc/jit/serialization/export_module.cpp"], "labels": []}, "284ff04792": {"title": "[quant] Support set API for EmbeddingBag quantization (#43433)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43433\n\nAdd support for torch.quint8 dtype\n\nTest Plan: Imported from OSS\n\nReviewed By: radkris-git\n\nDifferential Revision: D23277002\n\nfbshipit-source-id: 4204bc62f124b4fd481aaa6aa47b9437978c43ee", "pr_number": "43433", "files_changed": ["test/quantization/test_quantize.py", "torch/quantization/quantize.py"], "labels": ["merged"]}, "87905b5856": {"title": "[pytorch] add option to include autograd for code analyzer (#43155)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43155\n\nUpdate the code_analyzer build.sh script to be able to take additional build flags in the mobile build/analysis\n\nTest Plan:\nCheckout associated PR or copy contents of build.sh into PyTorch repo (must be run from root of PyTorch repo)\n\nTo run with inclusion of autograd dependencies (note BUILD_MOBILE_AUTOGRAD is still an experimental build flag): `ANALYZE_TORCH=1 DEPLOY=1 BASE_OPS_FILE=/path/to/baseopsfile MOBILE_BUILD_FLAGS='-DBUILD_MOBILE_AUTOGRAD=ON' tools/code_analyzer/build.sh`\n\nReviewed By: ljk53\n\nDifferential Revision: D23065754\n\nfbshipit-source-id: d83a7ad62ad366a84725430ed020adf4d56687bd", "pr_number": "43155", "files_changed": ["tools/code_analyzer/build.sh"], "labels": ["merged"]}, "7b243a4d46": {"title": "[quant][graphmode[fx][test][refactor] Refactor tests for graph mode quantization on fx (#43445)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43445\n\nchanged the interface for checkGraphModule to make the arguments more explicit\nas requested in https://github.com/pytorch/pytorch/pull/43437\n\nTest Plan:\nTestQuantizeFx\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23280586\n\nfbshipit-source-id: 5b5859e326d149a5aacb1d15cbeee69667cc9109", "pr_number": "43445", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "f80b695a75": {"title": "Properly format db.h and db.cc (#43027)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43027\n\nFormat db.h and db.cc using the default formatter.\n\nThis change was split off of D22705434.\n\nTest Plan: Wait for sandcastle.\n\nReviewed By: rohithmenon, marksantaniello\n\nDifferential Revision: D23113765\n\nfbshipit-source-id: 3f02d55bfb055bda0fcba5122336fa001562d42e", "pr_number": "43027", "files_changed": ["caffe2/core/db.cc", "caffe2/core/db.h"], "labels": ["fb-exported", "merged"]}, "675f3f0482": {"title": "Fix \"save binary size\" steps (#43529)", "body": "Summary:\n`pip3` alias might not be available, so call `python3 -mpip` to be on the safe side\nShould fix failures like that:\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/203448/workflows/3837b2d6-b089-4a19-b797-38bdf989c82e/jobs/6913032/parallel-runs/0/steps/0-109\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43529\n\nReviewed By: seemethere\n\nDifferential Revision: D23307306\n\nPulled By: malfet\n\nfbshipit-source-id: b55e6782b29f1a1f56787902cbb85b3c3d20370c", "pr_number": "43529", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml"], "labels": ["merged"]}, "d1d32003bb": {"title": "force pytorch tensors to contiguous before calling c2 ops", "body": "Summary: per title, makes c2 wrappers safer as contiguity of torch inputs is not guaranteed\n\nTest Plan: covered by existing tests\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23310137\n\nfbshipit-source-id: 3fe12abc7e394b8762098d032200778018e5b591", "pr_number": null, "files_changed": ["caffe2/core/operator.h"], "labels": []}, "cbdaa20c88": {"title": "[serialize] Expose zip file alignment calculation functions (#43531)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43531\n\nIt's useful for building some tooling out of tree to manipulate zip files in PyTorch-y way\n\nTest Plan: contbuild\n\nReviewed By: houseroad\n\nDifferential Revision: D23277361\n\nfbshipit-source-id: e15fad20e792d1e41018d32fd48295cfe74bea8c", "pr_number": "43531", "files_changed": ["caffe2/serialize/inline_container.cc", "caffe2/serialize/inline_container.h"], "labels": ["fb-exported", "merged"]}, "f02753fabb": {"title": "Support AMP in nn.parallel (#43102)", "body": "Summary:\nTake care of the state of autocast in `parallel_apply`, so there is no need to decorate model implementations.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43102\n\nReviewed By: ngimel\n\nDifferential Revision: D23294610\n\nPulled By: mrshenli\n\nfbshipit-source-id: 0fbe0c79de976c88cadf2ceb3f2de99d9342d762", "pr_number": "43102", "files_changed": ["test/distributed/test_data_parallel.py", "torch/nn/parallel/parallel_apply.py"], "labels": ["merged", "module: data parallel", "open source", "triaged"]}, "b430347a60": {"title": "Address JIT/Mypy issue with torch._VF (#43454)", "body": "Summary:\n- `torch._VF` is a hack to work around the lack of support for `torch.functional` in the JIT\n- that hack hides `torch._VF` functions from Mypy\n- could be worked around by re-introducing a stub file for `torch.functional`, but that's undesirable\n- so instead try to make both happy at the same time: the type ignore comments are needed for Mypy, and don't seem to affect the JIT after excluding them from the `get_type_line()` logic\n\nEncountered this issue while trying to make `mypy` run on `torch/functional.py` in gh-43446.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43454\n\nReviewed By: glaringlee\n\nDifferential Revision: D23305579\n\nPulled By: malfet\n\nfbshipit-source-id: 50e490693c1e53054927b57fd9acc7dca57e88ca", "pr_number": "43454", "files_changed": ["torch/_VF.py", "torch/functional.py", "torch/jit/annotations.py"], "labels": ["merged", "module: typing", "oncall: jit", "open source"]}, "6a2d7a05c4": {"title": "fix typo in test_dataloader test_multiprocessing_contexts (#43343)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/22990 added a multiprocessing_context argument to DataLoader, but a typo in the test causes the wrong DataLoader class to be used.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43343\n\nReviewed By: glaringlee\n\nDifferential Revision: D23299452\n\nPulled By: malfet\n\nfbshipit-source-id: 9489c48b83bce36f46d350cad902f7ad96e1eec4", "pr_number": "43343", "files_changed": ["test/test_dataloader.py"], "labels": ["merged", "module: dataloader", "module: multiprocessing", "open source"]}, "8ecfa9d9a2": {"title": "[cmake] End support for python3.5 for pytorch (#43105)", "body": "Summary:\nPyTorch uses f-string in its python codes.\nPython support for f-string started with version 3.6\nUsing python version 3.5 or older fails the build with latest release/master.\nThis patch checks the version of the python used for build and mandates it to be 3.6 or higher.\n\nSigned-off-by: Parichay Kapoor <kparichay@gmail.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43105\n\nReviewed By: glaringlee\n\nDifferential Revision: D23301481\n\nPulled By: malfet\n\nfbshipit-source-id: e9b4f7bffce7384c8ade3b7d131b10cf58f5e8a0", "pr_number": "43105", "files_changed": ["cmake/Dependencies.cmake"], "labels": ["merged", "module: build", "open source", "triaged"]}, "1089ff404c": {"title": "Refactored the duplicate code into a function in _ConvNd (#43525)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43525\n\nReviewed By: ngimel\n\nDifferential Revision: D23306593\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 3427cd2b9132a203858477b6c858d59b00e1282e", "pr_number": "43525", "files_changed": ["torch/nn/quantized/modules/conv.py"], "labels": ["merged", "open source"]}, "76894062dc": {"title": "move wholearchive to link option (#43485)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43216\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43485\n\nReviewed By: glaringlee\n\nDifferential Revision: D23318735\n\nPulled By: malfet\n\nfbshipit-source-id: 90c316d3d5ed51afcff356e6d9219950f119a902", "pr_number": "43485", "files_changed": ["cmake/public/utils.cmake"], "labels": ["merged", "module: build", "module: windows", "open source", "triaged"]}, "f8e9e7ad4a": {"title": "Allocating warp to an input index in compute_cuda_kernel (#43354)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43354\n\nInstead of assigning a thread to an input index for repeating that index, we assign a warp to an index. This helps us in avoiding the costly uncoaelesced memory accesses and brach divergence which occur when each thread is repeating the index.\n\nTest Plan: Run trainer to test\n\nReviewed By: ngimel\n\nDifferential Revision: D23230917\n\nfbshipit-source-id: 731e912c844f1d859b0384fcaebafe69cb4ab56a", "pr_number": "43354", "files_changed": ["aten/src/ATen/native/cuda/Repeat.cu"], "labels": ["fb-exported", "merged"]}, "f32ca57c5e": {"title": "Fix typo in LSTMCell document (#43395)", "body": "Summary:\nFixes typo in document\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43395\n\nReviewed By: mruberry\n\nDifferential Revision: D23312561\n\nPulled By: ngimel\n\nfbshipit-source-id: 28340c96faf52c17acfe9f6b1dd94b71ea4d60ce", "pr_number": "43395", "files_changed": ["torch/nn/modules/rnn.py"], "labels": ["merged", "open source"]}, "3dcfe84861": {"title": "Grammatical corrections (#43473)", "body": "Summary:\n**Few documentation corrections.**\n\n1. [...] If there is hard-to-debug error in one of your TorchScript **models**, you can use this flag [...]\n2. [...] Since TorchScript (scripting and tracing) **is** disabled with this flag [...]\n\n**Before corrections (as of now):**\n![before-fix](https://user-images.githubusercontent.com/45713346/90977203-d8bc2580-e543-11ea-9609-fbdf5689dcb9.jpg)\n\n**After corrections:**\n![after-fix](https://user-images.githubusercontent.com/45713346/90977209-dbb71600-e543-11ea-8259-011618efd95b.jpg)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43473\n\nReviewed By: mruberry\n\nDifferential Revision: D23296167\n\nPulled By: ngimel\n\nfbshipit-source-id: 932c9b25cc79d6e266e5ddb3744573b0bd63d925", "pr_number": "43473", "files_changed": ["docs/source/jit.rst"], "labels": ["merged", "open source"]}, "ebc0fc4dfc": {"title": "Polish the nightly.py docs in CONTRIBUTING a little (#43494)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43494\n\nReviewed By: mruberry\n\nDifferential Revision: D23296032\n\nPulled By: ngimel\n\nfbshipit-source-id: c85a6d4c39cbb60644f79136a6f21fd49c813b61", "pr_number": "43494", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged", "open source"]}, "9420c773d0": {"title": "Revert D23299452: [pytorch][PR] fix typo in test_dataloader test_multiprocessing_contexts", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23299452 (https://github.com/pytorch/pytorch/commit/6a2d7a05c4f3552a2a7d8223781f8b0f71f0264a)\n\nOriginal commit changeset: 9489c48b83bc\n\nfbshipit-source-id: e8c15d338dd89d8e92f3710e9cf149149bd2e763", "pr_number": null, "files_changed": ["test/test_dataloader.py"], "labels": []}, "62dcd253e3": {"title": "[quant][graphmode][fx] Testing torchvision (#43526)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43526\n\nAdd tests for graph mode quantization on torchvision and make sure it matches\ncurrent eager mode quantization\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23306683\n\nfbshipit-source-id: 30d27e225d4557bfc1d9aa462086e416aa9a9c0e", "pr_number": "43526", "files_changed": ["test/quantization/test_quantize_fx.py", "test/test_quantization.py", "torch/fx/graph_module.py", "torch/quantization/fx/fuse.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "3df398a3a8": {"title": "Update the QR documentation to include a warning about when the QR.backward is well-defined. (#43547)", "body": "Summary:\nAs per title.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43547\n\nReviewed By: mruberry\n\nDifferential Revision: D23318829\n\nPulled By: albanD\n\nfbshipit-source-id: 4764ebe1ad440e881b1c4c88b16fb569ef8eb0fa", "pr_number": "43547", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "9b05fbd92e": {"title": "Correct the windows docs (#43479)", "body": "Summary:\nFixes https://discuss.pytorch.org/t/i-cannot-use-the-pytorch-that-was-built-successfully-from-source-dll-initialization-routine-failed-error-loading-caffe2-detectron-ops-gpu-dll/93243/5?u=peterjc123.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43479\n\nReviewed By: mrshenli, ngimel\n\nDifferential Revision: D23294211\n\nPulled By: ezyang\n\nfbshipit-source-id: d67df7d0355c2783153d780c94f959758b246d36", "pr_number": "43479", "files_changed": ["docs/source/notes/windows.rst"], "labels": ["merged", "open source"]}, "5ca6cbbd93": {"title": "Remove unnecessary copies in ProcessGroupGloo for multiple inputs allreduce (#43543)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43543\n\nCloses https://github.com/pytorch/pytorch/issues/14691. This is not needed in the multiple outputs case, because gloo allreduce\nwill broadcast the result tensor to all the outputs. See\nhttps://github.com/facebookincubator/gloo/issues/152 and commit\nhttps://github.com/facebookincubator/gloo/commit/9cabb5aaa4f02356bc8db05e5630cb550b3f5b5c\nfor more details. Came across this when debugging https://github.com/pytorch/pytorch/pull/42577.\n\nThis effectively reverts https://github.com/pytorch/pytorch/pull/14688 while still keeping the tests.\n\nTested by ensuring `test_allreduce_basics` in `test_c10d.py` still works as expected.\nghstack-source-id: 110636498\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D23173945\n\nfbshipit-source-id: d1ae08f84b4ac9919c53080949b8fffcb2fe63a8", "pr_number": "43543", "files_changed": ["torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["merged"]}, "05f27b18fb": {"title": "Back out D23047144 \"[2/3][lite interpreter] add metadata when saving and loading models for mobile\"", "body": "Summary:\nOriginal commit changeset: f368d00f7bae\n\nBack out \"[2/3][lite interpreter] add metadata when saving and loading models for mobile\"\n\nD23047144 (https://github.com/pytorch/pytorch/commit/e37f871e875e8b7127a0f3496c1c5e52ff8643ac)\n\nPull Request: https://github.com/pytorch/pytorch/pull/43516\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: xcheng16\n\nDifferential Revision: D23304639\n\nfbshipit-source-id: 970ca3438c1858f8656cbcf831ffee2c4a551110", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/serialization/export.h", "torch/csrc/jit/serialization/export_module.cpp"], "labels": []}, "be637fd5f6": {"title": "Revert D23306683: [quant][graphmode][fx] Testing torchvision", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23306683 (https://github.com/pytorch/pytorch/commit/62dcd253e377d83baee83c62ebcd526105346384)\n\nOriginal commit changeset: 30d27e225d45\n\nfbshipit-source-id: e661334d187d3d6756facd36f2ebdb3ab2cd2e26", "pr_number": null, "files_changed": ["test/quantization/test_quantize_fx.py", "test/test_quantization.py", "torch/fx/graph_module.py", "torch/quantization/fx/fuse.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "348e78b086": {"title": "Evenly distribute output grad into all matching inputs for min/max/median (#43519)", "body": "Summary:\ncc: ngimel mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43519\n\nReviewed By: albanD\n\nDifferential Revision: D23312235\n\nPulled By: ngimel\n\nfbshipit-source-id: 678bda54996df7f29acf96add928bb7042fc2069", "pr_number": "43519", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp"], "labels": ["merged", "open source", "topic: bc-breaking", "triaged"]}, "c9f125bf70": {"title": "Black to Block for various files (#42913)", "body": "Summary:\nFixes  https://github.com/pytorch/pytorch/issues/41735 #41736 https://github.com/pytorch/pytorch/issues/41737 #41738 all areas where black is mentioned is replaced to block\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42913\n\nReviewed By: houseroad\n\nDifferential Revision: D23112873\n\nPulled By: malfet\n\nfbshipit-source-id: a515b56dc2ed20aa75741c577988d95f750b364c", "pr_number": "42913", "files_changed": ["torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset7.py", "torch/onnx/symbolic_opset8.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "b3f8834033": {"title": "Batching rule for torch.pow, torch.result_type (#43515)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43515\n\nThis PR adds a batching rule for torch.pow. This required adding a\nbatching rule for torch.result_type.\n\nTest Plan: - added new tests: `pytest test/test_vmap.py -v`\n\nReviewed By: cpuhrsch\n\nDifferential Revision: D23302737\n\nPulled By: zou3519\n\nfbshipit-source-id: 2cade358750f6cc3abf45f81f2394900600927cc", "pr_number": "43515", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["merged"]}, "58666982fb": {"title": "check in intel nnpi 1007 into fbcode/tp2", "body": "Summary: As title\n\nTest Plan:\n* Details of conducted tests can be found in https://fb.workplace.com/groups/527892364588452/permalink/615694119141609/\n* Sandcastle\n\nReviewed By: arunm-git\n\nDifferential Revision: D23198458\n\nfbshipit-source-id: dd8d34a985dced66a5624a21e5d4a7e9a499ce39", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/test/test_layernorm_nnpi_fp16.py"], "labels": []}, "f35e069622": {"title": "Back out \"Make grad point to bucket buffer in DDP to save memory usage\" (#43557)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43557\n\nbackout the diff that caused some errors in pytext distributed training\n\nTest Plan: Tested by rayhou who verified reverting the diff works\n\nDifferential Revision: D23320238\n\nfbshipit-source-id: caa0fe74404059e336cd95fdb41373f58ecf486e", "pr_number": "43557", "files_changed": ["test/distributed/test_distributed.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/functions/accumulate_grad.h", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/nn/parallel/distributed.py"], "labels": ["fb-exported", "merged"]}, "f6b7c6da19": {"title": "[TensorExpr] Fuser: move canHandle and some other auxiliary functions into TensorExprFuser class. (#43170)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43170\n\nDifferential Revision: D23178227\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 3c3a0215344fb5942c4f3078023fef32ad062fe9", "pr_number": "43170", "files_changed": ["torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["merged", "oncall: jit"]}, "8dc4b415eb": {"title": "[TensorExpr] Fuser: only require input shapes to be known (output shapes can be inferred). (#43171)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43171\n\nDifferential Revision: D23178228\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: e3465066e0cc4274d28db655de274a51c67594c4", "pr_number": "43171", "files_changed": ["torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["merged", "oncall: jit"]}, "d18566c617": {"title": "[TensorExpr] Fuser: disallow aten::slice nodes. (#43365)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43365\n\nWe don't have shape inference for them yet.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23253418\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 9c38778b8a616e70f6b2cb5aab03d3c2013b34b0", "pr_number": "43365", "files_changed": ["test/test_tensorexpr.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["merged", "oncall: jit"]}, "b763666f9f": {"title": "[JIT] Subgraph utils: add an optional vmap argument to the API to allow retrieving value mappings. (#43235)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43235\n\nThis functionality is needed when we want to not lose track of\nnodes/values as we merge and unmerge them into other nodes. For\ninstance, if we have a side data structure with some meta information\nabout values or nodes, this new functionality would allow to keep that\nmetadata up to date after merging and unmerging nodes.\n\nDifferential Revision: D23202648\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 350d21a5d462454166f8a61b51d833551c49fcc9", "pr_number": "43235", "files_changed": ["test/cpp/jit/test_subgraph_utils.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/passes/utils/subgraph_utils.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.h"], "labels": ["merged", "oncall: jit"]}, "3ec24f02af": {"title": "[TensorExpr] Start using typecheck in the fuser. (#43173)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43173\n\nWith this change the fuser starts to generate typechecks for inputs of\nfusion group. For each fusion group we generate a typecheck and an if\nnode: the true block contains the fused subgraph, the false block\ncontains unoptimized original subgraph.\n\nDifferential Revision: D23178230\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: f56e9529613263fb3e6575869fdb49973c7a520b", "pr_number": "43173", "files_changed": ["test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/cpp/tensorexpr/tests.h", "test/test_jit_fuser_te.py", "test/test_tensorexpr.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.h", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["merged", "oncall: jit"]}, "c1553ff94b": {"title": "Benchmarks: temporarily disable profiling-te configuration. (#43603)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43603\n\nWe are in the midst of landing a big reword of profiling executor and\nbenchmarks are expected to fail while we are in the transitional state.\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23334818\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 99ff17c6f8ee18d003f6ee76ff0e719cea68c170", "pr_number": "43603", "files_changed": ["benchmarks/fastrnns/test_bench.py"], "labels": ["merged"]}, "2b70f82737": {"title": "fix typo in test_dataloader test_multiprocessing_contexts (take 2) (#43588)", "body": "Summary:\n2nd attempt to land https://github.com/pytorch/pytorch/pull/43343\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43588\n\nReviewed By: seemethere\n\nDifferential Revision: D23332284\n\nPulled By: malfet\n\nfbshipit-source-id: d78faf468c56af2f176dbdd2ce4bd51f0b5df6fd", "pr_number": "43588", "files_changed": ["test/test_dataloader.py"], "labels": ["merged"]}, "573940f8d7": {"title": "Fix type annotation errors in torch.functional (#43446)", "body": "Summary:\nCloses gh-42968\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43446\n\nReviewed By: albanD\n\nDifferential Revision: D23280962\n\nPulled By: malfet\n\nfbshipit-source-id: de5386a95a20ecc814c39cbec3e4252112340b3a", "pr_number": "43446", "files_changed": ["mypy.ini", "test/onnx/expect/TestOperators.test_meshgrid.expect", "torch/functional.py", "torch/jit/annotations.py", "torch/nn/functional.pyi.in", "torch/overrides.py", "torch/quantization/_equalize.py"], "labels": ["merged", "module: typing", "open source"]}, "7beeef2c69": {"title": ".jenkins: Remove openssh installs (#43597)", "body": "Summary:\nopenssh should be installed by either the circleci machines or from the\njenkins workers so we shouldn't need to install it ourselves in order to\nget ssh functionality\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43597\n\nReviewed By: ezyang\n\nDifferential Revision: D23333479\n\nPulled By: seemethere\n\nfbshipit-source-id: 17a1ad0200a9df7d4818ab1ed44c8488ec8888fb", "pr_number": "43597", "files_changed": [".jenkins/pytorch/build.sh", ".jenkins/pytorch/multigpu-test.sh", ".jenkins/pytorch/test.sh"], "labels": ["merged", "module: ci"]}, "42f6c3b1f4": {"title": "Raise error on device mismatch in addmm (#43505)", "body": "Summary:\nFixes gh-42282\n\nThis adds a device-mismatch check to `addmm` on CPU and CUDA. Although it seems like the dispatcher is always selecting the CUDA version here if any of the inputs are on GPU. So in theory the CPU check is unnecessary, but probably better to err on the side of caution.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43505\n\nReviewed By: mruberry\n\nDifferential Revision: D23331651\n\nPulled By: ngimel\n\nfbshipit-source-id: 8eb2f64f13d87e3ca816bacec9d91fe285d83ea0", "pr_number": "43505", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu", "test/test_cuda.py"], "labels": ["merged", "open source", "triaged"]}, "51861cc9b1": {"title": ".circleci: Add CUDA 11 to nightly binary builds (#43366)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43366\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D23348556\n\nPulled By: seemethere\n\nfbshipit-source-id: 0cd129c5c27ffceec80636384762c3ff7bf74fdc", "pr_number": "43366", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/cimodel/data/dimensions.py", ".circleci/config.yml", ".circleci/scripts/binary_linux_test.sh", ".circleci/scripts/setup_ci_environment.sh"], "labels": ["merged"]}, "25dcc28cd6": {"title": "[jit][static] Replace deepcopy with copy (#43182)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43182\n\nWe should avoid using `deepcopy` on the module because it involves copying the weights.\n\nComparing the implementation of `c10::ivalue::Object::copy()` vs `c10::ivalue::Object::deepcopy()`, the only difference is `deepcopy` copies the attributes (slots) while `copy` does not.\n\nReviewed By: bwasti\n\nDifferential Revision: D23171770\n\nfbshipit-source-id: 3cd711c6a2a19ea31d1ac1ab2703a0248b5a4ef3", "pr_number": "43182", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "2a4d312027": {"title": "Allow GPU skip decorators to report the right number of GPUs required in (#43468)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43468\n\nCloses https://github.com/pytorch/pytorch/issues/41378.\nhttps://github.com/pytorch/pytorch/pull/41973 enhanced the skip decorators to\nreport the right no. of GPUs required, but this information was not passed to\nthe main process where the message is actually displayed. This PR uses a\n`multiprocessing.Manager()` so that the dictionary modification is reflected\ncorrectly in the main process.\nghstack-source-id: 110684228\n\nTest Plan:\nWith this diff, we can run a test in such as in https://github.com/pytorch/pytorch/pull/42577 that requires 4 GPUs on a 2 GPU machine, and we get the expected message:\n\n```\ntest_ddp_uneven_inputs_replicated_error (test_distributed.TestDistBackend) ... skipped 'Need at least 4 CUDA devices'\n```\n\nReviewed By: mrshenli\n\nDifferential Revision: D23285790\n\nfbshipit-source-id: ac32456ef3d0b1d8f1337a24dba9f342c736ca18", "pr_number": "43468", "files_changed": ["torch/testing/_internal/common_distributed.py"], "labels": ["merged"]}, "a91e1cedc5": {"title": "Reduce number of hypothesis tests in CI (#43591)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43591\n\n100 randomized inputs vs 50 doesn't change the balance that much but speed up test runtime\n\nTest Plan: CI\n\nReviewed By: orionr, seemethere\n\nDifferential Revision: D23332393\n\nfbshipit-source-id: 7a8ff9127ee3e045a83658a7a670a844f3862987", "pr_number": "43591", "files_changed": ["caffe2/python/hypothesis_test_util.py", "torch/testing/_internal/common_utils.py"], "labels": ["fb-exported", "merged"]}, "6459f0a077": {"title": "added rocm 3.7 docker image (#43576)", "body": "Summary:\nAdded bionic rocm 3.7 docker image\n\n- jeffdaily\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43576\n\nReviewed By: malfet\n\nDifferential Revision: D23352310\n\nPulled By: seemethere\n\nfbshipit-source-id: fd544b3825d8c25587f5765332c0a8ed1fa63c6e", "pr_number": "43576", "files_changed": [".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh"], "labels": ["merged", "module: build", "module: rocm", "open source", "triaged"]}, "db1fbc5729": {"title": "[OACR][NLU] Add aten::str operator (#43573)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43573\n\nWe recently updated the Stella NLU model in D23307228, and the App started to crash with `Following ops cannot be found:{aten::str, }`.\n\nTest Plan: Verified by installing the assistant-playground app on Android.\n\nReviewed By: czlx0701\n\nDifferential Revision: D23325409\n\nfbshipit-source-id: d670242868774bb0aef4be5c8212bc3a3f2f667c", "pr_number": "43573", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "306eb3def7": {"title": "Additional error checking for `torch.cuda.nccl` APIs. (#43247)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43247\n\n`torch.cuda.nccl` APIs didn't throw appropriate errors when called\nwith inputs/outputs that were of the wrong type and it resulted in some cryptic\nerrors instead.\n\nAdding some error checks with explicit error messages for these APIs.\nghstack-source-id: 110683546\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23206069\n\nfbshipit-source-id: 8107b39d27f4b7c921aa238ef37c051a9ef4d65b", "pr_number": "43247", "files_changed": ["test/distributed/test_nccl.py", "torch/cuda/nccl.py"], "labels": ["merged"]}, "0521c71241": {"title": "[D23047144 Duplicate][2/3][lite interpreter] add metadata when saving and loading models for mobile (#43584)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43584\n\n1. add `metadata.pkl` to `.bc` file which includes the model info that we are interested in\n2. load `metadata.pkl` as a attribute `unordered_map<string, string>` in the module\nghstack-source-id: 110730013\n\nTest Plan:\n- CI\n```buck build //xplat/caffe2:jit_module_saving\n```\n```buck build //xplat/caffe2:torch_mobile_core\n```\n\nReviewed By: xcheng16\n\nDifferential Revision: D23330080\n\nfbshipit-source-id: 5d65bd730b4b566730930d3754fa1bf16aa3957e", "pr_number": "43584", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/serialization/export.h", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["merged", "oncall: jit"]}, "769b9381fc": {"title": "DDP Communication hook: Fix the way we pass future result to buckets. (#43307)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43307\n\nI identified a bug with DDP communication hook while I was trying accuracy benchmarks: I was getting `loss=nan`.\n\nLooks like when we re-`initialize_bucketviews` with the value of `future_work`, as `Reducer::mark_variable_ready_dense` does `bucket_view.copy_(grad)` it wasn't copying the `grads` back to the contents since `bucket_view` wouldn't have any relationship with `contents` after re-intitializing it with something else. As we have multiple iterations, this was causing problems.\nI solved this by adding two states for `bucket_view`:\n```\n    // bucket_views_in[i].copy_(grad) and\n    // grad.copy_(bucket_views_out[i])\n    // provide convenient ways to move grad data in/out of contents.\n    std::vector<at::Tensor> bucket_views_in;\n    std::vector<at::Tensor> bucket_views_out;\n```\n\nI included two additional unit tests where we run multiple iterations for better test coverage:\n1) `test_accumulate_gradients_no_sync_allreduce_hook`\n2) `test_accumulate_gradients_no_sync_allreduce_with_then_hook`.\n\nghstack-source-id: 110728299\n\nTest Plan:\nRun `python test/distributed/test_c10d.py`, some perf&accuracy benchmarks.\n\nNew tests:\n`test_accumulate_gradients_no_sync_allreduce_hook`\n`test_accumulate_gradients_no_sync_allreduce_with_then_hook`\n\nAcc benchmark results look okay:\nf214188350\n\nReviewed By: agolynski\n\nDifferential Revision: D23229309\n\nfbshipit-source-id: 329470036cbc05ac12049055828495fdb548a082", "pr_number": "43307", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/comm.h", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h"], "labels": ["merged"]}, "5a15f56668": {"title": "match batchmatmul on 1.0.0.6 (#43559)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43559\n\n- remove mkl strided gemm since it was acting weird in some cases, use the plain for loop for gemm for now, it will have performance implications but this closes the gap for the ctr_instagram_5x model\n- reproduced the failure scenario of batchmatmul on ctr_instagram_5x by increasing the dimensions of the inputs\n- added an option in netrunner to skip bmm if needed\n\nTest Plan:\n- net runner passes with ctr_instagram 5x\n- bmm unit test repros the discrepancy fixed\n\nReviewed By: amylittleyang\n\nDifferential Revision: D23320857\n\nfbshipit-source-id: 7d5cfb23c1b0d684e1ef766f1c1cd47bb86c9757", "pr_number": "43559", "files_changed": ["caffe2/contrib/fakelowp/fp16_gemm_utils.cc", "caffe2/contrib/fakelowp/test/test_batchmatmul_nnpi_fp16.py"], "labels": ["fb-exported", "merged"]}, "6c28df7ceb": {"title": "[fx] add test for args/kwargs handling (#43640)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43640\n\n+ added a `self.checkGraphModule` utility function to wrap the common\ntest assert pattern.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23356262\n\nPulled By: suo\n\nfbshipit-source-id: a50626dcb01246d0dbd442204a8db5958cae23ab", "pr_number": "43640", "files_changed": ["test/test_fx.py"], "labels": ["merged"]}, "cf26050e29": {"title": "[pytorch] Move TensorIteratorConfig method implementation to cpp file (#43554)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43554\n\nMove function implementations in the TensorIteratorConfig Class from TensorIterator.h to TensorIterator.cpp to avoid this issue: https://github.com/pytorch/pytorch/issues/43300\n\nReviewed By: malfet\n\nDifferential Revision: D23319007\n\nfbshipit-source-id: 6cc3474994ea3094a294f795ac6998c572d6fb9b", "pr_number": "43554", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h"], "labels": ["fb-exported", "merged", "triaged"]}, "88e35fb8bd": {"title": "Skip SVD tests when no lapack (#43566)", "body": "Summary:\nThese tests are failing on one of my system that does not have lapack\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43566\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23325378\n\nPulled By: mruberry\n\nfbshipit-source-id: 5d795e460df0a2a06b37182d3d4084d8c5c8e751", "pr_number": "43566", "files_changed": ["test/test_torch.py"], "labels": ["merged", "open source"]}, "1bda5e480c": {"title": "Add Python code coverage (#43600)", "body": "Summary:\nReplace  `test` with  `coverage_test` stage for `pytorch-linux-bionic-py3.8-gcc9` configuration\nAdd `coverage.xml` to the list of ignored files\nAdd `codecov.yml` that maps installed pytorch folders back to original locations\nCleanup coverage option utilization in `run_test.py` and adapt it towards combining coverage reports across the runs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43600\n\nReviewed By: seemethere\n\nDifferential Revision: D23351877\n\nPulled By: malfet\n\nfbshipit-source-id: acf78ae4c8f3e23920a76cce1d50f2821b83eb06", "pr_number": "43600", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".gitignore", ".jenkins/pytorch/test.sh", "codecov.yml", "test/run_test.py"], "labels": ["merged"]}, "9ca338a9d4": {"title": "[ONNX] Modified slice node in inplace ops pass (#43275)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42292\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43275\n\nReviewed By: hl475\n\nDifferential Revision: D23352540\n\nPulled By: houseroad\n\nfbshipit-source-id: 7fce3087c333efe3db4b03e9b678d0bee418e93a", "pr_number": "43275", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.cpp"], "labels": ["merged", "oncall: jit", "open source"]}, "033b7ae3ef": {"title": "implement NumPy-like functionality maximum, minimum (#42579)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/38349\n\nImplement NumPy-like functions `maximum` and `minimum`.\nThe `maximum` and `minimum` functions compute input tensors element-wise, returning a new array with the element-wise maxima/minima.\n\nIf one of the elements being compared is a NaN, then that element is returned, both `maximum` and `minimum` functions do not support complex inputs.\n\nThis PR also promotes the overloaded versions of torch.max and torch.min, by re-dispatching binary `torch.max` and `torch.min` to `torch.maximum` and `torch.minimum`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42579\n\nReviewed By: mrshenli\n\nDifferential Revision: D23153081\n\nPulled By: mruberry\n\nfbshipit-source-id: 803506c912440326d06faa1b71964ec06775eac1", "pr_number": "42579", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["merged", "module: numpy", "open source", "triaged"]}, "0bf27d64f4": {"title": "Fix NaN propagation in fuser's min/max implementation (#43590)", "body": "Summary:\nfmax/fmin propagate the number if one argument is NaN, which doesn't match the eager mode behavior.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43590\n\nReviewed By: mruberry\n\nDifferential Revision: D23338664\n\nPulled By: bertmaher\n\nfbshipit-source-id: b0316a6f01fcf8946ba77621efa18f339379b2d0", "pr_number": "43590", "files_changed": ["test/test_jit_fuser.py", "torch/csrc/jit/codegen/fuser/codegen.cpp"], "labels": ["merged", "oncall: jit"]}, "f73e32cd04": {"title": "Reduce amount of work done within a global lock within ParallelLoadOp (#43508)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43508\n\nDifferential Revision: D22952007\n\nfbshipit-source-id: 11e28d20175271e6068edce8cb36f9fcf867a02a", "pr_number": "43508", "files_changed": ["caffe2/operators/load_save_op_util.cc", "caffe2/operators/load_save_op_util.h"], "labels": ["fb-exported", "merged"]}, "00c1501bc0": {"title": "[JIT] Cast return values of functions returning Any (#42259)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42259\n\n**Summary**\nThis commit modifies IR generation to insert explicit cast that cast\neach return value to `Any` when a function is annotated as returning `Any`.\nThis precludes the failure in type unification (see below) that caused\nthis issue.\n\nIssue #41962 reported that the use of an `Any` return type in\ncombination with different code paths returning values of different\ntypes causes a segmentation fault. This is because the exit transform\npass tries to unify the different return types, fails, but silently sets\nthe type of the if node to c10::nullopt. This causes problems later in\nshape analysis when that type object is dereferenced.\n\n**Test Plan**\nThis commit adds a unit test that checks that a function similar to the\none in #41962 can be scripted and executed.\n\n**Fixes**\nThis commit fixes #41962.\n\nDifferential Revision: D22883244\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison, yf225\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 523d002d846239df0222cd07f0d519956e521c5f", "pr_number": "42259", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/test_jit_py3.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/passes/shape_analysis.cpp"], "labels": ["merged", "oncall: jit"]}, "c4e5ab6ff2": {"title": "[TensorExpr] Disable a flaky test. (#43678)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43678\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D23363651\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 9557fbfda28633cea169836b02d034e9c950bc71", "pr_number": "43678", "files_changed": ["test/test_tensorexpr.py"], "labels": ["merged"]}, "28be3ef2f2": {"title": "Fix hipify script for pytorch extensions (#43528)", "body": "Summary:\nPyTorch extensions can have .cpp or .h files which contain CUDA code that needs to be hipified. The current hipify script logic has overly strict conditions to determine which files get considered for hipification: https://github.com/pytorch/pytorch/blob/master/torch/utils/hipify/hipify_python.py#L146\n\nThese conditions might apply well to pytorch/caffe2 source code, but are overconstrained for third-party extensions.\n`is_pytorch_file` conditions: https://github.com/pytorch/pytorch/blob/master/torch/utils/hipify/hipify_python.py#L549\n`is_caffe2_gpu_file` conditions: https://github.com/pytorch/pytorch/blob/master/torch/utils/hipify/hipify_python.py#L561\n\nThis PR relaxes these conditions if we're hipifying a pytorch extension (specified by `is_pytorch_extension=True`) and considers all the file extensions specified using the `extensions` parameter: https://github.com/pytorch/pytorch/blob/master/torch/utils/hipify/hipify_python.py#L820\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43528\n\nReviewed By: mruberry\n\nDifferential Revision: D23328272\n\nPulled By: ngimel\n\nfbshipit-source-id: 1e9c3a54ae2da65ac596a7ecd5539f3e14eeed88", "pr_number": "43528", "files_changed": ["torch/utils/hipify/hipify_python.py"], "labels": ["merged", "open source", "triaged"]}, "01b5c06254": {"title": "[fix] handle empty args in chain_matmul (#43553)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41817\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43553\n\nReviewed By: agolynski\n\nDifferential Revision: D23342586\n\nPulled By: mruberry\n\nfbshipit-source-id: c6349f8fa9fcefcf03681d92c085a21265d1e690", "pr_number": "43553", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "test/test_torch.py"], "labels": ["merged", "open source", "topic: linear algebra", "triaged"]}, "79e6aaeb4c": {"title": "pull empty() out of use_c10_dispatcher: full (#43572)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43572\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D23326019\n\nPulled By: bhosmer\n\nfbshipit-source-id: 10a4d7ffe33b4be4ae45396725456c6097ce1757", "pr_number": "43572", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/test/basic.cpp", "aten/src/ATen/test/extension_backend_test.cpp", "test/cpp_extensions/msnpu_extension.cpp"], "labels": ["merged"]}, "a070c619b9": {"title": "[FX] Native callables in FX lowering (#43426)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43426\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23273427\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 3a9d04486c72933d8afd9c181578fe98c3d825b0", "pr_number": "43426", "files_changed": ["test/cpp/jit/test_custom_class.cpp", "test/test_fx.py", "torch/fx/__init__.py"], "labels": ["merged", "oncall: jit"]}, "f63d06a57b": {"title": "Fix docs for kwargs, a-e (#43583)", "body": "Summary:\nTo reduce the chance of conflicts, not all ops are fixed. Ops starting with letter `f` will be fixed in separate PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43583\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23330347\n\nPulled By: mruberry\n\nfbshipit-source-id: 3387cb1e495faebd16fb183039197c6d90972ad4", "pr_number": "43583", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "48e08f884e": {"title": "C++ APIs TransformerEncoder (#43187)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43187\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23182770\n\nPulled By: glaringlee\n\nfbshipit-source-id: 968846138d4b1c391a74277216111dba8b72d683", "pr_number": "43187", "files_changed": ["test/cpp/api/transformer.cpp", "torch/csrc/api/include/torch/nn/modules.h", "torch/csrc/api/include/torch/nn/modules/transformercoder.h", "torch/csrc/api/include/torch/nn/options.h", "torch/csrc/api/include/torch/nn/options/transformercoder.h", "torch/csrc/api/src/nn/modules/transformer.cpp", "torch/csrc/api/src/nn/options/transformer.cpp"], "labels": ["merged"]}, "de84db2a9d": {"title": "[TensorExpr] Add aten::sum lowering to the kernel (#43585)", "body": "Summary:\nHandles all dimensions and selected dimensions, per PyTorch semantics.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43585\n\nTest Plan: test_tensorexpr\n\nReviewed By: bertmaher\n\nDifferential Revision: D23362382\n\nPulled By: asuhan\n\nfbshipit-source-id: e8d8f1197a026be0b46603b0807d996a0de5d58c", "pr_number": "43585", "files_changed": ["test/cpp/tensorexpr/test_kernel.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["merged", "oncall: jit"]}, "c25d0015f0": {"title": "Autograd code clean up (#43167)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43167\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23222358\n\nPulled By: anjali411\n\nfbshipit-source-id: b738c63b294bcee7d680fa64c6300007d988d218", "pr_number": "43167", "files_changed": ["torch/autograd/gradcheck.py"], "labels": ["merged"]}, "288a2effa0": {"title": "Operator generator based on templated selective build. (#43456)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43456\n\nIntroduce the template OperatorGenerator, which returns an optional Operator. It's null if the templated bool value is null.\n\nRegisterOperators() is updated to take the optional Operator. A null will not be registered.\n\nWith this update the selective operator registration can be done at compile time. Tests are added to show an operator can be registered if it's in a whitelist and it will not be registered if it's not in the whitelist.\n\nTest Plan: Imported from OSS\n\nReviewed By: ljk53\n\nDifferential Revision: D23283563\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 456e0c72b2f335256be800aeabb797bd83bcf0b3", "pr_number": "43456", "files_changed": ["aten/src/ATen/core/op_registration/op_whitelist.h", "test/cpp/jit/test_custom_operators.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/runtime/custom_operator.h", "torch/csrc/jit/runtime/operator.h"], "labels": ["merged", "oncall: jit"]}, "73dcfc5e78": {"title": "Update RNN op registration format (#43599)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43599\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D23350223\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 94c528799e31b2ffb02cff675604e7cce639687f", "pr_number": "43599", "files_changed": ["aten/src/ATen/native/RNN.cpp"], "labels": ["merged"]}, "5a1aa0e21e": {"title": "[reland][quant][graphmode][fx] Add e2e test on torchvision (#43587)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43587\n\nAdd tests for graph mode quantization on torchvision and make sure it matches\ncurrent eager mode quantization\n\nTest Plan:\nImported from OSS\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23331253\n\nfbshipit-source-id: 0445a44145d99837a2c975684cd0a0b7d965c8f9", "pr_number": "43587", "files_changed": ["test/quantization/test_quantize_fx.py", "test/test_quantization.py", "torch/fx/graph_module.py", "torch/quantization/fx/fuse.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "3830998ac3": {"title": "[fx] When generating names, avoid shadowing builtins (#43653)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43653\n\nWhen nodes are created without an explicit name, a name is generated for\nit based on the target. In these cases, we need to avoid shadowing\nbuiltin names. Otherwise, code like:\n```\na.foo.bar\n```\nresults in pretty-printed code like:\n```\ngetattr = a.foo\ngetattr_1 = getattr.bar\n```\n\nWhile this is technically allowed in Python, it's probably a bad idea,\nand more importantly is not supported by TorchScript (where `getattr` is\nhardcoded).\n\nThis PR changes the name generation logic to avoid shadowing all\nbuiltins and langauge keywords. We already do this for PyTorch\nbuilt-ins, so just extend that logic. So now the generated code will\nlook like:\n\n```\ngetattr_1 = a.foo\ngetattr_2 = getattr_1.bar\n```\nFixes #43522\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23357420\n\nPulled By: suo\n\nfbshipit-source-id: 91e9974adc22987eca6007a2af4fb4fe67f192a8", "pr_number": "43653", "files_changed": ["test/test_fx.py", "torch/fx/graph.py"], "labels": ["merged"]}, "bff741a849": {"title": "Improve save_for_mobile cxx binary (#43721)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43721\n\nWe can combine optimization pass and save_for_mobile together to reduce friction. Since lite interpreter model can also be used in full JIT, I don't think we need the option to save it as full JIT model.\n\nAlso\n- improved usage message\n- print op list before and after optimization pass\n\nTest Plan:\n```\nbuck run //xplat/caffe2:optimize_for_mobile -- --model=/home/linbin/sparkspot.pt\n\nBuilding: finished in 12.4 sec (100%) 2597/2597 jobs, 2 updated\n  Total time: 12.5 sec\n\npt_operator_library(\n        name = \"old_op_library\",\n        ops = [\n                \"aten::_convolution\",\n                \"aten::adaptive_avg_pool2d\",\n                \"aten::add_.Tensor\",\n                \"aten::batch_norm\",\n                \"aten::mul.Tensor\",\n                \"aten::relu_\",\n                \"aten::softplus\",\n                \"aten::sub.Tensor\",\n        ],\n)\n\npt_operator_library(\n        name = \"new_op_library\",\n        ops = [\n                \"aten::adaptive_avg_pool2d\",\n                \"aten::add_.Tensor\",\n                \"aten::batch_norm\",\n                \"aten::mul.Tensor\",\n                \"aten::relu_\",\n                \"aten::softplus\",\n                \"aten::sub.Tensor\",\n                \"prepacked::conv2d_clamp_run\",\n        ],\n)\n\nThe optimized model for lite interpreter was saved to /home/linbin/sparkspot_mobile_optimized.bc\n```\n\n```\nbuck run //xplat/caffe2:optimize_for_mobile -- --model=/home/linbin/sparkspot.pt --backend=vulkan\n```\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23363533\n\nfbshipit-source-id: f7fd61aaeda5944de5bf198e7f93cacf8368babd", "pr_number": "43721", "files_changed": ["binaries/optimize_for_mobile.cc"], "labels": ["fb-exported", "merged"]}, "04ccd3ed77": {"title": "Fix bazel dependencies (#43688)", "body": "Summary:\nAdd `header_template_rule` to `substitution.bzl`\nUse it in BUILD.bazel to specify dependencies on autogenerated headers\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43688\n\nTest Plan: bazel build --sandbox_writable_path=$HOME/.ccache -c dbg :caffe2\n\nReviewed By: seemethere\n\nDifferential Revision: D23374702\n\nPulled By: malfet\n\nfbshipit-source-id: 180dd996d1382df86258bb6abab9f2c7e964152e", "pr_number": "43688", "files_changed": ["BUILD.bazel", "third_party/substitution.bzl"], "labels": ["merged"]}, "cdc3e232e9": {"title": "Add `__str__` and `__repr__` bindings to SourceRange (#43601)", "body": "Summary:\nAdded the bindings for `__str__` and `__repr__` methods for SourceRange\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43601\n\nTest Plan:\n`python test/test_jit.py`\n\ncc gmagogsfm\n\nReviewed By: agolynski\n\nDifferential Revision: D23366500\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: ab4be6e8f9ad5f67a323554437878198483f4320", "pr_number": "43601", "files_changed": ["test/test_jit.py", "torch/csrc/jit/python/python_tree_views.cpp"], "labels": ["merged", "oncall: jit", "open source", "triaged"]}, "5da97a38d1": {"title": "Check if input is ChannelsLast or ChannelsLast3d for quantized AdaptivePool3d. (#42780)", "body": "Summary:\ncc z-a-f, vkuzo. This serves as a very simple first step to the issue mentioned in https://github.com/pytorch/pytorch/issues/42779.\n\n# Description\nSince `ChannelsLast` and `ChannelsLast3d` are not equivalent [(MemoryFormat.h)](https://github.com/pytorch/pytorch/blob/4e93844ab168ee0cf1aaa1b4712d6aad0e2972f8/c10/core/MemoryFormat.h#L27), the \"fast\" path for `NDHWC` is ignored.\n\nThis PR would produce the expected behaviour for 4 (5 if including batch) dimensional tensors.\n\n# Benchmarks\n## Notes\n- For channels `< 8`, it is actually slower than before.\n- For `qint32`, it is actually `2x` slower than before.\n- For channels `> 8`, the execution time decreases up to `9-10` times in the benchmarks.\n- While execution time does improve, it remains slower than the `contiguous` variant when channels `> 64`.\n\n## C++\n<img width=\"1667\" alt=\"before_after_py\" src=\"https://user-images.githubusercontent.com/37529096/89711911-5da22d80-d9e1-11ea-9b30-0c23d46c2c93.png\">\n\n## Python\n<img width=\"1523\" alt=\"before_after_cpp\" src=\"https://user-images.githubusercontent.com/37529096/89711906-58dd7980-d9e1-11ea-9696-1963f394198a.png\">\n\n## Reproduce\nSee https://github.com/pytorch/pytorch/issues/42779.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42780\n\nReviewed By: smessmer\n\nDifferential Revision: D23035424\n\nPulled By: z-a-f\n\nfbshipit-source-id: 15594846f66b73c22d2371eb8e47c472324d6139", "pr_number": "42780", "files_changed": ["aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp"], "labels": ["merged", "open source"]}, "d1c4d75c14": {"title": "Add API for unexecuted op (#43629)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43629\n\nWe have a few places where we count the size a block / subgraph - it's nice to have a shared API to ignore operators that are not executed in the optimized graph (will be used when i add a new profiling node in PR ^^)\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23358807\n\nPulled By: eellison\n\nfbshipit-source-id: 62c745d9025de94bdafd9f748f7c5a8574cace3f", "pr_number": "43629", "files_changed": ["torch/csrc/jit/ir/ir.h", "torch/csrc/jit/passes/create_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/loop_unrolling.cpp"], "labels": ["merged", "oncall: jit"]}, "e189ef5577": {"title": "Refactor pass to class (#43630)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43630\n\nNo functional changes here - just refactoring specialize autograd zero to a class, and standardizing its API to take in a shared_ptr<Graph>\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23358805\n\nPulled By: eellison\n\nfbshipit-source-id: 42e19ef2e14df66b44592252497a47d03cb07a7f", "pr_number": "43630", "files_changed": ["torch/csrc/jit/passes/specialize_autogradzero.cpp", "torch/csrc/jit/passes/specialize_autogradzero.h", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["merged", "oncall: jit"]}, "a4cf4c2437": {"title": "refactor tests (#43631)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43631\n\nI added a new test for just profiler stuff - I don't think the test should go in test_jit.py. Maybe this should just go in test_tensorexpr_fuser, but I'm not really testing tensorexpr stuff either... LMK\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23358810\n\nPulled By: eellison\n\nfbshipit-source-id: 074238e1b60e4c4a919a052b7a5312b790ad5d82", "pr_number": "43631", "files_changed": ["test/jit/test_profiler.py", "test/test_jit.py", "test/test_jit_fuser.py", "test/test_jit_fuser_te.py", "torch/testing/_internal/common_utils.py", "torch/testing/_internal/jit_utils.py"], "labels": ["merged", "oncall: jit"]}, "a19fd3a388": {"title": "Add undefined specializations in backward (#43632)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43632\n\nSpecialize the backward graph by guarding on the undefinedness of the input tensors. The graph will look like:\n```\nty1, ty2, succesful_checks = prim::TypeCheck(...)\nif (succesful_checks)\n-> optimized graph\nelse:\n-> fallback graph\n```\n\nSpecializing on the undefinedness of tensors allows us to clean up the\n```\nif any_defined(inputs):\n outputs = <original_computation>\nelse:\n outputs = autograd zero tensors\n```\nblocks that make up the backward graph, so that we can fuse the original_computation nodes together.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23358808\n\nPulled By: eellison\n\nfbshipit-source-id: f5bb28f78a4a3082ecc688a8fe0345a8a098c091", "pr_number": "43632", "files_changed": ["test/jit/test_profiler.py", "torch/csrc/jit/passes/specialize_autogradzero.cpp", "torch/csrc/jit/runtime/autodiff.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["merged", "oncall: jit"]}, "01f974eb1e": {"title": "Specialize optionals for grad_sum_to_size (#43633)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43633\n\nIn the backward graph, _grad_sum_to_size is inserted whenever a possibly broadcasting op is called:\"\n`\"aten::_grad_sum_to_size(Tensor(a) self, int[]? size) -> Tensor(a)\"`\n If a broadcast occurred, a sum is called, otherwise the second input is None and it is a no-op. Most of the time, it's a no-op (in the fast RNNs benchmark > 90% of the time).\n\nWe can get rid of this op by profiling the optionality of the second input. I added `prim::profile_optional` to do this, which counts the number of times it saw a None value and the number of times it saw a value present. When specializing the backward graph, we insert checks for values we profiled as None, and in the optimized block can remove the grad_sum_to_size calls that use those values.\n\nIn the future we may revisit this when NNC supports reductions and we want to replace grad_sum_to_size with sums as well, but I think this is worth landing now.\n\nTest Plan: Imported from OSS\n\nReviewed By: bwasti, ZolotukhinM\n\nDifferential Revision: D23358809\n\nPulled By: eellison\n\nfbshipit-source-id: a30a148ca581370789d57ba082d23cbf7ef2cd4d", "pr_number": "43633", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_profiler.py", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/ir.h", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/specialize_autogradzero.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/profiling_record.cpp", "torch/csrc/jit/runtime/profiling_record.h", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["merged", "oncall: jit"]}, "9a2d4d550e": {"title": "update build flags for benchmark binaries", "body": "Summary:\nSuggested by Shoaib Meenai, we should use mode/ndk_libcxx to replace mode/gnustl.\n\nThis diff updated all build flags for caffe2 and pytorch in aibench. For easy management, I created two mode files in xplat/caffe2/mode, and delete buckconfig.ptmobile.pep.\n\nTest Plan:\ncaffe2\n```\nbuck run aibench:run_bench -- -b aibench/specifications/models/caffe2/squeezenet/squeezenet.json --remote --devices s9f\n```\nhttps://our.intern.facebook.com/intern/aibench/details/433604719423848\n\nfull jit\n```\nbuck run aibench:run_bench -- -b aibench/specifications/models/pytorch/fbnet/fbnet_mobile_inference.json --platform android/full_jit --framework pytorch --remote --devices SM-G960F-8.0.0-26\n```\nhttps://our.intern.facebook.com/intern/aibench/details/189359776958060\n\nlite interpreter\n```\nbuck run aibench:run_bench -- -b aibench/specifications/models/pytorch/fbnet/fbnet_mobile_inference.json --platform android --framework pytorch --remote --devices s9f\n```\nhttps://our.intern.facebook.com/intern/aibench/details/568178969092066\n\nReviewed By: smeenai\n\nDifferential Revision: D23338089\n\nfbshipit-source-id: 62f4ae2beb004ceaab1f73f4de8ff9e0c152d5ee", "pr_number": null, "files_changed": ["mode/aibench_caffe2_android", "mode/aibench_pytorch_android"], "labels": []}, "3afd24d62c": {"title": "[pytorch] check in default generated op dependency graph (#43570)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43570\n\nAdd the default op dependency graph to the source tree - use it if user runs\ncustom build in dynamic dispatch mode without providing the graph.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23326988\n\nPulled By: ljk53\n\nfbshipit-source-id: 5fefe90ca08bb0ca20284e87b70fe1dba8c66084", "pr_number": "43570", "files_changed": ["cmake/Codegen.cmake", "tools/code_analyzer/default_op_deps.yaml"], "labels": ["merged"]}, "3a0e35c9f2": {"title": "[pytorch] deprecate static dispatch (#43564)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43564\n\nStatic dispatch was originally introduced for mobile selective build.\n\nSince we have added selective build support for dynamic dispatch and\ntested it in FB production for months, we can deprecate static dispatch\nto reduce the complexity of the codebase.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23324452\n\nPulled By: ljk53\n\nfbshipit-source-id: d2970257616a8c6337f90249076fca1ae93090c7", "pr_number": "43564", "files_changed": [".circleci/cimodel/data/simple/mobile_definitions.py", ".circleci/config.yml", ".jenkins/pytorch/build-mobile.sh", "BUILD.bazel", "CMakeLists.txt", "aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/cudnn/AutocastRNN.cpp", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/TensorProperties.cpp", "aten/src/ATen/templates/TensorMethods.cpp", "c10/macros/cmake_macros.h.in", "caffe2/core/macros.h.in", "cmake/Codegen.cmake", "cmake/Summary.cmake", "scripts/build_android.sh", "scripts/build_ios.sh", "scripts/build_mobile.sh", "test/mobile/custom_build/build.sh", "tools/code_analyzer/build.sh", "tools/jit/gen_unboxing_wrappers.py", "torch/csrc/jit/runtime/register_c10_ops.cpp"], "labels": ["merged", "oncall: jit"]}, "8032dbc117": {"title": "Add Rowwise Prune PyTorch op (#42708)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42708\n\nAdd rowwise prune pytorch op.\n\nThis operator introduces sparsity to the 'weights' matrix with the help\nof the importance indicator 'mask'.\n\nA row is considered important and not pruned if the mask value for that\nparticular row is 1(True) and not important otherwise.\n\nTest Plan:\nbuck test caffe2/torch/fb/sparsenn:test -- rowwise_prune\nbuck test caffe2/test:pruning\n\nReviewed By: supriyar\n\nDifferential Revision: D22849432\n\nfbshipit-source-id: 456f4f77c04158cdc3830b2e69de541c7272a46d", "pr_number": "42708", "files_changed": ["aten/src/ATen/native/RowwisePrune.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_pruning_op.py", "torch/overrides.py"], "labels": ["fb-exported"]}, "87d7c362b1": {"title": "[JIT] Add JIT support for torch.no_grad (#41371)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41371\n\n**Summary**\nThis commit enables the use of `torch.no_grad()` in a with item of a\nwith statement within JIT. Note that the use of this context manager as\na decorator is not supported.\n\n**Test Plan**\nThis commit adds a test case to the existing with statements tests for\n`torch.no_grad()`.\n\n**Fixes**\nThis commit fixes #40259.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D22649519\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 7fa675d04835377666dfd0ca4e6bc393dc541ab9", "pr_number": "41371", "files_changed": ["test/jit/test_with.py", "test/test_jit.py", "torch/_jit_internal.py", "torch/autograd/grad_mode.py", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/jit/__init__.py", "torch/jit/_builtins.py", "torch/jit/_script.py", "torch/jit/frontend.py"], "labels": ["merged", "oncall: jit"]}, "1a21c92364": {"title": "[ONNX] Update in scatter ONNX export when scalar src has different type (#43440)", "body": "Summary:\n`torch.scatter` allows `src` to be of different type when `src` is a scalar. This requires a an explicit cast op to be inserted in the ONNX graph because ONNX `ScatterElements` does not allow different types. This PR updates the export of `torch.scatter` with this logic.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43440\n\nReviewed By: hl475\n\nDifferential Revision: D23352317\n\nPulled By: houseroad\n\nfbshipit-source-id: c9eeddeebb67fc3c40ad01def134799ef2b4dea6", "pr_number": "43440", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source", "triaged"]}, "654ab209c6": {"title": "[JIT] Disable broken tests (#43750)", "body": "Summary:\nThese started failing since **https://github.com/pytorch/pytorch/pull/43633** for indecipherable reasons; temporarily disable. The errors on the PRs were\n```\nDownloading workspace layers\n  workflows/workspaces/3ca9ca71-7449-4ae1-bb7b-b7612629cc62/0/8607ba99-5ced-473b-b60a-0025b48739a6/0/105.tar.gz - 8.4 MB\nApplying workspace layers\n  8607ba99-5ced-473b-b60a-0025b48739a6\n```\nwhich is not too helpful...\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43750\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23388060\n\nPulled By: eellison\n\nfbshipit-source-id: 96afa0160ec948049f3e194787a0a7ddbeb5124a", "pr_number": "43750", "files_changed": ["test/test_cpp_extensions_jit.py"], "labels": ["merged"]}, "f06d3904f2": {"title": "Land `code_coverage_tool` to `caffe2/tools` folder", "body": "Summary:\nMove `code_coverage_tool` from `experimental` folder to `caffe2/tools` folder.\n\nNot sure if the fb related code is something we don't want to share with the oss. Can reviewers please help me check with `fbcode_coverage.py` and files in `fbcode/` folder?\n\nTest Plan: Test locally\n\nReviewed By: malfet\n\nDifferential Revision: D23379383\n\nfbshipit-source-id: f6782389ebb1b147eaf6d3664b5955db79d24ff3", "pr_number": null, "files_changed": ["tools/code_coverage_tool/README.md", "tools/code_coverage_tool/oss_coverage.py", "tools/code_coverage_tool/package/__init__.py", "tools/code_coverage_tool/package/oss/__init__.py", "tools/code_coverage_tool/package/oss/cov_json.py", "tools/code_coverage_tool/package/oss/init.py", "tools/code_coverage_tool/package/oss/run.py", "tools/code_coverage_tool/package/oss/utils.py", "tools/code_coverage_tool/package/tool/__init__.py", "tools/code_coverage_tool/package/tool/clang_coverage.py", "tools/code_coverage_tool/package/tool/gcc_coverage.py", "tools/code_coverage_tool/package/tool/parser/__init__.py", "tools/code_coverage_tool/package/tool/parser/coverage_record.py", "tools/code_coverage_tool/package/tool/parser/gcov_coverage_parser.py", "tools/code_coverage_tool/package/tool/parser/llvm_coverage_parser.py", "tools/code_coverage_tool/package/tool/parser/llvm_coverage_segment.py", "tools/code_coverage_tool/package/tool/print_report.py", "tools/code_coverage_tool/package/tool/summarize_jsons.py", "tools/code_coverage_tool/package/tool/utils.py", "tools/code_coverage_tool/package/util/__init__.py", "tools/code_coverage_tool/package/util/setting.py", "tools/code_coverage_tool/package/util/utils.py", "tools/code_coverage_tool/package/util/utils_init.py"], "labels": []}, "c7787f7fbf": {"title": "[numpy compatibility]Fix `argmin/argmax` when multiple max/min values (#42004)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41998\nFixes https://github.com/pytorch/pytorch/issues/22853\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42004\n\nReviewed By: ngimel\n\nDifferential Revision: D23049003\n\nPulled By: mruberry\n\nfbshipit-source-id: a6fddbadfec4b8696730550859395ce4f0cf50d6", "pr_number": "42004", "files_changed": ["aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "caffe2/python/operator_test/arg_ops_test.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/onnx/symbolic_opset12.py"], "labels": ["merged", "module: numpy", "open source", "topic: bc-breaking", "triaged"]}, "b29375840a": {"title": "Revert D23379383: Land `code_coverage_tool` to `caffe2/tools` folder", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23379383 (https://github.com/pytorch/pytorch/commit/f06d3904f2c7b6e47ffa532f4a22b9e18941bb90)\n\nOriginal commit changeset: f6782389ebb1\n\nfbshipit-source-id: 33a26761deb58dfe81314ea912bf485c5fc962b7", "pr_number": null, "files_changed": ["tools/code_coverage_tool/README.md", "tools/code_coverage_tool/oss_coverage.py", "tools/code_coverage_tool/package/__init__.py", "tools/code_coverage_tool/package/oss/__init__.py", "tools/code_coverage_tool/package/oss/cov_json.py", "tools/code_coverage_tool/package/oss/init.py", "tools/code_coverage_tool/package/oss/run.py", "tools/code_coverage_tool/package/oss/utils.py", "tools/code_coverage_tool/package/tool/__init__.py", "tools/code_coverage_tool/package/tool/clang_coverage.py", "tools/code_coverage_tool/package/tool/gcc_coverage.py", "tools/code_coverage_tool/package/tool/parser/__init__.py", "tools/code_coverage_tool/package/tool/parser/coverage_record.py", "tools/code_coverage_tool/package/tool/parser/gcov_coverage_parser.py", "tools/code_coverage_tool/package/tool/parser/llvm_coverage_parser.py", "tools/code_coverage_tool/package/tool/parser/llvm_coverage_segment.py", "tools/code_coverage_tool/package/tool/print_report.py", "tools/code_coverage_tool/package/tool/summarize_jsons.py", "tools/code_coverage_tool/package/tool/utils.py", "tools/code_coverage_tool/package/util/__init__.py", "tools/code_coverage_tool/package/util/setting.py", "tools/code_coverage_tool/package/util/utils.py", "tools/code_coverage_tool/package/util/utils_init.py"], "labels": []}, "8e507ad00e": {"title": "Update the div formula for numerical stability (#43627)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43414\n\nSee the issue for numerical improvements and quick benchmark.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43627\n\nReviewed By: agolynski\n\nDifferential Revision: D23350124\n\nPulled By: albanD\n\nfbshipit-source-id: 19d51640b3f200db37c32d2233a4244480e5a15b", "pr_number": "43627", "files_changed": ["tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "0ab83f7f9f": {"title": "Fixed undefined behavior in BatchedFallback (#43705)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43705\n\nThis was causing fb-internal flakiness. I'm surprised that the ASAN\nbuilds don't catch this behavior.\n\nThe problem is that dereferencing the end() pointer of a vector is\nundefined behavior. This PR fixes one callsite where BatchedFallback\ndereferences the end() pointer and adds an assert to make sure another\ncallsite doesn't do that.\n\nTest Plan:\n- Make sure all tests pass (`pytest test/test_vmap.py -v`)\n- It's hard to write a new test for this because most of the time this\ndoesn't cause a crash. It really depends on what lives at the end()\npointer.\n\nReviewed By: ezyang\n\nDifferential Revision: D23373352\n\nPulled By: zou3519\n\nfbshipit-source-id: 61ea0be80dc006f6d4e73f2c5badd75096f63e56", "pr_number": "43705", "files_changed": ["aten/src/ATen/BatchedFallback.cpp"], "labels": ["merged"]}, "bdee8e02c0": {"title": "TensorIterator: Check memory overlap in all `unary_op`s (#43418)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43418\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23298651\n\nPulled By: zou3519\n\nfbshipit-source-id: 84be498f5375813fd10cf30b8beabbd2d15210a3", "pr_number": "43418", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Pow.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/Activation.cu", "test/test_nn.py"], "labels": ["merged", "open source"]}, "065ebdb92f": {"title": "TensorIterator: Check for memory overlap in all `binary_op`s (#43419)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43419\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23298655\n\nPulled By: zou3519\n\nfbshipit-source-id: 82e0ff308a6a7e46b4342d57ddb4c1d73745411a", "pr_number": "43419", "files_changed": ["aten/src/ATen/MemoryOverlap.h", "aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Pow.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/cpu/LerpKernel.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp", "test/test_nn.py", "test/test_torch.py"], "labels": ["merged", "open source"]}, "dc0722e9b7": {"title": "TensorIterator: Check for memory overlap in all `compare_op`s (#43420)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43420\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23298650\n\nPulled By: zou3519\n\nfbshipit-source-id: 171cd17a3012880a5d248ffd0ea6942fbfb6606f", "pr_number": "43420", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/test/tensor_iterator_test.cpp", "test/test_torch.py"], "labels": ["merged", "open source"]}, "c177d25edf": {"title": "TensorIterator: Check for memory overlap in all `nullary_op`s (#43421)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43421\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23298654\n\nPulled By: zou3519\n\nfbshipit-source-id: 71b401f6ea1e3b50b830fef650927cc5b3fb940f", "pr_number": "43421", "files_changed": ["aten/src/ATen/native/Fill.cpp", "aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "test/test_torch.py"], "labels": ["merged", "open source"]}, "9063bcee04": {"title": "Don't proceed into setup.py too far if Python version is unsupported (#42870)", "body": "Summary:\nThis prevents confusing errors when the interpreter encounters some\nsyntax errors in the middle.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42870\n\nReviewed By: albanD\n\nDifferential Revision: D23269265\n\nPulled By: ezyang\n\nfbshipit-source-id: 61f62cbe294078ad4a909fa87aa93abd08c26344", "pr_number": "42870", "files_changed": ["setup.py"], "labels": ["merge-this-please", "merged", "module: build", "open source", "triaged"]}, "58a7e73a95": {"title": "[TensorExpr] Block Codegen (#40054)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40054\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D22061350\n\nPulled By: protonu\n\nfbshipit-source-id: 004f7c316629b16610ecdbb97e43036c72c65067", "pr_number": "40054", "files_changed": ["test/test_tensorexpr.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/tensorexpr/analysis.h", "torch/csrc/jit/tensorexpr/block_codegen.cpp", "torch/csrc/jit/tensorexpr/block_codegen.h", "torch/csrc/jit/tensorexpr/codegen.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.h", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["merged", "oncall: jit"]}, "b630c1870d": {"title": "Add stateful XNNPack deconvolution2d operator to torch. (#43233)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43233\n\nXNNPack is already being used for the convolution2d operation. Add the\nability for it to be used with transpose convolution.\n\nTest Plan: buck run caffe2/test:xnnpack_integration\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23184249\n\nfbshipit-source-id: 3fa728ce1eaca154d24e60f800d5e946d768c8b7", "pr_number": "43233", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/xnnpack/Common.h", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Convolution.h", "aten/src/ATen/native/xnnpack/Engine.h", "aten/src/ATen/native/xnnpack/OpContext.cpp", "aten/src/ATen/native/xnnpack/OpContext.h", "aten/src/ATen/native/xnnpack/RegisterOpContextClass.cpp", "aten/src/ATen/native/xnnpack/Shim.cpp", "test/test_xnnpack_integration.py", "torch/csrc/jit/passes/graph_rewrite_helper.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.cpp"], "labels": ["fb-exported", "merged", "oncall: jit"]}, "a76184fe1e": {"title": "grammatical error fix (#43697)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43697\n\nReviewed By: malfet\n\nDifferential Revision: D23397655\n\nPulled By: mrshenli\n\nfbshipit-source-id: fb447dcde4f83bc6650f0faa0728a1867cfa5213", "pr_number": "43697", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "1f7434d1ea": {"title": "Fix 'module' to 'model' in quantize_dynamic doc (#43693)", "body": "Summary:\nFixes issue https://github.com/pytorch/pytorch/issues/43503\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43693\n\nReviewed By: malfet\n\nDifferential Revision: D23397641\n\nPulled By: mrshenli\n\nfbshipit-source-id: bc216cea4f0a30c035e84a6cfebabd3755ef1305", "pr_number": "43693", "files_changed": ["torch/quantization/quantize.py"], "labels": ["merged", "open source"]}, "b72da0cf28": {"title": "OneDNN: report error for dilation max_pooling and replace AT_ERROR with TORCH_CHECK in oneDNN codes (#43538)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/43514.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43538\n\nReviewed By: agolynski\n\nDifferential Revision: D23364302\n\nPulled By: ngimel\n\nfbshipit-source-id: 8d17752cf33dcacd34504e32b5e523e607cfb497", "pr_number": "43538", "files_changed": ["aten/src/ATen/native/mkldnn/BinaryOps.cpp", "aten/src/ATen/native/mkldnn/Conv.cpp", "aten/src/ATen/native/mkldnn/Linear.cpp", "aten/src/ATen/native/mkldnn/MkldnnTensorMath.cpp", "aten/src/ATen/native/mkldnn/Normalization.cpp", "aten/src/ATen/native/mkldnn/Pooling.cpp", "aten/src/ATen/native/mkldnn/Relu.cpp", "aten/src/ATen/native/mkldnn/SoftMax.cpp", "aten/src/ATen/native/mkldnn/TensorFactories.cpp", "aten/src/ATen/native/mkldnn/TensorShape.cpp", "aten/src/ATen/native/mkldnn/UnaryOps.cpp", "test/test_mkldnn.py"], "labels": ["merged", "open source"]}, "be3ec6ab3e": {"title": "[caffe2][torch] correctly re-raise Manifold StorageException", "body": "Summary:\n1) Manifold raises StorageException when it see's an error: https://fburl.com/diffusion/kit3me8a\n2) torch re-raises exception: https://fburl.com/diffusion/zbw9wmpu\nIssue here, that in StorageException first argument is bool canRetry while re-raising happens with first argument being str as in all Python exceptions.\n\nTest Plan:\nExisting tests should pass. +\n```\nIn [1]: from manifold.clients.python import StorageException\nIn [2]: getattr(StorageException, \"message\", None)\nOut[2]: <attribute 'message' of 'manifold.blobstore.blobstore.types.StorageException' objects>\nIn [3]: getattr(Exception, \"message\", None) is None\nOut[3]: True\n\nReviewed By: haijunz\n\nDifferential Revision: D23195514\n\nfbshipit-source-id: baa1667dbba4086db6ec93f009e400611ac9b938", "pr_number": null, "files_changed": ["torch/_utils.py"], "labels": []}, "dc5d365514": {"title": "Fix bug in caching allocator. (#43719)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43719\n\nAccidentally this slipped through: with guard did not update the current\ncontext\n\nTest Plan: cpu_caching_allocator_test\n\nReviewed By: linbinyu\n\nDifferential Revision: D23374453\n\nfbshipit-source-id: 1d3ef21cc390d0a8bde98fb1b5c2175b40ab571b", "pr_number": "43719", "files_changed": ["c10/core/CPUCachingAllocator.cpp"], "labels": ["fb-exported", "merged"]}, "f4695203c2": {"title": "Fixes fft function calls for C++ API (#43749)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43732.\n\nRequires importing the fft namespace in the C++ API, just like the Python API does, to avoid clobbering torch::fft the function.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43749\n\nReviewed By: glaringlee\n\nDifferential Revision: D23391544\n\nPulled By: mruberry\n\nfbshipit-source-id: d477d0b6d9a689d5c154ad6c31213a7d96fdf271", "pr_number": "43749", "files_changed": ["test/cpp/api/fft.cpp", "torch/csrc/api/include/torch/all.h"], "labels": ["merged"]}, "bcec8cc3f9": {"title": "Add amax/amin (#43092)", "body": "Summary:\nAdd a max/min operator that only return values.\n\n## Some important decision to discuss\n| **Question**                          | **Current State** |\n|---------------------------------------|-------------------|\n| Expose torch.max_values to python?    | No                |\n| Remove max_values and only keep amax? | Yes               |\n| Should amax support named tensors?    | Not in this PR    |\n\n## Numpy compatibility\n\nReference: https://numpy.org/doc/stable/reference/generated/numpy.amax.html\n\n| Parameter                                                                                                                                                                                                                                              | PyTorch Behavior                                                                  |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|\n| `axis`:  None or int or tuple of ints, optional. Axis or axes along which to operate. By default, flattened input is used. If this is a tuple of ints, the maximum is selected over multiple axes, instead of a single axis or all the axes as before. | Named `dim`, behavior same as `torch.sum` (https://github.com/pytorch/pytorch/issues/29137)                                |\n| `out`: ndarray, optional. Alternative output array in which to place the result. Must be of the same shape and buffer length as the expected output.                                                                                                   | Same                                                                              |\n| `keepdims`: bool, optional. If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.                                      | implemented as `keepdim`                                                          |\n| `initial`: scalar, optional. The minimum value of an output element. Must be present to allow computation on empty slice.                                                                                                                              | Not implemented in this PR. Better to implement for all reductions in the future. |\n| `where`: array_like of bool, optional. Elements to compare for the maximum.                                                                                                                                                                            | Not implemented in this PR. Better to implement for all reductions in the future. |\n\n**Note from numpy:**\n> NaN values are propagated, that is if at least one item is NaN, the corresponding max value will be NaN as well. To ignore NaN values (MATLAB behavior), please use nanmax.\n\nPyTorch has the same behavior\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43092\n\nReviewed By: ngimel\n\nDifferential Revision: D23360705\n\nPulled By: mruberry\n\nfbshipit-source-id: 5bdeb08a2465836764a5a6fc1a6cc370ae1ec09d", "pr_number": "43092", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/reduce_ops_test.cpp", "docs/source/tensors.rst", "docs/source/torch.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/Functions.cpp", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["merged", "open source", "triaged"]}, "776c2d495f": {"title": "[JIT] IRParser: store list attributes as generic ivalue lists. (#43785)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43785\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23400565\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: e248eb1854c4ec40da9455d4279ea6e47b1f2a16", "pr_number": "43785", "files_changed": ["test/cpp/jit/test_irparser.cpp", "torch/csrc/jit/ir/irparser.cpp"], "labels": ["merged", "oncall: jit"]}, "b23e9cdd64": {"title": ".circleci: Add slash to end of s3 cp (#43792)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43792\n\nThis fixes the issue we had with the nightlies not being uploaded\nproperly, basically what was happening was that `aws s3 cp` doesn't\nautomatically distinguish between prefixes that are already\n\"directories\" vs a single file with the same name.\n\nThis means that if you'd like to upload a file to a \"directory\" in S3\nyou need to suffix your destination with a slash.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D23402074\n\nPulled By: seemethere\n\nfbshipit-source-id: 6085595283fcbbbab0836ccdfe0f8aa2a6abd7c8", "pr_number": "43792", "files_changed": [".circleci/scripts/binary_upload.sh"], "labels": ["merged", "module: ci"]}, "89e2a3591e": {"title": "Add 1% threshold to codecov (#43783)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43783\n\nReviewed By: seemethere\n\nDifferential Revision: D23402196\n\nPulled By: malfet\n\nfbshipit-source-id: bd11d6edc6d1f15bd227636a549b9ea7b3aca256", "pr_number": "43783", "files_changed": ["codecov.yml"], "labels": ["merged", "module: ci"]}, "0564d7a652": {"title": "Land code coverage tool for OSS (#43778)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43778\n\nMove code_coverage_tool from experimental folder to caffe2/tools folder.\n\nDelete `TODO` and fb-related code.\n\nTest Plan: Test locally\n\nReviewed By: malfet\n\nDifferential Revision: D23399983\n\nfbshipit-source-id: 92316fd3cc88409d087d2dc6ed0be674155b3762", "pr_number": "43778", "files_changed": ["tools/code_coverage/README.md", "tools/code_coverage/oss_coverage.py", "tools/code_coverage/package/__init__.py", "tools/code_coverage/package/oss/__init__.py", "tools/code_coverage/package/oss/cov_json.py", "tools/code_coverage/package/oss/init.py", "tools/code_coverage/package/oss/run.py", "tools/code_coverage/package/oss/utils.py", "tools/code_coverage/package/tool/__init__.py", "tools/code_coverage/package/tool/clang_coverage.py", "tools/code_coverage/package/tool/gcc_coverage.py", "tools/code_coverage/package/tool/parser/__init__.py", "tools/code_coverage/package/tool/parser/coverage_record.py", "tools/code_coverage/package/tool/parser/gcov_coverage_parser.py", "tools/code_coverage/package/tool/parser/llvm_coverage_parser.py", "tools/code_coverage/package/tool/parser/llvm_coverage_segment.py", "tools/code_coverage/package/tool/print_report.py", "tools/code_coverage/package/tool/summarize_jsons.py", "tools/code_coverage/package/tool/utils.py", "tools/code_coverage/package/util/__init__.py", "tools/code_coverage/package/util/setting.py", "tools/code_coverage/package/util/utils.py", "tools/code_coverage/package/util/utils_init.py"], "labels": ["fb-exported", "merged"]}, "20abfc21e4": {"title": "Adds arctanh, arcsinh aliases, simplifies arc* alias dispatch (#43762)", "body": "Summary:\nAdds two more \"missing\" NumPy aliases: arctanh and arcsinh, and simplifies the dispatch of other arc* aliases.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43762\n\nReviewed By: ngimel\n\nDifferential Revision: D23396370\n\nPulled By: mruberry\n\nfbshipit-source-id: 43eb0c62536615fed221d460c1dec289526fb23c", "pr_number": "43762", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_op_aliases.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py"], "labels": ["merged", "oncall: jit"]}, "4cb8d306e6": {"title": "Add _foreach_add_(TensorList tensors, Scalar scalar) API (#42531)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42531\n\n[First PR: Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar)](https://github.com/pytorch/pytorch/pull/41554).\n\n**Motivation**\n[GitHub issue](https://github.com/pytorch/pytorch/issues/38655)\nCurrent PyTorch optimizer implementations are not efficient in cases when we work with a lot of small feature tensors. Starting a lot of kernels slows down the whole process. We need to reduce the number of kernels that we start.\nAs an example, we should be looking at [NVIDIAs Apex](https://github.com/NVIDIA/apex).\nIn order to track progress, we will pick PyTorchs DCGAN model with Adam optimizer and once the optimizer is reimplemented with tensor lists, benchmark the model performance against original model version, Apexs version with original Adam optimizer and it\u2019s FusedAdam optimizer.\n\n**Current API restrictions**\n- List can't be empty (will fixed in upcoming PRs).\n- All tensors in the list must have the same dtype, device and size.\n\n**Broadcasting**\nAt this point we don't support broadcasting.\n\n**What is 'Fast' and 'Slow' route**\nIn particular cases, we cant process an op with a fast list CUDA kernel. Still, we can do with a regular for-loop where the op will be applied to each tensor individually through the dispatch mechanisms. There are a few checks that decide whether the op will be performed via a 'fast' or 'slow' path.\nTo go the fast route,\n- All tensors must have strided layout\n- All tensors must be dense and not have overlapping memory\n- The resulting tensor type must be the same.\n\n---------------\n**In this PR**\n- Adding a `std::vector<Tensor> _foreach_add_(TensorList tensors, Scalar scalar)` API\n- Resolving some additional comments from previous [PR](https://github.com/pytorch/pytorch/pull/41554).\n\n**Tests**\nTested via unit tests\n\n**TODO**\n1. Properly handle empty lists\n\n**Plan for the next PRs**\n1. APIs\n- Binary Ops for list with Scalar\n- Binary Ops for list with list\n- Unary Ops for list\n- Pointwise Ops\n\n2. Complete tasks from TODO\n3. Rewrite PyTorch optimizers to use for-each operators for performance gains.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23331892\n\nPulled By: izdeby\n\nfbshipit-source-id: c585b72e1e87f6f273f904f75445618915665c4c", "pr_number": "42531", "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/cuda/ForeachFunctors.cuh", "aten/src/ATen/native/cuda/ForeachTensorAddScalar.cu", "aten/src/ATen/native/cuda/ForeachUtils.cuh", "aten/src/ATen/native/cuda/MultiTensorApply.cuh", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "test/test_foreach.py", "torch/csrc/autograd/VariableTypeUtils.h"], "labels": ["merged"]}, "af4ecb3c11": {"title": "quantized conv: add support for graph mode BC testing, and increase coverage (#43524)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43524\n\n1. adds support for testing BC on data format and numerics for graph mode\nquantized modules\n2. using the above, adds coverage for quantized conv2d on graph mode\n\nTest Plan:\n```\npython test/test_quantization.py TestSerialization.test_conv2d_nobias\npython test/test_quantization.py TestSerialization.test_conv2d_graph\npython test/test_quantization.py TestSerialization.test_conv2d_nobias_graph\n```\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23335222\n\nfbshipit-source-id: 0c9e93a940bbf6c676c2576eb62fcc725247588b", "pr_number": "43524", "files_changed": ["test/quantization/serialized/TestSerialization.test_conv2d_graph.expected.pt", "test/quantization/serialized/TestSerialization.test_conv2d_graph.input.pt", "test/quantization/serialized/TestSerialization.test_conv2d_graph.scripted.pt", "test/quantization/serialized/TestSerialization.test_conv2d_graph.traced.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias.expected.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias.input.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias.scripted.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias.state_dict.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias.traced.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias_graph.expected.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias_graph.input.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias_graph.scripted.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias_graph.traced.pt", "test/quantization/test_backward_compatibility.py"], "labels": ["merged"]}, "3f5ea2367e": {"title": "Adding a version serialization type to ConvPackedParam (#43086)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43086\n\nThis PR changes the format of `ConvPackedParam` in a nearly backwards-compatible way:\n* a new format is introduced which has more flexibility and a lower on-disk size\n* custom pickle functions are added to `ConvPackedParams` which know how to load the old format\n* the custom pickle functions are **not** BC because the output type of `__getstate__` has changed.  We expect this to be acceptable as no user flows are actually broken (loading a v1 model with v2 code works), which is why we whitelist the failure.\n\nTest plan (TODO finalize):\n\n```\n// adhoc testing of saving v1 and loading in v2: https://gist.github.com/vkuzo/f3616c5de1b3109cb2a1f504feed69be\n\n// test that loading models with v1 conv params format works and leads to the same numerics\npython test/test_quantization.py TestSerialization.test_conv2d_graph\npython test/test_quantization.py TestSerialization.test_conv2d_nobias_graph\n\n// test that saving and loading models with v2 conv params format works and leads to same numerics\npython test/test_quantization.py TestSerialization.test_conv2d_graph_v2\npython test/test_quantization.py TestSerialization.test_conv2d_nobias_graph_v2\n\n// TODO before land:\n// test numerics for a real model\n// test legacy ONNX path\n```\n\nNote: this is a newer copy of https://github.com/pytorch/pytorch/pull/40003\n\nTest Plan: Imported from OSS\n\nReviewed By: dreiss\n\nDifferential Revision: D23347832\n\nPulled By: vkuzo\n\nfbshipit-source-id: 06bbe4666421ebad25dc54004c3b49a481d3cc92", "pr_number": "43086", "files_changed": ["aten/src/ATen/native/quantized/cpu/conv_serialization.h", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/quantization/serialized/TestSerialization.test_conv2d_graph_v2.expected.pt", "test/quantization/serialized/TestSerialization.test_conv2d_graph_v2.input.pt", "test/quantization/serialized/TestSerialization.test_conv2d_graph_v2.scripted.pt", "test/quantization/serialized/TestSerialization.test_conv2d_graph_v2.traced.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias_graph_v2.expected.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias_graph_v2.input.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias_graph_v2.scripted.pt", "test/quantization/serialized/TestSerialization.test_conv2d_nobias_graph_v2.traced.pt", "test/quantization/test_backward_compatibility.py", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp", "torch/custom_class.h"], "labels": ["merged", "oncall: jit"]}, "7d517cf96f": {"title": "[NCCL] Dedicated stream to run all FutureNCCL callbacks. (#43447)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43447\n\nTwo main better-engineering motivations to run all FutureNCCL callbacks on a dedicated stream:\n1. Each time a then callback was called, we would get a stream from the pool and run the callback on that stream. If we observe the stream traces using that approach, we would see a lot of streams and debugging would become more complicated. If we have a dedicated stream to run all then callback operations, the trace results will be much cleaner and easier to follow.\n2. getStreamFromPool may eventually return the default stream or a stream that is used for other operations. This can cause slowdowns.\n\nUnless then callback takes longer than preceding allreduce, this approach will be as performant as the previous approach.\nghstack-source-id: 110909401\n\nTest Plan:\nPerf trace runs to validate the desired behavior:\nSee the dedicated stream 152 is running the then callback operations:\n\n{F299759342}\n\nI run pytorch.benchmark.main.workflow using resnet50 and 32 GPUs registering allreduce with then hook.\nSee f213777896 [traces](https://www.internalfb.com/intern/perfdoctor/results?run_id=26197585)\n\nAfter updates, same observation: see f214890101\n\nReviewed By: malfet\n\nDifferential Revision: D23277575\n\nfbshipit-source-id: 67a89900ed7b70f3daa92505f75049c547d6b4d9", "pr_number": "43447", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["merged"]}, "3f0120edb4": {"title": "Revert D23360705: [pytorch][PR] Add amax/amin", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23360705 (https://github.com/pytorch/pytorch/commit/bcec8cc3f9b14a79895af026891e9fd5f4a01b0a)\n\nOriginal commit changeset: 5bdeb08a2465\n\nfbshipit-source-id: 76a9e199823c7585e55328bad0778bcd8cd49381", "pr_number": null, "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/reduce_ops_test.cpp", "docs/source/tensors.rst", "docs/source/torch.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/Functions.cpp", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "633d239409": {"title": "[torch.fx] Pass placeholders through delegate too (#43432)", "body": "Summary:\nIt's useful if we add additional attributed to nodes in the graph - it's easier to set the attribute on all nodes, even if the value would happen to be None.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43432\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23276433\n\nPulled By: dzhulgakov\n\nfbshipit-source-id: c69e7cb723bbbb4dba3b508a3d6c0e456fe610df", "pr_number": "43432", "files_changed": ["test/test_fx.py", "torch/fx/graph.py", "torch/fx/symbolic_trace.py", "torch/quantization/fx/quantize.py"], "labels": []}, "a1eae6d158": {"title": "Implementing NumPy-like function torch.heaviside() (#42523)", "body": "Summary:\n- Related with https://github.com/pytorch/pytorch/issues/38349\n- Implementing the NumPy-like function `torch.heaviside()` .\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42523\n\nReviewed By: glaringlee\n\nDifferential Revision: D23391941\n\nPulled By: mruberry\n\nfbshipit-source-id: 7b942321a62567a5fc0a3679a289f4c4c19e6134", "pr_number": "42523", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["open source", "triaged"]}, "cd0bab8d8d": {"title": "[ONNX] Where op (#41544)", "body": "Summary:\nExtending where op export\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41544\n\nReviewed By: malfet\n\nDifferential Revision: D23279515\n\nPulled By: bzinodev\n\nfbshipit-source-id: 4627c95ba18c8a5ac8d06839c343e06e71c46aa7", "pr_number": "41544", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset9.py"], "labels": ["module: onnx", "open source", "triaged"]}, "68b9daa9bf": {"title": "Add `torch.linalg.norm` (#42749)", "body": "Summary:\nAdds `torch.linalg.norm` function that matches the behavior of `numpy.linalg.norm`.\n\nAdditional changes:\n* Add support for dimension wrapping in `frobenius_norm` and `nuclear_norm`\n* Fix `out` argument behavior for `nuclear_norm`\n* Fix issue where `frobenius_norm` allowed duplicates in `dim` argument\n* Add `_norm_matrix`\n\nCloses https://github.com/pytorch/pytorch/issues/24802\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42749\n\nReviewed By: ngimel\n\nDifferential Revision: D23336234\n\nPulled By: mruberry\n\nfbshipit-source-id: f0aba3089a3a0bf856aa9c4215e673ff34228fac", "pr_number": "42749", "files_changed": ["aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/linalg.rst", "test/test_linalg.py", "test/test_torch.py", "tools/autograd/templates/python_linalg_functions.cpp", "torch/csrc/api/include/torch/linalg.h", "torch/linalg/__init__.py"], "labels": ["oncall: jit", "open source", "triaged"]}, "1a79d7bb28": {"title": "DDP communication hook examples (#43310)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43310\n\nIn this diff, we prepared some example DDP communication hooks [#40848](https://github.com/pytorch/pytorch/pull/40848):\n\n1\\. `allreduce_hook`: This DDP communication hook just calls ``allreduce`` using ``GradBucket`` tensors. Once gradient tensors are aggregated across all workers, its ``then`` callback takes the mean and returns the result. If user registers this hook DDP results is expected to be same as the case where no hook was registered. Hence, this won't change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior.\n\n2\\. `allgather_then_aggregate_hook` Similar to ``allreduce_hook``, this hook first gathers ``GradBucket`` tensors and its ``then`` callback aggregates the gathered gradient tensors and takes mean. Instead of ``allreduce`` this hook uses ``allgather``. Note that with W workers, both the computation and communication time scale as O(W) for allgather compared to O(logW) for allreduce. Therefore, this hook is expected to be much slower than ``allreduce_hook`` although both essentially do the same thing with the gradients.\n\n3\\. `fp16_compress_hook` This DDP communication hook implements a simple gradient compression approach that converts ``GradBucket`` tensors whose type is assumed to be ``torch.float32`` to half-precision floating point format (``torch.float16``). It allreduces those ``float16`` gradient tensors. Once compressed gradient tensors are allreduced, its then callback called ``decompress`` converts the aggregated result back to ``float32`` and takes the mean.\n\n4\\. `quantization_pertensor_hook` does quantization per tensor and uses the idea in https://pytorch.org/docs/master/generated/torch.quantize_per_tensor.html.  Note that we separately send scale and zero_point (two floats per rank) before quantized tensors.\n\n5\\. `quantization_perchannel_hook` does quantization per channel similar to https://pytorch.org/docs/master/generated/torch.quantize_per_channel.html. The main motivation is that after the initial QSGD study diff, we realized that for considerably large gradient tensors such as a tensor that contains 6 million floats quantizing dividing it into smaller channels (512 float chunks) and quantizing independently may significantly increase the resolution and result with lower error.\nghstack-source-id: 110923269\n\nTest Plan:\npython torch/distributed/algorithms/ddp_comm_hooks/test_ddp_hooks.py\nCouldn't download test skip set, leaving all tests enabled...\n.....\n----------------------------------------------------------------------\nRan 4 tests in 26.724s\n\nOK\n\nInternal testing:\n```\nbuck run mode/dev-nosan //caffe2/test/distributed/algorithms/ddp_comm_hooks:test_ddp_hooks\n```\n\nReviewed By: malfet\n\nDifferential Revision: D22937999\n\nfbshipit-source-id: 274452e7932414570999cb978ae77a97eb3fb0ec", "pr_number": "43310", "files_changed": ["test/distributed/algorithms/ddp_comm_hooks/test_ddp_hooks.py", "test/run_test.py", "torch/distributed/algorithms/ddp_comm_hooks/__init__.py", "torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py", "torch/distributed/algorithms/ddp_comm_hooks/quantization_hooks.py"], "labels": []}, "47e489b135": {"title": "Make ExtraFilesMap return bytes instead of str (#43241)", "body": "Summary:\nIn case we want to store binary files using `ScriptModule.save(..., _extra_files=...)` functionality. With python3 we can just use bytes only and not bother about it.\n\nI had to do a copy-pasta from pybind sources, maybe we should upstream it, but it'd mean adding a bunch of template arguments to `bind_map` which is a bind untidy.\n\nLet me know if there's a better place to park this function (it seems to be the only invocation of `bind_map` so I put it in the same file)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43241\n\nReviewed By: zdevito\n\nDifferential Revision: D23205244\n\nPulled By: dzhulgakov\n\nfbshipit-source-id: 8f291eb4294945fe1c581c620d48ba2e81b3dd9c", "pr_number": "43241", "files_changed": ["test/jit/test_async.py", "test/jit/test_save_load.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/python/script_init.cpp", "torch/jit/_script.py", "torch/jit/_serialization.py"], "labels": ["oncall: jit"]}, "64906497cd": {"title": "Revert D23391941: [pytorch][PR] Implementing NumPy-like function torch.heaviside()", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23391941 (https://github.com/pytorch/pytorch/commit/a1eae6d158fdfd3ca9b4b09c56eb3da631b74bcb)\n\nOriginal commit changeset: 7b942321a625\n\nfbshipit-source-id: c2a7418a1fedaa9493300945c30e2392fc0d08ee", "pr_number": null, "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": []}, "13c7c6227e": {"title": "Python/C++ API Parity: TransformerDecoder (#42886)", "body": "Summary:\nFixes #{[37756](https://github.com/pytorch/pytorch/issues/37756)}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42886\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23385631\n\nPulled By: glaringlee\n\nfbshipit-source-id: 610a2fabb4c25b2dfd37b33287215bb8872d653d", "pr_number": "42886", "files_changed": ["test/cpp/api/transformer.cpp", "torch/csrc/api/include/torch/nn/modules/transformercoder.h", "torch/csrc/api/include/torch/nn/options/transformercoder.h", "torch/csrc/api/src/nn/modules/transformer.cpp", "torch/csrc/api/src/nn/options/transformer.cpp"], "labels": ["module: cpp", "open source", "triaged"]}, "eae92b7187": {"title": "Updated README.md by correcting grammatical errors (#43779)", "body": "Summary:\nFixed grammatical errors and punctuation so that it be can more understandable.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43779\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23407849\n\nPulled By: malfet\n\nfbshipit-source-id: 09c064ce68d0f37f8023c2ecae8775fc00541a2c", "pr_number": "43779", "files_changed": ["README.md"], "labels": ["module: docs", "open source", "triaged"]}, "8997a4b56b": {"title": "[typing] Enable typing in torch.quantization.fuse_modules typechecks \u2026 (#43786)", "body": "Summary:\n\u2026during CI\n\nFixes #{42971}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43786\n\nReviewed By: malfet\n\nDifferential Revision: D23403258\n\nPulled By: yizhouyu\n\nfbshipit-source-id: 4cd24a4fcf1408341a210fa50f574887b6db5e0e", "pr_number": "43786", "files_changed": ["mypy.ini", "torch/quantization/fuse_modules.py"], "labels": []}, "58148c85f4": {"title": "Use template OperatorGenerator for prim and special operator registration (#43481)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43481\n\nApply OperatorGenerator for prim and special operator registration. It does not affect the existing build by default. However, if a whitelist of operator exists, only the operators in the whitelist will be registered. It has the potential to save up to 200 KB binary size, depending on the usage.\n\nTest Plan: Imported from OSS\n\nReviewed By: ljk53\n\nDifferential Revision: D23287251\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 3ca39fbba645bad8d69e69195f3680e4f6d633c5", "pr_number": "43481", "files_changed": ["torch/csrc/jit/runtime/register_ops_utils.h", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp"], "labels": ["oncall: jit"]}, "6aaae3b08b": {"title": "[ONNX] Addition of diagnostic tool API (#43020)", "body": "Summary:\nAdded initial diagnostic tool API\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43020\n\nReviewed By: malfet\n\nDifferential Revision: D23398459\n\nPulled By: bzinodev\n\nfbshipit-source-id: 7a6d9164a19e3ba51676fbcf645c4d358825eb42", "pr_number": "43020", "files_changed": ["test/onnx/test_utility_funs.py", "torch/onnx/__init__.py", "torch/onnx/utils.py"], "labels": ["open source", "triaged"]}, "8538a79bfe": {"title": "[jit][static] Basic executor (#43647)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43647\n\nNothing fancy, just a basic implementation of the graph executor without using stack machine.\n\nReviewed By: bwasti\n\nDifferential Revision: D23208413\n\nfbshipit-source-id: e483bb6ad7ba8591bbe1767e669654d82f42c356", "pr_number": "43647", "files_changed": ["benchmarks/static_runtime/deep_wide_pt.cc", "benchmarks/static_runtime/deep_wide_pt.h", "benchmarks/static_runtime/deep_wide_pt_bench.cc", "benchmarks/static_runtime/test_static_runtime.cc", "test/test_static_runtime.py", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h"], "labels": ["fb-exported", "oncall: jit"]}, "000739c31a": {"title": "Function calls for fallback paths (#43274)", "body": "Summary:\nThis PR adds API to package unoptimized/fallback blocks as function calls. It's mainly meant to be used by TensorExpressionsFuser and SpecializeAutogradZero passes as both specialize the original graph but would also like to provide a fallback path in case the assumptions under which the graph was specialized do not hold for some inputs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43274\n\nReviewed By: malfet\n\nDifferential Revision: D23406961\n\nPulled By: Krovatkin\n\nfbshipit-source-id: ef21fc9ad886953461b09418d02c75c58375490c", "pr_number": "43274", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/graph_executor.h", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.h", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["oncall: jit"]}, "931b8b4ac8": {"title": "Use ivalue::Future in autograd engine and DistEngine. (#43676)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43676\n\nThis is one part of https://github.com/pytorch/pytorch/issues/41574 to\nensure we consolidate everything around ivalue::Future.\n\nI've removed the use of torch/csrc/utils/future.h from the autograd engines and\nused ivalue::Future instead.\nghstack-source-id: 110895545\n\nTest Plan: waitforbuildbot.\n\nReviewed By: albanD\n\nDifferential Revision: D23362415\n\nfbshipit-source-id: aa109b3f8acf0814d59fc5264a85a8c27ef4bdb6", "pr_number": "43676", "files_changed": ["torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_engine.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp"], "labels": []}, "d10056652b": {"title": "Enable `torch.half` for `lt` and `masked_select` (#43704)", "body": "Summary:\nEnable testing of those options in `TestTorchDeviceTypeCPU.test_logical_cpu` and `TestTorchDeviceTypeCPU.test_masked_select_cpu_float16`\nAdd `view_as_real` testing for `torch.complex32` type\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43704\n\nReviewed By: albanD\n\nDifferential Revision: D23373070\n\nPulled By: malfet\n\nfbshipit-source-id: 00f17f23b48513379a414227aea91e2d3c0dd5f9", "pr_number": "43704", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cpu/FillKernel.cpp", "aten/src/ATen/native/cpu/IndexKernel.cpp", "c10/core/Scalar.h", "test/test_torch.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/testing/__init__.py"], "labels": ["oncall: jit"]}, "8a41fa4718": {"title": "[Selective Build] Move register_prim_ops and register_special_ops to app level (#43539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43539\n\nMove the two source files out of the base internal mobile library to the app level. Make it ready for app-based selective build. Opensource build should not be affected. The file list change in build_variables.bzl affects internal build only.\n\nghstack-source-id: 111006135\n\nTest Plan: CI\n\nReviewed By: ljk53\n\nDifferential Revision: D23287661\n\nfbshipit-source-id: 9b2d688544e79e0fca9c84730ef0259952cd8abe", "pr_number": "43539", "files_changed": ["tools/build_variables.bzl"], "labels": []}, "60ad7e9c04": {"title": "[TensorExpr] Make sum available from Python (#43730)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43730\n\nTest Plan:\npython test/test_jit_fuser_te.py -k TestTEFuser.test_sum\ntest_tensorexpr --gtest_filter=TensorExprTest.KernelSum*\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23407600\n\nPulled By: asuhan\n\nfbshipit-source-id: e6da4690ae6d802f9be012e39e61b7467aa5285c", "pr_number": "43730", "files_changed": ["test/cpp/tensorexpr/test_kernel.cpp", "test/test_jit_fuser_te.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["oncall: jit"]}, "550fb2fd52": {"title": "Expand the coverage of test_blas_empty (#43822)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43822\n\nReviewed By: mruberry\n\nDifferential Revision: D23413359\n\nPulled By: ngimel\n\nfbshipit-source-id: fcdb337e32ed2d1c791fa0762d5233b346b26d14", "pr_number": "43822", "files_changed": ["aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "test/test_torch.py"], "labels": ["open source"]}, "f31b111a35": {"title": "Add the sls tensor train op (#33525)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33525\n\nReviewed By: wx1988\n\nDifferential Revision: D19987020\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: e3ca7b00a374a75ee42716c4e6236bf168ebebf1", "pr_number": "33525", "files_changed": ["caffe2/operators/lengths_reducer_ops.cc", "caffe2/operators/lengths_reducer_ops.h", "caffe2/python/hypothesis_test.py"], "labels": []}, "45ba836876": {"title": "Revert \"Revert D23252335: Refactor Vulkan context into its own files. Use RAII.\" (#43628)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43628\n\nThis reverts commit 6c772515ed1a87ec676382492ff3c019c6d194c3.\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23356714\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: a44af3b3c7b00a097eae1b0c9a00fdabc7ab6f86", "pr_number": "43628", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/vulkan/api/Common.cpp", "aten/src/ATen/native/vulkan/api/Common.h", "aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/api.h", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/vulkan_api_test.cpp"], "labels": []}, "cc52386096": {"title": "Revert D19987020: [pytorch][PR] Add the sls tensor train op", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19987020 (https://github.com/pytorch/pytorch/commit/f31b111a352dd9de5e432f137913a0f3c106eb9a)\n\nOriginal commit changeset: e3ca7b00a374\n\nfbshipit-source-id: a600c747a45dfb51e0882196e382a21ccaa7b989", "pr_number": null, "files_changed": ["caffe2/operators/lengths_reducer_ops.cc", "caffe2/operators/lengths_reducer_ops.h", "caffe2/python/hypothesis_test.py"], "labels": []}, "7f967c08b8": {"title": "Document the beta=0 behavior of BLAS functions (#43823)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43823\n\nReviewed By: mruberry\n\nDifferential Revision: D23413899\n\nPulled By: ngimel\n\nfbshipit-source-id: d3c4e5631db729a3f3d5eb9290c76cb1aa529f74", "pr_number": "43823", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source"]}, "4e39c310eb": {"title": "Move torch/csrc/utils/hash.h to c10/util/hash.h. (#42503)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42503\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252331\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 3c4c0e27b9a7eec8560e374c2a3ba5f1c65dae48", "pr_number": "42503", "files_changed": ["c10/util/hash.h", "torch/csrc/autograd/edge.h", "torch/csrc/cuda/nccl.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/jit/codegen/fuser/arg_spec.h", "torch/csrc/jit/codegen/fuser/compiler.cpp", "torch/csrc/jit/codegen/fuser/kernel_spec.h", "torch/csrc/jit/codegen/fuser/tensor_desc.h", "torch/csrc/jit/ir/node_hashing.cpp", "torch/csrc/jit/ir/type_hashing.cpp", "torch/csrc/jit/passes/lower_graph.cpp", "torch/csrc/jit/python/python_arg_flatten.h", "torch/csrc/jit/runtime/argument_spec.h", "torch/csrc/utils/hash.h", "torch/lib/c10d/ProcessGroupGloo.hpp"], "labels": ["oncall: jit"]}, "6373063a98": {"title": "Generic Vulkan object cache. (#42394)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42394\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252340\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 34e753964b94153ed6ed1fcaa7f3b4a7c6b5f340", "pr_number": "42394", "files_changed": ["aten/src/ATen/native/vulkan/api/Cache.h"], "labels": []}, "287fb273cd": {"title": "Vulkan (source and binary) shader and shader layout cache. (#42325)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42325\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252336\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: f3f26c78366be45c90a370db9194d88defbf08d8", "pr_number": "42325", "files_changed": ["aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/Shader.cpp", "aten/src/ATen/native/vulkan/api/Shader.h", "aten/src/ATen/native/vulkan/api/api.h"], "labels": []}, "387dc24c92": {"title": "Vulkan memory allocator. (#42786)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42786\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252332\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 14e848ad81b4ba1367e8cf719343a51995457827", "pr_number": "42786", "files_changed": ["aten/src/ATen/native/vulkan/api/Allocator.cpp", "aten/src/ATen/native/vulkan/api/Allocator.h", "aten/src/ATen/native/vulkan/api/vk_mem_alloc.h"], "labels": []}, "15aaeb8867": {"title": "Vulkan pipeline and pipeline layout cache. (#42395)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42395\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252334\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 6b4e88f9794a7879d47a1cdb671076d50f1944d9", "pr_number": "42395", "files_changed": ["aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/Pipeline.cpp", "aten/src/ATen/native/vulkan/api/Pipeline.h", "aten/src/ATen/native/vulkan/api/api.h"], "labels": []}, "87e8f50aae": {"title": "Vulkan descriptor and descriptor layout cache. (#42642)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42642\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252337\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 075acc8c093e639bb24a0d4653d5c922b36a1128", "pr_number": "42642", "files_changed": ["aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/Descriptor.cpp", "aten/src/ATen/native/vulkan/api/Descriptor.h", "aten/src/ATen/native/vulkan/api/api.h"], "labels": []}, "d1df098956": {"title": "Vulkan resource cache. (#42709)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42709\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252339\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 977ab3fdedfe98789a48dd263127529d8be0ed37", "pr_number": "42709", "files_changed": ["aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/Resource.cpp", "aten/src/ATen/native/vulkan/api/Resource.h", "aten/src/ATen/native/vulkan/api/api.h"], "labels": []}, "628db9699f": {"title": "Vulkan command buffer and pool. (#42930)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42930\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23252333\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 738385e0058edf3d3b34173e1b1011356adb7b3c", "pr_number": "42930", "files_changed": ["aten/src/ATen/native/vulkan/api/Command.cpp", "aten/src/ATen/native/vulkan/api/Command.h", "aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/api.h"], "labels": []}, "ab3ea95e90": {"title": "#include <string> in loopnest.h (#43835)", "body": "Summary:\nThis file is causing compiling failure on my gcc-10.1\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43835\n\nReviewed By: bhosmer\n\nDifferential Revision: D23416417\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: d0c2998347438fb729212574d52ce20dd6faae85", "pr_number": "43835", "files_changed": ["torch/csrc/jit/tensorexpr/loopnest.h"], "labels": ["oncall: jit", "open source"]}, "1830e4f08c": {"title": "Remove unnamed namespace in headers (#43689)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43689\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison, asuhan\n\nDifferential Revision: D23367636\n\nPulled By: bertmaher\n\nfbshipit-source-id: ddb6d34d2f7cadff3a591c3650e1dd1b401c3d2d", "pr_number": "43689", "files_changed": ["torch/csrc/jit/tensorexpr/ir_simplifier.h", "torch/csrc/jit/tensorexpr/reduction.h", "torch/csrc/jit/tensorexpr/tensor.h"], "labels": ["oncall: jit"]}, "5021ec826b": {"title": "Fix docs for kwargs, f-p (#43586)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43586\n\nReviewed By: glaringlee\n\nDifferential Revision: D23390667\n\nPulled By: mruberry\n\nfbshipit-source-id: dd51a4a48ff4e2fc10675ec817a206041957982f", "pr_number": "43586", "files_changed": ["torch/_torch_docs.py"], "labels": ["module: docs", "open source", "triaged"]}, "7b835eb887": {"title": "Update CUDA11 docker container (#42200)", "body": "Summary:\n- no more `-rc`\n- add magma\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42200\n\nReviewed By: ZolotukhinM, mruberry\n\nDifferential Revision: D23411686\n\nPulled By: malfet\n\nfbshipit-source-id: 04532bc1cc65b3e14ddf29e8bf61a7a3b4c706ad", "pr_number": "42200", "files_changed": [".circleci/docker/build.sh", ".circleci/docker/common/install_conda.sh"], "labels": ["open source", "triaged"]}, "3dc9645430": {"title": "Disable RocM CircleCI jobs (#42630)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42630\n\nReviewed By: seemethere\n\nDifferential Revision: D22957640\n\nPulled By: malfet\n\nfbshipit-source-id: 9f7d633310c653fcd14e66755168c0e559307b69", "pr_number": "42630", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml"], "labels": []}, "3aeb70db0b": {"title": "Documents sub properly, adds subtract alias (#43850)", "body": "Summary:\n`torch.sub` was undocumented, so this PR adds its documentation, analogous to `torch.add`'s documentation, and adds the alias `torch.subtract` for `torch.sub`, too. This alias comes from NumPy (see https://numpy.org/doc/stable/reference/generated/numpy.subtract.html?highlight=subtract#numpy.subtract)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43850\n\nReviewed By: ngimel\n\nDifferential Revision: D23416908\n\nPulled By: mruberry\n\nfbshipit-source-id: 6c4d2ebaf6ecae91f3a6efe484ce6c4dad96f016", "pr_number": "43850", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_op_aliases.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py"], "labels": ["module: docs", "module: numpy", "oncall: jit"]}, "08126c9153": {"title": "[ONNX] Utilize ONNX shape inference for ONNX exporter (#40628)", "body": "Summary:\nIt is often that the conversion from torch operator to onnx operator requires input rank/dtype/shape to be known. Previously, the conversion depends on tracer to provide these info, leaving a gap in conversion of scripted modules.\n\nWe are extending the export with support from onnx shape inference. If enabled, onnx shape inference will be called whenever an onnx node is created. This is the first PR introducing the initial look of the feature. More and more cases will be supported following this PR.\n\n* Added pass to run onnx shape inference on a given node. The node has to have namespace `onnx`.\n* Moved helper functions from `export.cpp` to a common place for re-use.\n* This feature is currently experimental, and can be turned on through flag `onnx_shape_inference` in internal api `torch.onnx._export`.\n* Currently skipping ONNX Sequence ops, If/Loop and ConstantOfShape due to limitations. Support will be added in the future.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40628\n\nReviewed By: mrshenli\n\nDifferential Revision: D22709746\n\nPulled By: bzinodev\n\nfbshipit-source-id: b52aeeae00667e66e0b0c1144022f7af9a8b2948", "pr_number": "40628", "files_changed": [".github/workflows/lint.yml", "aten/src/ATen/core/interned_strings.h", "caffe2/CMakeLists.txt", "test/onnx/test_pytorch_common.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "tools/build_variables.bzl", "tools/git-pre-commit", "torch/csrc/jit/passes/onnx/constant_fold.cpp", "torch/csrc/jit/passes/onnx/helper.cpp", "torch/csrc/jit/passes/onnx/helper.h", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/passes/onnx/shape_type_inference.cpp", "torch/csrc/jit/passes/onnx/shape_type_inference.h", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/serialization/export.cpp", "torch/csrc/jit/serialization/onnx.cpp", "torch/csrc/jit/serialization/onnx.h", "torch/onnx/symbolic_helper.py", "torch/onnx/utils.py"], "labels": ["module: onnx", "oncall: jit", "open source", "triaged"]}, "8fb7c50250": {"title": "Enable complex blas for ROCm. (#43744)", "body": "Summary:\nRevert \"Skips some complex tests on ROCm (https://github.com/pytorch/pytorch/issues/42759)\".  This reverts commit 55b1706775726418ddc5dd3b7756ea0388c0817c.\n\nUse new cuda_to_hip_mappings.py from https://github.com/pytorch/pytorch/issues/43004.\n\nFixes https://github.com/pytorch/pytorch/pull/42383#issuecomment-670771922\n\nCC sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43744\n\nReviewed By: glaringlee\n\nDifferential Revision: D23391263\n\nPulled By: ngimel\n\nfbshipit-source-id: ddf734cea3ba69c24f0d79cf1b87c05cdb45ec3d", "pr_number": "43744", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "test/test_torch.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["module: rocm", "open source"]}, "a860be898e": {"title": "[resubmit] Add amax/amin (#43819)", "body": "Summary:\nResubmit for landing next week.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43819\n\nReviewed By: ngimel\n\nDifferential Revision: D23421906\n\nPulled By: mruberry\n\nfbshipit-source-id: 23dd60d1e365bb1197d660c3bfad7ee07ba3e97f", "pr_number": "43819", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/reduce_ops_test.cpp", "docs/source/tensors.rst", "docs/source/torch.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/Functions.cpp", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source"]}, "1dcc4fb6b7": {"title": "Kill unused _pointwise_loss function. (#43523)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43523\n\nThe code is also wrong, see https://github.com/pytorch/pytorch/issues/43228.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23305461\n\nPulled By: gchanan\n\nfbshipit-source-id: 9fe516d87a4243d5ce3c29e8822417709a1d6346", "pr_number": "43523", "files_changed": ["torch/nn/functional.py"], "labels": []}, "1cdb9d2ab5": {"title": "Test runner for batched gradient computation with vmap (#43664)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43664\n\nThis PR implements the test runner for batched gradient computation with\nvmap. It also implements the batching rule for sigmoid_backward and\ntests that one can compute batched gradients with sigmoid (and batched\n2nd gradients).\n\nTest Plan: - New tests: `python test/test_vmap.py -v`\n\nReviewed By: ezyang\n\nDifferential Revision: D23358555\n\nPulled By: zou3519\n\nfbshipit-source-id: 7bb05b845a41b638b7cca45a5eff1fbfb542a51f", "pr_number": "43664", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": []}, "576880febf": {"title": "Print all traceback for nested backwards in detect_anomaly (#43626)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43405.\n\nThis pull request adds a feature of printing all tracebacks if a `detect_anomaly` mode detects `nan` in nested backward operations.\nThe way I did it is by assigning a node as a parent to all nodes it produces during its backward calculation. Then if one of the children produces `nan`, it will print the traceback from the parent and grand parents (if any).\n\nThe parent is assigned in `parent_node_` member in `Node` class which is accessible in C++ by function `node->parent()` and in Python by `node.parent_function`.\nA node has a parent iff:\n\n1. it is created from a backward operation, and\n2. created when anomaly mode and grad mode are both enabled.\n\nAn example of this feature:\n\n    import torch\n\n    def example():\n        x = torch.tensor(1.0, requires_grad=True)\n        y = torch.tensor(1e-8, requires_grad=True)  # small to induce nan in n-th backward\n        a = x * y\n        b = x * y\n        z1 = a / b  # can produce nan in n-th backward as long as https://github.com/pytorch/pytorch/issues/43414 is unsolved\n        z = z1 * z1\n        gy , = torch.autograd.grad( z , (y,), create_graph=True)\n        gy2, = torch.autograd.grad(gy , (y,), create_graph=True)\n        gy3, = torch.autograd.grad(gy2, (y,), create_graph=True)\n        gy4, = torch.autograd.grad(gy3, (y,), create_graph=True)\n        return gy4\n\n    with torch.autograd.detect_anomaly():\n        gy4 = example()\n\nwith output:\n\n    example.py:16: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n      with torch.autograd.detect_anomaly():\n    /home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py:190: UserWarning: Error detected in DivBackward0. Traceback of forward call that caused the error:\n      File \"example.py\", line 17, in <module>\n        gy4 = example()\n      File \"example.py\", line 12, in example\n        gy3, = torch.autograd.grad(gy2, (y,), create_graph=True)\n      File \"/home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 190, in grad\n        return Variable._execution_engine.run_backward(\n     (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:61.)\n      return Variable._execution_engine.run_backward(\n    /home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py:190: UserWarning:\n\n    Traceback of forward call that induces the previous calculation:\n      File \"example.py\", line 17, in <module>\n        gy4 = example()\n      File \"example.py\", line 11, in example\n        gy2, = torch.autograd.grad(gy , (y,), create_graph=True)\n      File \"/home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 190, in grad\n        return Variable._execution_engine.run_backward(\n     (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:65.)\n      return Variable._execution_engine.run_backward(\n    /home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py:190: UserWarning:\n\n    Traceback of forward call that induces the previous calculation:\n      File \"example.py\", line 17, in <module>\n        gy4 = example()\n      File \"example.py\", line 8, in example\n        z1 = a / b  # can produce nan in n-th backward as long as https://github.com/pytorch/pytorch/issues/43414 is unsolved\n     (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:65.)\n      return Variable._execution_engine.run_backward(\n    Traceback (most recent call last):\n      File \"example.py\", line 17, in <module>\n        gy4 = example()\n      File \"example.py\", line 13, in example\n        gy4, = torch.autograd.grad(gy3, (y,), create_graph=True)\n      File \"/home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 190, in grad\n        return Variable._execution_engine.run_backward(\n    RuntimeError: Function 'DivBackward0' returned nan values in its 1th output.\n\ncc & thanks to albanD\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43626\n\nReviewed By: malfet\n\nDifferential Revision: D23397499\n\nPulled By: albanD\n\nfbshipit-source-id: aa7435ec2a7f0d23a7a02ab7db751c198faf3b7d", "pr_number": "43626", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/anomaly_mode.h", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/function.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/python_anomaly_mode.cpp", "torch/csrc/autograd/python_anomaly_mode.h", "torch/csrc/autograd/python_function.cpp"], "labels": ["open source"]}, "6ea89166bd": {"title": "Rewrite of ATen code generator (#42629)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42629\n\nHow to approach reviewing this diff:\n\n- The new codegen itself lives in `tools/codegen`. Start with `gen.py`, then read `model.py` and them the `api/` folder. The comments at the top of the files describe what is going on. The CLI interface of the new codegen is similar to the old one, but (1) it is no longer necessary to explicitly specify cwrap inputs (and now we will error if you do so) and (2) the default settings for source and install dir are much better; to the extent that if you run the codegen from the root source directory as just `python -m tools.codegen.gen`, something reasonable will happen.\n- The old codegen is (nearly) entirely deleted; every Python file in `aten/src/ATen` was deleted except for `common_with_cwrap.py`, which now permanently finds its home in `tools/shared/cwrap_common.py` (previously cmake copied the file there), and `code_template.py`, which now lives in `tools/codegen/code_template.py`. We remove the copying logic for `common_with_cwrap.py`.\n- All of the inputs to the old codegen are deleted.\n- Build rules now have to be adjusted to not refer to files that no longer exist, and to abide by the (slightly modified) CLI.\n- LegacyTHFunctions files have been generated and checked in. We expect these to be deleted as these final functions get ported to ATen. The deletion process is straightforward; just delete the functions of the ones you are porting. There are 39 more functions left to port.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D23183978\n\nPulled By: ezyang\n\nfbshipit-source-id: 6073ba432ad182c7284a97147b05f0574a02f763", "pr_number": "42629", "files_changed": [".circleci/scripts/cpp_doc_push_script.sh", ".github/workflows/lint.yml", ".gitignore", ".jenkins/caffe2/build.sh", ".jenkins/pytorch/macos-common.sh", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "BUILD.bazel", "README.md", "aten/src/ATen/Declarations.cwrap", "aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/code_template.py", "aten/src/ATen/common_with_cwrap.py", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/cwrap_parser.py", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/gen_backend_select_register.py", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "aten/src/ATen/nn.yaml", "aten/src/ATen/nn_parse.py", "aten/src/ATen/preprocess_declarations.py", "caffe2/CMakeLists.txt", "caffe2/contrib/aten/gen_op.py", "cmake/Codegen.cmake", "docs/cpp/source/check-doxygen.sh", "mypy-strict.ini", "mypy.ini", "requirements.txt", "setup.py", "test/backward_compatibility/check_backward_compatibility.py", "test/test_type_hints.py", "tools/autograd/gen_autograd.py", "tools/autograd/gen_python_functions.py", "tools/autograd/gen_variable_type.py", "tools/autograd/utils.py", "tools/codegen/__init__.py", "tools/codegen/api/__init__.py", "tools/codegen/api/cpp.py", "tools/codegen/api/dispatcher.py", "tools/codegen/api/legacy_dispatcher.py", "tools/codegen/api/types.py", "tools/codegen/code_template.py", "tools/codegen/gen.py", "tools/codegen/local.py", "tools/codegen/model.py", "tools/setup_helpers/gen.py", "tools/shared/cwrap_common.py"], "labels": []}, "b8d34547ee": {"title": "[quant][graphmode][fx][fix] enable per channel quantization for functional ops (#43534)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43534\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23310857\n\nfbshipit-source-id: ff7a681ee55bcc51f564e9de78319249b989366c", "pr_number": "43534", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "42c895de4d": {"title": "Properly check that reduction strings are valid for l1_loss, smoothl1_loss, and mse_loss. (#43527)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43527\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23306786\n\nPulled By: gchanan\n\nfbshipit-source-id: f3b7c9c02ae02813da116cb6b247a95727c47587", "pr_number": "43527", "files_changed": ["test/test_nn.py", "torch/nn/functional.py"], "labels": ["topic: bc-breaking"]}, "eb4199b0a7": {"title": "[quant][graphmode][fx] Add top level APIs (#43581)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43581\n\nAdd similar APIs like eager and graph mode on torchscript\n- fuse_fx\n- quantize_fx (for both post training static and qat)\n- quantize_dynamic_fx (for post training dynamic)\n- prepare_fx (for both post training static and qat)\n- prepare_dynamic_fx (for post training dynamic)\n- convert_fx (for all modes)\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23385091\n\nfbshipit-source-id: b789e54e1a0f3af6b026fd568281984e253e0433", "pr_number": "43581", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/__init__.py", "torch/quantization/_quantize_fx.py", "torch/quantization/fx/__init__.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/quantize.py", "torch/quantization/quant_type.py", "torch/quantization/quantize_fx.py", "torch/quantization/quantize_jit.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "ef08f92076": {"title": "[quant][graphmode][fx] Add support for weight prepack folding (#43728)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43728\n\nTrace back from the weight node util we hit getattr, reconstruct the graph module with the traced nodes\nand run the graph module to pack the weight. then replace the original chain of ops with the packed weight.\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23385090\n\nfbshipit-source-id: 11341f0af525a02ecec36f163a9cd35dee3744a1", "pr_number": "43728", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["fx"]}, "63dbef3038": {"title": "Better msg (#43848)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43848\n\nMissing space in logging.\n\nTest Plan: build\n\nReviewed By: hl475\n\nDifferential Revision: D23416698\n\nfbshipit-source-id: bf7c494f33836601f5f380c03a0910f419c2e62b", "pr_number": "43848", "files_changed": ["caffe2/opt/custom/freeze_quantization_params.cc"], "labels": ["fb-exported"]}, "1390cad2d8": {"title": "[NNC] Hook up registerizer to Cuda codegen [2/x] (#42878)", "body": "Summary:\nInsert the registerizer into the Cuda Codegen pass list, to enable scalar replacement and close the gap in simple reduction performance.\n\nFirst up the good stuff, benchmark before:\n```\n          Column sum          Caffe2             NNC          Simple          Better\n           (10, 100)          5.7917          9.7037          6.9386          6.0448\n          (100, 100)          5.9338          14.972          7.1139          6.3254\n        (100, 10000)          21.453          741.54          145.74          12.555\n        (1000, 1000)          8.0678          122.75          22.833          9.0778\n\n             Row sum          Caffe2             NNC          Simple          Better\n           (10, 100)          5.4502          7.9661          6.1469          5.5587\n          (100, 100)          5.7613          13.897           21.49          5.5808\n        (100, 10000)          21.702          82.398          75.462          22.793\n        (1000, 1000)          22.527             129          176.51          22.517\n\n```\n\nAfter:\n```\n          Column sum          Caffe2             NNC          Simple          Better\n           (10, 100)          6.0458          9.4966          7.1094           6.056\n          (100, 100)          5.9299          9.1482          7.1693           6.593\n        (100, 10000)          21.739          121.97          162.63          14.376\n        (1000, 1000)          9.2374           29.01          26.883          10.127\n\n             Row sum          Caffe2             NNC          Simple          Better\n           (10, 100)          5.9773          8.1792          7.2307          5.8941\n          (100, 100)          6.1456          9.3155          24.563          5.8163\n        (100, 10000)          25.384          30.212          88.531          27.185\n        (1000, 1000)          26.517          32.702          209.31          26.537\n```\n\nSpeedup about 3-8x depending on the size of the data (increasing with bigger inputs).\n\nThe gap between NNC and simple is closed or eliminated - remaining issue appears to be kernel launch overhead. Next up is getting us closer to the _Better_ kernel.\n\nIt required a lot of refactoring and bug fixes on the way:\n* Refactored flattening of parallelized loops out of the CudaPrinter and into its own stage, so we can transform the graph in the stage between flattening and printing (where registerization occurs).\n* Made AtomicAddFuser less pessimistic, it will now recognize that if an Add to a buffer is dependent on all used Block and Thread vars then it has no overlap and does not need to be atomic. This allows registerization to apply to these stores.\n* Fixed PrioritizeLoad mutator so that it does not attempt to separate the Store and Load to the same buffer (i.e. reduction case).\n* Moved CudaAnalysis earlier in the process, allowing later stages to use the analyzed bufs.\n* Fixed a bug in the Registerizer where when adding a default initializer statement it would use the dtype of the underlying var (which is always kHandle) instead of the dtype of the Buf.\n* Fixed a bug in the IRMutator where Allocate statements logic was inverted to be replaced only if they did not change.\n* Added simplification of simple Division patterns to the IRSimplifier.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42878\n\nReviewed By: glaringlee\n\nDifferential Revision: D23382499\n\nPulled By: nickgg\n\nfbshipit-source-id: 3640a98fd843723abad9f54e67070d48c96fe949", "pr_number": "42878", "files_changed": ["test/cpp/tensorexpr/test_registerizer.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.h", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.h", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/ir_printer.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.h", "torch/csrc/jit/tensorexpr/registerizer.cpp", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["oncall: jit"]}, "89452a67de": {"title": "[fx] GraphModule.src -> GraphModule.code (#43655)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43655\n\nPure, unadulerated bikeshed. The good stuff.\n\nThis makes things more consistent with ScriptModule.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23401528\n\nPulled By: suo\n\nfbshipit-source-id: 7dd8396365f118abcd045434acd9348545314f44", "pr_number": "43655", "files_changed": ["torch/fx/__init__.py", "torch/fx/graph_module.py"], "labels": []}, "c5d0f091b2": {"title": "addmm/addmv should accept complex alpha and beta (#43827)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43827\n\nReviewed By: malfet\n\nDifferential Revision: D23415869\n\nPulled By: ngimel\n\nfbshipit-source-id: a47b76df5fb751f76d36697f5fd95c69dd3a6efe", "pr_number": "43827", "files_changed": ["aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "test/test_torch.py"], "labels": ["module: complex", "open source"]}, "4ef12be900": {"title": "Add __complex__ (#43844)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/43833\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43844\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23422000\n\nPulled By: ngimel\n\nfbshipit-source-id: ebc6a27a9b04c77c3977e6c184cefce9e817cc2f", "pr_number": "43844", "files_changed": ["test/test_torch.py", "tools/autograd/templates/python_variable_methods.cpp", "tools/pyi/gen_pyi.py", "torch/overrides.py"], "labels": ["module: complex", "open source"]}, "69dd0bab90": {"title": "[RPC profiling] Add test to ensure using record_function works for RPC (#43657)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43657\n\nWe didn't have a test that ensures functions ran over RPC that are being profiled can use `with record_function()` to profile specific blocks in the function execution. This is useful if the user wants certain information about specific blocks in the function ran over RPC composed of many torch ops and some custom logic, for example.\n\nCurrently, this will not work if the function is TorchScripted since `with record_function()` is not torchscriptable yet. We can add support for this in future PRs so that torchscript RPC functions can also be profiled like this.\nghstack-source-id: 111033981\n\nReviewed By: mrshenli\n\nDifferential Revision: D23355215\n\nfbshipit-source-id: 318d92e285afebfeeb2a7896b4959412c5c241d4", "pr_number": "43657", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "2bede78a05": {"title": "add qr_backward functionality for wide case (#42216)", "body": "Summary:\nUnblocks implementation of https://github.com/pytorch/pytorch/issues/27036. Note that this PR ***does not*** fix #{27036}.\nCurrently QR decomposition only has support for square and tall (a.k.a. skinny) case.\nThis PR adds functionality for wide A matrix/tensors, includes 3 unit tests for the new case\nand restructures the `qr_backward` method to use the same Walther method as a helper.\n\ncc albanD t-vi\n\nI don't have a gpu machine so haven't tested on cuda but everything passes on my local machine in cpu.\n\nThe basic idea of the PR is noted in the comments in the `Functions.cpp` file but I'll note here too for clarity:\n\nlet <img src=\"https://render.githubusercontent.com/render/math?math=A_{m,n}\"> be a matrix and <img src=\"https://render.githubusercontent.com/render/math?math=m < n\"> then partition <img src=\"https://render.githubusercontent.com/render/math?math=A_{m, n}\"> as  <img src=\"https://render.githubusercontent.com/render/math?math=A_{m,n} = [ X_{m,m} |\\ Y_{m, n-m} ]\">\nand take QR of <img src=\"https://render.githubusercontent.com/render/math?math=X\"> and call that one\n<img src=\"https://render.githubusercontent.com/render/math?math=X=QU\"> the <img src=\"https://render.githubusercontent.com/render/math?math=Q\"> here from <img src=\"https://render.githubusercontent.com/render/math?math=X\"> is the same as the <img src=\"https://render.githubusercontent.com/render/math?math=Q\"> from <img src=\"https://render.githubusercontent.com/render/math?math=QR\"> on entire <img src=\"https://render.githubusercontent.com/render/math?math=A\"> matrix. Then transform <img src=\"https://render.githubusercontent.com/render/math?math=Y\"> with the <img src=\"https://render.githubusercontent.com/render/math?math=Q\"> rotation got from <img src=\"https://render.githubusercontent.com/render/math?math=X\"> to get <img src=\"https://render.githubusercontent.com/render/math?math=V=Q^{T}Y\"> now <img src=\"https://render.githubusercontent.com/render/math?math=R= [U |\\ V] \"> and similarly for the grads of each piece, e.g. if <img src=\"https://render.githubusercontent.com/render/math?math=\\bar{A}\"> is  `grad_A` then\n<img src=\"https://render.githubusercontent.com/render/math?math=\\bar{A} = [ \\bar{X} |\\ \\bar{Y}]\"> and <img src=\"https://render.githubusercontent.com/render/math?math=\\bar{R} = [ \\bar{U} |\\ \\bar{V}]\"> and then\n<img src=\"https://render.githubusercontent.com/render/math?math=\\bar{Y} =  Q\\bar{V}\"> and\n<img src=\"https://render.githubusercontent.com/render/math?math=\\bar{V}\"> is the `narrow()` of `grad_R`.\n<img src=\"https://render.githubusercontent.com/render/math?math=\\bar{X}\"> is calculated very similar to the original Walther formula (exactly the same in the tall and square cases) but is slightly modified here for wide case matrices.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42216\n\nReviewed By: glaringlee\n\nDifferential Revision: D23373118\n\nPulled By: albanD\n\nfbshipit-source-id: 3702ba7e7e23923868c02cdb7e10a96036052344", "pr_number": "42216", "files_changed": ["tools/autograd/templates/Functions.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "topic: linear algebra", "triaged"]}, "1c0faa759e": {"title": "Update requires grad property (#43634)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43634\n\nBecause differentiable graphs detach the gradients of input Tensors, creating and inlining differentiable graphs changes the requires_grad property of tensors in the graph. In the legacy executor, this was not a problem as the Fuser would simply ignore the gradient property because it would be invariant that the LegacyExecutor only passed tensors with grad = False. This is not the case with the profiler, as the Fuser does it's own guarding.\n\nUpdating the type also helps with other typechecks, e.g. the ones specializing the backward, and with debugging the graph.\n\nOther possibilities considered were:\n- Fuser/Specialize AutogradZero always guards against requires_grad=False regardless of the profiled type\n- Re-profile forward execution of differentiable graph\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23358803\n\nPulled By: eellison\n\nfbshipit-source-id: b106998accd5d0f718527bc00177de9af5bad5fc", "pr_number": "43634", "files_changed": ["tools/build_variables.bzl", "torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/update_differentiable_graph_requires_grad.cpp", "torch/csrc/jit/passes/update_differentiable_graph_requires_grad.h", "torch/csrc/jit/runtime/autodiff.cpp"], "labels": ["oncall: jit"]}, "a7e7981c0b": {"title": "Use prim::TensorExprGroup interned symbol (#43635)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43635\n\nIntern the symbol, no functional changes. Aliasing need to be looked at but this should be done in a separate PR; this PR is just changing the symbol.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23358806\n\nPulled By: eellison\n\nfbshipit-source-id: f18bcd142a0daf514136f019ae607e4c3f45d9f8", "pr_number": "43635", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/backward_compatibility/check_backward_compatibility.py", "test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/test_jit_fuser_te.py", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/operator.cpp"], "labels": ["oncall: jit"]}, "259e5b7d71": {"title": "Add passes to profiling executor pipeline (#43636)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43636\n\nWe weren't running inlining in the forward graph of differentiable subgraphs, and we weren't getting rid of all profiles as part of optimization.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23358804\n\nPulled By: eellison\n\nfbshipit-source-id: 05ede5fa356a15ca385f899006cb5b35484ef620", "pr_number": "43636", "files_changed": ["torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["oncall: jit"]}, "5da8a7bf2d": {"title": "use types in the IR instead of vmap (#43742)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43742\n\nWe can remove all prim::profiles, update the values to their specialized profiled types, and then later guard the input graphs based on the input types of the fusion group. After that we remove specialized tensor types from the graph. This gets rid of having to update the vmap and removes all of the profile nodes in fusing.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D23385206\n\nPulled By: eellison\n\nfbshipit-source-id: 2c84bd1d1c38df0d7585e523c30f7bd28f399d7c", "pr_number": "43742", "files_changed": ["test/jit/test_profiler.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/remove_redundant_profiles.cpp", "torch/csrc/jit/passes/remove_redundant_profiles.h", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["oncall: jit"]}, "3c8b1d73c9": {"title": "Update aliasing in tensorexpr fuser (#43743)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43743\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D23385205\n\nPulled By: eellison\n\nfbshipit-source-id: 097a15d5bcf216453e1dd144d6117108b3deae4d", "pr_number": "43743", "files_changed": ["test/jit/test_profiler.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/runtime/interpreter.cpp"], "labels": ["oncall: jit"]}, "0394c5a283": {"title": "[fix] torch.multinomial : fix for 0 size dim (#43775)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43768\n\nTO-DO:\n* [x] Add test\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43775\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23421979\n\nPulled By: ngimel\n\nfbshipit-source-id: 949fcdd30f18d17ae1c372fa6ca6a0b8d0d538ce", "pr_number": "43775", "files_changed": ["aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/cuda/MultinomialKernel.cu", "test/test_torch.py"], "labels": ["module: cuda", "open source", "triaged"]}, "68304c527a": {"title": "Revert D23385090: [quant][graphmode][fx] Add support for weight prepack folding", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23385090 (https://github.com/pytorch/pytorch/commit/ef08f92076806b9f17290876db7c5ea87c09873c)\n\nOriginal commit changeset: 11341f0af525\n\nfbshipit-source-id: fe2bcdc16106923a2cee99eb5cc0a1e9c14ad2c5", "pr_number": null, "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "f7bae5b6b1": {"title": "Revert D23385091: [quant][graphmode][fx] Add top level APIs", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23385091 (https://github.com/pytorch/pytorch/commit/eb4199b0a72bd390780fef5291f72cf213646ff9)\n\nOriginal commit changeset: b789e54e1a0f\n\nfbshipit-source-id: dc3dd9169d34beab92488d78d42d7e7d05e771d1", "pr_number": null, "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/__init__.py", "torch/quantization/_quantize_fx.py", "torch/quantization/fx/__init__.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/quantize.py", "torch/quantization/quant_type.py", "torch/quantization/quantize_fx.py", "torch/quantization/quantize_jit.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "2f52748515": {"title": "Publish all_gather_object and gather_object docs (#43772)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43772\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23398495\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 032e1d628c0c0f2dec297226167471698c56b605", "pr_number": "43772", "files_changed": ["docs/source/distributed.rst", "torch/distributed/distributed_c10d.py"], "labels": []}, "4e4626a23d": {"title": "Join-based API to support DDP uneven inputs (#42577)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42577\n\nCloses https://github.com/pytorch/pytorch/issues/38174. Implements a join-based API to support training with the DDP module in the scenario where different processes have different no. of inputs. The implementation follows the description in https://github.com/pytorch/pytorch/issues/38174. Details are available in the RFC, but as a summary, we make the following changes:\n\n#### Approach\n1) Add a context manager `torch.nn.parallel.distributed.join`\n2) In the forward pass, we schedule a \"present\" allreduce where non-joined process contribute 1 and joined processes contribute 0. This lets us keep track of joined processes and know when all procs are joined.\n3) When a process depletes its input and exits the context manager, it enters \"joining\" mode and attempts to \"shadow\" the collective comm. calls made in the model's forward and backward pass. For example we schedule the same allreduces in the same order as the backward pass, but with zeros\n4) We adjust the allreduce division logic to divide by the effective world size (no. of non-joined procs) rather than the absolute world size to maintain correctness.\n5) At the end of training, the last joined process is selected to be the \"authoritative\" model copy\n\nWe also make some misc. changes such as adding a `rank` argument to `_distributed_broadcast_coalesced` and exposing some getters/setters on `Reducer` to support the above changes.\n\n#### How is it tested?\nWe have tests covering the following models/scenarios:\n- [x] Simple linear model\n- [x] Large convolutional model\n- [x] Large model with module buffers that are broadcast in the forward pass (resnet). We verify this with a helper function `will_sync_module_buffers` and ensure this is true for ResNet (due to batchnorm)\n- [x] Scenario where a rank calls join() without iterating at all, so without rebuilding buckets (which requires collective comm)\n- [x] Model with unused params (with find unused parameters=True)\n- [x] Scenarios where different processes iterate for a varying number of different iterations.\n- [x] Test consistency in tie-breaking when multiple ranks are the last ones to join\n- [x] Test that we divide by the effective world_size (no. of unjoined processes)\n\n#### Performance implications\n\n###### Trunk vs PR patched, 32 GPUs, batch size = 32\nP50, forward + backward + optimizer batch latency & total QPS: 0.121 264/s vs 0.121 264/s\nP50 backwards only batch latency & total QPS: 0.087 369/s vs 0.087 368/s\n\n###### join(enable=True) vs without join, 32 GPUs, batch size = 32, even inputs\nP50, forward + backward + optimizer batch latency & total QPS: 0.120 265/s vs 0.121 264/s\nP50 backwards only batch latency & total QPS: 0.088 364/s vs 0.087 368/s\n\n###### join(enable=False) vs without join, 32 GPUs, batch size = 32, even inputs\nP50 forward + backward + optimizer batch latency & total QPS: 0.121 264/s vs 0.121 264/s\nP50 backwards only batch latency & total QPS: 0.087 368/s vs 0.087 368/s\n\n###### join(enable=True) with uneven inputs (offset = 2000), 32 GPUs, batch size = 32\nP50 forward + backward + optimizer batch latency & total QPS: 0.183 174/s vs 0.121 264/s\nP50 backwards only batch latency & total QPS: 0.150 213/s vs 0.087 368/s\n\n###### join(enable=True) with uneven inputs ((offset = 2000)), 8 GPUs, batch size = 32\nP50 forward + backward + optimizer batch latency & total QPS: 0.104 308/s vs 0.104 308/s\nP50 backwards only batch latency & total QPS: 0.070 454/s vs 0.070 459/s\n\nThe 2 above uneven inputs benchmark was conducted 32 GPUs and 4 GPUs immediately depleting their inputs and entering \"join\" mode (i.e. not iterating at all), while the other 28 iterating as normal. It looks like there is a pretty significant perf hit for this case when there are uneven inputs and multi-node training. Strangely, when there is a single node (8 GPUs), this does not reproduce.\n\n#### Limitations\n1) This is only implemented for MPSD, not SPMD. Per a discussion with mrshenli we want to encourage the use of MPSD over SPMD for DDP.\n2) This does not currently work with SyncBN or custom collective calls made in the model's forward pass. This is because the `join` class only shadows the `broadcast` for buffers in the forward pass, the gradient allreduces in the bwd pass, unused parameters reduction, and (optionally) the rebuild buckets broadcasting in the backwards pass. Supporting this will require additional design thought.\n3) Has not been tested with the [DDP comm. hook](https://github.com/pytorch/pytorch/issues/39272) as this feature is still being finalized/in progress. We will add support for this in follow up PRs.\nghstack-source-id: 111033819\n\nReviewed By: mrshenli\n\nDifferential Revision: D22893859\n\nfbshipit-source-id: dd02a7aac6c6cd968db882c62892ee1c48817fbe", "pr_number": "42577", "files_changed": ["test/distributed/test_c10d.py", "test/distributed/test_distributed.py", "torch/csrc/distributed/c10d/comm.cpp", "torch/csrc/distributed/c10d/comm.h", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/nn/parallel/distributed.py", "torch/testing/_internal/common_distributed.py"], "labels": []}, "ffca81e38b": {"title": "[pytorch][bot] update mobile op deps (#43871)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43871\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23422523\n\nPulled By: ljk53\n\nfbshipit-source-id: 95f2a1b6a2d25b13618c65944a2b919922083fb8", "pr_number": "43871", "files_changed": ["tools/code_analyzer/default_op_deps.yaml"], "labels": []}, "3278beff44": {"title": "Skip target determination for codecov test (#43899)", "body": "Summary:\nPython code coverage tests should not rely on target determination as it will negatively impact the coverage score\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43899\n\nReviewed By: seemethere\n\nDifferential Revision: D23432069\n\nPulled By: malfet\n\nfbshipit-source-id: 341fcadafaab6bd96d33d23973e01f7d421a6593", "pr_number": "43899", "files_changed": [".jenkins/pytorch/test.sh"], "labels": []}, "7680d87a76": {"title": "Let linspace support bfloat16 and complex dtypes (#43578)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43578\n\nReviewed By: malfet\n\nDifferential Revision: D23413690\n\nPulled By: mruberry\n\nfbshipit-source-id: 8c24f7b054269e1317fe53d26d523fea4decb164", "pr_number": "43578", "files_changed": ["aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp", "aten/src/ATen/native/cuda/RangeFactories.cu", "test/test_tensor_creation_ops.py"], "labels": ["module: bfloat16", "module: complex", "open source", "triaged"]}, "3682df77db": {"title": "Implementing NumPy-like function torch.heaviside() (#42523)", "body": "Summary:\n- Related with https://github.com/pytorch/pytorch/issues/38349\n- Implementing the NumPy-like function `torch.heaviside()` .\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42523\n\nReviewed By: ngimel\n\nDifferential Revision: D23416743\n\nPulled By: mruberry\n\nfbshipit-source-id: 9975bd9c9fa73bd0958fe9879f79a692aeb722d5", "pr_number": "42523", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["open source", "triaged"]}, "da0e93a8c3": {"title": "Move `fbcode` related coverage code to `fb/` folder and add `TARGETS` (#43800)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43800\n\n1. Move fbcode related coverage code to fb/ folder and add TARGETS so that we can use buck run to run the tool and solved the import probelm.\n\n2. Write `README.md` to give users guidance about the tool\n\nTest Plan:\nOn devserver:\n```\nbuck run //caffe2/fb/code_coverage/tool:coverage -- //caffe2/c10:\n```\n\nMore examples in README.md\n\nReviewed By: malfet\n\nDifferential Revision: D23404988\n\nfbshipit-source-id: 4942cd0e0fb7bd28a5e884d9835b93f00adb7b92", "pr_number": "43800", "files_changed": ["tools/code_coverage/README.md", "tools/code_coverage/package/tool/clang_coverage.py", "tools/code_coverage/package/tool/print_report.py", "tools/code_coverage/package/tool/utils.py"], "labels": ["fb-exported"]}, "e941a462a3": {"title": "Enable gcc coverage in OSS (#43883)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43883\n\nCheck the result of GCC coverage in OSS is reasonable and ready to ship.\n\nThe amount of executable lines are not the same between `gcc` and `clang` because of the following reasons:\n* Lines following are counted in `clang` but not in `gcc`:\n1. empty line or line with only \u201c{\u201d or \u201c}\u201d\n3. some comments are counted in clang but not in gcc\n5. `#define ...` -- not supported by gcc according to official documentation\n\n* Besides, a statement that explains to more than one line will be counted as only one executable line in gcc, but several lines in clang\n\n## Advantage of `gcc` coverage\n1. Much faster\n- code coverage tool runtime is onle **4 min** (*ammazzzing!!*) by `gcc`, compared to **3 hours!!** by `clang`, to analyze all the tests' artifacts\n2. Use less disk\n- `Clang`'s artifacts will take as large as 170G, but `GCC` is 980M\n\nBesides, also update `README.md`.\n\nTest Plan:\nCompare the result in OSS `clang` and OSS `gcc` with the same command:\n```\npython oss_coverage.py --run-only atest test_nn.py --interested-folder=aten\n```\n\n----\n\n## GCC\n**Summary**\n> time: 0:15:45\nsummary percentage: 44.85%\n\n**Report and Log**\n[File Coverage Report](P140825162)\n[Line Coverage Report](P140825196)\n[Log](P140825385)\n\n------\n\n## CLANG\n\n**Summary**\n> time: 0:21:35\nsummary percentage: 44.08%\n\n**Report and Log**\n[File Coverage Report](P140825845)\n[Line Coverage Report](P140825923)\n[Log](P140825950)\n\n----------\n\n# Run all tests\n```\n# run all tests and get coverage over Pytorch\npython oss_coverage.py\n```\n**Summary**\n> time: 1:27:20. ( time to run tests:  1:23:33)\nsummary percentage: 56.62%\n\n**Report and Log**\n[File Coverage Report](P140837175)\n[Log](P140837121)\n\nReviewed By: malfet\n\nDifferential Revision: D23416772\n\nfbshipit-source-id: a6810fa4d8199690f10bd0a4f58a42ab2a22182b", "pr_number": "43883", "files_changed": ["tools/code_coverage/README.md", "tools/code_coverage/oss_coverage.py", "tools/code_coverage/package/oss/init.py", "tools/code_coverage/package/tool/summarize_jsons.py"], "labels": ["fb-exported"]}, "4c19a1e350": {"title": "Move torch/autograd/grad_mode.pyi stubs inline (#43415)", "body": "Summary:\n- Add `torch._C` bindings from `torch/csrc/autograd/init.cpp`\n- Renamed `torch._C.set_grad_enabled` to `torch._C._set_grad_enabled`\n  so it doesn't conflict with torch.set_grad_enabled anymore\n\nThis is a continuation of gh-38201. All I did was resolve merge conflicts and finish the annotation of `_DecoratorContextManager.__call__` that ezyang started in the first commit.\n\n~Reverts commit b5cd3a80bbc, which was only motivated by not having `typing_extensions` available.~ (JIT can't be made to understand `Literal[False]`, so keep as is).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43415\n\nReviewed By: ngimel\n\nDifferential Revision: D23301168\n\nPulled By: malfet\n\nfbshipit-source-id: cb5290f2e556b4036592655b9fe54564cbb036f6", "pr_number": "43415", "files_changed": ["torch/_C/__init__.pyi.in", "torch/autograd/grad_mode.py", "torch/autograd/grad_mode.pyi", "torch/csrc/autograd/init.cpp"], "labels": ["module: typing", "open source", "triaged"]}, "7137327646": {"title": "log message at per-test level for`perfpipe_pytorch_test_times` (#43752)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43752\n\nTest Plan:\n{F315930458}\n\n{F315930459}\n\nReviewed By: walterddr, malfet\n\nDifferential Revision: D23387998\n\nPulled By: dhuang29\n\nfbshipit-source-id: 2da8b607c049a6f8f21d98dbb25e664ea6229f27", "pr_number": "43752", "files_changed": ["test/print_test_stats.py"], "labels": ["open source"]}, "9b820fe904": {"title": "Fix ImportError in the OSS land. (#43912)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43912\n\nFixed the ImportError: cannot import name 'compute_ulp_error' from 'caffe2.python.oss.fakelowp.test_utils'\n\nTest Plan: test_op_nnpi_fp16.py\n\nReviewed By: hyuen\n\nDifferential Revision: D23435218\n\nfbshipit-source-id: be0b240ee62090d06fdc8efac85fb1c32803da0d", "pr_number": "43912", "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py"], "labels": ["fb-exported"]}, "f150f924d3": {"title": "[JIT] Specialize autograd zero: fix the guarding condition. (#43846)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43846\n\nWe are looking for tensors that are expected to be undefined (according\nto the profile info) and should be checking for them to satisfy the\nfollowing condition: \"not(have any non-zero)\", which is equivalent to\n\"tensor is all zeros\". The issue was that we've been checking tensors\nthat were expected *not* to be undefined.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23416198\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 71e22f552680f68f2af29f427b7355df9b1a4278", "pr_number": "43846", "files_changed": ["torch/csrc/jit/passes/specialize_autogradzero.cpp"], "labels": ["oncall: jit"]}, "d69d603061": {"title": "[JIT] Specialize autograd zero: actually remove the original graph after we created its versioned copy. (#43900)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43900\n\nThe original code assumed that the versioning if was inserted in the\nbeginning of the graph while in fact it was inserted in the end. We're\nnow also not removing `profile_optional` nodes and rely on DCE to clean\nit up later (the reason we're not doing it is that deletion could\ninvalidate the insertion point being used).\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23432175\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 1bf55affaa3f17af1bf71bad3ef64edf71a3e3fb", "pr_number": "43900", "files_changed": ["torch/csrc/jit/passes/specialize_autogradzero.cpp"], "labels": ["oncall: jit"]}, "98b846cd1d": {"title": "[JIT] Remove loop peeling from the profiling executor pipeline. (#43847)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43847\n\nIt seems to slowdown two fastRNN benchmarks and does not speed up\nothers.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23416197\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 598144561979e84bcf6bccf9b0ca786f5af18383", "pr_number": "43847", "files_changed": ["test/test_jit.py", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["oncall: jit"]}, "f73ba88946": {"title": "Avoid resizing in MinMaxObserver (#43789)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43789\n\nSince it's single element.. In some cases we may not be able to resize the\nbuffers.\n\nTest Plan: unit tests\n\nReviewed By: supriyar\n\nDifferential Revision: D23393108\n\nfbshipit-source-id: 46cd7f73ed42a05093662213978a01ee726433eb", "pr_number": "43789", "files_changed": ["torch/quantization/observer.py"], "labels": ["fb-exported"]}, "ee53a335c0": {"title": "[ONNX] Floordiv (#43022)", "body": "Summary:\nAdd export of floordiv op\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43022\n\nReviewed By: houseroad\n\nDifferential Revision: D23398493\n\nPulled By: bzinodev\n\nfbshipit-source-id: f929a88b3bc0c3867e8fbc4e50afdf0c0c71553d", "pr_number": "43022", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["module: onnx", "open source", "triaged"]}, "deb5fde51c": {"title": "[TensorExpr] Make KernelSumMultipleAxes much faster (#43905)", "body": "Summary:\nReduce input size, skip the dtype conversion.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43905\n\nTest Plan: test_tensorexpr --gtest_filter=TensorExprTest.KernelSum*\n\nReviewed By: ailzhang\n\nDifferential Revision: D23433398\n\nPulled By: asuhan\n\nfbshipit-source-id: 0d95ced3c1382f10595a9e5745bf4bef007cc913", "pr_number": "43905", "files_changed": ["test/cpp/tensorexpr/test_kernel.cpp"], "labels": []}, "7db7da7151": {"title": "[reland][quant][graphmode][fx] Add top level APIs (#43581) (#43901)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43901\n\nAdd similar APIs like eager and graph mode on torchscript\n- fuse_fx\n- quantize_fx (for both post training static and qat)\n- quantize_dynamic_fx (for post training dynamic)\n- prepare_fx (for both post training static and qat)\n- prepare_dynamic_fx (for post training dynamic)\n- convert_fx (for all modes)\n\nTest Plan:\nImported from OSS\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23432430\n\nfbshipit-source-id: fc99eb75cbecd6ee7a3aa6c8ec71cd499ff7e3c1", "pr_number": "43901", "files_changed": ["mypy.ini", "test/quantization/test_quantize_fx.py", "torch/quantization/__init__.py", "torch/quantization/_quantize_fx.py", "torch/quantization/fx/__init__.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/quantize.py", "torch/quantization/quant_type.py", "torch/quantization/quantize_fx.py", "torch/quantization/quantize_jit.py", "torch/testing/_internal/common_quantization.py"], "labels": ["fx"]}, "602209751e": {"title": "[quant][graphmode][fix] Fix insert quant dequant for observers without qparams (#43606)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43606\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23335106\n\nfbshipit-source-id: 84af2884d52118c069fc43a9f166dc336a8a87c8", "pr_number": "43606", "files_changed": ["torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": ["oncall: jit"]}, "da32bf4cc6": {"title": "Move type annotations for remaining torch.utils stub files inline (#43406)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43406\n\nReviewed By: mruberry\n\nDifferential Revision: D23319736\n\nPulled By: malfet\n\nfbshipit-source-id: e25fbb49f27aa4893590b022441303d6d98263a9", "pr_number": "43406", "files_changed": ["mypy.ini", "torch/utils/data/__init__.py", "torch/utils/data/__init__.pyi", "torch/utils/data/distributed.py", "torch/utils/data/distributed.pyi", "torch/utils/hooks.py", "torch/utils/hooks.pyi"], "labels": ["module: typing", "open source", "triaged"]}, "f17d7a5556": {"title": "Fix exception chaining in `torch/` (#43836)", "body": "Summary:\n## Motivation\nFixes https://github.com/pytorch/pytorch/issues/43770.\n\n## Description of the change\nThis PR fixes exception chaining only in files under `torch/` where appropriate.\nTo fix exception chaining, I used either:\n1. `raise new_exception from old_exception` where `new_exception` itself seems not descriptive enough to debug or `old_exception` delivers valuable information.\n2. `raise new_exception from None` where raising both of `new_exception` and `old_exception` seems a bit noisy and redundant.\nI subjectively chose which one to use from the above options.\n\n## List of lines containing raise in except clause:\nI wrote [this simple script](https://gist.github.com/akihironitta/4223c1b32404b36c1b349d70c4c93b4d) using [ast](https://docs.python.org/3.8/library/ast.html#module-ast) to list lines where `raise`ing in `except` clause.\n\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/jit/annotations.py#L35\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/jit/annotations.py#L150\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/jit/annotations.py#L158\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/jit/annotations.py#L231\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/jit/_trace.py#L432\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/nn/utils/prune.py#L192\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/cuda/nvtx.py#L7\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/utils/cpp_extension.py#L1537\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/utils/tensorboard/_pytorch_graph.py#L292\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/utils/data/dataloader.py#L835\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/utils/data/dataloader.py#L849\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/utils/data/dataloader.py#L856\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_utils.py#L186\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_utils.py#L189\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_utils.py#L424\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_utils.py#L1279\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_utils.py#L1283\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_utils.py#L1356\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_utils.py#L1388\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_utils.py#L1391\n- [ ] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_utils.py#L1412\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/codegen/random_topo_test.py#L310\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/codegen/random_topo_test.py#L329\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/codegen/random_topo_test.py#L332\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/jit_utils.py#L183\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/testing/_internal/common_nn.py#L4789\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/onnx/utils.py#L367\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/onnx/utils.py#L659\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/onnx/utils.py#L892\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/onnx/utils.py#L897\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/serialization.py#L108\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/serialization.py#L754\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/distributed/rpc/_testing/faulty_agent_backend_registry.py#L76\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/distributed/rpc/backend_registry.py#L260\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/distributed/distributed_c10d.py#L184\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/_utils_internal.py#L57\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/hub.py#L494\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/contrib/_tensorboard_vis.py#L16\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/distributions/lowrank_multivariate_normal.py#L100\n- [x] https://github.com/pytorch/pytorch/blob/000739c31ad34909e64124e0d39b2f49249458e9/torch/distributions/constraint_registry.py#L142\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43836\n\nReviewed By: ailzhang\n\nDifferential Revision: D23431212\n\nPulled By: malfet\n\nfbshipit-source-id: 5f7f41b391164a5ad0efc06e55cd58c23408a921", "pr_number": "43836", "files_changed": ["torch/_utils_internal.py", "torch/contrib/_tensorboard_vis.py", "torch/distributed/distributed_c10d.py", "torch/distributions/constraint_registry.py", "torch/distributions/lowrank_multivariate_normal.py", "torch/jit/_trace.py", "torch/jit/annotations.py", "torch/onnx/utils.py", "torch/serialization.py", "torch/testing/_internal/codegen/random_topo_test.py", "torch/testing/_internal/common_utils.py", "torch/utils/cpp_extension.py", "torch/utils/data/dataloader.py"], "labels": ["oncall: jit", "open source", "triaged"]}, "3c2f6d2ecf": {"title": "[caffe2] Extend dedup SparseAdagrad fusion with stochastic rounding FP16 (#43124)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43124\n\nAdd the stochastic rounding FP16 support for dedup version of SparseAdagrad fusion.\nghstack-source-id: 111037723\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient \\(caffe2\\.caffe2\\.fb\\.net_transforms\\.tests\\.fuse_sparse_ops_test\\.TestFuseSparseOps\\)' --print-passing-details\n```\n\nhttps://our.intern.facebook.com/intern/testinfra/testrun/5629499566042000\n\n```\nbuck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_mean_gradient \\(caffe2\\.caffe2\\.fb\\.net_transforms\\.tests\\.fuse_sparse_ops_test\\.TestFuseSparseOps\\)' --print-passing-details\n```\n\nhttps://our.intern.facebook.com/intern/testinfra/testrun/1125900076333177\n\nReviewed By: xianjiec\n\nDifferential Revision: D22893851\n\nfbshipit-source-id: 81c7a7fe4b0d2de0e6b4fc965c5d23210213c46c", "pr_number": "43124", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cu"], "labels": []}, "69fbc705d8": {"title": "Remained changes of #43578 (#43921)", "body": "Summary:\nNot full https://github.com/pytorch/pytorch/issues/43578 was merged. This PR is the remained part.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43921\n\nReviewed By: ailzhang\n\nDifferential Revision: D23438504\n\nPulled By: mruberry\n\nfbshipit-source-id: 9c5e26346dfc423b7a440b8a986420a27349090f", "pr_number": "43921", "files_changed": ["test/test_tensor_creation_ops.py"], "labels": ["open source"]}, "d7ee84c9b5": {"title": "Update determinism documentation (#41692)", "body": "Summary:\nAdd user-facing documentation for set_deterministic\nAlso update grammar and readability in Reproducibility page\n\nIssue https://github.com/pytorch/pytorch/issues/15359\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41692\n\nReviewed By: ailzhang\n\nDifferential Revision: D23433061\n\nPulled By: mruberry\n\nfbshipit-source-id: 4c4552950803c2aaf80f7bb4792d2095706d07cf", "pr_number": "41692", "files_changed": ["docs/source/notes/randomness.rst", "docs/source/torch.rst", "torch/__init__.py"], "labels": ["open source", "triaged"]}, "69080e9e7e": {"title": "simplify profile text output by displaying only top-level ops statistics (#42262)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42262\n\nTest Plan:\nImported from OSS\n```\n==================================================================================================================================================================================\nTEST\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\nName                           Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Input Shapes\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\naten::add_                     3.61%            462.489us        3.61%            462.489us        462.489us        1                [[3, 20], [3, 20], []]\naten::slice                    1.95%            249.571us        1.95%            250.018us        250.018us        1                [[3, 80], [], [], [], []]\naten::lstm                     1.89%            242.534us        22.41%           2.872ms          2.872ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::lstm                     1.68%            215.852us        18.18%           2.330ms          2.330ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::lstm                     1.68%            215.767us        18.49%           2.370ms          2.370ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::lstm                     1.60%            205.014us        20.15%           2.582ms          2.582ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::lstm                     1.55%            198.213us        18.53%           2.375ms          2.375ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::addmm                    0.95%            122.359us        1.01%            129.857us        129.857us        1                [[80], [3, 20], [20, 80], [], []]\naten::stack                    0.29%            36.745us         0.63%            80.179us         80.179us         1                [[], []]\naten::add_                     0.28%            35.694us         0.28%            35.694us         35.694us         1                [[3, 20], [3, 20], []]\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\nSelf CPU time total: 12.817ms\n\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\nName                           Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Input Shapes\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\naten::mul                      11.45%           1.467ms          12.88%           1.651ms          11.006us         150              [[3, 20], [3, 20]]\naten::lstm                     8.41%            1.077ms          97.76%           12.529ms         2.506ms          5                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::addmm                    7.65%            979.982us        11.38%           1.459ms          29.182us         50               [[80], [3, 20], [20, 80], [], []]\naten::sigmoid_                 6.78%            869.295us        9.74%            1.249ms          8.327us          150              [[3, 20]]\naten::add_                     5.82%            745.801us        5.82%            745.801us        14.916us         50               [[3, 20], [3, 20], []]\naten::slice                    5.58%            715.532us        6.61%            847.445us        4.237us          200              [[3, 80], [], [], [], []]\naten::unsafe_split             4.24%            544.015us        13.25%           1.698ms          33.957us         50               [[3, 80], [], []]\naten::tanh                     3.11%            398.881us        6.05%            775.024us        15.500us         50               [[3, 20]]\naten::empty                    3.04%            389.055us        3.04%            389.055us        1.319us          295              [[], [], [], [], [], []]\naten::sigmoid                  2.96%            379.686us        2.96%            379.686us        2.531us          150              [[3, 20], [3, 20]]\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\nSelf CPU time total: 12.817ms\n\n==================================================================================================================================================================================\nTEST\n==================================================================================================================================================================================\nThis report only display top-level ops statistics\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\nName                           Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Input Shapes\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\naten::lstm                     1.89%            242.534us        22.41%           2.872ms          2.872ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::lstm                     1.68%            215.852us        18.18%           2.330ms          2.330ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::lstm                     1.68%            215.767us        18.49%           2.370ms          2.370ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::lstm                     1.60%            205.014us        20.15%           2.582ms          2.582ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\naten::lstm                     1.55%            198.213us        18.53%           2.375ms          2.375ms          1                [[5, 3, 10], [], [], [], [], [], [], [], []]\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\nSelf CPU time total: 12.817ms\n\n==================================================================================================================================================================================\nThis report only display top-level ops statistics\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\nName                           Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Input Shapes\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\naten::lstm                     8.41%            1.077ms          97.76%           12.529ms         2.506ms          5                [[5, 3, 10], [], [], [], [], [], [], [], []]\n-----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------------------------------------\nSelf CPU time total: 12.817ms\n\nTotal time based on python measurements:  13.206ms\nCPU time measurement python side overhead: 3.03%\n```\n\nReviewed By: ilia-cher\n\nDifferential Revision: D22830328\n\nPulled By: ilia-cher\n\nfbshipit-source-id: c9a71be7b23a8f84784117c788faa43caa96f545", "pr_number": "42262", "files_changed": ["test/test_autograd.py", "torch/autograd/profiler.py"], "labels": []}, "f229d2c07b": {"title": "Revert D23335106: [quant][graphmode][fix] Fix insert quant dequant for observers without qparams", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23335106 (https://github.com/pytorch/pytorch/commit/602209751e825b478f2718b38be0ffe7a768e20b)\n\nOriginal commit changeset: 84af2884d521\n\nfbshipit-source-id: 8d227fe2048b532016407d8ecfbaa6ffd1c313fd", "pr_number": null, "files_changed": ["torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": []}, "85d91a3230": {"title": "[TensorExpr] Check statements in test_kernel.cpp (#43911)", "body": "Summary:\nCheck statements and fix all the warnings.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43911\n\nTest Plan: test_tensorexpr\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23441092\n\nPulled By: asuhan\n\nfbshipit-source-id: f671eef4b4eb9b51acb15054131152ae650fedbd", "pr_number": "43911", "files_changed": ["test/cpp/tensorexpr/test_kernel.cpp"], "labels": []}, "6da26cf0d9": {"title": "Update torch.range warning message regarding the removal version number (#43569)", "body": "Summary:\n`torch.range` still hasn't been removed way after version 0.5. This PR fixes the warning message. Alternatively, we can remove `torch.range`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43569\n\nReviewed By: ngimel\n\nDifferential Revision: D23408233\n\nPulled By: mruberry\n\nfbshipit-source-id: 86c4f9f018ea5eddaf80b78a3c54dfa41cfc6fa6", "pr_number": "43569", "files_changed": ["tools/autograd/templates/python_torch_functions.cpp", "torch/_torch_docs.py"], "labels": ["open source", "topic: operator", "triaged"]}, "825c109eb7": {"title": "[reland][quant][graphmode][fx] Add support for weight prepack folding (#43728) (#43902)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43902\n\nTrace back from the weight node util we hit getattr, reconstruct the graph module with the traced nodes\nand run the graph module to pack the weight. then replace the original chain of ops with the packed weight.\n\nTest Plan:\nImported from OSS\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23432431\n\nfbshipit-source-id: 657f21a8287494f7f87687a9d618ca46376d3aa3", "pr_number": "43902", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["fx"]}, "f1624b82b5": {"title": "Preserve python backtrace in autograd engine errors. (#43684)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43684\n\nThis PR attempts to address #42560 by capturing the appropriate\nexception_ptr in the autograd engine and passing it over to the Future.\n\nAs part of this change, there is a significant change the Future API where we\nnow only accept an exception_ptr as part of setError.\n\nFor the example in #42560, the exception trace would now look like:\n\n```\n> Traceback (most recent call last):\n>   File \"test_autograd.py\", line 6914, in test_preserve_backtrace\n>     Foo.apply(t).sum().backward()\n>   File \"torch/tensor.py\", line 214, in backward\n>     torch.autograd.backward(self, gradient, retain_graph, create_graph)\n>   File \"torch/autograd/__init__.py\", line 127, in backward\n>     allow_unreachable=True)  # allow_unreachable flag\n>   File \"torch/autograd/function.py\", line 87, in apply\n>     return self._forward_cls.backward(self, *args)\n>   File \"test_autograd.py\", line 6910, in backward\n>     raise ValueError(\"something\")\n> ValueError: something\n```\nghstack-source-id: 111109637\n\nTest Plan: waitforbuildbot\n\nReviewed By: albanD\n\nDifferential Revision: D23365408\n\nfbshipit-source-id: 1470c4776ec8053ea92a6ee1663460a3bae6edc5", "pr_number": "43684", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/test/ivalue_test.cpp", "test/cpp/jit/test_misc.cpp", "test/test_autograd.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/torchscript_functions.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["oncall: jit"]}, "820c4b05a9": {"title": "[ONNX] Update slice symbolic function (#42935)", "body": "Summary:\nDuring scripting, combination of shape (or size()) and slice (e.g x.shape[2:]) produces following error:\n slice() missing 1 required positional argument: 'step'\nThis happens because aten::slice has 2 signatures:\n\n- aten::slice(Tensor self, int dim, int start, int end, int step) -> Tensor\n- aten::slice(t[] l, int start, int end, int step) -> t[]\n\nand when a list is passed instead of tensor the 2nd of the two slice signatures is called, and since it has 4 instead of 5 arguments it produces the above exception.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42935\n\nReviewed By: houseroad\n\nDifferential Revision: D23398435\n\nPulled By: bzinodev\n\nfbshipit-source-id: 4151a8f878c520cea199b265973fb476b17801fe", "pr_number": "42935", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset9.py"], "labels": ["module: onnx", "open source", "triaged"]}, "c14a3613a8": {"title": "Fix NaN propagation in TE fuser's min/max implementation (#43609)", "body": "Summary:\nPer eager mode source-of-truth, NaNs shall be propagated by min/max.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43609\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23349184\n\nPulled By: bertmaher\n\nfbshipit-source-id: 094eb8b89a02b27d5ecf3988d0f473c0f91e4afb", "pr_number": "43609", "files_changed": ["test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/tests.h", "test/test_jit_fuser_te.py", "test/test_tensorexpr.py", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/eval.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp"], "labels": ["oncall: jit"]}, "fab012aa28": {"title": "Revert \"Added support for Huber Loss (#37599)\" (#43351)", "body": "Summary:\nThis reverts commit 11e5174926d807a540fc7b54fb45a26ec0c5d9c0 due to [comment](https://github.com/pytorch/pytorch/pull/37599#pullrequestreview-471950192).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43351\n\nReviewed By: pbelevich, seemethere\n\nDifferential Revision: D23249511\n\nPulled By: vincentqb\n\nfbshipit-source-id: 18b8b346f00eaf0ef7376b06579d404a84add4de", "pr_number": "43351", "files_changed": ["torch/nn/functional.py", "torch/nn/modules/loss.py"], "labels": []}, "a67246b2d4": {"title": "Add reduction string test for ctc_loss. (#43884)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43884\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23427907\n\nPulled By: gchanan\n\nfbshipit-source-id: 889bd92e9d3e0528b57e3952fc83e25bc7abe293", "pr_number": "43884", "files_changed": ["test/test_nn.py"], "labels": []}, "0b2694cd11": {"title": "Support work.result() to get result tensors for allreduce for Gloo, NCCL backends (#43386)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43386\n\nResolves #43178\n\nghstack-source-id: 111109716\n\nTest Plan: Added checks to existing unit test and ran it on gpu devserver.\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23216393\n\nfbshipit-source-id: fed5e37fbabbd2ac4a9055b20057fffe3c416c0b", "pr_number": "43386", "files_changed": ["test/distributed/test_distributed.py", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp"], "labels": []}, "2789a4023b": {"title": "TestVmapOperators: add structured tests that batching rules get invoked (#43731)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43731\n\nAfter this PR, for each test in TestVmapOperators, TestVmapOperators\ntests that the test never invokes the slow vmap fallback path. The\nrationale behind this change is that TestVmapOperators is used for\ntesting batching rules and we want confidence that the batching rules\nactually get invoked.\n\nWe set this up using a similar mechanism to the CUDA memory leak check:\n(https://github.com/pytorch/pytorch/blob/bff741a8497887c8ee22ffa9f0208565072a74dc/torch/testing/_internal/common_utils.py#L506-L511)\n\nThis PR also implements the batching rule for `to.dtype_layout`; the new\ntesting caught that we were testing vmap on `to.dtype_layout` but it\ndidn't actually have a batching rule implemented!\n\nTest Plan: - New tests in `pytest test/test_vmap.py -v` that test the mechanism.\n\nReviewed By: ezyang\n\nDifferential Revision: D23380729\n\nPulled By: zou3519\n\nfbshipit-source-id: 6a4b97a7fa7b4e1c5be6ad80d6761e0d5b97bb8c", "pr_number": "43731", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": []}, "fa12e225d3": {"title": "Batching rule for torch.mv (#43780)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43780\n\nThe general strategy is:\n- unsqueeze the physical inputs enough\n- pass the unsqueezed physical inputs to at::matmul\n- squeeze any extra dimensions\n\nTest Plan: - `pytest test/test_vmap.py -v`\n\nReviewed By: ezyang\n\nDifferential Revision: D23400842\n\nPulled By: zou3519\n\nfbshipit-source-id: c550eeb935747c08e3b083609ed307a4374b9096", "pr_number": "43780", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": []}, "dbc4218f11": {"title": "Batching rules for: torch.bmm, torch.dot (#43781)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43781\n\nTest Plan: - `pytest test/test_vmap.py -v`\n\nReviewed By: ezyang\n\nDifferential Revision: D23400843\n\nPulled By: zou3519\n\nfbshipit-source-id: a901bba6dc2d8435d314cb4dac85bbd5cd4ee2a5", "pr_number": "43781", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": []}, "9b98bcecfa": {"title": "torch.cat and torch.stack batching rules (#43798)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43798\n\nThese are relatively straightforward.\n\nTest Plan: - `pytest test/test_vmap.py -v`\n\nReviewed By: ezyang\n\nDifferential Revision: D23405000\n\nPulled By: zou3519\n\nfbshipit-source-id: 65c78da3dee43652636bdb0a65b636fca69e765d", "pr_number": "43798", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": []}, "b1f19c20d6": {"title": "Run function check and out check in TestTensorDeviceOps (#43830)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43830\n\nReviewed By: ailzhang\n\nDifferential Revision: D23438101\n\nPulled By: mruberry\n\nfbshipit-source-id: b581ce779ea2f50ea8dfec51d5469031ec7a0a67", "pr_number": "43830", "files_changed": ["test/test_torch.py"], "labels": ["open source", "triaged"]}, "a044c039c0": {"title": "updated documentation to streamline setup (#42850)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42850\n\nReviewed By: mrshenli\n\nDifferential Revision: D23449055\n\nPulled By: osandoval-fb\n\nfbshipit-source-id: 6db695d4fe5f6d9b7bb2895c85c855db4779516b", "pr_number": "42850", "files_changed": ["CONTRIBUTING.md", "README.md"], "labels": []}, "13a48ac1f3": {"title": "MaxPool1d without indices optimization (#43745)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43745\n\nThis is part of a larger effort to refactor and optimize the pooling code. Previously I started working on MaxPool2d here https://github.com/pytorch/pytorch/pull/43267 but since it uses MaxPool1d as a subroutine, it made more sense to work on 1D first and get it tested and optimized and then move up to 2D and then 3D.\n\nBelow are some benchmarking results, the python script I used is under the results.\n\n## Benchmarking\n```\nName (time in us)                            Min                   Max                Mean             StdDev              Median                 IQR            Outliers  OPS (Kops/s)            Rounds  Iterations\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_googlenet[(3, 2, 0, 1, 0)-new]      79.7659 (1.03)     1,059.6327 (5.32)      90.6280 (1.01)     19.1196 (1.41)      84.2176 (1.01)       2.4289 (1.0)     1079;2818       11.0341 (0.99)       9055           1\ntest_googlenet[(3, 2, 0, 1, 0)-old]     505.1531 (6.55)       830.8962 (4.17)     563.4763 (6.29)     65.3974 (4.81)     538.3361 (6.43)      80.5371 (33.16)      242;99        1.7747 (0.16)       1742           1\ntest_googlenet[(3, 2, 0, 1, 1)-new]      80.2949 (1.04)       233.0020 (1.17)      97.6498 (1.09)     19.1228 (1.41)      89.2282 (1.07)      18.5743 (7.65)     1858;741       10.2407 (0.92)       9587           1\ntest_googlenet[(3, 2, 0, 1, 1)-old]     513.5350 (6.66)       977.4677 (4.91)     594.4559 (6.63)     69.9372 (5.15)     577.9080 (6.90)      79.8218 (32.86)      503;84        1.6822 (0.15)       1675           1\ntest_googlenet[(3, 2, 1, 1, 0)-new]      77.1061 (1.0)        199.1168 (1.0)       89.6529 (1.0)      13.5864 (1.0)       83.7557 (1.0)        7.5139 (3.09)    1419;1556       11.1541 (1.0)        7434           1\ntest_googlenet[(3, 2, 1, 1, 0)-old]     543.6055 (7.05)       964.5708 (4.84)     636.9867 (7.11)     84.0732 (6.19)     616.7777 (7.36)     100.4562 (41.36)      434;65        1.5699 (0.14)       1552           1\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_inception[(3, 2, 0, 1, 0)-new]      84.5827 (1.00)       184.2827 (1.0)       90.5438 (1.01)      9.6324 (1.0)       89.3027 (1.05)      4.5672 (1.03)      637;759       11.0444 (0.99)       6274           1\ntest_inception[(3, 2, 0, 1, 0)-old]     641.2268 (7.59)     1,704.8977 (9.25)     686.9383 (7.65)     57.2499 (5.94)     682.5905 (8.01)     58.3753 (13.17)       86;21        1.4557 (0.13)        802           1\ntest_inception[(3, 2, 0, 1, 1)-new]      84.5008 (1.0)      1,093.6335 (5.93)      89.8233 (1.0)      14.0443 (1.46)      85.2682 (1.0)       4.4331 (1.0)      802;1106       11.1330 (1.0)        9190           1\ntest_inception[(3, 2, 0, 1, 1)-old]     643.7078 (7.62)       851.4188 (4.62)     687.4905 (7.65)     41.1116 (4.27)     685.1386 (8.04)     60.2733 (13.60)      286;14        1.4546 (0.13)       1300           1\ntest_inception[(3, 2, 1, 1, 0)-new]     106.0739 (1.26)       258.5649 (1.40)     115.3597 (1.28)     17.5436 (1.82)     106.9643 (1.25)      5.5470 (1.25)     894;1402        8.6685 (0.78)       7635           1\ntest_inception[(3, 2, 1, 1, 0)-old]     651.0504 (7.70)       955.2278 (5.18)     698.0295 (7.77)     45.5097 (4.72)     692.8109 (8.13)     64.6794 (14.59)      145;15        1.4326 (0.13)        909           1\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_large_batch_size[new]       2.9608 (1.0)        5.1127 (1.0)        3.3096 (1.0)      0.1936 (1.0)        3.3131 (1.0)      0.2093 (1.0)          71;6  302.1515 (1.0)         297           1\ntest_large_batch_size[old]     130.6583 (44.13)    152.9521 (29.92)    137.1385 (41.44)    7.4352 (38.40)    135.1784 (40.80)    5.1358 (24.53)         1;1    7.2919 (0.02)          7           1\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_large_channel_size[new]      2.9696 (1.0)       5.5595 (1.0)       3.5997 (1.0)      0.5836 (1.0)       3.3497 (1.0)      0.3445 (1.0)         58;54  277.8014 (1.0)         277           1\ntest_large_channel_size[old]     19.6838 (6.63)     22.6637 (4.08)     21.1775 (5.88)     0.8610 (1.48)     21.3739 (6.38)     1.4930 (4.33)         13;0   47.2199 (0.17)         36           1\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_large_width[new]      1.7714 (1.0)       2.4104 (1.0)       1.8988 (1.0)      0.0767 (1.0)       1.8911 (1.0)      0.0885 (1.0)         86;13  526.6454 (1.0)         373           1\ntest_large_width[old]     19.5708 (11.05)    22.8755 (9.49)     20.7987 (10.95)    0.7009 (9.14)     20.6623 (10.93)    0.8584 (9.70)         14;1   48.0799 (0.09)         46           1\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_multithreaded[new]      15.0560 (1.0)       24.2891 (1.0)       16.1627 (1.0)      1.5657 (1.0)       15.7182 (1.0)      0.7598 (1.0)           4;6  61.8709 (1.0)          65           1\ntest_multithreaded[old]     115.7614 (7.69)     120.9670 (4.98)     118.3004 (7.32)     1.6259 (1.04)     118.4164 (7.53)     1.9613 (2.58)          2;0   8.4531 (0.14)          8           1\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nLegend:\n  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n  OPS: Operations Per Second, computed as 1 / Mean\n```\n\n### Benchmarking script\nTo run the benchmark make sure you have pytest-benchmark installed with `pip install pytest-benchmark` and use the following command: `pytest benchmark.py --benchmark-sort='name'`\n\n```\nimport torch\nimport pytest\n\ndef _test_speedup(benchmark, batches=1, channels=32, width=32,\n                  kernel_size=2, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False):\n    torch.set_num_threads(1)\n    x = torch.randn((batches, channels, width))\n    model = torch.nn.MaxPool1d(kernel_size, stride, padding, dilation, return_indices, ceil_mode)\n    benchmark(model, x)\n\npytest.mark.benchmark(group=\"inception\")\npytest.mark.parametrize(\"return_indices\", [True, False], ids=[\"old\", \"new\"])\npytest.mark.parametrize(\"params\", [(3, 2), (3, 2, 0, 1, True), (3, 2, 1)],\n                         ids=[\"(3, 2, 0, 1, 0)\",\n                              \"(3, 2, 0, 1, 1)\",\n                              \"(3, 2, 1, 1, 0)\"])\ndef test_inception(benchmark, params, return_indices):\n    _test_speedup(benchmark, 10, 64, 147, *params, return_indices=return_indices)\n\npytest.mark.benchmark(group=\"googlenet\")\npytest.mark.parametrize(\"return_indices\", [True, False], ids=[\"old\", \"new\"])\npytest.mark.parametrize(\"params\", [(3, 2), (3, 2, 0, 1, True), (3, 2, 1)],\n                         ids=[\"(3, 2, 0, 1, 0)\",\n                              \"(3, 2, 0, 1, 1)\",\n                              \"(3, 2, 1, 1, 0)\"])\ndef test_googlenet(benchmark, params, return_indices):\n    _test_speedup(benchmark, 10, 64, 112, *params, return_indices=return_indices)\n\npytest.mark.benchmark(group=\"large batch size\")\npytest.mark.parametrize(\"return_indices\", [True, False], ids=[\"old\", \"new\"])\ndef test_large_batch_size(benchmark, return_indices):\n    _test_speedup(benchmark, 100000, 1, 32, return_indices=return_indices)\n\npytest.mark.benchmark(group=\"large channel size\")\npytest.mark.parametrize(\"return_indices\", [True, False], ids=[\"old\", \"new\"])\ndef test_large_channel_size(benchmark, return_indices):\n    _test_speedup(benchmark, 1, 100000, 32, return_indices=return_indices)\n\npytest.mark.benchmark(group=\"large width\")\npytest.mark.parametrize(\"return_indices\", [True, False], ids=[\"old\", \"new\"])\ndef test_large_width(benchmark, return_indices):\n    _test_speedup(benchmark, 1, 32, 100000, return_indices=return_indices)\n\npytest.mark.benchmark(group=\"multithreading\")\npytest.mark.parametrize(\"return_indices\", [True, False], ids=[\"old\", \"new\"])\ndef test_multithreaded(benchmark, return_indices):\n    x = torch.randn((40, 10000, 32))\n    model = torch.nn.MaxPool1d(2, return_indices=return_indices)\n    benchmark(model, x)\n```\n\n## Discussion\n\nThe new algorithm is on average 7x faster than the old one. But because the old algorithm had many issues with how it parallelized the code and made use of the cache, one can come up with input parameters (like large batch size) that will make the new algorithm much faster than the original one.\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D23425348\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 3fa3f9b8e71200da48424a95510124a83f50d7b2", "pr_number": "43745", "files_changed": ["aten/src/ATen/native/MaxPooling.cpp", "aten/src/ATen/native/MaxPooling.h", "aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/cpu/MaxPooling.cpp", "test/test_nn.py", "torch/nn/modules/pooling.py"], "labels": []}, "224232032c": {"title": "Move Autograd to an alias dispatch key (#43070)", "body": "Summary:\nThis PR moves `DispatchKey::Autograd` to an alias dispatch key mapping to `AutogradCPU, AutogradCUDA, AutogradXLA, AutogradOther, AutogradPrivate*` keys.\n\nA few things are handled in this PR:\n- Update alias dispatch key mapping and precompute dispatchTable logic\n- Move `Autograd` key from `always_included` set to TensorImpl constructor.\n- Update `dummyTensor` constructor to take `requires_grad` as optional argument so that it's closer to the real application in op_registration_test.\n- Use `BackendSelect` key for both backend select before and after autograd layer. (1 liner in backend_select codegen)\n\nA few planned followups ordered by priority:\n- [cleanup] Update `test_dispatch.py` to include testing `Autograd`.\n- [cleanup] Add Math alias key and move catchAll to Math. (to remove 2.2 in `computeDispatchTableEntryWithDebug`)\n- [new feature] Add support for Math in native_functions.yaml\n- [cleanup] Add iterator like functionality to DispatchKeySet\n- [cleanup/large] Only add Autograd backend keys when tensor requires grad. (cc: ljk53 ?)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43070\n\nReviewed By: ezyang\n\nDifferential Revision: D23281535\n\nPulled By: ailzhang\n\nfbshipit-source-id: 9ad00b17142e9b83304f63cf599f785500f28f71", "pr_number": "43070", "files_changed": ["aten/src/ATen/core/LegacyTypeDispatch.h", "aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/boxing/impl/test_helpers.h", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/library.cpp", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "aten/src/ATen/templates/TensorBody.h", "c10/core/Backend.h", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/TensorOptions.h", "test/test_dispatch.py", "tools/codegen/gen.py", "torch/csrc/utils/python_dispatch.cpp", "torch/csrc/utils/tensor_new.cpp"], "labels": []}, "e3cb582e05": {"title": "Error printing extension support for multiline errors (#43807)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43807\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23407457\n\nPulled By: Lilyjjo\n\nfbshipit-source-id: 05a6a50dc39c00474d9087ef56028a2c183aa53a", "pr_number": "43807", "files_changed": ["docs/source/jit_language_reference.rst", "test/test_jit.py", "torch/csrc/jit/frontend/source_range.cpp"], "labels": ["oncall: jit"]}, "76ca365661": {"title": "[pytorch][bot] update mobile op deps (#43937)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43937\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23443927\n\nPulled By: ljk53\n\nfbshipit-source-id: 526ca08dfb5bd32527bff98b243da90dbbf2ea49", "pr_number": "43937", "files_changed": ["tools/code_analyzer/default_op_deps.yaml"], "labels": []}, "8ca3913f47": {"title": "Introduce BUILD_CAFFE2 flag (#43673)", "body": "Summary:\nintroduce BUILD_CAFFE2 flag. default to `ON`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43673\n\nReviewed By: malfet\n\nDifferential Revision: D23381035\n\nPulled By: walterddr\n\nfbshipit-source-id: 1f4582987fa0c4a911f0b18d311c04fdbf8dd8f0", "pr_number": "43673", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "CMakeLists.txt", "cmake/Summary.cmake"], "labels": ["module: build", "triaged"]}, "63a0bb0ab9": {"title": "Add typing annotations for torch.nn.quantized.dynamic.modules.rnn (#43186)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43185\n\nxref: [gh-43072](https://github.com/pytorch/pytorch/issues/43072)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43186\n\nReviewed By: ezyang\n\nDifferential Revision: D23441259\n\nPulled By: malfet\n\nfbshipit-source-id: 80265ae7f3a70f0087e620969dbd4aa8ca17c317", "pr_number": "43186", "files_changed": ["mypy.ini", "torch/nn/quantized/dynamic/modules/rnn.py"], "labels": ["module: typing", "oncall: quantization", "open source", "triaged"]}, "7035cd0f84": {"title": "Revert D23216393: Support work.result() to get result tensors for allreduce for Gloo, NCCL backends", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23216393 (https://github.com/pytorch/pytorch/commit/0b2694cd11a1bddecc01121a882baa2ed1b35d37)\n\nOriginal commit changeset: fed5e37fbabb\n\nfbshipit-source-id: 27fbeb1617066fa3f271a681cb089622027d6689", "pr_number": null, "files_changed": ["test/distributed/test_distributed.py", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp"], "labels": []}, "bc64efae48": {"title": "Back out \"Revert D19987020: [pytorch][PR] Add the sls tensor train op\" (#43938)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43938\n\nresubmit\n\nTest Plan: unit test included\n\nReviewed By: mruberry\n\nDifferential Revision: D23443493\n\nfbshipit-source-id: 7b68f8f7d1be58bee2154e9a498b5b6a09d11670", "pr_number": "43938", "files_changed": ["caffe2/operators/lengths_reducer_ops.cc", "caffe2/operators/lengths_reducer_ops.h", "caffe2/python/hypothesis_test.py"], "labels": ["fb-exported"]}, "db6bd9d60b": {"title": "rename input argunment `interested-folder` to `interest-only` -- be consistent with other arguments (#43889)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43889\n\n1. rename input argunment `interested-folder` to `interest-only` -- be consistent with `run-only`, `coverage-only` and be shorted\n\nTest Plan: Test on devserver and linux docker.\n\nReviewed By: malfet\n\nDifferential Revision: D23417338\n\nfbshipit-source-id: ce9711e75ca3a1c30801ad6bd1a620f3b06819c5", "pr_number": "43889", "files_changed": ["tools/code_coverage/package/oss/init.py", "tools/code_coverage/package/util/utils_init.py"], "labels": ["fb-exported"]}, "5472426b9f": {"title": "Reset `DataLoader` workers instead of creating new ones (#35795)", "body": "Summary:\nThis PR needs discussion as it changes the behavior of `DataLoader`. It can be closed if its not considered a good practice.\n\nCurrently, the `DataLoader` spawns a new `_BaseDataLoaderIter` object every epoch,\nIn the case of the multiprocess DataLoader, every epoch the worker processes are re-created and they make a copy of the original `Dataset` object.\nIf users want to cache data or do some tracking on their datasets, all their data will be wiped out every epoch. Notice that this doesn't happen when the number of workers is 0. giving some inconsistencies with the multiprocess and serial data loaders.\n\nThis PR keeps the `_BaseDataLoaderIter` object alive and just resets it within epochs, so the workers remain active and so their own `Dataset` objects. People seem to file issues about this often.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/35795\n\nReviewed By: ailzhang\n\nDifferential Revision: D23426612\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e16950036bae35548cd0cfa78faa06b6c232a2ea", "pr_number": "35795", "files_changed": ["docs/source/data.rst", "test/test_dataloader.py", "torch/utils/data/_utils/worker.py", "torch/utils/data/dataloader.py"], "labels": ["open source", "triaged"]}, "06c277f38e": {"title": "[TVM] Support slice op (#43969)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43969\n\nReviewed By: yinghai\n\nDifferential Revision: D23413340\n\nfbshipit-source-id: 20168bd573b81ce538e3589b72aba9590c3c055e", "pr_number": "43969", "files_changed": ["caffe2/opt/tvm_transformer.cc"], "labels": ["fb-exported"]}, "73f7d63bc9": {"title": "[FX] Support tensor-valued constants (#43666)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43666\n\nTest Plan: Imported from OSS\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23359110\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 8569a2db0ef081ea7d8e81d7ba26a92bc12ed423", "pr_number": "43666", "files_changed": ["test/test_fx.py", "torch/fx/symbolic_trace.py"], "labels": []}, "a1a23669f2": {"title": "[FX] Pickle serialization of GraphModule via forward source (#43674)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43674\n\nTest Plan: Imported from OSS\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23362396\n\nPulled By: jamesr66a\n\nfbshipit-source-id: cb8181edff70643b7bbe548cc6b0957328d4eedd", "pr_number": "43674", "files_changed": ["test/test_fx.py", "torch/fx/graph_module.py"], "labels": []}, "bacee6aa2e": {"title": "Selective meta programming preparation for prim ops (#43540)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43540\n\nselected_mobile_ops.h is generated at BUCK build time, which contains the whitelist of root operators. It's used for templated selective build when XPLAT_MOBILE_BUILD is defined.\n\nghstack-source-id: 111014372\n\nTest Plan: CI and BSB\n\nReviewed By: ljk53\n\nDifferential Revision: D22618309\n\nfbshipit-source-id: ddf813904892f99c3f4ae0cd14ce8b27727be5a2", "pr_number": "43540", "files_changed": ["aten/src/ATen/core/op_registration/op_whitelist.h"], "labels": []}, "24eea364f7": {"title": "Check SparseAdam params are dense on init (#41966) (#43668)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41966\n\nRaises a value error if user attempts to create SparseAdam optimizer with sparse parameter tensors.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43668\n\nReviewed By: glaringlee\n\nDifferential Revision: D23388109\n\nPulled By: ranman\n\nfbshipit-source-id: 1fbcc7527d49eac6fae9ce51b3307c609a6ca38b", "pr_number": "43668", "files_changed": ["test/test_optim.py", "torch/optim/sparse_adam.py"], "labels": []}, "93fbbaab2a": {"title": "Update `README.md` in oss (#43893)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43893\n\nUpdate `README.md` in oss, provide more examples, start from the most common use to specified use. Make `README.md` be more friendly and more specific.\n\nTest Plan: `README.md` doesn't need test.\n\nReviewed By: malfet, seemethere\n\nDifferential Revision: D23420203\n\nfbshipit-source-id: 1a4c146393fbcaf2893321e7892740edf5d0c248", "pr_number": "43893", "files_changed": ["tools/code_coverage/README.md"], "labels": ["fb-exported"]}, "5e97f251a8": {"title": "Enable TF32 support for cuDNN (#40737)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40737\n\nReviewed By: mruberry\n\nDifferential Revision: D22801525\n\nPulled By: ngimel\n\nfbshipit-source-id: ac7f7e728b4b3e01925337e8c9996f26a6433fd2", "pr_number": "40737", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "aten/src/ATen/native/cudnn/RNN.cpp", "aten/src/ATen/native/native_functions.yaml", "caffe2/serialize/inline_container.h", "docs/source/notes/cuda.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_lite_interpreter.cpp", "test/test_cuda.py", "test/test_nn.py", "tools/autograd/derivatives.yaml", "torch/_C/__init__.pyi.in", "torch/backends/cudnn/__init__.py", "torch/csrc/Module.cpp", "torch/csrc/jit/passes/fold_conv_bn.cpp", "torch/csrc/jit/passes/graph_rewrite_helper.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/onnx/symbolic_opset9.py", "torch/testing/_internal/autocast_test_lists.py", "torch/testing/_internal/common_cuda.py", "torch/testing/_internal/common_nn.py"], "labels": ["open source", "triaged"]}, "e49dd9fa05": {"title": "Delete `raise_from` from `torch._six` (#43981)", "body": "Summary:\nNo need for compatibility wrapper in Python3+ world\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43981\n\nReviewed By: seemethere\n\nDifferential Revision: D23458325\n\nPulled By: malfet\n\nfbshipit-source-id: 00f822895625f4867c22376fe558c50316f5974d", "pr_number": "43981", "files_changed": ["torch/_six.py", "torch/cuda/__init__.py"], "labels": ["better-engineering", "triaged"]}, "ec7f14943c": {"title": "[OSS] Update README.md -- Explain more complex arguments and functionalities", "body": "Summary: Update `README.md` for oss to explain the usage of `--run` `--export` `--summary`\n\nTest Plan: Test locally.\n\nReviewed By: malfet\n\nDifferential Revision: D23431508\n\nfbshipit-source-id: 368b8dd8cd5099f39c7f5bc985203c417bf7af39", "pr_number": null, "files_changed": ["tools/code_coverage/README.md"], "labels": []}, "8d53df30ea": {"title": "[FX] Better error when unpacking Proxy (#43740)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43740\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D23380964\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 9658ef1c50d0f9c4de38781a7485002487f6d3f7", "pr_number": "43740", "files_changed": ["test/test_fx.py", "torch/fx/proxy.py"], "labels": ["fx"]}, "d15b9d980c": {"title": "[quant][graphmode][fx][refactor] Move patterns to separate files (#43891)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43891\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23429759\n\nfbshipit-source-id: f19add96beb7c8bac323ad78f74588ca1393040c", "pr_number": "43891", "files_changed": ["torch/quantization/fx/fuse.py", "torch/quantization/fx/fusion_patterns.py", "torch/quantization/fx/pattern_utils.py", "torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "0ffe3d84d5": {"title": "[quant][graphmode][fx] Support dynamic quantization without calibration (#43892)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43892\n\nRun weight observer in the convert function, so user do not need to run calibration\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23429758\n\nfbshipit-source-id: 5bc222e3b731789ff7a86463c449690a58dffb7b", "pr_number": "43892", "files_changed": ["torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "4134b7abfa": {"title": "Pass CC env variable as ccbin argument to nvcc (#43931)", "body": "Summary:\nThis is the common behavior when one builds PyTorch (or any other CUDA project) using CMake, so it should be held true for Torch CUDA extensions as well.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43931\n\nReviewed By: ezyang, seemethere\n\nDifferential Revision: D23441793\n\nPulled By: malfet\n\nfbshipit-source-id: 1af392107a94840331014fda970ef640dc094ae4", "pr_number": "43931", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["module: cpp-extensions", "triaged"]}, "fbea2ee917": {"title": "broadcast_object API for c10d (#43887)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43887\n\nAs part of addressing #23232, this PR adds support for `broadcast_object_list` which is an API to broadcast arbitrary picklable objects to all the other ranks.  This has been a long-requested feature, so would be good for Pytorch to natively support this.\n\nThe implementation approach follows a similar approach as https://github.com/pytorch/pytorch/pull/42189. The input is a list of objects to be broadcasted and it is in place, meaning all ranks part of the group will have their input list modified to contain the broadcasted objects from the src rank.\n\nNote that the API is designed to match the tensor-based collectives other than supporting async_op. For now, it is a blocking call. If we see demand to support async_op, we will have to make more progress on merging work/future to support this.\nghstack-source-id: 111180436\n\nReviewed By: mrshenli\n\nDifferential Revision: D23422577\n\nfbshipit-source-id: fa700abb86eff7128dc29129a0823e83caf4ab0e", "pr_number": "43887", "files_changed": ["docs/source/distributed.rst", "test/distributed/test_distributed.py", "torch/distributed/distributed_c10d.py"], "labels": []}, "8fd9fe93be": {"title": "[quant][graphmode][fx] Support dynamic quantization without calibration (#43952)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43952\n\nRun weight observer for dynamic quantization before inserting quant/dequant node\n\nTest Plan: Imported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23452123\n\nfbshipit-source-id: c322808fa8025bbadba36c2e5ab89f59e85de468", "pr_number": "43952", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "9db90fe1f3": {"title": "[TensorExpr] Remove unused functions in kernel.cpp (#43966)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43966\n\nTest Plan: build.\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23456660\n\nPulled By: asuhan\n\nfbshipit-source-id: c13411b61cf62dd5d038e7246f79a8682822b472", "pr_number": "43966", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["oncall: jit"]}, "263412e536": {"title": "Rename is_complex_t -> is_complex (#39906)", "body": "Summary:\n`is_complex_t` is a bad name. For example in std, there are `std::is_same` but not `std::is_same_t`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39906\n\nReviewed By: mrshenli\n\nDifferential Revision: D22665013\n\nPulled By: anjali411\n\nfbshipit-source-id: 4b71745f5e2ea2d8cf5845d95ada4556c87e040d", "pr_number": "39906", "files_changed": ["aten/src/ATen/NumericUtils.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/native/cpu/zmath.h", "aten/src/ATen/native/cuda/ScanKernels.cu", "c10/core/Scalar.h", "c10/util/Half.h", "c10/util/TypeCast.h", "c10/util/complex_utils.h"], "labels": ["module: complex", "open source", "triaged"]}, "137a4fcc3b": {"title": "Back out \"Selective meta programming preparation for prim ops\"", "body": "Summary:\nThe diff D22618309 (https://github.com/pytorch/pytorch/commit/bacee6aa2ee1ea2e1c178af109a20b8033f25e85) breaks CYA ACP e2e tests. (https://www.internalfb.com/intern/ods/chart/?rapido=%7B%22queries%22%3A[%7B%22entity%22%3A%22regex(assistant%5C%5C.cya%5C%5C..*acp.*)%2C%5Cn%2C%20!regex(assistant%5C%5C.cya%5C%5C..*fair.*)%2C%22%2C%22key%22%3A%22overview.pct_passed_x_1000%2C%22%2C%22transform%22%3A%22formula(%2F%20%241%201000.0)%2C%22%2C%22reduce_keys%22%3Atrue%2C%22datatypes%22%3A[%22raw%22]%2C%22reduce%22%3A%22%22%2C%22id%22%3A%22ds1%22%2C%22source%22%3A%22ods%22%2C%22active%22%3Atrue%7D]%2C%22period%22%3A%7B%22minutes_back%22%3A720%2C%22time_type%22%3A%22dynamic%22%7D%7D&view=%7B%22type%22%3A%22line_chart_client%22%2C%22params%22%3A%7B%22title%22%3A%22Pass%20Rates%20of%20All%20Continuous%20Runs%20in%20PROD%22%2C%22haspoints%22%3Afalse%2C%22state%22%3A%22published%22%2C%22title_use_v2%22%3Atrue%2C%22tooltip_outside%22%3Atrue%2C%22series_names_preg_replace_list%22%3A[%7B%22series_name_preg_replace_list_group%22%3Anull%2C%22pattern%22%3A%22%2Fassistant%5C%5C.cya%5C%5C.(%5C%5Cw%2B)%5C%5C.([%5E%3A]%2B)%3A%3A.*%2F%22%2C%22replacement%22%3A%22%241%2F%242%22%7D]%2C%22sort_by_series_name%22%3A%22ASC%22%2C%22use_y_axis_hints_as_limits%22%3Atrue%7D%7D&version=2)\n\nSo I back out the diff.\n\nTest Plan:\n```\ncya test -n aloha.acp.arv2.prod --tp ~/tmp/cyaTests/assistant/cya/aloha_acp/whatsapp_call_who_ondevice_oacr.yaml --device_no_new_conn --retries 0\nInstalling: finished in 13.4 sec\nMore details at https://www.internalfb.com/intern/buck/build/c48882e8-1032-43ca-ba8f-8\nRunning \"aloha.acp.arv2.prod (acp)\" [1 tests] with endpoint \"https://prod.facebookvirtualassistant.com\"\n.\n  %100.0 tests passed:  1/1\n  Avg turn duration:    12.6s\n  P99 turn duration:    24.4s\n  CTP report:  https://our.intern.facebook.com/intern/testinfra/testrun/2814749804232321\n\n[jaeholee@32384.od ~/fbsource (7934576f)]$\n```\n\nDifferential Revision: D23464555\n\nfbshipit-source-id: b2c712a512a207c4813585f4ee57fdb5607317c6", "pr_number": null, "files_changed": ["aten/src/ATen/core/op_registration/op_whitelist.h"], "labels": []}, "c259146477": {"title": "add missing NEON {vld1,vst1}_*_x2 intrinsics (#43683)", "body": "Summary:\nWorkaround for issue https://github.com/pytorch/pytorch/issues/43265.\nAdd the missing intrinsics until gcc-7 gets the missing patches backported.\n\nFixes https://github.com/pytorch/pytorch/issues/43265.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43683\n\nReviewed By: albanD\n\nDifferential Revision: D23467867\n\nPulled By: malfet\n\nfbshipit-source-id: 7c138dd3de3c45852a60f2cfe8b4d7f7cf76bc7e", "pr_number": "43683", "files_changed": ["CMakeLists.txt", "aten/src/ATen/cpu/vec256/intrinsics.h", "aten/src/ATen/cpu/vec256/missing_vld1_neon.h"], "labels": ["open source", "triaged"]}, "1dd658f28f": {"title": "[Codemod][GleanFbcode] Remove dead includes in caffe2/test (#43953)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43953\n\nReviewed By: malfet\n\nDifferential Revision: D23445556\n\nfbshipit-source-id: 89cd6833aa06f35c5d3c99d698abb08cd61ae4ab", "pr_number": "43953", "files_changed": ["test/cpp/api/operations.cpp", "test/cpp/api/transformer.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/test_gpu.cpp", "test/cpp/rpc/test_e2e_process_group.cpp", "test/cpp/rpc/test_e2e_tensorpipe.cpp", "test/cpp/tensorexpr/test_boundsinference.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_train.cpp"], "labels": ["fb-exported", "oncall: jit"]}, "7000c2efb5": {"title": "[2/2][PyTorch][Mobile] Added mobile module metadata logging (#43853)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43853\n\nAdd QPL logging for mobile module's metadata\nghstack-source-id: 111113492\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\n- CI\n\n- Load the model trained by `mobile_model_util.py`\n\n- Local QPL logger standard output.\n{F319012106}\n\nReviewed By: xcheng16\n\nDifferential Revision: D23417304\n\nfbshipit-source-id: 7bc834f39e616be1eccfae698b3bccdf2f7146e5", "pr_number": "43853", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/observer.h"], "labels": ["oncall: jit"]}, "4bb5d33076": {"title": "is_numpy_scalar should also consider bool and complex types (#43644)", "body": "Summary:\nBefore this PR,\n\n```python\nimport torch\nimport numpy as np\n\na = torch.tensor([1, 2], dtype=torch.bool)\nc = np.array([1, 2], dtype=np.bool)\nprint(a[0] == c[0])\n\na = torch.tensor([1, 2], dtype=torch.complex64)\nc = np.array([1, 2], dtype=np.complex64)\nprint(a[0] == c[0])\n\n # This case is still broken\na = torch.tensor([1 + 1j, 2 + 2j], dtype=torch.complex64)\nc = np.array([1 + 1j, 2 + 2j], dtype=np.complex64)\nprint(a[0] == c[0])\n```\n\noutputs\n\n```\nFalse\nFalse\nFalse\n```\n\nAfter this PR, it outputs:\n\n```\ntensor(True)\n/home/user/src/pytorch/torch/tensor.py:25: ComplexWarning: Casting complex values to real discards the imaginary part return f(*args, **kwargs)\ntensor(True)\ntensor(False)\n```\n\nRelated issue: https://github.com/pytorch/pytorch/issues/43579\n\ncc anjali411 mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43644\n\nReviewed By: ailzhang\n\nDifferential Revision: D23425569\n\nPulled By: anjali411\n\nfbshipit-source-id: a868209376b30cea601295e54015c47803923054", "pr_number": "43644", "files_changed": ["test/test_torch.py", "torch/csrc/utils/tensor_numpy.cpp"], "labels": ["module: complex", "module: numpy", "open source", "triaged"]}, "95f912ab13": {"title": "Use NewCriterionTest in test_cpp_api_parity.py. (#43954)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43954\n\nCriterionTest is basically dead -- see https://github.com/pytorch/pytorch/pull/43769 and https://github.com/pytorch/pytorch/pull/43776.\n\nThe only exception is the cpp parity test, but the difference there doesn't actually have any effect -- the get_target has unpack=True, but all examples don't require unpacking (I checked).\n\nAs a pre-requisite for merging these tests, have the cpp parity test start using the NewCriterionTest.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23452144\n\nPulled By: gchanan\n\nfbshipit-source-id: 5dca1eb0878b882c93431d3b0e880b5bb1764522", "pr_number": "43954", "files_changed": ["test/test_cpp_api_parity.py"], "labels": []}, "c61a16b237": {"title": "Kill dead code in common_nn as part of merging Criterion and NewCriterionTests. (#43956)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43956\n\nSee https://github.com/pytorch/pytorch/pull/43769 and https://github.com/pytorch/pytorch/pull/43776 for proof this code is dead.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23452217\n\nPulled By: gchanan\n\nfbshipit-source-id: 6850aab2daaa1c321a6b7714f6f113f364f41973", "pr_number": "43956", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "6512032699": {"title": "[Static Runtime] Add OSS build for static runtime benchmarks (#43881)", "body": "Summary:\nAdds CMake option.  Build with:\n\n```\nBUILD_STATIC_RUNTIME_BENCHMARK=ON python setup.py install\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43881\n\nReviewed By: hlu1\n\nDifferential Revision: D23430708\n\nPulled By: bwasti\n\nfbshipit-source-id: a39bf54e8d4d044a4a3e4273a5b9a887daa033ec", "pr_number": "43881", "files_changed": ["CMakeLists.txt", "benchmarks/static_runtime/CMakeLists.txt", "caffe2/CMakeLists.txt", "cmake/Summary.cmake"], "labels": []}, "8722952dbd": {"title": "Add benchmark for channel_shuffle operator (#43509)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43509\n\nTest Plan: Imported from OSS\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23299972\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 6189d209859da5a41067eb9e8317e3bf7a0fc754", "pr_number": "43509", "files_changed": ["benchmarks/operator_benchmark/benchmark_all_other_test.py", "benchmarks/operator_benchmark/pt/channel_shuffle_test.py"], "labels": []}, "cd58114c6c": {"title": "Adjust level of verbosity of debug dumps in graph executor T74227880 (#43682)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43682\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23397980\n\nPulled By: Lilyjjo\n\nfbshipit-source-id: b0114efbd63b2a29eb14086b0a8963880023c2a8", "pr_number": "43682", "files_changed": ["torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["oncall: jit"]}, "5807bb92d3": {"title": "TensorIteratorConfig: Check memory overlap by default (#43422)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43422\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D23298653\n\nPulled By: zou3519\n\nfbshipit-source-id: a7b66a8a828f4b35e31e8be0c07e7fe9339181f2", "pr_number": "43422", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/DistributionTemplates.h", "aten/src/ATen/native/FunctionOfAMatrixUtils.cpp", "aten/src/ATen/native/GatedLinearUnit.cpp", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnfoldBackward.h", "aten/src/ATen/native/cpu/LerpKernel.cpp", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/Shape.cu", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "c88ac25679": {"title": "Check for internal memory overlap in some indexing-type functions (#43423)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43423\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23298652\n\nPulled By: zou3519\n\nfbshipit-source-id: c13c59aec0c6967ef0d6365d782c1f4c98c04227", "pr_number": "43423", "files_changed": ["aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "test/test_distributions.py", "test/test_torch.py", "torch/distributions/geometric.py", "torch/distributions/multinomial.py"], "labels": ["open source", "topic: deprecation", "triaged"]}, "14ebb2c67c": {"title": "Allow no-bias MKLDNN Linear call (#43703)", "body": "Summary:\nMKLDNN linear incorrectly assumes that bias is defined and will fail for no-bias calls.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43703\n\nReviewed By: glaringlee\n\nDifferential Revision: D23373182\n\nPulled By: bwasti\n\nfbshipit-source-id: 1e817674838a07d237c02eebe235c386cf5b191e", "pr_number": "43703", "files_changed": ["aten/src/ATen/native/mkldnn/Linear.cpp"], "labels": []}, "b6b5ebc345": {"title": "Add `torch.vdot` (#43004)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42747\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43004\n\nReviewed By: mruberry\n\nDifferential Revision: D23318935\n\nPulled By: anjali411\n\nfbshipit-source-id: 12d4824b7cb42bb9ca703172c54ec5c663d9e325", "pr_number": "43004", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/BlasKernel.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["module: complex", "open source", "triaged"]}, "6f5282adc8": {"title": "add quantization debug util to pretty print FX graphs (#43910)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43910\n\nAdds a debug function to get a representation of all nodes in the\ngraph, such as\n\n```\nname          op      target         args               kwargs\nx             plchdr  x              ()                 {}\nlinear_weight gt_prm  linear.weight  ()                 {}\nadd_1         cl_fun  <bi_fun add>   (x, linear_weight) {}\nlinear_1      cl_mod  linear         (add_1,)           {}\nrelu_1        cl_meth relu           (linear_1,)        {}\nsum_1         cl_fun  <bi_meth sum>  (relu_1,)          {'dim': -1}\ntopk_1        cl_fun  <bi_meth topk> (sum_1, 3)         {}\n```\n\nusing only Python STL. This is useful for printing internal state of\ngraphs when working on FX code.\n\nHas some on-by-default logic to shorten things so that node reprs for\ntoy models and unit tests fit into 80 chars.\n\nFlexible on function name and location, I care more that this is\naccessible from both inside PT as well as from debug scripts which\nare not checked in.\n\nTest Plan:\nsee\nhttps://gist.github.com/vkuzo/ed0a50e5d6dc7442668b03bb417bd603 for\nexample usage\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23435029\n\nfbshipit-source-id: 1a2df797156a19cedd705e9e700ba7098b5a1376", "pr_number": "43910", "files_changed": ["torch/quantization/fx/utils.py", "torch/quantization/quantize_fx.py"], "labels": ["fx"]}, "77ef77e5fa": {"title": "fx quant: rename matches -> is_match (#43914)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43914\n\nRenames `matches` function to `is_match`, since there is also\na list named `matches` we are passing around in `Quantizer`,\nand would be good to decrease name conflicts.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFxOps\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23435601\n\nfbshipit-source-id: 394af11e0120cfb07dedc79d5219247330d4dfd6", "pr_number": "43914", "files_changed": ["torch/quantization/fx/fuse.py", "torch/quantization/fx/pattern_utils.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "df8da5cb5a": {"title": "fx quant: make load_arg function more clear (#43923)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43923\n\nReadability improvements to `Quantizer.convert.load_arg`, makes\nthings easier to read.\n1. add docblock\n2. `arg` -> `arg_or_args`, to match what's actually happening\n3. `loaded_arg` -> `loaded_args`, to match what's actually happening\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFxOps\npython test/test_quantization.py TestQuantizeFx\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23438745\n\nfbshipit-source-id: f886b324d2e2e33458b72381499e37dccfc3bd30", "pr_number": "43923", "files_changed": ["torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "a6789074fc": {"title": "Implement ChannelShuffle op with XNNPACK (#43602)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43602\n\nTest Plan: Imported from OSS\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23334952\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 858ef3db599b1c521ba3a1855c9a3c35fe3b02b0", "pr_number": "43602", "files_changed": ["aten/src/ATen/native/ChanelShuffle.cpp", "aten/src/ATen/native/xnnpack/ChannelShuffle.cpp", "aten/src/ATen/native/xnnpack/Engine.h"], "labels": []}, "73f009a2aa": {"title": "refactor manual function definitions (#43711)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43711\n\nthis makes them available in forward if needed\n\nNo change to the file content, just a copy-paste.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D23454146\n\nPulled By: albanD\n\nfbshipit-source-id: 6269a4aaf02ed53870fadf8b769ac960e49af195", "pr_number": "43711", "files_changed": [".github/workflows/lint.yml", "caffe2/CMakeLists.txt", "tools/autograd/templates/Functions.cpp", "tools/autograd/templates/VariableType.cpp", "tools/build_variables.bzl", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h"], "labels": []}, "276158fd05": {"title": ".circleci: Remove un-needed steps from binary builds (#43974)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43974\n\nWe already install devtoolset7 in our docker images for binary builds\nand tclsh shouldn't be needed since we're not relying on unbuffer\nanymore\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D23462531\n\nPulled By: seemethere\n\nfbshipit-source-id: 83cbb8b0782054f0b543dab8d11fa6ac57685272", "pr_number": "43974", "files_changed": [".circleci/config.yml", ".circleci/scripts/binary_linux_build.sh", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml"], "labels": ["module: ci"]}, "544a56ef69": {"title": "[JIT] Always map node output in vmap (#43988)", "body": "Summary:\nPreviously when merging a node without a subgraph, we would merge the node's outputs to the corresponding subgraph values, but when merging a node with a subgraph the node's outputs would be absent in the value mapping. This PR makes it so they are included.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43988\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23462116\n\nPulled By: eellison\n\nfbshipit-source-id: 232c081261e9ae040df0accca34b1b96a5a5af57", "pr_number": "43988", "files_changed": ["test/cpp/jit/test_subgraph_utils.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.h"], "labels": ["oncall: jit"]}, "b167402e2e": {"title": "[redo] Fix SyncBatchNorm forward pass for non-default process group (#43861)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43861\n\nThis is a redo of https://github.com/pytorch/pytorch/pull/38874, and\nfixing my original bug from\nhttps://github.com/pytorch/pytorch/pull/38246.\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23418816\n\nfbshipit-source-id: 2a3a3d67fc2d03bb0bf30a87cce4e805ac8839fb", "pr_number": "43861", "files_changed": ["torch/nn/modules/_functions.py"], "labels": []}, "4716284904": {"title": "Update persons_of_interest.rst (#44031)", "body": "Summary:\nAdding Geeta to the POI for TorchServe\n\ncc chauhang\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44031\n\nReviewed By: jspisak\n\nDifferential Revision: D23476439\n\nPulled By: soumith\n\nfbshipit-source-id: 6936d46c201e1437143d85e1dce24da355857628", "pr_number": "44031", "files_changed": ["docs/source/community/persons_of_interest.rst"], "labels": []}, "f6f9d22228": {"title": "[ONNX] Export KLDivLoss (#41858)", "body": "Summary:\nEnable export for KLDivLoss\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41858\n\nReviewed By: mrshenli\n\nDifferential Revision: D22918004\n\nPulled By: bzinodev\n\nfbshipit-source-id: e3debf77a4cf0eae0df6ed5a72ee91c43e482b62", "pr_number": "41858", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["module: onnx", "open source", "triaged"]}, "297c938729": {"title": "Add _foreach_add(TensorList tl1, TensorList tl2) and _foreach_add_(TensorList tl1, TensorList tl2) APIs (#42533)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42533\n\n[First PR: Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar)](https://github.com/pytorch/pytorch/pull/41554).\n\n**Motivation**\n[GitHub issue](https://github.com/pytorch/pytorch/issues/38655)\nCurrent PyTorch optimizer implementations are not efficient in cases when we work with a lot of small feature tensors. Starting a lot of kernels slows down the whole process. We need to reduce the number of kernels that we start.\nAs an example, we should be looking at [NVIDIAs Apex](https://github.com/NVIDIA/apex).\nIn order to track progress, we will pick PyTorchs DCGAN model with Adam optimizer and once the optimizer is reimplemented with tensor lists, benchmark the model performance against original model version, Apexs version with original Adam optimizer and it\u2019s FusedAdam optimizer.\n\n**Current API restrictions**\n- List can't be empty (will fixed in upcoming PRs).\n- All tensors in the list must have the same dtype, device and size.\n\n**Broadcasting**\nAt this point we don't support broadcasting.\n\n**What is 'Fast' and 'Slow' route**\nIn particular cases, we cant process an op with a fast list CUDA kernel. Still, we can do with a regular for-loop where the op will be applied to each tensor individually through the dispatch mechanisms. There are a few checks that decide whether the op will be performed via a 'fast' or 'slow' path.\nTo go the fast route,\n- All tensors must have strided layout\n- All tensors must be dense and not have overlapping memory\n- The resulting tensor type must be the same.\n\n----------------\n**In this PR**\n- Adding a `_foreach_add(TensorList tl1, TensorList tl2)` API\n- Adding a `_foreach_add_(TensorList tl1, TensorList tl2)` API\n\n**Tests**\nTested via unit tests\n\n**TODO**\n1. Properly handle empty lists\n\n**Plan for the next PRs**\n1. APIs\n- Binary Ops for list with Scalar\n- Binary Ops for list with list\n- Unary Ops for list\n- Pointwise Ops\n\n2. Complete tasks from TODO\n3. Rewrite PyTorch optimizers to use for-each operators for performance gains.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23331894\n\nPulled By: izdeby\n\nfbshipit-source-id: 876dd1bc82750f609b9e3ba23c8cad94d8d6041c", "pr_number": "42533", "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/cuda/ForeachFunctors.cuh", "aten/src/ATen/native/cuda/ForeachTensorAddList.cu", "aten/src/ATen/native/cuda/ForeachTensorAddScalar.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_foreach.py", "tools/codegen/model.py"], "labels": []}, "7a77d1c5c2": {"title": "[FX] Only copy over forward() from exec (#44006)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44006\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23466542\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 12a1839ddc65333e3e3d511eeb53206f06546a87", "pr_number": "44006", "files_changed": ["test/test_fx.py", "torch/fx/graph_module.py"], "labels": ["fx"]}, "f15e27265f": {"title": "[torch.fx] Add support for custom op (#43248)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43248\n\nWe add the support of __torch_function__ override for C++ custom op. The logic is the same as the other components, like torch.nn.Module.\nRefactored some code a little bit to make it reusable.\n\nTest Plan: buck test //caffe2/test:fx -- test_torch_custom_ops\n\nReviewed By: bradleyhd\n\nDifferential Revision: D23203204\n\nfbshipit-source-id: c462a86e407e46c777171da32d7a40860acf061e", "pr_number": "43248", "files_changed": ["test/test_fx.py", "torch/csrc/jit/python/init.cpp", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["fb-exported"]}, "f9efcb646b": {"title": "fx quant: clarify state in Quantizer object (#43927)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43927\n\nAdds uninitialized placeholders for various state\nused throughout the Quantizer object, with documentation\non what they are. No logic change.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFxOps\npython test/test_quantization.py TestQuantizeFx\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23439473\n\nfbshipit-source-id: d4ae83331cf20d81a7f974f88664ccddca063ffc", "pr_number": "43927", "files_changed": ["torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "129f406062": {"title": "Make torch.conj() a composite function and return self for real tensors (#43270)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43270\n\n`torch.conj` is a very commonly used operator for complex tensors, but it's mathematically a no op for real tensors. Switching to tensorflow gradients for complex tensors (as discussed in #41857) would involve adding `torch.conj()` to the backward definitions for a lot of operators. In order to preserve autograd performance for real tensors and maintain numpy compatibility for `torch.conj`, this PR updates `torch.conj()` which behaves the same for complex tensors but performs a view/returns `self` tensor for tensors of non-complex dtypes. The documentation states that the returned tensor for a real input shouldn't be mutated. We could perhaps return an immutable tensor for this case in future when that functionality is available (zdevito ezyang ).\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23460493\n\nPulled By: anjali411\n\nfbshipit-source-id: 3b3bf0af55423b77ff2d0e29f5d2c160291ae3d9", "pr_number": "43270", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_torch_docs.py"], "labels": ["module: complex", "topic: bc-breaking"]}, "2f044d4ee5": {"title": "Fix CI build (#44068)", "body": "Summary:\nSome of our machines have only 1 device.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44068\n\nReviewed By: wanchaol\n\nDifferential Revision: D23485730\n\nPulled By: izdeby\n\nfbshipit-source-id: df6bc0aba18feefc50c56a8f376103352fa2a2ea", "pr_number": "44068", "files_changed": ["test/test_foreach.py"], "labels": []}, "041573c8cd": {"title": "Add Cost Inference for AdaGrad and RowWiseSparseAdagrad", "body": "Summary: Add cost inference for AdaGrad and RowWiseSparseAdagrad\n\nTest Plan:\nRan `buck test caffe2/caffe2/python/operator_test:adagrad_test`\nResult: https://our.intern.facebook.com/intern/testinfra/testrun/5629499567799494\n\nReviewed By: bwasti\n\nDifferential Revision: D23442607\n\nfbshipit-source-id: 67800fb82475696512ad19a43067774247f8b230", "pr_number": null, "files_changed": ["caffe2/sgd/adagrad_op.cc"], "labels": []}, "33d51a9b32": {"title": "Respect canFuseOn{CPU,GPU} in TE fuser (#43967)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43967\n\nTest Plan: Imported from OSS\n\nReviewed By: asuhan\n\nDifferential Revision: D23469048\n\nPulled By: bertmaher\n\nfbshipit-source-id: 1005a7ae08974059ff9d467492caa3a388070eeb", "pr_number": "43967", "files_changed": ["benchmarks/fastrnns/bench.py", "benchmarks/fastrnns/fuser.py", "benchmarks/fastrnns/test_bench.py", "benchmarks/tensorexpr/__main__.py", "benchmarks/tensorexpr/benchmark.py", "test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/jit/test_profiler.py", "test/test_jit_fuser_te.py", "test/test_tensorexpr.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/jit/_fuser.py"], "labels": ["oncall: jit"]}, "78994d165f": {"title": "min_max kernel: add CUDA (#42868)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42868\n\nAdds a CUDA kernel for the _min_max function.\n\nNote: this is a re-submit of https://github.com/pytorch/pytorch/pull/41805,\nwas faster to resubmit than to ressurect that one.  Thanks to durumu\nfor writing the original implementation!\n\nFuture PRs will add index support, docs, and hook this up to observers.\n\nTest Plan:\n```\npython test/test_torch.py TestTorchDeviceTypeCUDA.test_minmax_cuda_float32\n```\n\nBasic benchmarking shows a 50% reduction in time to calculate min + max:\nhttps://gist.github.com/vkuzo/b7dd91196345ad8bce77f2e700f10cf9\n\nTODO\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23057766\n\nfbshipit-source-id: 70644d2471cf5dae0a69343fba614fb486bb0891", "pr_number": "42868", "files_changed": ["aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": []}, "834279f4ab": {"title": "_min_max_val.dim: CPU implementation (#42894)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42894\n\nContinuing the min_max kernel implementation, this PR adds the\nCPU path when a dim is specified.  Next PR will replicate for CUDA.\n\nNote: after a discussion with ngimel, we are taking the fast path\nof calculating the values only and not the indices, since that is what\nis needed for quantization, and calculating indices would require support\nfor reductions on 4 outputs which is additional work.  So, the API\ndoesn't fully match `min.dim` and `max.dim`.\n\nFlexible on the name, let me know if something else is better.\n\nTest Plan:\ncorrectness:\n```\npython test/test_torch.py TestTorchDeviceTypeCPU.test_minmax_cpu_float32\n```\n\nperformance: seeing a 49% speedup on a min+max tensor with similar shapes\nto what we care about for quantization observers (bench:\nhttps://gist.github.com/vkuzo/b3f24d67060e916128a51777f9b89326). For\nother shapes (more dims, different dim sizes, etc), I've noticed a\nspeedup as low as 20%, but we don't have a good use case to optimize\nthat so perhaps we can save that for a future PR.\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23086798\n\nfbshipit-source-id: b24ce827d179191c30eccf31ab0b2b76139b0ad5", "pr_number": "42894", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": []}, "486a9fdab2": {"title": "_min_max.dim: CUDA implementation (#42943)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42943\n\nAdds a CUDA kernel for _min_max_val.dim\n\nTest Plan:\ncorrectness:\n```\npython test/test_torch.py TestTorchDeviceTypeCUDA.test_minmax_cuda_float32\n```\n\nperformance: ~50% savings on a tensor representative of quantization workloads: https://gist.github.com/vkuzo/3e16c645e07a79dd66bcd50629ff5db0\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23086797\n\nfbshipit-source-id: 04a2d310f64a388d48ab8131538dbd287900ca4a", "pr_number": "42943", "files_changed": ["aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": []}, "6a6552576d": {"title": "rename _min_max to _aminmax (#44001)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44001\n\nThis is to align with the naming in numpy and in\nhttps://github.com/pytorch/pytorch/pull/43092\n\nTest Plan:\n```\npython test/test_torch.py TestTorchDeviceTypeCPU.test_aminmax_cpu_float32\npython test/test_torch.py TestTorchDeviceTypeCUDA.test_aminmax_cuda_float32\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23465298\n\nfbshipit-source-id: b599035507156cefa53942db05f93242a21c8d06", "pr_number": "44001", "files_changed": ["aten/src/ATen/native/ReduceAllOps.cpp", "aten/src/ATen/native/ReduceAllOps.h", "aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": []}, "b2aaf212aa": {"title": "[TensorExpr] Add option to enforce TensorExprKernel fallbacks. (#43972)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43972\n\nIt is useful when debugging a bug to disable NNC backend to see whether\nthe bug is there or in the fuser logic.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23455624\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: f7c0452a29b860afc806e2d58acf35aa89afc060", "pr_number": "43972", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["oncall: jit"]}, "b2a9c3baa9": {"title": "[TVM] Support fp16 weights in c2_frontend (#44070)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44070\n\nReviewed By: yinghai\n\nDifferential Revision: D23444253\n\nfbshipit-source-id: 0bfa98172dfae835eba5ca7cbe30383ba964c2a6", "pr_number": "44070", "files_changed": ["caffe2/opt/tvm_transformer.cc"], "labels": ["fb-exported"]}, "37658b144b": {"title": "Remove useless py2 compatibility import __future__, part 1 (#43808)", "body": "Summary:\nTo avoid conflicts, this PR does not remove all imports. More are coming in further PRs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43808\n\nReviewed By: wanchaol\n\nDifferential Revision: D23436675\n\nPulled By: ailzhang\n\nfbshipit-source-id: ccc21a1955c244f0804277e9e47e54bfd23455cd", "pr_number": "43808", "files_changed": ["torch/nn/qat/__init__.py", "torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py", "torch/nn/quantized/__init__.py", "torch/nn/quantized/dynamic/__init__.py", "torch/nn/quantized/dynamic/modules/embeddingbag.py", "torch/nn/quantized/dynamic/modules/linear.py", "torch/nn/quantized/dynamic/modules/rnn.py", "torch/nn/quantized/functional.py", "torch/nn/quantized/modules/activation.py", "torch/nn/quantized/modules/batchnorm.py", "torch/nn/quantized/modules/conv.py", "torch/nn/quantized/modules/linear.py", "torch/nn/quantized/modules/normalization.py"], "labels": ["open source"]}, "a76a56d761": {"title": "Add \"torch/testing/_internal/data/*.pt\" to .gitignore (#43941)", "body": "Summary:\nI usually get this extra \"legacy_conv2d.pt\" file in my git \"changed files\". I found that this is from tests with `download_file`\nhttps://github.com/pytorch/pytorch/blob/42c895de4d3a8ea539c26473b761e49ff7309996/test/test_nn.py#L410-L426\n\nand its definition (see `data_dir` for download output location)\nhttps://github.com/pytorch/pytorch/blob/f17d7a5556940156b81b595bb39783192ba1e16f/torch/testing/_internal/common_utils.py#L1338-L1357\n\nI assume a file \"generated\" by test should not be tracked in VCS? Also, if the file is updated on the server, users may still use the old version of it if they have already downloaded that before.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43941\n\nReviewed By: anjali411\n\nDifferential Revision: D23451264\n\nPulled By: ezyang\n\nfbshipit-source-id: 7fcdfb24685a7e483914cc46b3b024df798bf7f7", "pr_number": "43941", "files_changed": [".gitignore"], "labels": ["open source"]}, "f5ba489f93": {"title": "Move dependent configs to CUDA-10.2 (#44057)", "body": "Summary:\nMove `multigpu`, `noavx` and `slow` test configs to CUDA-10.2, but keep them a master only tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44057\n\nReviewed By: walterddr, seemethere\n\nDifferential Revision: D23482732\n\nPulled By: malfet\n\nfbshipit-source-id: a6b050701cbc1d8f176ebb302f7f5076a78f1f58", "pr_number": "44057", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml"], "labels": ["module: ci", "triaged"]}, "bc45c47aa3": {"title": "Expand the coverage of test_addmm and test_addmm_sizes (#43831)", "body": "Summary:\n- This test is very fast and very important, so it makes no sense in marking it as slowTest\n- This test is should also run on CUDA\n- This test should check alpha and beta support\n- This test should check `out=` support\n- manual computation should use list instead of index_put because list is much faster\n- precision for TF32 needs to be fixed. Will do it in future PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43831\n\nReviewed By: ailzhang\n\nDifferential Revision: D23435032\n\nPulled By: ngimel\n\nfbshipit-source-id: d1b8350addf1e2fe180fdf3df243f38d95aa3f5a", "pr_number": "43831", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu", "test/test_torch.py"], "labels": ["open source"]}, "32e0cedc53": {"title": "[ONNX] Move tests to test_pytorch_onnx_onnxruntime (#42684)", "body": "Summary:\nMove tests to test_pytorch_onnx_onnxruntime from test_utility_fun\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42684\n\nReviewed By: smessmer\n\nDifferential Revision: D23480360\n\nPulled By: bzinodev\n\nfbshipit-source-id: 8876ba0a0c3e1d7104511de7a5cca5262b32f574", "pr_number": "42684", "files_changed": ["test/onnx/test_models.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_utility_funs.py"], "labels": ["open source", "triaged"]}, "ae7699829c": {"title": "Remove THC max and min, which are longer used (#43903)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43903\n\nReviewed By: smessmer\n\nDifferential Revision: D23493225\n\nPulled By: ezyang\n\nfbshipit-source-id: bc89d8221f3351da0ef3cff468ffe6a91dae96a6", "pr_number": "43903", "files_changed": ["aten/src/THC/generic/THCTensorMathPointwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.h"], "labels": ["open source", "triaged"]}, "3da82aee03": {"title": "[JIT] Remove profile nodes before BatchMM. (#43961)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43961\n\nCurrently we're removing prim::profile nodes and embed the type info\ndirectly in the IR right before the fuser, because it is difficult to\nfuse in a presence of prim::profile nodes. It turns out that BatchMM has\na similar problem: it doesn't work when there are prim::profile nodes in\nthe graph. These two passes run next to each other, so we could simply\nremove prim::profile nodes slightly earlier: before the BatchMM pass.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23453266\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 92cb50863962109b3c0e0112e56c1f2cb7467ff1", "pr_number": "43961", "files_changed": ["torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.h", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["oncall: jit"]}, "40fec4e739": {"title": "[TensorExpr] Fuser: do not fuse ops with 0-dim tensors. (#44073)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44073\n\nWe don't have a proper support on NNC and JIT IR->NNC lowering side for it yet.\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23487905\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: da0da7478fc8ce7b455176c95d8fd610c94352c1", "pr_number": "44073", "files_changed": ["test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/cpp/tensorexpr/tests.h", "test/test_jit_fuser_te.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["oncall: jit"]}, "ab7606702c": {"title": "Rectified a few grammatical errors in documentation (#43695)", "body": "Summary:\nRectified a few grammatical errors in documentation of pytorch.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43695\n\nReviewed By: anjali411\n\nDifferential Revision: D23451600\n\nPulled By: ezyang\n\nfbshipit-source-id: bc7b34c240fde1b31cac811080befa2ff2989395", "pr_number": "43695", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source"]}, "665feda15b": {"title": "Adds opinfo-based autograd tests and (un)supported dtype tests (#43451)", "body": "Summary:\nThis PR adds a new test suite, test_ops.py, designed for generic tests across all operators with OpInfos. It currently has two kinds of tests:\n\n- it validates that the OpInfo has the correct supported dtypes by verifying that unsupported dtypes throw an error and supported dtypes do not\n- it runs grad and gradgrad checks on each op and its variants (method and inplace) that has an OpInfo\n\nThis is a significant expansion and simplification of the current autogenerated autograd tests, which spend considerable processing their inputs. As an alternative, this PR extends OpInfos with \"SampleInputs\" that are much easier to use. These sample inputs are analogous to the existing tuples in`method_tests()`.\n\nFuture PRs will extend OpInfo-based testing to other uses of `method_tests()`, like test_jit.py, to ensure that new operator tests can be implemented entirely using an OpInfo.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43451\n\nReviewed By: albanD\n\nDifferential Revision: D23481723\n\nPulled By: mruberry\n\nfbshipit-source-id: 0c2cdeacc1fdaaf8c69bcd060d623fa3db3d6459", "pr_number": "43451", "files_changed": ["test/run_test.py", "test/test_ops.py", "test/test_unary_ufuncs.py", "torch/autograd/gradcheck.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": []}, "5f89aa36cf": {"title": "Actually run backward criterion tests. (#44030)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44030\n\nThis looks to have been a mistake from https://github.com/pytorch/pytorch/pull/9287.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23476274\n\nPulled By: gchanan\n\nfbshipit-source-id: 81ed9d0c9a40d49153fc97cd69fdcd469bec0c73", "pr_number": "44030", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "68a1fbe308": {"title": "Allow criterion backwards test on modules requiring extra args (i.e. CTCLoss). (#44050)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44050\n\nWe don't actually turn on the CTCLoss tests since they fail, but this allows you to toggle check_forward_only and for the code to actually run.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23481091\n\nPulled By: gchanan\n\nfbshipit-source-id: f2a3b0a2dee27341933c5d25f1e37a878b04b9f6", "pr_number": "44050", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "b6d5973e13": {"title": "Delete THCStream.cpp (#43733)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43733\n\nReviewed By: malfet\n\nDifferential Revision: D23405121\n\nPulled By: ezyang\n\nfbshipit-source-id: 95fa80b5dcb11abaf4d2507af15646a98029c80d", "pr_number": "43733", "files_changed": ["aten/src/THC/THCStream.cpp"], "labels": ["open source"]}, "24ca6aab02": {"title": "Improves type-checking guards. (#43339)", "body": "Summary:\nPR https://github.com/pytorch/pytorch/issues/38157 fixed type checking for mypy by including `if False` guards on some type-checker-only imports. However other typecheckers - [like pyright](https://github.com/microsoft/pylance-release/issues/262#issuecomment-677758245) - will respect this logic and ignore the imports. Using [`if TYPE_CHECKING`](https://docs.python.org/3/library/typing.html#typing.TYPE_CHECKING) instead means both mypy and pyright will work correctly.\n\n[For background, an example of where the current code fails](https://github.com/microsoft/pylance-release/issues/262) is if you make a file `tmp.py` with the contents\n```python\nimport torch\ntorch.ones((1,))\n```\nThen [`pyright tmp.py --lib`](https://github.com/microsoft/pyright#command-line) will fail with a `\"ones\" is not a known member of module` error. This is because it can't find the `_VariableFunctions.pyi` stub file, as pyright respects the `if False` logic. After adding the `TYPE_CHECKING` guard, all works correctly.\n\nCredit to erictraut for suggesting the fix.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43339\n\nReviewed By: agolynski\n\nDifferential Revision: D23348142\n\nPulled By: ezyang\n\nfbshipit-source-id: c8a58122a7b0016845c311da39a1cc48748ba03f", "pr_number": "43339", "files_changed": ["torch/__init__.py"], "labels": ["open source", "triaged"]}, "15643de941": {"title": "With fixes, Back out \"Back out \"Selective meta programming preparation for prim ops\"\"", "body": "Summary: Original commit changeset: b2c712a512a2\n\nTest Plan: CI\n\nReviewed By: jiatongzhou\n\nDifferential Revision: D23477710\n\nfbshipit-source-id: 177ee56a82234376b7a5c3fc33441f8acfd59fea", "pr_number": null, "files_changed": ["aten/src/ATen/core/op_registration/op_whitelist.h"], "labels": []}, "cae52b4036": {"title": "Merge CriterionTest into NewCriterionTest. (#44055)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44055\n\nThere is no functional change here.  Another patch will rename NewCriterionTest to CriterionTest.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23482572\n\nPulled By: gchanan\n\nfbshipit-source-id: de364579067e2cc9de7df6767491f8fa3a685de2", "pr_number": "44055", "files_changed": ["test/cpp_api_parity/utils.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "b6e2b1eac7": {"title": "BatchedFallback: stop emitting the entire schema in the fallback warning (#44051)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44051\n\nInstead, just emit the operator name. The entire schema is pretty wordy\nand doesn't add any additional information.\n\nTest Plan: - modified test: `pytest test/test_vmap.py -v`\n\nReviewed By: ezyang\n\nDifferential Revision: D23481184\n\nPulled By: zou3519\n\nfbshipit-source-id: 9fbda61fc63565507b04c8b87e0e326a2036effa", "pr_number": "44051", "files_changed": ["aten/src/ATen/BatchedFallback.cpp", "test/test_vmap.py"], "labels": []}, "768c2b0fb2": {"title": "Fix THPVariable_float_scalar (#43842)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43842\n\nReviewed By: ailzhang\n\nDifferential Revision: D23426892\n\nPulled By: ezyang\n\nfbshipit-source-id: 63318721fb3f4a57d417f9a87e57c74f6d4e6e18", "pr_number": "43842", "files_changed": ["tools/autograd/templates/python_variable_methods.cpp"], "labels": ["open source"]}, "98320061ad": {"title": "DDP Communication hook: (Patch) Fix the way we pass future result to buckets. (#43734)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43734\n\nFollowing the additional GH comments on the original PR https://github.com/pytorch/pytorch/pull/43307.\nghstack-source-id: 111327130\n\nTest Plan: Run `python test/distributed/test_c10d.py`\n\nReviewed By: smessmer\n\nDifferential Revision: D23380288\n\nfbshipit-source-id: 4b8889341c57b3701f0efa4edbe1d7bbc2a82ced", "pr_number": "43734", "files_changed": ["torch/csrc/distributed/c10d/reducer.cpp"], "labels": []}, "c10f30647f": {"title": "Fix CUDA debug nightly build failure (#44085)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43607.\nTested in https://github.com/pytorch/pytorch/pull/44007.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44085\n\nReviewed By: malfet\n\nDifferential Revision: D23493663\n\nPulled By: ezyang\n\nfbshipit-source-id: 4c01f3fc5a52814a23773a56b980c455851c2686", "pr_number": "44085", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["open source"]}, "7d95eb8633": {"title": "[fbgemm] manual submodule update (#44082)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44082\n\nAutomated submodule is running into some test failures and I am not sure how can I rebase that.\n\nautomated submodule update:\nhttps://github.com/pytorch/pytorch/pull/43817\n\nTest Plan: CI tests\n\nReviewed By: jianyuh\n\nDifferential Revision: D23489240\n\nfbshipit-source-id: a49b01786ebf0a59b719a0abf22398e1eafa90af", "pr_number": "44082", "files_changed": ["third_party/fbgemm"], "labels": ["fb-exported"]}, "5973b44d9e": {"title": "Rename NewCriterionTest to CriterionTest. (#44056)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44056\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23482573\n\nPulled By: gchanan\n\nfbshipit-source-id: dde0f1624330dc85f48e5a0b9d98fb55fdb72f68", "pr_number": "44056", "files_changed": ["test/cpp_api_parity/utils.py", "test/test_cpp_api_parity.py", "test/test_nn.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "c59e11bfbb": {"title": "Add soft error reporting to capture all the inference runtime failure. (#44078)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44078\n\nWhen PyTorch mobile inference failed and throw exception, if caller catch and not crash the app, we are not able to track all the inference failures.\n\nSo we are adding native soft error reporting to capture all the failures occurring during module loading and running including both crashing and on-crashing failures. Since c10::Error has good error messaging stack handling (D21202891 (https://github.com/pytorch/pytorch/commit/a058e938f944b6834b5d237735d715a79974b656)), we are utilizing it for the error handling and message print out.\nghstack-source-id: 111307080\n\nTest Plan:\nVerified that the soft error reporting is sent through module.cpp when operator is missing, make sure a logview mid is generated with stack trace: https://www.internalfb.com/intern/logview/details/facebook_android_softerrors/5dd347d1398c1a9a73c804b20f7c2179/?selected-logview-tab=latest.\n\nError message with context is logged below:\n\n```\nsoft_error.cpp\t\t[PyTorchMobileInference] : Error occured during model running entry point: Could not run 'aten::embedding' with arguments from the 'CPU' backend. 'aten::embedding' is only available for these backends: [BackendSelect, Named, Autograd, Autocast, Batched, VmapMode].\n\nBackendSelect: fallthrough registered at xplat/caffe2/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at xplat/caffe2/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nAutograd: fallthrough registered at xplat/caffe2/aten/src/ATen/core/VariableFallbackKernel.cpp:31 [backend fallback]\nAutocast: fallthrough registered at xplat/caffe2/aten/src/ATen/autocast_mode.cpp:253 [backend fallback]\nBatched: registered at xplat/caffe2/aten/src/ATen/BatchingRegistrations.cpp:317 [backend fallback]\nVmapMode: fallthrough registered at xplat/caffe2/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n\nException raised from reportError at xplat/caffe2/aten/src/ATen/core/dispatch/OperatorEntry.cpp:261 (m\n```\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23428636\n\nfbshipit-source-id: 82d5d9c054300dff18d144f264389402d0b55a8a", "pr_number": "44078", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/observer.h"], "labels": ["oncall: jit"]}, "f96b91332f": {"title": "[caffe2.proto] Add AOTConfig (#44020)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44020\n\nPull Request resolved: https://github.com/pytorch/glow/pull/4853\n\nAdd AOT config\n\nReviewed By: yinghai\n\nDifferential Revision: D23414435\n\nfbshipit-source-id: 3c48acf29889fcf63def37a48de382e675e0e1f3", "pr_number": "44020", "files_changed": ["caffe2/proto/caffe2.proto"], "labels": ["fb-exported"]}, "d11603de38": {"title": "[TensorExpr] Benchmarks: set number of profiling runs to 2 for PE. (#44112)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44112\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23500904\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: d0dd54752b7ea5ae11f33e865c96d2d61e98d573", "pr_number": "44112", "files_changed": ["benchmarks/fastrnns/fuser.py"], "labels": []}, "de672e874d": {"title": "[JIT] Improve error message for unsupported Optional types (#44054)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44054\n\n**Summary**\nThis commit improves the error message that is printed when an\n`Optional` type annotation with an unsupported contained type is\nencountered. At present, the `Optional` is printed as-is, and\n`Optional[T]` is syntatic sugar for `Union[T, None]`, so that is what\nshows up in the error message and can be confusing. This commit modifies\nthe error message so that it prints `T` instead of `Union[T, None]`.\n\n**Test Plan**\nContinuous integration.\n\nExample of old message:\n```\nAssertionError: Unsupported annotation typing.Union[typing.List, NoneType] could not be resolved.\n```\nExample of new message:\n```\nAssertionError: Unsupported annotation typing.Union[typing.List, NoneType] could not be resolved because typing.List could not be resolved.\n```\n\n**Fixes**\nThis commit fixes #42859.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23490365\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 2aa9233718e78cf1ba3501ae11f5c6f0089e29cd", "pr_number": "44054", "files_changed": ["torch/jit/annotations.py"], "labels": ["oncall: jit"]}, "91b0d1866a": {"title": "add tanh + quantize unit test (#44076)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44076\n\nadd fakelowp test for tanh + quantize\n\nTest Plan: net runner\n\nReviewed By: venkatacrc\n\nDifferential Revision: D23339662\n\nfbshipit-source-id: 96c2cea12b41bf3df24aa46e601e053dca8e9481", "pr_number": "44076", "files_changed": ["caffe2/contrib/fakelowp/quant_lut_fp16_fake_op.cc", "caffe2/contrib/fakelowp/quant_lut_fp16_fake_op.h", "caffe2/contrib/fakelowp/test/test_fusions.py", "caffe2/opt/fakefp16_transform.cc"], "labels": ["fb-exported"]}, "42f9897983": {"title": "Mark bucketize as not subject to autograd (#44102)", "body": "Summary:\nBucketize returns integers, currently this triggers an internal assert, so we apply the mechanism for this case (also used for argmax etc.).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44102\n\nReviewed By: zou3519\n\nDifferential Revision: D23500048\n\nPulled By: albanD\n\nfbshipit-source-id: fdd869cd1feead6616b532b3e188bd5512adedea", "pr_number": "44102", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_variable_type.py"], "labels": ["open source", "triaged"]}, "49215d7f26": {"title": "For CriterionTests, have check_gradgrad actually only affect gradgrad checks. (#44060)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44060\n\nRight now it skips grad checks as well.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23484018\n\nPulled By: gchanan\n\nfbshipit-source-id: 24a8f1af41f9918aaa62bc3cd78b139b2f8de1e1", "pr_number": "44060", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "bfa1fa5249": {"title": "Update rocm-3.5.1 build job to rocm-3.7 (#44123)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44123\n\nReviewed By: seemethere\n\nDifferential Revision: D23504193\n\nPulled By: malfet\n\nfbshipit-source-id: 3570dc0aa879a3fdd43f3ecd41ee9e745006cfde", "pr_number": "44123", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": []}, "55ff9aa185": {"title": "Test TE fuser unary ops and fix sigmoid(half) (#44094)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44094\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23494950\n\nPulled By: bertmaher\n\nfbshipit-source-id: 676c4e57267c4ad92065ea90b06323918dd5b0de", "pr_number": "44094", "files_changed": ["test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/tests.h", "test/test_jit_fuser_te.py", "torch/csrc/jit/tensorexpr/codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_half_support.h"], "labels": ["oncall: jit"]}, "ba65cce2a2": {"title": "Fix transposed conv2d rewrite pattern to account for convolution api (#44035)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44035\n\nchange\n\nAlso added test so as to capture such cases for future.\n\nTest Plan:\npython test/test_xnnpack_integration.py\n\nImported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23476773\n\nfbshipit-source-id: a62c4429351c909245106a70b4c60b1bacffa817", "pr_number": "44035", "files_changed": ["test/test_xnnpack_integration.py", "torch/csrc/jit/passes/graph_rewrite_helper.cpp"], "labels": ["oncall: jit"]}, "a153f69417": {"title": "Fix replaceAtenConvolution for BC. (#44036)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44036\n\nRunning replaceAtenConvolution on older traced model wont work as\n_convolution signature has changed and replaceAtenConvolution was\nchanged to account for that.\nBut we did not preserve the old behavior during that. This change\nrestores the old behavior while keeing the new one.\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23476775\n\nfbshipit-source-id: 73a0c2b7387f2a8d82a8d26070d0059972126836", "pr_number": "44036", "files_changed": ["torch/csrc/jit/passes/graph_rewrite_helper.cpp"], "labels": ["oncall: jit"]}, "442684cb25": {"title": "Enable typechecks for torch.nn.modules.[activation|upsampling] (#44093)", "body": "Summary:\nAdd missing `hardsigmoid`, `silu`, `hardswish` and `multi_head_attention_forward` to functional.pyi.in\n Embed some typing annotations into functional.py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44093\n\nReviewed By: ezyang\n\nDifferential Revision: D23494384\n\nPulled By: malfet\n\nfbshipit-source-id: 27023c16ff5951ceaebb78799c4629efa25f7c5c", "pr_number": "44093", "files_changed": ["mypy.ini", "torch/nn/functional.py", "torch/nn/functional.pyi.in", "torch/nn/modules/activation.py", "torch/nn/modules/upsampling.py"], "labels": []}, "3806c939bd": {"title": "Polish DDP join API docstrings (#43973)", "body": "Summary:\nPolishes DDP join api docstrings and makes a few minor cosmetic changes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43973\n\nReviewed By: zou3519\n\nDifferential Revision: D23467238\n\nPulled By: rohan-varma\n\nfbshipit-source-id: faf0ee56585fca5cc16f6891ea88032336b3be56", "pr_number": "43973", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": []}, "d0421ff1cc": {"title": "Benchmarks: add scripts for FastRNNs results comparison. (#44134)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44134\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23505810\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: d0b3d70d4c2a44a8c3773631d09a25a98ec59370", "pr_number": "44134", "files_changed": ["benchmarks/compare-fastrnn-results.py", "benchmarks/compare.sh"], "labels": []}, "f3da9e3b50": {"title": "Enable Enum pickling/unpickling. (#43188)", "body": "Summary:\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* **https://github.com/pytorch/pytorch/issues/43188 Enable Enum pickling/unpickling.**\n* https://github.com/pytorch/pytorch/issues/42963 Add Enum TorchScript serialization and deserialization support\n* https://github.com/pytorch/pytorch/issues/42874 Fix enum constant printing and add FileCheck to all Enum tests\n* https://github.com/pytorch/pytorch/issues/43121 Add Enum convert back to Python object support\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43188\n\nReviewed By: zdevito\n\nDifferential Revision: D23365141\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: f0c93d4ac614dec047ad8640eb6bd9c74159b558", "pr_number": "43188", "files_changed": ["test/jit/test_enum.py", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/unpickler.cpp"], "labels": ["oncall: jit"]}, "352a32e7f3": {"title": "[caffe2] fix clang build", "body": "Summary:\n* multiple -Wpessimizing-moves\n* `static` within  `__host__` `__device__` function\n\nTest Plan:\n```lang=bash\nbuck build -c fbcode.cuda_use_clang=true fblearner/flow/projects/dper:workflow\n```\n\nReviewed By: andrewjcg\n\nDifferential Revision: D23506573\n\nfbshipit-source-id: 1490a1267e39e067d3ef836ef9b1cd5d7a28f724", "pr_number": null, "files_changed": ["aten/src/ATen/native/cuda/ForeachTensorAddList.cu", "aten/src/ATen/native/cuda/ForeachTensorAddScalar.cu", "aten/src/ATen/native/cuda/Math.cuh"], "labels": []}, "e05fa2f553": {"title": "[quant] Prep for conv_transpose packing (#39714)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39714\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22087071\n\nPulled By: z-a-f\n\nfbshipit-source-id: 507f8a414026eb4c9926f68c1e94d2f56119bca6", "pr_number": "39714", "files_changed": ["aten/src/ATen/native/quantized/cpu/conv_packed_params.h", "aten/src/ATen/native/quantized/cpu/conv_serialization.h", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.h", "aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack_utils.h", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp"], "labels": ["oncall: jit", "oncall: quantization"]}, "f91bdbeabd": {"title": "Enable function calls in TEFuser and SpecializeAutogradZero (#43866)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43866\n\nReviewed By: ezyang\n\nDifferential Revision: D23452798\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 2cff4c905bf1b5d9de56e7869458ffa6fce1f1b5", "pr_number": "43866", "files_changed": ["test/cpp/jit/test_misc.cpp", "test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/test_jit.py", "torch/csrc/jit/passes/specialize_autogradzero.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/graph_executor.h", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.h", "torch/csrc/jit/runtime/profiling_record.cpp", "torch/csrc/jit/runtime/profiling_record.h"], "labels": ["oncall: jit"]}, "9b3c72d46e": {"title": "[pytorch] Make mobile find_method return an optional (#43965)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43965\n\nAs part of a larger effort to unify the API between the lite interpreter and full JIT:\n- implement torch::jit::mobile::Method, a proxy for torch::jit::mobile::Function\n- add support for overloaded operator() to mobile Method and Function\n- mobile find_method now returns a c10::optional<Method> (so signature matches full jit)\n- moves some implementation of Function from module.cpp to function.cpp\nghstack-source-id: 111161942\n\nTest Plan: CI\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23330762\n\nfbshipit-source-id: bf0ba0d711d9566c92af31772057ecd35983ee6d", "pr_number": "43965", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/method.h", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/python/script_init.cpp"], "labels": ["oncall: jit"]}, "6868bf95c6": {"title": "[JIT] Fuser match on schemas not node kind (#44083)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44083\n\nMatch on the complete schema of a node instead of its node kind when deciding to fuse it. Previously we matched on node kind, which could fail with something like `aten::add(int, int)` and if a new overload was added to an op without corresponding NNC support we would fuse it.\n\nFollow ups are:\n - bail when an output tensor type isnt uniquely determined by the input types (e.g. aten::add and the second input could be either a float or an int)\n- remove NNC lowering for _tanh_backward & _sigmoid_backward\n- Validate that we support all of the overloads here. I optimistically added ops that included Tensors, it's possible that we do not support every overload here. This isn't a regression, and this PR is at least improving our failures in that regard.\n\nI can do any of these as part of this PR if desired, but there are a number of failures people have run into that this PR fixes so I think it would be good to land this sooner than later.\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23503704\n\nPulled By: eellison\n\nfbshipit-source-id: 3ce971fb1bc3a7f1cbaa38f1ed853e2db3d67c18", "pr_number": "44083", "files_changed": ["test/jit/test_profiler.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["oncall: jit"]}, "e64879e180": {"title": "[tensorexpr] Alias analysis tests (#44110)", "body": "Summary:\nSome tests for alias analysis.\n\nThe first aliases at the module level and the second at the input level.\n\nPlease let me know if there are other alias situations!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44110\n\nReviewed By: nickgg\n\nDifferential Revision: D23509473\n\nPulled By: bwasti\n\nfbshipit-source-id: fbfe71a1d40152c8fbbd8d631f0a54589b791c34", "pr_number": "44110", "files_changed": ["test/test_tensorexpr.py"], "labels": []}, "74f18476a2": {"title": "[jit] fix segfault in attribute lookup on loaded ScriptModules (#43284)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43284\n\nThe IR emitter looks for attributes on modules like:\n1. Check the JIT type for the attribute\n2. Check the originating Python class, in order to fulfill requests for, e.g. static methods or ignored methods.\n\nIn the case where you do:\n```\ninner_module = torch.jit.load(\"inner.pt\")\nwrapped = Wrapper(inner_module)  # wrap the loaded ScriptModule in an nn.Module\ntorch.jit.script(wrapped)\n```\n\nThe IR emitter may check for attributes on `inner_module`. There is no\noriginating Python class for `inner_module`, since it was directly\ncompiled from the serialized format.\n\nDue to a bug in the code, we don't guard for this case an a segfault\nresults if the wrapper asks for an undefined attribute. The lookup in\nthis case looks like:\n1. Check the JIT type for the attribute (not there!)\n2. Check the originating Python class (this is a nullptr! segfault!)\n\nThis PR guards this case and properly just raises an attribute missing\ncompiler error instead of segfaulting.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23224337\n\nPulled By: suo\n\nfbshipit-source-id: 0cf3060c427f2253286f76f646765ec37b9c4c49", "pr_number": "43284", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/concrete_module_type.cpp", "torch/csrc/jit/frontend/concrete_module_type.h", "torch/csrc/jit/python/python_sugared_value.cpp"], "labels": ["oncall: jit"]}, "9dd8670d7d": {"title": "[jit] Better match behavior of loaded ScriptModules vs. freshly created ones (#43298)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43298\n\nIR emitter uses `ModuleValue` to represent ScriptModules and emit IR for\nattribute access, submodule access, etc.\n\n`ModuleValue` relies on two pieces of information, the JIT type of the\nmodule, and the `ConcreteModuleType`, which encapsulates Python-only\ninformation about the module.\n\nScriptModules loaded from a package used to create a dummy\nConcreteModuleType without any info in it. This led to divergences in\nbehavior during compilation.\n\nThis PR makes the two ways of constructing a ConcreteModuleType equivalent,\nmodulo any py-only information (which, by definition, is never present in\npackaged files anyway).\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23228738\n\nPulled By: suo\n\nfbshipit-source-id: f6a660f42272640ca1a1bb8c4ee7edfa2d1b07cc", "pr_number": "43298", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/concrete_module_type.cpp", "torch/csrc/jit/frontend/concrete_module_type.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/script_init.cpp"], "labels": ["oncall: jit"]}, "7816d53798": {"title": "[JIT] Add mypy type annotations for JIT (#43862)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43862\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23491151\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 88367b89896cf409bb9ac3db7490d6779efdc3a4", "pr_number": "43862", "files_changed": [".gitignore", "mypy.ini", "tools/pyi/gen_pyi.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/runtime/logging.h", "torch/jit/_builtins.py", "torch/jit/_recursive.py", "torch/jit/_script.py", "torch/jit/_serialization.py", "torch/jit/_state.py", "torch/jit/_trace.py", "torch/jit/annotations.py", "torch/jit/frontend.py", "torch/jit/quantized.py", "torch/jit/supported_ops.py", "torch/jit/unsupported_tensor_ops.py", "torch/quantization/quantize_fx.py"], "labels": ["module: typing", "oncall: jit"]}, "71510c60ad": {"title": "fx qat: respect device affinity (#44115)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44115\n\nFixes device affinity in the FX prepare pass for QAT. Before this PR, observers\nwere always created on CPU. After this PR, observers are created on the\nsame device as the rest of the model. This will enable QAT prepare to\nwork regardless of whether users move the model to cuda before or after\ncalling this pass.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFx.test_qat_prepare_device_affinity\n```\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23502291\n\nfbshipit-source-id: ec4ed20c21748a56a25e3395b35ab8640d71b5a8", "pr_number": "44115", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "3105d8a9b2": {"title": "[TensorExpr] Fuser: rely on input types when checking whether a device is supported. (#44139)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44139\n\nAlso, make sure that we're checking that condition when we're starting a\nnew fusion group, not only when we merge a node into an existing fusion\ngroup. Oh, and one more: add a test checking that we're rejecting graphs\nwith unspecified shapes.\n\nDifferential Revision: D23507510\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 9c268825ac785671d7c90faf2aff2a3e5985ac5b", "pr_number": "44139", "files_changed": ["test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["oncall: jit"]}, "bc4a00c197": {"title": "[TVM] Support Fused8BitRowwiseQuantizedToFloat op (#44098)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44098\n\nReviewed By: yinghai\n\nDifferential Revision: D23470129\n\nfbshipit-source-id: 1959e2167859f7cbc16e1423b957072bbc743ece", "pr_number": "44098", "files_changed": ["caffe2/opt/tvm_transformer.cc"], "labels": ["fb-exported"]}, "70aecd2a7f": {"title": "[NNC] make inlining immediate (take 2) and fix bugs (#43885)", "body": "Summary:\nA rework of `computeInline` which makes it work a bit better, particularly when combined with other transformations. Previously we stored Functions that were inlined and then deferred the actual inlining of the function body until prepareForCodgen was called. This has an issue when transformations are applied to the LoopNest: the function body can be different from what appears in the root_stmt and result in inlining that a) fails, b) reverses other transformations or c) a weird unpredictable combination of the two.\n\nThis PR changes that behaviour so that the inlining occurs in the root stmt immediately, which means it reflects any previous transformations and any future transformations have a true view of the internal IR. It also has the benefit that inspecting the root statement gives an accurate view of it without needing to call prepareForCodgen. I also removed the difference between `computeInline` and `computeInlineWithRand` and we handle calls to `rand()` in all branches.\n\nThis is a rework of https://github.com/pytorch/pytorch/issues/38696, with the agreed changes from ZolotukhinM and zheng-xq: we should only inline if the dimensions are trivial (ie. they are vars not exprs).\n\nThis PR is mostly tests, and I fixed a bunch of bugs I found along the way. Partial list:\n* When inlining an expression involving rand, we would create random vars equal to the dimensionality of the enclosing Tensor not the produced Tensor - meaning we'd use an incorrect value if the inlined tensor was smaller. E.g: `X[i] = rand(); A[i, j] = X[i]` would produce a tensor where `A[0, 0] != A[0, 1]`. This is fixed by inserting the Let binding of the random variable at the correct loop body.\n* When inlining we'd replace all calls to `rand()` rather than just those present in the Tensor being inlined.\n* `rand()` was treated symbolically by the simplifier and we would aggregate or cancel calls to `rand()`. Have fixed the hasher to hash all calls to `rand()` distinctly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43885\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23503636\n\nPulled By: nickgg\n\nfbshipit-source-id: cdbdc902b7a14d269911d978a74a1c11eab004fa", "pr_number": "43885", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/analysis.h", "torch/csrc/jit/tensorexpr/function.cpp", "torch/csrc/jit/tensorexpr/hash_provider.cpp", "torch/csrc/jit/tensorexpr/hash_provider.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.h", "torch/csrc/jit/tensorexpr/tensor.h"], "labels": ["oncall: jit"]}, "c40e3f9f98": {"title": "[android][jni] Support Tensor MemoryFormat in java wrappers (#40785)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40785\n\nThe main goal of this change is to support creating Tensors specifying blob in NHWC (ChannelsLast) format.\n\nChannelsLast is supported only for 4-dim tensors, this is enforced on LibTorch side, I have not added asserts on java side in case that this limitation will be changed in future and not to have double asserts.\n\nAdditional changes in `aten/src/ATen/templates/Functions.h`:\n\n`from_blob` creates `at::empty({0}, options)` tensor first and sets it Storage with sizes and strides afterwards.\n\nBut as ChannelsLast is only for 4-dim tensors - it fails on that creation, as dim==1.\n\nI've added `zero_sizes()` function that returns `{0, 0, 0, 0}` for ChannelsLast and ChannelsLast3d.\n\nTest Plan: Imported from OSS\n\nReviewed By: dreiss\n\nDifferential Revision: D22396244\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 02582d748a554e0f859aefe71cd2c1e321fb8979", "pr_number": "40785", "files_changed": ["android/pytorch_android/generate_test_torchscripts.py", "android/pytorch_android/src/androidTest/java/org/pytorch/PytorchTestBase.java", "android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/src/main/java/org/pytorch/MemoryFormat.java", "android/pytorch_android/src/main/java/org/pytorch/Tensor.java", "android/pytorch_android/test_asset.jit", "aten/src/ATen/templates/Functions.h"], "labels": []}, "69e38828f5": {"title": "[quant] conv_transpose2d_prepack/conv_transpose2d_unpack (#40351)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40351\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22158983\n\nPulled By: z-a-f\n\nfbshipit-source-id: 3ca064c2d826609724b2740fcc9b9eb40556168d", "pr_number": "40351", "files_changed": ["aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qconv_unpack.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py", "torch/testing/_internal/hypothesis_utils.py"], "labels": ["oncall: quantization"]}, "538d3bd364": {"title": "Enable CUDA 11 jobs for Windows nightly builds (#44086)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/pull/43366/files#r474333051.\nTesting with https://github.com/pytorch/pytorch/pull/44007.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44086\n\nReviewed By: ezyang\n\nDifferential Revision: D23493553\n\nPulled By: malfet\n\nfbshipit-source-id: 34b3e5b2e8dece5e97db9d507c34d61d33bd0863", "pr_number": "44086", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/config.yml"], "labels": ["open source"]}, "b60ffcdfdd": {"title": "Enable typechecks for torch.nn.quantized.modules.linear (#44154)", "body": "Summary:\nAlso import `Optional` directly from `typing` rather than from `_jit_internal`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44154\n\nReviewed By: seemethere\n\nDifferential Revision: D23511833\n\nPulled By: malfet\n\nfbshipit-source-id: f78c5fd679c002b218e4d287a9e56fa198171981", "pr_number": "44154", "files_changed": ["mypy.ini", "torch/jit/quantized.py", "torch/nn/quantized/dynamic/modules/embeddingbag.py", "torch/nn/quantized/modules/linear.py"], "labels": ["oncall: jit"]}, "addfd7a9b9": {"title": "Add tests against autograd precedence and multiple dispatch. (#44037)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44037\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23480154\n\nPulled By: ailzhang\n\nfbshipit-source-id: 28b68e67975397c76ce6c73ceaeec9d5cc934635", "pr_number": "44037", "files_changed": ["aten/src/ATen/core/op_registration/op_registration_test.cpp"], "labels": []}, "d221256888": {"title": "[Message] Add what to do for missing operators.", "body": "Summary: As title.\n\nTest Plan: N/A\n\nReviewed By: gaurav-work\n\nDifferential Revision: D23502416\n\nfbshipit-source-id: a341eb10030e3f319266019ba4c02d9d9a0a6298", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/import.cpp"], "labels": []}, "2f8a43341d": {"title": "Add API for onnxifi with AOT Glow ONNX (#44021)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44021\n\nPull Request resolved: https://github.com/pytorch/glow/pull/4854\n\nTest Plan: Added `test_onnxifi_aot.py`\n\nReviewed By: yinghai\n\nDifferential Revision: D23307003\n\nfbshipit-source-id: e6d4f3e394f96fd22f80eb2b8a686cf8171a54c0", "pr_number": "44021", "files_changed": ["caffe2/opt/onnxifi_op.cc", "caffe2/python/pybind_state.cc", "caffe2/python/pybind_state.h"], "labels": ["fb-exported"]}, "a37c199b8b": {"title": "[c2][cuda] small improvement to dedup adagrad by avoiding recompute of x_ij (#44173)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44173\n\nit has small 10~15% speed improvement\n\nTest Plan:\n== Correctness ==\n`buck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient '`\n\nReviewed By: jianyuh\n\nDifferential Revision: D23494030\n\nfbshipit-source-id: cdb7ee716a7e559903b72ed9f93bf106813f88fa", "pr_number": "44173", "files_changed": ["caffe2/sgd/adagrad_fused_op_gpu.cu"], "labels": ["fb-exported"]}, "98ad5ff41f": {"title": "[te] Disable reductions by default (#44122)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44122\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D23504769\n\nPulled By: bertmaher\n\nfbshipit-source-id: 1889217cd22da529e46ab30c9319a5646267e4ec", "pr_number": "44122", "files_changed": ["test/test_jit_fuser_te.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.h", "torch/csrc/jit/python/init.cpp"], "labels": ["oncall: jit"]}, "f8f35fddd4": {"title": "Optimize code path for adaptive_avg_pool2d when output size is (1, 1) (#43986)", "body": "Summary:\nBenchmark:\n\ncode: https://github.com/xwang233/code-snippet/blob/master/adaptive-avg-pool2d-output-1x1/adap.ipynb\n\n| shape | time_before (ms) | time_after (ms) |\n| --- | --- | --- |\n| (2, 3, 4, 4), torch.contiguous_format, cpu  |  0.035 |  0.031 |\n| (2, 3, 4, 4), torch.contiguous_format, cuda  |  0.041 |  0.031 |\n| (2, 3, 4, 4), torch.channels_last, cpu  |  0.027 |  0.029 |\n| (2, 3, 4, 4), torch.channels_last, cuda  |  0.031 |  0.034 |\n| (2, 3, 4, 4), non_contiguous, cpu  |  0.037 |  0.026 |\n| (2, 3, 4, 4), non_contiguous, cuda  |  0.062 |  0.033 |\n| (4, 16, 32, 32), torch.contiguous_format, cpu  |  0.063 |  0.055 |\n| (4, 16, 32, 32), torch.contiguous_format, cuda  |  0.043 |  0.031 |\n| (4, 16, 32, 32), torch.channels_last, cpu  |  0.052 |  0.064 |\n| (4, 16, 32, 32), torch.channels_last, cuda  |  0.190 |  0.033 |\n| (4, 16, 32, 32), non_contiguous, cpu  |  0.048 |  0.035 |\n| (4, 16, 32, 32), non_contiguous, cuda  |  0.062 |  0.033 |\n| (8, 128, 64, 64), torch.contiguous_format, cpu  |  0.120 |  0.109 |\n| (8, 128, 64, 64), torch.contiguous_format, cuda  |  0.043 |  0.044 |\n| (8, 128, 64, 64), torch.channels_last, cpu  |  1.303 |  0.260 |\n| (8, 128, 64, 64), torch.channels_last, cuda  |  1.237 |  0.049 |\n| (8, 128, 64, 64), non_contiguous, cpu  |  0.132 |  0.128 |\n| (8, 128, 64, 64), non_contiguous, cuda  |  0.062 |  0.031 |\n| (16, 256, 224, 224), torch.contiguous_format, cpu  |  17.232 |  14.807 |\n| (16, 256, 224, 224), torch.contiguous_format, cuda  |  1.930 |  1.930 |\n| (16, 256, 224, 224), torch.channels_last, cpu  |  245.025 |  24.345 |\n| (16, 256, 224, 224), torch.channels_last, cuda  |  15.593 |  1.944 |\n| (16, 256, 224, 224), non_contiguous, cpu  |  11.738 |  6.460 |\n| (16, 256, 224, 224), non_contiguous, cuda  |  0.524 |  0.251 |\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43986\n\nReviewed By: anjali411\n\nDifferential Revision: D23468286\n\nPulled By: ngimel\n\nfbshipit-source-id: cc181f705feacb2f86df420d648cc59fda69fdb7", "pr_number": "43986", "files_changed": ["aten/src/ATen/native/AdaptiveAveragePooling.cpp", "test/test_nn.py"], "labels": ["open source"]}, "28b1360d24": {"title": "[Codemod][FBSourceGoogleJavaFormatLinter] Daily `arc lint --take GOOGLEJAVAFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D23536088\n\nfbshipit-source-id: d4c6c26ed5bad4e8c1b80ac1c05bd86b36cb6aaa", "pr_number": null, "files_changed": ["android/pytorch_android/src/androidTest/java/org/pytorch/PytorchTestBase.java"], "labels": []}, "0c01f136f3": {"title": "[BE] Use f-string in various Python functions (#44161)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44161\n\nReviewed By: seemethere\n\nDifferential Revision: D23515874\n\nPulled By: malfet\n\nfbshipit-source-id: 868cf65aedd58fce943c08f8e079e84e0a36df1f", "pr_number": "44161", "files_changed": ["torch/__init__.py", "torch/_classes.py", "torch/_jit_internal.py", "torch/functional.py", "torch/jit/annotations.py", "torch/quasirandom.py", "torch/serialization.py", "torch/storage.py", "torch/tensor.py"], "labels": ["oncall: jit"]}, "9a5a732866": {"title": "Register some backwards functions as operators (#44052)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44052\n\nSummary\n=======\n\nThis PR registers the following backwards functions as operators:\n- slice_backward\n- select_backward\n- gather_backward\n- index_select_backward (the backward function for index_select)\n- select_index_backward (prevously known as index_select_backward, but is actually the backward function for max.dim, min.dim, etc)\n\nIn the future, I'd like to register more backward functions as operators\nso that we can write batching rules for the backward functions. Batching\nrules for backward functions makes it so that we can compute batched\ngradients.\n\nMotivation\n==========\nThe rationale behind this PR is that a lot of backwards functions (27 in total)\nare incompatible with BatchedTensor due to using in-place operations.\nSometimes we can allow the in-place operations, but other times we can't.\nFor example, consider select_backward:\n\n```\nTensor select_backward(const Tensor& grad, IntArrayRef input_sizes, int64_t dim, int64_t index) {\n  auto grad_input = at::zeros(input_sizes, grad.options());\n  grad_input.select(dim, index).copy_(grad);\n  return grad_input;\n}\n```\n\nand consider the following code:\n```\nx = torch.randn(5, requires_grad=True)\ndef select_grad(v):\n    torch.autograd.grad(x[0], x, v)\n\nvs = torch.randn(B0)\nbatched_grads = vmap(select_grad)(vs)\n```\n\nFor the batched gradient use case, `grad` is a BatchedTensor.\nThe physical version of `grad` has size `(B0,)`.\nHowever, select_backward creates a `grad_input` of shape `(5)`, and\ntries to copy `grad` to a slice of it.\n\nOther approaches\n================\n\nI've considered the following:\n- register select_backward as an operator (this PR)\n- have a branch inside select_backward for if `grad` is batched.\n    - this is OK, but what if we have more tensor extensions that want to override this?\n- modify select_backward to work with BatchedTensor, by creating a new operator for the \"select + copy_ behavior\".\n    - select + copy_ isn't used elsewhere in derivative formulas so this doesn't seem useful\n\nTest Plan\n=========\n\n- `pytest test/test_autograd.py -v`\n- Registering backward functions may impact performance. I benchmarked\nselect_backward to see if registering it as an operator led to any noticable\nperformance overheads: https://gist.github.com/zou3519/56d6cb53775649047b0e66de6f0007dc.\nThe TL;DR is that the overhead is pretty minimal.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang, fbhuba\n\nDifferential Revision: D23481183\n\nPulled By: zou3519\n\nfbshipit-source-id: 125af62eb95824626dc83d06bbc513262ee27350", "pr_number": "44052", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h"], "labels": []}, "6cecf7ec68": {"title": "Enable test_cublas_config_deterministic_error for windows (#42796)", "body": "Summary:\ntest_cublas_config_deterministic_error can pass for windows, so enable it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42796\n\nReviewed By: seemethere\n\nDifferential Revision: D23520002\n\nPulled By: malfet\n\nfbshipit-source-id: eccedbbf202b1cada795071a34e266b2c635c2cf", "pr_number": "42796", "files_changed": ["test/test_torch.py"], "labels": ["merge-this-please", "open source", "triaged"]}, "6aba58cfd3": {"title": "Limit MAX_JOBS to 18 for linux binary builds (#44168)", "body": "Summary:\nBecause those jobs are running in Docker2XLarge+ container that has 20 cores\nUnfortunately `nproc` returns number of cores available on the host rather than number of cores available to container\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44168\n\nReviewed By: walterddr, ngimel\n\nDifferential Revision: D23539558\n\nPulled By: malfet\n\nfbshipit-source-id: 3df858722e153a8fcbe8ef6370b1a9c1993ada5b", "pr_number": "44168", "files_changed": [".circleci/scripts/binary_linux_build.sh"], "labels": []}, "4d431881d1": {"title": "Control NCCL build parallelism via MAX_JOBS environment var (#44167)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44167\n\nReviewed By: walterddr, ngimel\n\nDifferential Revision: D23522419\n\nPulled By: malfet\n\nfbshipit-source-id: 31b25a71fef3e470bdf382eb3698e267326fa354", "pr_number": "44167", "files_changed": ["cmake/External/nccl.cmake"], "labels": []}, "2a1fc56694": {"title": "replace the white list from default mappings (#41802)", "body": "Summary:\nReplaced \"whitelist\" from default_mappings.py\nFixes https://github.com/pytorch/pytorch/issues/41756\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41802\n\nReviewed By: ngimel\n\nDifferential Revision: D23521452\n\nPulled By: malfet\n\nfbshipit-source-id: 019a2d5c06dc59dc53d6c48b70fb35b216299cf4", "pr_number": "41802", "files_changed": ["torch/quantization/_numeric_suite.py", "torch/quantization/default_mappings.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["oncall: quantization", "open source", "triaged"]}, "f38e7aee71": {"title": "Updates to SCCACHE for ROCm case (#44155)", "body": "Summary:\n- Collecting sccache trace logs\n- Change the SCCACHE_IDLE_TIMEOUT to unlimited\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44155\n\nReviewed By: ngimel\n\nDifferential Revision: D23516192\n\nPulled By: malfet\n\nfbshipit-source-id: aa93052d7b9a1832eeaa8e81ee8706aeb9f7a508", "pr_number": "44155", "files_changed": [".jenkins/caffe2/common.sh", ".jenkins/pytorch/common.sh"], "labels": ["open source"]}, "0e3cf6b8d2": {"title": "[pytorch] remove code analyzer build folder between builds (#44148)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44148\n\nAutomatically remove the build_code_analyzer folder each time build.sh is run\nghstack-source-id: 111458413\n\nTest Plan:\nRun build.sh with different options and compare the outputs (should be different).\nEx:\n`ANALYZE_TORCH=1 DEPLOY=1 BASE_OPS_FILE=/path/to/baseops MOBILE_BUILD_FLAGS='-DBUILD_MOBILE_AUTOGRAD=OFF' tools/code_analyzer/build.sh `\n\nshould produce a shorter file than\n`ANALYZE_TORCH=1 DEPLOY=1 BASE_OPS_FILE=/path/to/baseops MOBILE_BUILD_FLAGS='-DBUILD_MOBILE_AUTOGRAD=ON' tools/code_analyzer/build.sh`\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23503886\n\nfbshipit-source-id: 9b95d4365540da0bd2d27760e1315caed5f44eec", "pr_number": "44148", "files_changed": ["tools/code_analyzer/build.sh"], "labels": []}, "af13faf18b": {"title": "[FX] __str__ for GraphModule and Graph (#44166)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44166\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23520801\n\nPulled By: jamesr66a\n\nfbshipit-source-id: f77e3466e435127ec01e66291964395f32a18992", "pr_number": "44166", "files_changed": ["test/test_fx.py", "torch/fx/graph.py", "torch/fx/graph_module.py"], "labels": ["fx"]}, "539d029d8c": {"title": "[ONNX] Fix split export using slice (#43670)", "body": "Summary:\nFix for exporting split with fixed output shape using slice.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43670\n\nReviewed By: houseroad\n\nDifferential Revision: D23420318\n\nPulled By: bzinodev\n\nfbshipit-source-id: 09c2b58049fe32dca2f2977d91dd64de6ee9a72f", "pr_number": "43670", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py"], "labels": ["open source"]}, "6474057c76": {"title": "Revert D23503636: [pytorch][PR] [NNC] make inlining immediate (take 2) and fix bugs", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23503636 (https://github.com/pytorch/pytorch/commit/70aecd2a7f48dfe7180002fb1d9b9694ac1fcb7f)\n\nOriginal commit changeset: cdbdc902b7a1\n\nfbshipit-source-id: b5164835f874a56213de4bed9ad690164eae9230", "pr_number": null, "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/analysis.h", "torch/csrc/jit/tensorexpr/function.cpp", "torch/csrc/jit/tensorexpr/hash_provider.cpp", "torch/csrc/jit/tensorexpr/hash_provider.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.h", "torch/csrc/jit/tensorexpr/tensor.h"], "labels": []}, "0c2bc4fe20": {"title": "Revert D23468286: [pytorch][PR] Optimize code path for adaptive_avg_pool2d when output size is (1, 1)", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23468286 (https://github.com/pytorch/pytorch/commit/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8)\n\nOriginal commit changeset: cc181f705fea\n\nfbshipit-source-id: 3a1db0eef849e0c2f3c0c64040d2a8b799644fa3", "pr_number": null, "files_changed": ["aten/src/ATen/native/AdaptiveAveragePooling.cpp", "test/test_nn.py"], "labels": []}, "2ad5a82c43": {"title": "[fx] get rid of graph_module.root (#44092)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44092\n\ninstead submodules and weights are installed directly on the\ngraph_module by transferring the original modules. This makes it more\nlikely that scripting will succeed (since we no longer have submodules\nthat are not used in the trace). It also prevents layered transforms\nfrom having to special case handling of the `root` module. GraphModules\ncan now be re-traced as part of the input to other transforms.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23504210\n\nPulled By: zdevito\n\nfbshipit-source-id: f79e5c4cbfc52eb0ffb5d6ed89b37ce35a7dc467", "pr_number": "44092", "files_changed": ["test/fx/quantization.py", "test/quantization/test_quantize_fx.py", "test/test_fx.py", "torch/fx/graph_module.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["fx"]}, "4562b212db": {"title": "Fix potential divide by zero for CostInferenceForRowWiseSparseAdagrad", "body": "Summary: Fix the potential divide by zero error in CostInferenceForRowWiseSparseAdagrad, when n has zero elements\n\nTest Plan:\nRan buck test caffe2/caffe2/python/operator_test:adagrad_test\nResult: https://our.intern.facebook.com/intern/testinfra/testrun/562950122086369\n\nReviewed By: idning\n\nDifferential Revision: D23520763\n\nfbshipit-source-id: 191345bd24f5179a9dbdb41c6784eab102cfe89c", "pr_number": null, "files_changed": ["caffe2/sgd/adagrad_op.cc"], "labels": []}, "70bbd08402": {"title": "[FX] Fix forward merge conflict breakage (#44221)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44221\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23547373\n\nPulled By: jamesr66a\n\nfbshipit-source-id: df47fce0f6ff2988093208fc8370544b7985288d", "pr_number": "44221", "files_changed": ["test/test_fx.py"], "labels": []}, "3d7c22a2ce": {"title": "[ONNX] Enable new scripting passes for functionalization and remove_mutation (#43791)", "body": "Summary:\nDuplicate of https://github.com/pytorch/pytorch/issues/41413\nThis PR initiates the process of updating the torchsciprt backend interface used by ONNX exporter.\n\nReplace jit lower graph pass by freeze module pass\n\nEnable ScriptModule tests for ONNX operator tests (ORT backend) and model tests by default.\n\nReplace jit remove_inplace_ops pass with remove_mutation and consolidation all passes for handling inplace ops.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43791\n\nReviewed By: houseroad\n\nDifferential Revision: D23421872\n\nPulled By: bzinodev\n\nfbshipit-source-id: a98710c45ee905748ec58385e2a232de2486331b", "pr_number": "43791", "files_changed": ["test/jit/test_onnx_export.py", "test/onnx/test_models.py", "test/onnx/test_models_onnxruntime.py", "test/onnx/test_pytorch_common.py", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_utility_funs.py", "test/onnx/verify.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/lower_tuples.cpp", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.cpp", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.h", "torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp", "torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.h", "torch/csrc/jit/passes/remove_mutation.cpp", "torch/csrc/jit/passes/remove_mutation.h", "torch/csrc/jit/python/init.cpp", "torch/onnx/utils.py"], "labels": ["oncall: jit", "open source"]}, "8f37ad8290": {"title": "[BUILD] Guard '#pragma unroll' with COMPILING_FOR_MIN_SIZE", "body": "Summary: Disable  unroll hints when COMPILING_FOR_MIN_SIZE is on. We were seeing hundreds of errors in the build because the optimization was not being performed.\n\nTest Plan: Smoke builds\n\nDifferential Revision: D23513255\n\nfbshipit-source-id: 87da2fdc3c1146e8ffcacf14a49d5151d313f367", "pr_number": null, "files_changed": ["aten/src/ATen/native/cpu/CatKernel.cpp", "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "aten/src/ATen/native/cpu/SumKernel.cpp"], "labels": []}, "8b17fd2516": {"title": "Add remote_parameters() into RemoteModule class. (#43906)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43906\n\nThis method returns a list of RRefs of remote parameters that can be fed into the DistributedOptimizer.\n\nOriginal PR issue: RemoteModule enhancements #40550\n\nTest Plan: buck test caffe2/test/distributed/rpc:process_group_agent -- RemoteModule\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23399586\n\nfbshipit-source-id: 4b0f1ccf2e47c8a9e4f79cb2c8668f3cdbdff820", "pr_number": "43906", "files_changed": ["torch/distributed/nn/api/remote_module.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["fb-exported"]}, "3699274ce2": {"title": "[DPER3] AOT integration", "body": "Summary: Integrate aot flow with model exporter.\n\nTest Plan:\nbuck test dper3/dper3_backend/delivery/tests:dper3_model_export_test\n\nreplayer test see D23407733\n\nReviewed By: ipiszy\n\nDifferential Revision: D23313689\n\nfbshipit-source-id: 39ae8d578ed28ddd6510db959b65974a5ff62888", "pr_number": null, "files_changed": ["caffe2/proto/metanet.proto", "caffe2/python/predictor/predictor_py_utils.py"], "labels": []}, "f3bf6a41ca": {"title": "[ONNX] Update repeat op (#43430)", "body": "Summary:\nUpdate repeat op so that the inputs to sizes argument can a mixture of dynamic and constant inputs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43430\n\nReviewed By: houseroad\n\nDifferential Revision: D23494257\n\nPulled By: bzinodev\n\nfbshipit-source-id: 90c5e90e4f73e98f3a9d5c8772850e72cecdf0d4", "pr_number": "43430", "files_changed": ["test/onnx/expect/TestOperators.test_repeat.expect", "test/onnx/expect/TestOperators.test_repeat_dim_overflow.expect", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset8.py", "torch/onnx/symbolic_opset9.py"], "labels": ["open source", "triaged"]}, "15e99b6ff6": {"title": "Compile less legacy code when BUILD_CAFFE2 is set to False (#44079)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44079\n\nReviewed By: walterddr\n\nDifferential Revision: D23490149\n\nPulled By: malfet\n\nfbshipit-source-id: a76382c30d83127d180ec63ac15093a7297aae53", "pr_number": "44079", "files_changed": ["caffe2/CMakeLists.txt", "caffe2/core/CMakeLists.txt", "caffe2/proto/CMakeLists.txt", "caffe2/utils/CMakeLists.txt", "torch/csrc/jit/serialization/import.cpp"], "labels": ["caffe2", "ci/all", "module: ci", "oncall: jit", "triaged"]}, "398409f072": {"title": "[PyTorch][Mobile] Insert the module name as `name()` to metadata dict if metadata doesn't contain \"model_name\" (#44227)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44227\n\nAs title\nghstack-source-id: 111490242\n\nTest Plan: CI\n\nReviewed By: xcheng16\n\nDifferential Revision: D23549149\n\nfbshipit-source-id: fad742a8d4e6f844f83495514cd60ff2bf0d5bcb", "pr_number": "44227", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/module.cpp"], "labels": ["oncall: jit"]}, "8c64bb4f47": {"title": "[dper3] replace LengthsGather lowlevel module's PT implemetnatio to use caffe2 op", "body": "Summary: Use a more efficient C++ implementation in a caffe2 op to get rid of control flow statements here.\n\nTest Plan:\n- Ran `buck test dper3/dper3/modules/low_level_modules/tests:single_operators_test`\n- Ran `buck-out/gen/dper3/dper3_models/experimental/pytorch/ads_model_generation_script.par --model_type=\"inline_cvr_post_imp\" --model_version=\"april_2020\" --gen_inference_model` and observed files getting generated:\n```\n[ashenoy@devbig086.ash8 ~/fbsource/fbcode] ls -l /tmp/ashenoy/inline_cvr_post_imp_april_2020/\ntotal 278332\n-rw-r--r--. 1 ashenoy users 71376941 Sep  3 23:10 serialized_inline_cvr_post_imp_april_2020_model_inference.pt\n-rw-r--r--. 1 ashenoy users 71437424 Sep  3 22:09 serialized_inline_cvr_post_imp_april_2020_model_inference_shrunk.pt\n-rw-r--r--. 1 ashenoy users    14952 Sep  3 22:38 serialized_inline_cvr_post_imp_april_2020_model_io_metadata_map.pt\n-rw-r--r--. 1 ashenoy users    14952 Sep  3 21:42 serialized_inline_cvr_post_imp_april_2020_model_io_metadata_map_shrunk.pt\n-rw-r--r--. 1 ashenoy users 67001662 Sep  3 22:38 serialized_inline_cvr_post_imp_april_2020_model_main.pt\n-rw-r--r--. 1 ashenoy users 67126415 Sep  3 21:42 serialized_inline_cvr_post_imp_april_2020_model_main_shrunk.pt\n-rw-r--r--. 1 ashenoy users  3945257 Sep  3 22:34 serialized_inline_cvr_post_imp_april_2020_model_preproc.pt\n-rw-r--r--. 1 ashenoy users  4077266 Sep  3 21:37 serialized_inline_cvr_post_imp_april_2020_model_preproc_shrunk.pt\n```\n- Ran `buck-out/gen/dper3/dper3_models/experimental/pytorch/ads_model_generation_script.par --model_type=\"ctr_mbl_feed\" --model_version=\"april_2020\" --gen_inference_model` and observed model files getting generated:\n```\n[ashenoy@devbig086.ash8 ~/fbsource/fbcode] ls -l /tmp/ashenoy/ctr_mbl_feed_april_2020/\ntotal 170304\n-rw-r--r--. 1 ashenoy users  2641870 Sep  3 23:06 ctr_mbl_feed_april_2020_prod_eval_training_options\n-rw-r--r--. 1 ashenoy users  2641870 Sep  3 23:06 ctr_mbl_feed_april_2020_prod_train_training_options\n-rw-r--r--. 1 ashenoy users 42225079 Sep  3 23:59 serialized_ctr_mbl_feed_april_2020_model_inference.pt\n-rw-r--r--. 1 ashenoy users 42576708 Sep  3 22:33 serialized_ctr_mbl_feed_april_2020_model_inference_shrunk.pt\n-rw-r--r--. 1 ashenoy users    11194 Sep  3 23:29 serialized_ctr_mbl_feed_april_2020_model_io_metadata_map.pt\n-rw-r--r--. 1 ashenoy users    11194 Sep  3 22:05 serialized_ctr_mbl_feed_april_2020_model_io_metadata_map_shrunk.pt\n-rw-r--r--. 1 ashenoy users 39239139 Sep  3 23:29 serialized_ctr_mbl_feed_april_2020_model_main.pt\n-rw-r--r--. 1 ashenoy users 39250842 Sep  3 22:05 serialized_ctr_mbl_feed_april_2020_model_main_shrunk.pt\n-rw-r--r--. 1 ashenoy users  2839097 Sep  3 23:24 serialized_ctr_mbl_feed_april_2020_model_preproc.pt\n-rw-r--r--. 1 ashenoy users  2944239 Sep  3 22:01 serialized_ctr_mbl_feed_april_2020_model_preproc_shrunk.pt\n```\n\nReviewed By: houseroad\n\nDifferential Revision: D23519521\n\nfbshipit-source-id: ed9bd16a8af3cca3a865d9614d67d07f01d8b18a", "pr_number": null, "files_changed": ["caffe2/operators/utility_ops.cc", "caffe2/operators/utility_ops.h"], "labels": []}, "a940f5ea5d": {"title": "torchscript graph mode quant: remove benchmark filter (#44165)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44165\n\nAllows convolutions to be quantized if `torch.cudnn.backends.benchmark`\nflag was set.\n\nNot for land yet, just testing.\n\nTest Plan:\nin the gist below, the resulting graph now has quantized convolutions\nhttps://gist.github.com/vkuzo/622213cb12faa0996b6700b08d6ab2f0\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23518775\n\nfbshipit-source-id: 294f678c6afbd3feeb89b7a6655bc66ac9f8bfbc", "pr_number": "44165", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/graph_rewrite_helper.cpp"], "labels": ["oncall: jit"]}, "618b4dd763": {"title": "fx quant prepare: clarify naming (#44125)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44125\n\nIn `Quantizer._prepare`, `observed` was used for two different variables\nwith different types.  Making the names a bit cleaner and removing the\nname conflict.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFx\npython test/test_quantization.py TestQuantizeFxOps\n```\n\nImported from OSS\n\nReviewed By: dskhudia\n\nDifferential Revision: D23504109\n\nfbshipit-source-id: 0f73eac3d6dd5f72ad5574a4d47d33808a70174a", "pr_number": "44125", "files_changed": ["torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "d07a36e0c1": {"title": "Revert D23490149: [pytorch][PR] Compile less legacy code when BUILD_CAFFE2 is set to False", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23490149 (https://github.com/pytorch/pytorch/commit/15e99b6ff6ee88ef955ee526d35856451536ee99)\n\nOriginal commit changeset: a76382c30d83\n\nfbshipit-source-id: 75057fa9af2c19eb976962552118bf0a99911b38", "pr_number": null, "files_changed": ["caffe2/CMakeLists.txt", "caffe2/core/CMakeLists.txt", "caffe2/proto/CMakeLists.txt", "caffe2/utils/CMakeLists.txt", "torch/csrc/jit/serialization/import.cpp"], "labels": []}, "5a0d65b06b": {"title": "Further expand coverage of addmm/addmv, fix 0 stride (#43980)", "body": "Summary:\n- test beta=0, self=nan\n- test transposes\n- fixes broadcasting of addmv\n- not supporting tf32 yet, will do it in future PR together with other testing fixes\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43980\n\nReviewed By: mruberry\n\nDifferential Revision: D23507559\n\nPulled By: ngimel\n\nfbshipit-source-id: 14ee39d1a0e13b9482932bede3fccb61fe6d086d", "pr_number": "43980", "files_changed": ["aten/src/ATen/native/cuda/Blas.cu", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "df67f0beab": {"title": "[TensorExpr fuser] Guard nodes that have tensor output properties determined by non-tensor inputs (#44137)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44137\n\nWe only insert guards on Tensor types, so we rely on the output\nof a node being uniquely determined by its input types.\nbail if any non-Tensor input affects the output type\nand cannot be reasoned about statically\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23543602\n\nPulled By: eellison\n\nfbshipit-source-id: abd6fe0b1fd7fe6fc251694d4cd442b19c032dd7", "pr_number": "44137", "files_changed": ["test/jit/test_profiler.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["oncall: jit"]}, "5bd2902796": {"title": "[JIT] Remove references to no longer generated _tanh_backward and _sigmoid_backward (#44138)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44138\n\nIf you look at the sigmoid and tanh backward they are composed of other ops: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/runtime/symbolic_script.cpp#L786\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/runtime/symbolic_script.cpp#L164\n\nSo tanh_backward and sigmoid_backward are no longer generated / legacy ops.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23543603\n\nPulled By: eellison\n\nfbshipit-source-id: ce8353e53043cf969b536aac47c9576d66d4ce02", "pr_number": "44138", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "test/cpp/tensorexpr/test_aten.cpp", "test/cpp/tensorexpr/tests.h", "test/test_tensorexpr.py", "torch/csrc/jit/codegen/fuser/codegen.cpp", "torch/csrc/jit/passes/guard_elimination.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["oncall: jit"]}, "15a7368115": {"title": "Add const to getTensors method of GradBucket. (#44126)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44126\n\nAdd const to getTensors method of GradBucket.\n\nTest Plan: buck test caffe2/torch/lib/c10d:ProcessGroupGlooTest\n\nReviewed By: sinannasir, jiayisuse\n\nDifferential Revision: D23504088\n\nfbshipit-source-id: 427d9591042e0c03cde02629c1146ff1e5e027f9", "pr_number": "44126", "files_changed": ["torch/csrc/distributed/c10d/comm.cpp", "torch/csrc/distributed/c10d/comm.h"], "labels": ["fb-exported"]}, "a0ae416d60": {"title": "[quant] Support aten::embedding_bag quantization in graph mode (#43989)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43989\n\nWhen we trace the model it produces aten::embedding_bag node in the graph,\nAdd necessary passes in graph mode to help support quantizing it as well\n\nTest Plan:\npython test/test_quantization.py TestQuantizeDynamicJitOps.test_embedding_bag\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23460485\n\nfbshipit-source-id: 328c5e1816cfebb10ba951113f657665b6d17575", "pr_number": "43989", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/insert_observers.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": ["oncall: jit"]}, "164b96c34c": {"title": "[quant][pyper] make embedding_bag quantization static (#44008)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44008\n\nembedding_bag requires only quantization of weights (no dynamic quantization of inputs)\nSo the type of quantization is essentially static (without calibration)\nThis will enable pyper to do fc and embedding_bag quantization using the same API call\n\nTest Plan:\npython test/test_quantization.py test_embedding_bag\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23467019\n\nfbshipit-source-id: 41a61a17ee34bcb737ba5b4e19fb7a576d4aeaf9", "pr_number": "44008", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/insert_observers.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": ["oncall: jit"]}, "199c73be0f": {"title": "[quant][pyper] Support quantization of ops in fork-wait subgraph (#44048)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44048\n\nInline the fork-wait calls to make sure we can see the ops to be quantized in the main graph\n\nAlso fix the InlineForkWait JIT pass to account for the case where the aten::wait call isn't present in the main graph\nand we return future tensor from subgraph\n\nExample\n\n```\ngraph(%self.1 : __torch__.dper3.core.interop.___torch_mangle_6325.DperModuleWrapper,\n       %argument_1.1 : Tensor,\n       %argument_2.1 : Tensor):\n   %3 : Future[Tensor[]] = prim::fork_0(%self.1, %argument_1.1, %argument_2.1) # :0:0\n   return (%3)\n with prim::fork_0 = graph(%self.1 : __torch__.dper3.core.interop.___torch_mangle_5396.DperModuleWrapper,\n       %argument_1.1 : Tensor,\n       %argument_2.1 : Tensor):\n   %3 : __torch__.dper3.core.interop.___torch_mangle_6330.DperModuleWrapper = prim::GetAttr[name=\"x\"](%self.1)\n   %4 : __torch__.dper3.core.interop.___torch_mangle_5397.DperModuleWrapper = prim::GetAttr[name=\"y\"](%self.1)\n   %5 : __torch__.dper3.core.interop.___torch_mangle_6327.DperModuleWrapper = prim::GetAttr[name=\"z\"](%4)\n   %6 : Tensor = prim::CallMethod[name=\"forward\"](%5, %argument_1.1, %argument_2.1) # :0:0\n   %7 : None = prim::CallMethod[name=\"forward\"](%3, %6) # :0:0\n   %8 : Tensor[] = prim::ListConstruct(%6)\n   return (%8)\n```\n\nTest Plan:\npython test/test_quantization.py test_interface_with_fork\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23481003\n\nfbshipit-source-id: 2e756be73c248319da38e053f021888b40593032", "pr_number": "44048", "files_changed": ["test/quantization/test_quantize_jit.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/inline_fork_wait.cpp", "torch/csrc/jit/passes/quantization/insert_observers.cpp"], "labels": ["oncall: jit"]}, "396469f18c": {"title": "Explicitly forbidden the other inherited methods of RemoteModule. (#43895)", "body": "Summary:\nThrow exceptions when the methods except for forwardXXX are used.\n\nOriginal PR issue: RemoteModule enhancements #40550\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43895\n\nTest Plan: buck test test/distributed/rpc:process_group_agent -- RemoteModule\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23392842\n\nPulled By: SciPioneer\n\nfbshipit-source-id: 7c09a55a03f9f0b7e9f9264a42bfb907607f4651", "pr_number": "43895", "files_changed": ["torch/distributed/nn/api/remote_module.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["fb-exported"]}, "4fc29e9c43": {"title": "Revert D23519521: [dper3] replace LengthsGather lowlevel module's PT implemetnatio to use caffe2 op", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23519521 (https://github.com/pytorch/pytorch/commit/8c64bb4f4740b7d4441b8538a517fb1440b321c4)\n\nOriginal commit changeset: ed9bd16a8af3\n\nfbshipit-source-id: 33631299eabec05a1a272bfd0040d96203cf62a0", "pr_number": null, "files_changed": ["caffe2/operators/utility_ops.cc", "caffe2/operators/utility_ops.h"], "labels": []}, "719d29dab5": {"title": "Implement torch.i0 and torch.kaiser_window (#43132)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/38349\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43132\n\nReviewed By: smessmer\n\nDifferential Revision: D23479072\n\nPulled By: mruberry\n\nfbshipit-source-id: 4fb1de44830771c6a7222cf19f7728d9ac7c043b", "pr_number": "43132", "files_changed": ["aten/src/ATen/core/NamedRegistrations.cpp", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_bfloat16.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vec256_float_neon.h", "aten/src/ATen/cpu/vml.h", "aten/src/ATen/native/Distributions.h", "aten/src/ATen/native/Math.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["open source", "triaged"]}, "68297eeb1a": {"title": "Add support for integer dim arg in `torch.linalg.norm` (#43907)", "body": "Summary:\nSince PR https://github.com/pytorch/pytorch/issues/43262 is merged, this works now.\n\nPart of https://github.com/pytorch/pytorch/issues/24802\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43907\n\nReviewed By: anjali411\n\nDifferential Revision: D23471964\n\nPulled By: mruberry\n\nfbshipit-source-id: ef2f11f78343fc866f752c9691b0c1fa687353ba", "pr_number": "43907", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_linalg.py"], "labels": ["open source", "triaged"]}, "70c8daf439": {"title": "Apply selective build on RNN operators (#44132)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44132\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43985\n\nAdded\n```\ndef(detail::SelectiveStr<true>, ...)\nimpl(detail::SelectiveStr<true>, ...)\n```\nin torch/library, which can also be used for other templated selective registration.\n\nSize saves for this diff:\nfbios-pika: 78 KB\nigios: 87 KB\n\nTest Plan: Imported from OSS\n\nReviewed By: ljk53, smessmer\n\nDifferential Revision: D23459774\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 86d34cfe8e3f852602f203db06f23fa99af2c018", "pr_number": "44132", "files_changed": ["aten/src/ATen/native/RNN.cpp", "torch/library.h"], "labels": ["fb-exported"]}, "e358d516c8": {"title": "Revert D23549149: [PyTorch][Mobile] Insert the module name as `name()` to metadata dict if metadata doesn't contain \"model_name\"", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23549149 (https://github.com/pytorch/pytorch/commit/398409f07284cad25c500cc252518bbbf78f672e)\n\nOriginal commit changeset: fad742a8d4e6\n\nfbshipit-source-id: bd92a2033a804d3e6a2747b4fda4ca527991a993", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/module.cpp"], "labels": []}, "671160a963": {"title": "Revert D23557576: Revert D23519521: [dper3] replace LengthsGather lowlevel module's PT implemetnatio to use caffe2 op", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23557576\n\nOriginal commit changeset: 33631299eabe\n\nfbshipit-source-id: 704d36a16346f047b30e2da8be882062135f8617", "pr_number": null, "files_changed": ["caffe2/operators/utility_ops.cc", "caffe2/operators/utility_ops.h"], "labels": []}, "83a6e7d342": {"title": "Adds inequality testing aliases for better NumPy compatibility (#43870)", "body": "Summary:\nThis PR adds the following aliaes:\n\n- not_equal for torch.ne\n- greater for torch.gt\n- greater_equal for torch.ge\n- less for torch.lt\n- less_equal for torch.le\n\nThis aliases are consistent with NumPy's naming for these functions.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43870\n\nReviewed By: zou3519\n\nDifferential Revision: D23498975\n\nPulled By: mruberry\n\nfbshipit-source-id: 78560df98c9f7747e804a420c1e53fd1dd225002", "pr_number": "43870", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_op_aliases.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py"], "labels": ["module: numpy", "oncall: jit"]}, "bb861e1d69": {"title": "Ports CUDA var and std reduce all (with no out argument) to ATen, fixes var docs (#43858)", "body": "Summary:\nWhen var and std are called without args (other than unbiased) they currently call into TH or THC. This PR:\n\n- Removes the THC var_all and std_all functions and updates CUDA var and std to use the ATen reduction\n- Fixes var's docs, which listed its arguments in the incorrect order\n- Adds new tests comparing var and std with their NumPy counterparts\n\nPerformance appears to have improved as a result of this change. I ran experiments on 1D tensors, 1D tensors with every other element viewed ([::2]), 2D tensors and 2D transposed tensors. Some notable datapoints:\n\n- torch.randn((8000, 8000))\n  - var measured 0.0022215843200683594s on CUDA before the change\n  - var measured 0.0020322799682617188s on CUDA after the change\n- torch.randn((8000, 8000)).T\n  - var measured .015128850936889648 on CUDA before the change\n  - var measured 0.001912832260131836 on CUDA after the change\n- torch.randn(8000 ** 2)\n  - std measured 0.11031460762023926 on CUDA before the change\n  - std measured 0.0017833709716796875 on CUDA after the change\n\nTimings for var and std are, as expected, similar.\n\nOn the CPU, however, the performance change from making the analogous update was more complicated, and ngimel and I decided not to remove CPU var_all and std_all. ngimel wrote the following script that showcases how single-threaded CPU inference would suffer from this change:\n\n```\nimport torch\nimport numpy as np\nfrom torch.utils._benchmark import Timer\nfrom torch.utils._benchmark import Compare\nimport sys\nbase = 8\nmultiplier = 1\n\ndef stdfn(a):\n    meanv = a.mean()\n    ac = a-meanv\n    return torch.sqrt(((ac*ac).sum())/a.numel())\n\nresults = []\nnum_threads=1\nfor _ in range(7):\n    size = base*multiplier\n    input = torch.randn(size)\n\n    tasks = [(\"torch.var(input)\", \"torch_var\"),\n             (\"torch.var(input, dim=0)\", \"torch_var0\"),\n             (\"stdfn(input)\", \"stdfn\"),\n             (\"torch.sum(input, dim=0)\", \"torch_sum0\")\n            ]\n    timers = [Timer(stmt=stmt, num_threads=num_threads, label=\"Index\", sub_label=f\"{size}\",\n    description=label, globals=globals()) for stmt, label in tasks]\n    repeats = 3\n\n    for i, timer in enumerate(timers * repeats):\n        results.append(\n            timer.blocked_autorange()\n        )\n        print(f\"\\r{i + 1} / {len(timers) * repeats}\", end=\"\")\n        sys.stdout.flush()\n    multiplier *=10\nprint()\n\ncomparison = Compare(results)\n\ncomparison.print()\n```\n\nThe TH timings using this script on my devfair are:\n\n```\n[------------------------------ Index ------------------------------]\n        | torch_var | torch_var0 |  stdfn  | torch_sum0\n1 threads: ----------------------------------------------------------\n   8    |   16.0  |    5.6  |   40.9 |    5.0\n   80    |   15.9  |    6.1  |   41.6 |    4.9\n   800   |   16.7  |   12.0  |   42.3 |    5.0\n   8000   |   27.2  |   72.7  |   51.5 |    6.2\n   80000  |   129.0  |   715.0  |  133.0 |   18.0\n   800000  |  1099.8  |  6961.2  |  842.0 |   112.6\n   8000000 |  11879.8  |  68948.5  | 20138.4 |  1750.3\n```\n\nand the ATen timings are:\n\n```\n[------------------------------ Index ------------------------------]\n               |  torch_var  |  torch_var0  |   stdfn   |  torch_sum0\n1 threads: ----------------------------------------------------------\n      8              |       4.3   |       5.4    |     41.4  |       5.4\n      80            |       4.9   |       5.7    |     42.6  |       5.4\n      800          |      10.7   |      11.7    |     43.3  |       5.5\n      8000        |      69.3   |      72.2    |     52.8  |       6.6\n      80000      |     679.1   |     676.3    |    129.5  |      18.1\n      800000    |    6770.8   |    6728.8    |    819.8  |     109.7\n      8000000  |   65928.2   |   65538.7    |  19408.7  |    1699.4\n```\n\nwhich demonstrates that performance is analogous to calling the existing var and std with `dim=0` on a 1D tensor. This would be a significant performance hit. Another simple script shows the performance is mixed when using multiple threads, too:\n\n```\nimport torch\nimport time\n\n# Benchmarking var and std, 1D with varying sizes\nbase = 8\nmultiplier = 1\n\nop = torch.var\nreps = 1000\n\nfor _ in range(7):\n    size = base * multiplier\n    t = torch.randn(size)\n    elapsed = 0\n    for _ in range(reps):\n        start = time.time()\n        op(t)\n        end = time.time()\n        elapsed += end - start\n    multiplier *= 10\n\n    print(\"Size: \", size)\n    print(\"Avg. elapsed time: \", elapsed / reps)\n```\n\n```\nvar cpu TH vs ATen timings\n\nSize:  8\nAvg. elapsed time:  1.7853736877441406e-05 vs 4.9788951873779295e-06 (ATen wins)\nSize:  80\nAvg. elapsed time:  1.7803430557250977e-05 vs 6.156444549560547e-06 (ATen wins)\nSize:  800\nAvg. elapsed time:  1.8569469451904296e-05 vs 1.2302875518798827e-05 (ATen wins)\nSize:  8000\nAvg. elapsed time:  2.8756141662597655e-05 vs. 6.97789192199707e-05 (TH wins)\nSize:  80000\nAvg. elapsed time:  0.00026622867584228516 vs. 0.0002447957992553711 (ATen wins)\nSize:  800000\nAvg. elapsed time:  0.0010556647777557374 vs 0.00030616092681884767 (ATen wins)\nSize:  8000000\nAvg. elapsed time:  0.009990205764770508 vs 0.002938544034957886 (ATen wins)\n\nstd cpu TH vs ATen timings\n\nSize:  8\nAvg. elapsed time:  1.6681909561157225e-05 vs. 4.659652709960938e-06 (ATen wins)\nSize:  80\nAvg. elapsed time:  1.699185371398926e-05 vs. 5.431413650512695e-06 (ATen wins)\nSize:  800\nAvg. elapsed time:  1.768803596496582e-05 vs. 1.1279821395874023e-05 (ATen wins)\nSize:  8000\nAvg. elapsed time:  2.7791500091552735e-05  vs 7.031106948852539e-05 (TH wins)\nSize:  80000\nAvg. elapsed time:  0.00018650460243225096 vs 0.00024368906021118164 (TH wins)\nSize:  800000\nAvg. elapsed time:  0.0010522041320800782 vs 0.0003039860725402832 (ATen wins)\nSize:  8000000\nAvg. elapsed time:  0.009976618766784668 vs. 0.0029211788177490234 (ATen wins)\n```\n\nThese results show the TH solution still performs better than the ATen solution with default threading for some sizes.\n\nIt seems like removing CPU var_all and std_all will require an improvement in ATen reductions. https://github.com/pytorch/pytorch/issues/40570 has been updated with this information.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43858\n\nReviewed By: zou3519\n\nDifferential Revision: D23498981\n\nPulled By: mruberry\n\nfbshipit-source-id: 34bee046c4872d11c3f2ffa1b5beee8968b22050", "pr_number": "43858", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/generic/THCTensorMathReduce.cu", "test/backward_compatibility/check_backward_compatibility.py", "test/test_torch.py", "torch/_torch_docs.py"], "labels": []}, "ac1f471fe2": {"title": "Benchmarks: re-enable profiling-te configuration. (#44212)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44212\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23544563\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 98659e8860fa951d142e0f393731c4a769463c6c", "pr_number": "44212", "files_changed": [".jenkins/pytorch/test.sh", "benchmarks/fastrnns/conftest.py", "benchmarks/fastrnns/test_bench.py"], "labels": []}, "514f20ea51": {"title": "Histogram Binning Calibration", "body": "Summary:\nAdding a calibration module called histogram binning:\n\nDivide the prediction range (e.g., [0, 1]) into B bins. In each bin, use two parameters to store the number of positive examples and the number of examples that fall into this bucket. So we basically have a histogram for the model prediction.\n\nAs a result, for each bin, we have a statistical value for the real CTR (num_pos / num_example). We use this statistical value as the final calibrated prediction if the pre-cali prediction falls into the corresponding bin.\n\nIn this way, the predictions within each bin should be well-calibrated if we have sufficient examples. That is, we have a fine-grained calibrated model by this calibration module.\n\nTheoretically, this calibration layer can fix any uncalibrated model or prediction if we have sufficient bins and examples. It provides the potential to use any kind of training weight allocation to our training data, without worrying about the calibration issue.\n\nTest Plan:\nbuck test dper3/dper3/modules/calibration/tests:calibration_test -- test_histogram_binning_calibration\n\nbuck test dper3/dper3_models/ads_ranking/tests:model_paradigm_e2e_tests -- test_sparse_nn_histogram_binning_calibration\n\nAll tests passed.\n\nExample workflows:\nf215431958\n\n{F326445092}\n\nf215445048\n\n{F326445223}\n\nReviewed By: chenshouyuan\n\nDifferential Revision: D23356450\n\nfbshipit-source-id: c691b66c51ef33908c17575ce12e5bee5fb325ff", "pr_number": null, "files_changed": ["caffe2/operators/reduction_ops.h", "caffe2/operators/scale_op.h", "caffe2/operators/sqrt_op.cc", "caffe2/operators/utility_ops.h", "caffe2/utils/math_cpu.cc"], "labels": []}, "1b2da9ed82": {"title": "Expose alias key info in dumpState and update test_dispatch. (#44081)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44081\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23492794\n\nPulled By: ailzhang\n\nfbshipit-source-id: 27a2978591900463bda2e92e0201c9fd719f9792", "pr_number": "44081", "files_changed": ["aten/src/ATen/core/dispatch/OperatorEntry.cpp", "c10/core/DispatchKey.h", "test/test_dispatch.py", "torch/csrc/utils/python_dispatch.cpp"], "labels": []}, "626e410e1d": {"title": "Revert D23544563: Benchmarks: re-enable profiling-te configuration.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23544563 (https://github.com/pytorch/pytorch/commit/ac1f471fe20faf96282b4143e4193759f01a7d41)\n\nOriginal commit changeset: 98659e8860fa\n\nfbshipit-source-id: 5dab7044699f59c709e64d178758f5f462ebb788", "pr_number": null, "files_changed": [".jenkins/pytorch/test.sh", "benchmarks/fastrnns/conftest.py", "benchmarks/fastrnns/test_bench.py"], "labels": []}, "10dd25dcd1": {"title": "Add binary ops for _foreach APIs (#42536)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42536\n\n[First PR: Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar)](https://github.com/pytorch/pytorch/pull/41554).\n\n**Motivation**\n[GitHub issue](https://github.com/pytorch/pytorch/issues/38655)\nCurrent PyTorch optimizer implementations are not efficient in cases when we work with a lot of small feature tensors. Starting a lot of kernels slows down the whole process. We need to reduce the number of kernels that we start.\nAs an example, we should be looking at [NVIDIAs Apex](https://github.com/NVIDIA/apex).\nIn order to track progress, we will pick PyTorchs DCGAN model with Adam optimizer and once the optimizer is reimplemented with tensor lists, benchmark the model performance against original model version, Apexs version with original Adam optimizer and it\u2019s FusedAdam optimizer.\n\n**Current API restrictions**\n- List can't be empty (will fixed in upcoming PRs).\n- All tensors in the list must have the same dtype, device and size.\n\n**Broadcasting**\nAt this point we don't support broadcasting.\n\n**What is 'Fast' and 'Slow' route**\nIn particular cases, we cant process an op with a fast list CUDA kernel. Still, we can do with a regular for-loop where the op will be applied to each tensor individually through the dispatch mechanisms. There are a few checks that decide whether the op will be performed via a 'fast' or 'slow' path.\nTo go the fast route,\n- All tensors must have strided layout\n- All tensors must be dense and not have overlapping memory\n- The resulting tensor type must be the same.\n\n----------------\n**In this PR**\nAdding APIs:\n```\ntorch._foreach_sub(TensorList tl1, TensorList tl2)\ntorch._foreach_sub_(TensorList self, TensorList tl2)\ntorch._foreach_mul(TensorList tl1, TensorList tl2)\ntorch._foreach_mul_(TensorList self, TensorList tl2)\ntorch._foreach_div(TensorList tl1, TensorList tl2)\ntorch._foreach_div_(TensorList self, TensorList tl2)\n\ntorch._foreach_sub(TensorList tl1, Scalar scalar)\ntorch._foreach_sub_(TensorList self, Scalar scalar)\ntorch._foreach_mul(TensorList tl1, Scalar scalar)\ntorch._foreach_mul_(TensorList self, Scalar scalar)\ntorch._foreach_div(TensorList tl1, Scalar scalar)\ntorch._foreach_div(TensorList self, Scalar scalar)\n```\n\n**Tests**\nTested via unit tests\n\n**TODO**\n1. Properly handle empty lists\n2. Properly handle bool tensors\n\n**Plan for the next PRs**\n1. APIs\n- Unary Ops for list\n- Pointwise Ops\n\n2. Complete tasks from TODO\n3. Rewrite PyTorch optimizers to use for-each operators for performance gains.\n\nTest Plan: Imported from OSS\n\nReviewed By: cpuhrsch\n\nDifferential Revision: D23331891\n\nPulled By: izdeby\n\nfbshipit-source-id: 18c5937287e33e825b2e391e41864dd64e226f19", "pr_number": "42536", "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/cuda/ForeachBinaryOpList.cu", "aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu", "aten/src/ATen/native/cuda/ForeachFunctors.cuh", "aten/src/ATen/native/cuda/ForeachTensorAddList.cu", "aten/src/ATen/native/cuda/ForeachTensorAddScalar.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_foreach.py", "tools/codegen/model.py"], "labels": ["oncall: jit"]}, "589a2024c8": {"title": "Benchmarks: re-enable profiling-te configuration (try 2). (#44270)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44270\n\nThe previous PR (#44212) was reverted since I didn't update the\n`upload_scribe.py` script and it was looking for 'executor_and_fuser'\nfield in the json which now is replaced with two separate fields:\n'executor' and 'fuser'.\n\nDifferential Revision: D23561500\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 7fe86d34afa488a0e43d5ea2aaa7bc382337f470", "pr_number": "44270", "files_changed": [".jenkins/pytorch/test.sh", "benchmarks/fastrnns/conftest.py", "benchmarks/fastrnns/test_bench.py", "benchmarks/upload_scribe.py"], "labels": []}, "5d748e6d22": {"title": "[TensorExpr] Re-enable tests. (#44218)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44218\n\nDifferential Revision: D23546100\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 4c4c5378ec9891ef72b60ffb59081a009e0df049", "pr_number": "44218", "files_changed": ["test/test_tensorexpr.py"], "labels": []}, "0e64b02912": {"title": "FindCUDA error handling (#44236)", "body": "Summary:\nCheck return code of `nvcc --version` and if it's not zero, print warning and mark CUDA as not found.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44236\n\nTest Plan: Run `CUDA_NVCC_EXECUTABLE=/foo/bar cmake ../`\n\nReviewed By: ezyang\n\nDifferential Revision: D23552336\n\nPulled By: malfet\n\nfbshipit-source-id: cf9387140a8cdbc8dab12fcc4bfaf55ae8e6a502", "pr_number": "44236", "files_changed": ["cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake"], "labels": []}, "7c61f57bec": {"title": "test_ops: skipTest only takes a single argument (#44181)", "body": "Summary:\nFixes a broken skipTest from https://github.com/pytorch/pytorch/issues/43451, e.g. in the ROCm CI.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44181\n\nReviewed By: ngimel\n\nDifferential Revision: D23568608\n\nPulled By: malfet\n\nfbshipit-source-id: 557048bd5f0086ffac38d1c48255badb63869899", "pr_number": "44181", "files_changed": ["test/test_ops.py"], "labels": ["open source"]}, "6134ac17ba": {"title": "Revert D23561500: Benchmarks: re-enable profiling-te configuration (try 2).", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23561500 (https://github.com/pytorch/pytorch/commit/589a2024c8f6c7a986cab61a42452f2e8cc39f5e)\n\nOriginal commit changeset: 7fe86d34afa4\n\nfbshipit-source-id: 10e48f230402572fcece56662ad4413ac0bd3cb5", "pr_number": null, "files_changed": [".jenkins/pytorch/test.sh", "benchmarks/fastrnns/conftest.py", "benchmarks/fastrnns/test_bench.py", "benchmarks/upload_scribe.py"], "labels": []}, "cce5982c4c": {"title": "Add unary ops: exp and sqrt (#42537)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42537\n\n[First PR: Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar)](https://github.com/pytorch/pytorch/pull/41554).\n\n**Motivation**\n[GitHub issue](https://github.com/pytorch/pytorch/issues/38655)\nCurrent PyTorch optimizer implementations are not efficient in cases when we work with a lot of small feature tensors. Starting a lot of kernels slows down the whole process. We need to reduce the number of kernels that we start.\nAs an example, we should be looking at [NVIDIAs Apex](https://github.com/NVIDIA/apex).\nIn order to track progress, we will pick PyTorchs DCGAN model with Adam optimizer and once the optimizer is reimplemented with tensor lists, benchmark the model performance against original model version, Apexs version with original Adam optimizer and it\u2019s FusedAdam optimizer.\n\n**Current API restrictions**\n- List can't be empty (will fixed in upcoming PRs).\n- All tensors in the list must have the same dtype, device and size.\n\n**Broadcasting**\nAt this point we don't support broadcasting.\n\n**What is 'Fast' and 'Slow' route**\nIn particular cases, we cant process an op with a fast list CUDA kernel. Still, we can do with a regular for-loop where the op will be applied to each tensor individually through the dispatch mechanisms. There are a few checks that decide whether the op will be performed via a 'fast' or 'slow' path.\nTo go the fast route,\n- All tensors must have strided layout\n- All tensors must be dense and not have overlapping memory\n- The resulting tensor type must be the same.\n\n----------------\n**In this PR**\nAdding APIs:\n```\ntorch._foreach_exp(TensorList tl1)\ntorch._foreach_exp_(TensorList tl1)\ntorch._foreach_sqrt(TensorList tl1)\ntorch._foreach_sqrt_(TensorList tl1)\n```\n\n**Tests**\nTested via unit tests\n\n**TODO**\n1. Properly handle empty lists\n2. Properly handle bool tensors\n\n**Plan for the next PRs**\n1. APIs\n- Pointwise Ops\n\n2. Complete tasks from TODO\n3. Rewrite PyTorch optimizers to use for-each operators for performance gains.\n\nTest Plan: Imported from OSS\n\nReviewed By: cpuhrsch\n\nDifferential Revision: D23331889\n\nPulled By: izdeby\n\nfbshipit-source-id: 8b04673b8412957472ed56361954ca3884eb9376", "pr_number": "42537", "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/cuda/ForeachFunctors.cuh", "aten/src/ATen/native/cuda/ForeachPointwiseOp.cu", "aten/src/ATen/native/cuda/ForeachUnaryOp.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_foreach.py", "tools/codegen/model.py"], "labels": ["oncall: jit"]}, "5de805d8a7": {"title": "[dper3] Export Caffe2 operator LearningRate to PyTorch", "body": "Summary: Exports the operator to PyTorch, to be made into a low-level module.\n\nTest Plan:\n```\nbuck test //caffe2/caffe2/python/operator_test:torch_integration_test -- test_learning_rate\n```\n\nReviewed By: yf225\n\nDifferential Revision: D23545582\n\nfbshipit-source-id: 6b6d9aa6a47b2802ccef0f87c1263c6cc2d2fdf6", "pr_number": null, "files_changed": ["caffe2/python/operator_test/torch_integration_test.py", "caffe2/sgd/learning_rate_op.cc", "caffe2/sgd/learning_rate_op.h"], "labels": []}, "8d212d3f7a": {"title": "add 'run_duration' stats for binary builds to scuba (#44251)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44251\n\nReviewed By: seemethere\n\nDifferential Revision: D23575312\n\nPulled By: walterddr\n\nfbshipit-source-id: 29d737f5bee1540d6595d4d0ca1386b9ce5ab2ee", "pr_number": "44251", "files_changed": [".circleci/scripts/upload_binary_size_to_scuba.py"], "labels": []}, "de980f937b": {"title": "skip test_tanhquantize for now (#44312)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44312\n\nThis test is failing now when running on card. Let's disable it while Intel is investigating the issue.\n\nTest Plan: Sandcastle\n\nReviewed By: hyuen\n\nDifferential Revision: D23577475\n\nfbshipit-source-id: 84f957c69ed75e0e0f563858b8b8ad7a2158da4e", "pr_number": "44312", "files_changed": ["caffe2/contrib/fakelowp/test/test_fusions.py"], "labels": ["fb-exported"]}, "fd8e2064e0": {"title": "quant: switch observers to use min_max (#42957)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42957\n\nSwitches observers to use the new min_max function to calculate\nmin and max at the same time.  We see around 45-50% speedup on\nrepresentative input shapes on the microbenchmarks for all observers except `HistogramObserver`.\n\nTest Plan:\nCI for correctness\n\nperformance:\n```\ncd benchmarks/operator_benchmark\n// repeat (before diff, after diff) x (cpu, cuda)\npython -m pt.qobserver_test --tag_filter all --device cpu\n/*\n    * before, cpu: https://our.intern.facebook.com/intern/paste/P138633280/\n    * before, cuda: https://our.intern.facebook.com/intern/paste/P138639473/\n    * after, cpu: https://our.intern.facebook.com/intern/paste/P138635458/\n    * after, cuda: https://our.intern.facebook.com/intern/paste/P138636344/\n*/\n```\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23093995\n\nfbshipit-source-id: 9f416d144109b5b80baf089eb4bcfabe8fe358d5", "pr_number": "42957", "files_changed": ["torch/quantization/observer.py"], "labels": []}, "4e0ac120e9": {"title": "[FX] Only copy over training attr if it\\'s there (#44314)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44314\n\nTest Plan: Imported from OSS\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23578189\n\nPulled By: jamesr66a\n\nfbshipit-source-id: fb7643f28582bd5009a826663a937fbe188c50bc", "pr_number": "44314", "files_changed": ["torch/fx/graph_module.py"], "labels": ["fx"]}, "caf23d110f": {"title": "[JIT] Unshare types for modules that define() in __init__ (#44233)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44233\n\n**Summary**\nBy default, scripting tries to share concrete and JIT types across\ncompilations. However, this can lead to incorrect results if a module\nextends `torch.jit.ScriptModule`, and injects instance variables into\nmethods defined using `define`.\n\nThis commit detects when this has happened and disables type sharing\nfor the compilation of the module that uses `define` in `__init__`.\n\n**Test Plan**\nThis commit adds a test to TestTypeSharing that tests this scenario.\n\n**Fixes**\nThis commit fixes #43580.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23553870\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: d756e87fcf239befa0012998ce29eeb25728d3e1", "pr_number": "44233", "files_changed": ["test/jit/test_type_sharing.py", "torch/jit/_script.py"], "labels": ["oncall: jit"]}, "477f489137": {"title": "Don't register a fallback for private use to let extensions do it themselves (#44149)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44149\n\nThanks Christian Puhrsch for reporting.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: izdeby\n\nDifferential Revision: D23574739\n\nPulled By: ezyang\n\nfbshipit-source-id: 8c9d0d78e6970139e0103cd1e0004b743e3c7f9e", "pr_number": "44149", "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp"], "labels": []}, "de89261abe": {"title": "Reduce `sccache` log levels for RocM to a default state (#44310)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44310\n\nReviewed By: walterddr\n\nDifferential Revision: D23576966\n\nPulled By: malfet\n\nfbshipit-source-id: c7fa063ec2be92de8f3768aaa3e6a032913004f7", "pr_number": "44310", "files_changed": [".jenkins/caffe2/common.sh", ".jenkins/pytorch/common.sh"], "labels": []}, "63d62d3e44": {"title": "Skips test_addcmul_cuda if using ROCm (#44304)", "body": "Summary:\nThis test is failing consistently on linux-bionic-rocm3.7-py3.6-test2. Relevant log snippet:\n\n```\n03:43:11 FAIL: test_addcmul_cuda_float16 (__main__.TestForeachCUDA)\n03:43:11 ----------------------------------------------------------------------\n03:43:11 Traceback (most recent call last):\n03:43:11   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 818, in wrapper\n03:43:11     method(*args, **kwargs)\n03:43:11   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py\", line 258, in instantiated_test\n03:43:11     result = test(self, *args)\n03:43:11   File \"test_foreach.py\", line 83, in test_addcmul\n03:43:11     self._test_pointwise_op(device, dtype, torch._foreach_addcmul, torch._foreach_addcmul_, torch.addcmul)\n03:43:11   File \"test_foreach.py\", line 58, in _test_pointwise_op\n03:43:11     self.assertEqual(tensors, expected)\n03:43:11   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1153, in assertEqual\n03:43:11     exact_dtype=exact_dtype, exact_device=exact_device)\n03:43:11   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py\", line 1127, in assertEqual\n03:43:11     self.assertTrue(result, msg=msg)\n03:43:11 AssertionError: False is not true : Tensors failed to compare as equal! With rtol=0.001 and atol=1e-05, found 10 element(s) (out of 400) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.00048828125 (-0.46484375 vs. -0.46533203125), which occurred at index (11, 18).\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44304\n\nReviewed By: malfet, izdeby\n\nDifferential Revision: D23578316\n\nPulled By: mruberry\n\nfbshipit-source-id: 558eecf42677383e7deaa4961e12ef990ffbe28c", "pr_number": "44304", "files_changed": ["test/test_foreach.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "54931ebb7b": {"title": "Release saved variable from DifferentiableGraphBackward (#42994)", "body": "Summary:\nWhen the backward ops execute via the autograd engine evaluate_function(), the fn.release_variables() is called to release the SavedVariables. For the eager mode ops, this releases the saved inputs that was required for backward grad function. However, with TorchScript, we get a DifferentableGraph and the DifferentiableGraphBackward() doesn't implement a release_variables(). This leads to the SavedVariables to be alive longer. Implement release_variables() for DifferentiableGraphBackward to release these SavedVariables  early.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42994\n\nReviewed By: izdeby\n\nDifferential Revision: D23503172\n\nPulled By: albanD\n\nfbshipit-source-id: d87127498cfa72883ae6bb31d0e6c7056c4c36d4", "pr_number": "42994", "files_changed": ["test/jit/test_profiler.py", "test/test_jit.py", "torch/csrc/jit/runtime/graph_executor.cpp"], "labels": ["oncall: jit", "open source", "triaged"]}, "47ac9bb105": {"title": "Enable temp disabled tests in test_jit_fuser_te.py (#44222)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44222\n\nReviewed By: izdeby\n\nDifferential Revision: D23582214\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 27caa3ea02ce10b163212f6a45a81b446898953d", "pr_number": "44222", "files_changed": ["test/test_jit_fuser_te.py"], "labels": []}, "1fcccd6a18": {"title": "[FX] Minor fixups in Graph printout (#44214)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44214\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D23545501\n\nPulled By: jamesr66a\n\nfbshipit-source-id: dabb3b051ed4da213b2087979ade8a649288bd5d", "pr_number": "44214", "files_changed": ["torch/fx/graph.py"], "labels": ["fx"]}, "49e979bfde": {"title": "Set default compiler differently according to platform (#43890)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43890\n\n1. auto-detect `CXX` default compiler type in oss, and `clang` as default compiler type in fbcode (because auto-detecting will say `gcc` is the default compiler on devserver).\n\n2. change `compiler type` from str `\"CLANG\" \"GCC\"` to enum type\n3. rename function `get_cov_type` to `detect_compiler_type`\n4. auto-set the default pytorch folder for users in oss\n\nTest Plan:\non devserver:\n```\nbuck run :coverage //caffe2/c10:\n```\n\non oss:\n```\npython oss_coverage.py --run-only=atest\n```\n\nReviewed By: malfet\n\nDifferential Revision: D23420034\n\nfbshipit-source-id: c0ea88188578bb1343a286f2090eb8a74cdf3982", "pr_number": "43890", "files_changed": ["tools/code_coverage/README.md", "tools/code_coverage/package/oss/cov_json.py", "tools/code_coverage/package/oss/init.py", "tools/code_coverage/package/oss/utils.py", "tools/code_coverage/package/tool/summarize_jsons.py", "tools/code_coverage/package/util/setting.py", "tools/code_coverage/package/util/utils.py"], "labels": ["fb-exported"]}, "43e38d60d6": {"title": "[quant][graphmode][fx] Support quantize per channel in all cases (#44042)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44042\n\nMissed one case last time\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23479345\n\nfbshipit-source-id: 30e6713120c494e9fab5584de4df9b25bec83d32", "pr_number": "44042", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py", "torch/quantization/fx/utils.py"], "labels": ["fx"]}, "6dd53fb58d": {"title": "[fix] output of `embedding_bag` with non-contiguous weight (#44032)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43723\n\nuse weight.contiguous on fast-path as it expects contiguous tensor.\n\nTODO:\n* [x] Add tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44032\n\nReviewed By: izdeby\n\nDifferential Revision: D23502200\n\nPulled By: glaringlee\n\nfbshipit-source-id: 4a7b546b3e8b1ad35c287a634b4e990a1ccef874", "pr_number": "44032", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "test/test_nn.py"], "labels": ["module: operators (deprecated)", "open source", "triaged"]}, "00b5bd536f": {"title": "fx quant: add docblocks to _find_matches and _find_quants (#43928)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43928\n\nImproving readability, no logic change.\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23440249\n\nfbshipit-source-id: a7ebfc7ad15c73e26b9a94758e7254413cc17d29", "pr_number": "43928", "files_changed": ["torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "40d138f7c1": {"title": "Added alpha overloads for add/sub ops with lists (#43413)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43413\n\nTest Plan: Imported from OSS\n\nReviewed By: cpuhrsch\n\nDifferential Revision: D23331896\n\nPulled By: izdeby\n\nfbshipit-source-id: 2e7484339fec533e21224f18979fddbeca649d2c", "pr_number": "43413", "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/cuda/ForeachBinaryOpList.cu", "aten/src/ATen/native/cuda/ForeachFunctors.cuh", "aten/src/ATen/native/cuda/ForeachPointwiseOp.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_foreach.py"], "labels": []}, "0351d31722": {"title": "add rocm nightly build (#44250)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44250\n\nReviewed By: izdeby\n\nDifferential Revision: D23585431\n\nPulled By: walterddr\n\nfbshipit-source-id: c798707f5cb55f720e470bc40f30ab82718e0ddf", "pr_number": "44250", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/cimodel/data/binary_build_definitions.py", ".circleci/cimodel/data/dimensions.py", ".circleci/config.yml", ".circleci/scripts/binary_linux_build.sh"], "labels": ["ci/binaries"]}, "9f54bcc522": {"title": "[quant][graphmode][fx] Support inplace option (#43983)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43983\n\nSupport inplace option in apis\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23459260\n\nfbshipit-source-id: 80409c7984f17d1a4e13fb1eece8e18a69ee43b3", "pr_number": "43983", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/quantize.py", "torch/quantization/quantize_fx.py"], "labels": ["fx"]}, "be94dba429": {"title": "[NNC] fix support for FP16 in CudaCodgen (#44209)", "body": "Summary:\nFixes a bug where FP16 values could be incorrectly cast to a half type that doesn't have a cast operator by inserting the cuda specific cast to float during handling of the Cast node, not as a wrapper around printing Loads and Stores. Two main changes: the HalfChecker now inserts the casts to float explicitly in the IR, and the PrioritizeLoad mutator now consumes both Loads and a Cast which immediately preceded a load.\n\nTested with test_jit_fuser_te.py and test_tensorexpr.py, plus C++ tests obv.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44209\n\nReviewed By: izdeby\n\nDifferential Revision: D23575577\n\nPulled By: nickgg\n\nfbshipit-source-id: 808605aeb2af812758f96f9fdc11b07e08053b46", "pr_number": "44209", "files_changed": ["test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/tests.h", "test/test_jit_fuser_te.py", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_half_support.h"], "labels": ["oncall: jit"]}, "6269b6e0f0": {"title": "[quant][graphmode][fx][api] Call fuse in prepare (#43984)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43984\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23459261\n\nfbshipit-source-id: 6b56b0916d76df67b9cc2f4be1fcee905d604019", "pr_number": "43984", "files_changed": ["torch/quantization/quantize_fx.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "f9146b4598": {"title": "fix lint (#44346)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44346\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23589324\n\nPulled By: eellison\n\nfbshipit-source-id: a4e22b69196909ec200ac3e262f04d2aaf78e9cf", "pr_number": "44346", "files_changed": ["tools/code_coverage/package/oss/utils.py", "tools/code_coverage/package/tool/summarize_jsons.py"], "labels": []}, "f27be2f781": {"title": "[caffe2] fix wrong comment (#42735)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42735\n\nWe use reduced precision only for embedding table (not for momentum) in RowWiseSparseAdagrad\n\nTest Plan: .\n\nReviewed By: jianyuh\n\nDifferential Revision: D23003939\n\nfbshipit-source-id: 062290d94b160100bc4c2f48b797833819f8e88a", "pr_number": "42735", "files_changed": ["caffe2/sgd/rowwise_adagrad_fused.h"], "labels": ["fb-exported"]}, "6013a29fc0": {"title": "[quant] Support quantization of embedding lookup operators (#44207)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44207\n\nUse existing embedding_bag operator but set offsets to [0, 1, .. len(indices)]\n\nTest Plan:\npython test/test_quantization.py TestEmbeddingOps.test_embedding_byte\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23547385\n\nfbshipit-source-id: ccce348bc192c6a4a65a8eca4c8b90f99f40f1b1", "pr_number": "44207", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py", "test/test_quantization.py"], "labels": []}, "57b87aaf59": {"title": "[quant] Add quantized Embedding module (#44208)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44208\n\nAdd quantized module in static quantization namespace. Embedding\nquantization requires only weights to be quantized so it is static.\nInternally it calls the embedding_bag_byte op with the offsets set corresponding to the\nindices.\n\nFuture PR will move EmbeddingBag quantization from dynamic to static as well.\n\nTest Plan:\npython test/test_quantization.py test_embedding_api\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23547384\n\nfbshipit-source-id: eddc6fb144b4a771060e7bab5853656ccb4443f0", "pr_number": "44208", "files_changed": ["test/quantization/test_quantize.py", "test/quantization/test_quantized_module.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/embedding.py", "torch/quantization/default_mappings.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "646ffd4886": {"title": "[quant] Move EmbeddingBag eager quantization to static (#44217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44217\n\nMove the tests to static ones as well\n\nTest Plan:\npython test/test_quantization.py TestStaticQuantizedModule.test_embedding_bag_api\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23547386\n\nfbshipit-source-id: 41f81c31e1613098ecf6a7eff601c7dcd4b09c76", "pr_number": "44217", "files_changed": ["test/quantization/test_quantize.py", "test/quantization/test_quantized_module.py", "torch/nn/quantized/dynamic/modules/__init__.py", "torch/nn/quantized/dynamic/modules/embeddingbag.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/embedding.py", "torch/nn/quantized/modules/embedding_ops.py", "torch/quantization/default_mappings.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "bd8e38cd88": {"title": "[TensorExpr] Fuser: check node inputs' device before merging the node into a fusion group. (#44241)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44241\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23554192\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: fb03262520303152b83671603e08e7aecc24f5f2", "pr_number": "44241", "files_changed": ["test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["oncall: jit"]}, "ecc6358dbe": {"title": "Port nonzero cuda from THC to ATen (#44259)", "body": "Summary:\n1) Ports nonzero from THC to ATen\n2) replaces most thrust uses with cub, to avoid synchronization and to improve performance. There is still one necessary synchronization point, communicating number of nonzero elements from GPU to CPU\n3) slightly changes algorithm, now we first compute the number of nonzeros, and then allocate correct-sized output, instead of allocating full-sized output as was done before, to account for possibly all elements being non-zero\n4) unfortunately, since the last transforms are still done with thrust, 2) is slightly beside the point, however it is a step towards a future without thrust\n4) hard limits the number of elements in the input tensor to MAX_INT. Previous implementation allocated a Long tensor with the size ndim*nelements, so that would be at least 16 GB for a tensor with MAX_INT elements. It is reasonable to say that larger tensors could not be used anyway.\n\nBenchmarking is done for tensors with approximately half non-zeros\n<details><summary>Benchmarking script</summary>\n<p>\n\n```\nimport torch\nfrom torch.utils._benchmark import Timer\nfrom torch.utils._benchmark import Compare\nimport sys\n\ndevice = \"cuda\"\nresults = []\nfor numel in (1024 * 128,):#, 1024 * 1024, 1024 * 1024 * 128):\n    inp = torch.randint(2, (numel,), device=\"cuda\", dtype=torch.float)\n    for ndim in range(2,3):#(1,4):\n        if ndim == 1:\n            shape = (numel,)\n        elif ndim == 2:\n            shape = (1024, numel // 1024)\n        else:\n            shape = (1024, 128, numel // 1024 // 128)\n        inp = inp.reshape(shape)\n        repeats = 3\n        timer = Timer(stmt=\"torch.nonzero(inp, as_tuple=False)\", label=\"Nonzero\", sub_label=f\"number of elts {numel}\",\n        description = f\"ndim {ndim}\", globals=globals())\n        for i in range(repeats):\n            results.append(timer.blocked_autorange())\n        print(f\"\\rnumel {numel} ndim {ndim}\", end=\"\")\n        sys.stdout.flush()\n\ncomparison = Compare(results)\ncomparison.print()\n```\n</p>\n</details>\n\n### Results\nBefore:\n```\n[--------------------------- Nonzero ---------------------------]\n                                 |  ndim 1  |   ndim 2  |   ndim 3\n 1 threads: ------------------------------------------------------\n       number of elts 131072     |    55.2  |     71.7  |     90.5\n       number of elts 1048576    |   113.2  |    250.7  |    497.0\n       number of elts 134217728  |  8353.7  |  23809.2  |  54602.3\n\n Times are in microseconds (us).\n```\nAfter:\n```\n[-------------------------- Nonzero --------------------------]\n                                |  ndim 1  |  ndim 2  |  ndim 3\n1 threads: ----------------------------------------------------\n      number of elts 131072     |    48.6  |    79.1  |    90.2\n      number of elts 1048576    |    64.7  |   134.2  |   161.1\n      number of elts 134217728  |  3748.8  |  7881.3  |  9953.7\n\nTimes are in microseconds (us).\n\n```\nThere's a real regression for smallish 2D tensor due to added work of computing number of nonzero elements, however, for other sizes there are significant gains, and there are drastically lower memory requirements. Perf gains would be even larger for tensors with fewer nonzeros.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44259\n\nReviewed By: izdeby\n\nDifferential Revision: D23581955\n\nPulled By: ngimel\n\nfbshipit-source-id: 0b99a767fd60d674003d83f0848dc550d7a363dc", "pr_number": "44259", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/THCTensorMath.cu", "aten/src/THC/generic/THCTensorMath.cu", "aten/src/THC/generic/THCTensorMath.h", "aten/src/THC/generic/THCTensorRandom.cu", "test/test_torch.py", "torch/_torch_docs.py"], "labels": []}, "4aacfab221": {"title": "Resolve Autograd key for disable_variable_dispatch flag. (#44268)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44268\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23561042\n\nPulled By: ailzhang\n\nfbshipit-source-id: 6f35cd9a543bea3f9e294584f1db7c3622ebb741", "pr_number": "44268", "files_changed": ["aten/src/ATen/templates/TensorBody.h", "c10/core/impl/LocalDispatchKeySet.cpp"], "labels": []}, "1d01fcdc24": {"title": "[quant] fill_ path for quantized tensors (#43303)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43303\n\nTest Plan: Imported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23231947\n\nPulled By: z-a-f\n\nfbshipit-source-id: fd5110ff15a073f326ef590436f8c6e5a2608324", "pr_number": "43303", "files_changed": ["aten/src/ATen/native/Fill.cpp", "test/quantization/test_quantized_tensor.py"], "labels": []}, "b22abbe381": {"title": "Enable test_distributed to work with spawn mode (#41769)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41769\n\nCurrently the tests in `test_distributed` only work with the `fork` mode multiprocessing, this PR introduces support for `spawn` mode multiprocessing as well (while keeping the `fork` mode intact).\n\nMotivations for the change:\n1) Spawn multiprocessing is the default on MacOS, so it better emulates how MacOS users would use distributed\n2) With python 3.8+, spawn is the default on linux, so we should have test coverage for this\n3) PT multiprocessing suggests using spawn/forkserver over fork, for sharing cuda tensors: https://pytorch.org/docs/stable/multiprocessing.html\n4) Spawn is better supported with respect to certain sanitizers such as TSAN, so adding this sanitizer coverage may help us uncover issues.\n\nHow it is done:\n1) Move `test_distributed` tests in `_DistTestBase` class to a shared file `distributed_test` (similar to how the RPC tests are structured)\n2) For `Barrier`, refactor the setup of temp directories, as the current version did not work with spawn, each process would get a different randomly generated directory and thus would write to different barriers.\n3) Add all the relevant builds to run internally and in OSS.\nRunning test_distributed with spawn mode in OSS can be done with:\n`python test/run_test.py -i distributed/test_distributed_spawn -v`\n\nReviewed By: izdeby\n\nDifferential Revision: D22408023\n\nfbshipit-source-id: e206be16961fd80438f995e221f18139d7e6d2a9", "pr_number": "41769", "files_changed": ["test/distributed/test_distributed.py", "test/distributed/test_distributed_spawn.py", "test/run_test.py", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": []}, "106459acac": {"title": "Rename test_distributed to test_distributed_fork (#42932)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42932\n\nFollow up from https://github.com/pytorch/pytorch/pull/41769, rename `test_distributed` to `test_distributed_fork` to make it explicit that it forks.\n\nNew command to run test:\n`python test/run_test.py -i distributed/test_distributed_fork -v`\nghstack-source-id: 111632568\n\nTest Plan: `python test/run_test.py -i distributed/test_distributed_fork -v`\n\nReviewed By: izdeby\n\nDifferential Revision: D23072201\n\nfbshipit-source-id: 48581688b6c5193a309e803c3de38e70be980872", "pr_number": "42932", "files_changed": [".jenkins/pytorch/multigpu-test.sh", "test/distributed/test_distributed.py", "test/distributed/test_distributed_fork.py", "test/run_test.py", "test/test_determination.py"], "labels": []}, "135ebbde6d": {"title": "[Caffe2] Add RMSNormOp (#44338)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44338\n\nAdd RMSNormOp in Caffe2\n\nTest Plan: buck test mode/dev-nosan //caffe2/caffe2/python/operator_test:rms_norm_op_test\n\nReviewed By: houseroad\n\nDifferential Revision: D23546424\n\nfbshipit-source-id: 8f3940a0bb42230bfa647dc66b5e359cc84491c6", "pr_number": "44338", "files_changed": ["caffe2/operators/rms_norm_op.cc", "caffe2/operators/rms_norm_op.cu", "caffe2/operators/rms_norm_op.h", "caffe2/python/operator_test/rms_norm_op_test.py"], "labels": ["fb-exported"]}, "7a64b0c27a": {"title": "Export Node::isBefore/isAfter for PythonAPI (#44162)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44162\n\nThis diff exports Node::isBefore/isAfter method to PythonAPI.\n\nTest Plan: Tested locally. Please let me know if there is a set of unit tests to be passed.\n\nReviewed By: soumith\n\nDifferential Revision: D23514448\n\nfbshipit-source-id: 7ef709b036370217ffebef52fd93fbd68c464e89", "pr_number": "44162", "files_changed": ["torch/csrc/jit/python/python_ir.cpp"], "labels": ["oncall: jit"]}, "8acce55015": {"title": "Dump optimized graph when logging in already-optimized PE (#44315)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44315\n\nI find it more intuitive to dump the optimized graph if we have one;\nwhen I first saw the unoptimized graph being dumped I thought we had failed to\napply any optimizations.\n\nTest Plan: Observe output by hand\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D23578813\n\nPulled By: bertmaher\n\nfbshipit-source-id: e2161189fb0e1cd53aae980a153aea610871662a", "pr_number": "44315", "files_changed": ["torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["oncall: jit"]}, "d23f3170ef": {"title": "Remove pybind11 from required submodules (#44278)", "body": "Summary:\nThis can be taken from the system in which case it is not used from the submodule. Hence the check here limits the usage unnecessarily\n\nccing malfet\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44278\n\nReviewed By: malfet\n\nDifferential Revision: D23568552\n\nPulled By: ezyang\n\nfbshipit-source-id: 7fd2613251567f649b12eca0b1fe7663db9cb58d", "pr_number": "44278", "files_changed": ["setup.py"], "labels": ["module: build", "open source", "triaged"]}, "cfd3620b76": {"title": "Don't use VCOMP if Intel OMP is used (#44280)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44096.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44280\n\nReviewed By: malfet\n\nDifferential Revision: D23568557\n\nPulled By: ezyang\n\nfbshipit-source-id: bd627e497a9f71be9ba908852bf3ae437b1a5c94", "pr_number": "44280", "files_changed": ["caffe2/CMakeLists.txt", "modules/detectron/CMakeLists.txt"], "labels": ["merge-this-please", "module: openmp", "module: windows", "open source", "triaged"]}, "f044b17ae2": {"title": "Disable a test (#44348)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44348\n\nReviewed By: mrshenli\n\nDifferential Revision: D23592524\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 349057606ce39dd5de24314c9ba8f40516d2ae1c", "pr_number": "44348", "files_changed": ["test/test_jit_fuser_te.py"], "labels": []}, "a953a825cc": {"title": "Moves some of TestTorchMathOps to OpInfos (#44277)", "body": "Summary:\nThis PR fixes three OpInfo-related bugs and moves some functions from TestTorchMathOps to be tested using the OpInfo pattern. The bugs are:\n\n- A skip test path in test_ops.py incorrectly formatted its string argument\n- Decorating the tests in common_device_type.py was incorrectly always applying decorators to the original test, not the op-specific variant of the test. This could cause the same decorator to be applied multiple times, overriding past applications.\n- make_tensor was incorrectly constructing tensors in some cases\n\nThe functions moved are:\n\n- asin\n- asinh\n- sinh\n- acosh\n- tan\n- atan\n- atanh\n- tanh\n- log\n- log10\n- log1p\n- log2\n\nIn a follow-up PR more or all of the remaining functions in TestTorchMathOps will be refactored as OpInfo-based tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44277\n\nReviewed By: ngimel\n\nDifferential Revision: D23568330\n\nPulled By: mruberry\n\nfbshipit-source-id: 03e69fccdbfd560217c34ce4e9a5f20e10d05a5e", "pr_number": "44277", "files_changed": ["test/test_ops.py", "test/test_torch.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["module: tests"]}, "2a87742ffa": {"title": "Autocast wrappers for RNN cell apis (#44296)", "body": "Summary:\nShould fix https://github.com/pytorch/pytorch/issues/42605.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44296\n\nReviewed By: izdeby\n\nDifferential Revision: D23580447\n\nPulled By: ezyang\n\nfbshipit-source-id: 86027b693fd2b648f043ab781b84ffcc1f72854d", "pr_number": "44296", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "docs/source/amp.rst", "test/test_cuda.py", "torch/testing/_internal/autocast_test_lists.py"], "labels": ["open source"]}, "7c464eed16": {"title": "Skipping CUDA tests in ProcessGroupGloo and logs (#42488)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42488\n\nCurrently, ProcessGroupGloo tests do not emit logs if the test was\nskipped due CUDA not being available/not enough CUDA devices. This PR clarifies\nthe reason for skipping through these logs.\nghstack-source-id: 111638111\n\nTest Plan: tested on devvm and devgpu\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22879396\n\nfbshipit-source-id: d483ca46b5e22ed986521262c11a1c6dbfbe7efd", "pr_number": "42488", "files_changed": ["torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": []}, "960c088a58": {"title": "[te] Fix casting of unsigned char, and abs(int) (#44157)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44157\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D23528507\n\nPulled By: bertmaher\n\nfbshipit-source-id: c5ef0422a91a4665b616601bed8b7cd137be39f9", "pr_number": "44157", "files_changed": ["test/test_jit_fuser_te.py", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp"], "labels": ["oncall: jit"]}, "350130a69d": {"title": "Prevent the TE fuser from getting datatypes it can't handle (#44160)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44160\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D23528508\n\nPulled By: bertmaher\n\nfbshipit-source-id: 03b22725fb2666f441cb504b35397ea6d155bb85", "pr_number": "44160", "files_changed": ["test/test_jit_fuser_te.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["oncall: jit"]}, "6ec8fabc29": {"title": "Fix frac in CUDA fuser (#44152)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44152\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23528506\n\nfbshipit-source-id: bfd468d72fa55ce317f88ae83e1f2d5eee041aa0", "pr_number": "44152", "files_changed": ["torch/csrc/jit/codegen/fuser/codegen.cpp"], "labels": ["oncall: jit"]}, "683380fc91": {"title": "Use compile time cudnn version if linking with it statically (#44402)", "body": "Summary:\nThis should prevent torch_python from linking the entire cudnn library statically just to query its version\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44402\n\nReviewed By: seemethere\n\nDifferential Revision: D23602720\n\nPulled By: malfet\n\nfbshipit-source-id: 185b15b789bd48b1df178120801d140ea54ba569", "pr_number": "44402", "files_changed": ["torch/CMakeLists.txt", "torch/csrc/cuda/shared/cudnn.cpp"], "labels": []}, "758c2b96f5": {"title": "BUG: make cholesky_solve_out do broadcast, error checking (#43137)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42695\n\ntest, fix `cholesky_solve_out` to use error checking and broadcasting from `cholesky_solve`. Test segfaults before, passes after the fix.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43137\n\nReviewed By: izdeby\n\nDifferential Revision: D23568589\n\nPulled By: malfet\n\nfbshipit-source-id: 41b67ba964b55e59f1897eef0d96e0f6e1725bef", "pr_number": "43137", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "test/test_torch.py"], "labels": ["module: operators (deprecated)", "open source"]}, "364d03a67c": {"title": "Misc. FakeLowP OSS cleanup (#44331)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44331\n\n[10:22 AM] Cherckez, Tal\nsummary of issues(just to have a clear list):\n* std::clamp forces the user to use c++17\n* using setting without given fails the test\n* avoid using max_examples for tests\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: https://www.internalfb.com/intern/testinfra/testconsole/testrun/6192449509073222/\n\nReviewed By: hyuen\n\nDifferential Revision: D23581440\n\nfbshipit-source-id: fe9fbc341f8fca02352f531cc622fc1035d0300c", "pr_number": "44331", "files_changed": ["caffe2/contrib/fakelowp/quant_lut_fp16_fake_op.h", "caffe2/contrib/fakelowp/test/test_batchmatmul_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_batchnorm_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_fc_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_fusions.py", "caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py", "caffe2/contrib/fakelowp/test/test_layernorm_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp32.py"], "labels": ["fb-exported"]}, "37093f4d99": {"title": "Benchmarks: make fuser and executor configurable from command line. (#44291)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44291\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23569089\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: ec25b2f0bba303adaa46c3e85b1a9ce4fa3cf076", "pr_number": "44291", "files_changed": [".jenkins/pytorch/test.sh", "benchmarks/fastrnns/conftest.py", "benchmarks/fastrnns/test_bench.py", "benchmarks/upload_scribe.py"], "labels": []}, "ef4475f902": {"title": "[Reland] Optimize code path for adaptive_avg_pool2d when output size is (1, 1) (#44211)", "body": "Summary:\nReland of https://github.com/pytorch/pytorch/issues/43986\n\nDO NOT MERGE YET. XLA failure seems real.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44211\n\nReviewed By: mrshenli\n\nDifferential Revision: D23590505\n\nPulled By: ngimel\n\nfbshipit-source-id: 6ee516b0995bfff6efaf740474c82cb23055d274", "pr_number": "44211", "files_changed": ["aten/src/ATen/native/AdaptiveAveragePooling.cpp", "test/test_nn.py"], "labels": ["open source"]}, "15cbd1cf4b": {"title": "Preserve .ninja_log in build artifacts (#44390)", "body": "Summary:\nHelpful for later analysis on the build time trends\nAlso, same .whl files out of regular linux build job\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44390\n\nReviewed By: walterddr\n\nDifferential Revision: D23602049\n\nPulled By: malfet\n\nfbshipit-source-id: 4d55c9aa2d161a7998ad991a3da0436da83f70ad", "pr_number": "44390", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".jenkins/pytorch/build.sh"], "labels": []}, "1df24fd457": {"title": "[NCCL] Timeout Loop Thread for Async Error Handling (#41050)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41050\n\n**This Commit:**\nWe introduce a workVector to track live workNCCL objects corresponding to collective operations. Further, we introduce a workCleanupLoop, which busy-polls the vector of workNCCL objects and removes them upon completion.\n\n**This Stack:**\nThe purpose of this stack is to fix the hanging behavior observed in when using PyTorch DDP training with NCCL. In various situations (desynchronization, high GPU utilization, etc.), NCCL collectives may hang due to waiting on an unresponsive worker. This stack detects such hanging behavior and aborts timed-out collectives by throwing a user-visible exception, all with minimal perf regression. Training can then be restarted from a previous checkpoint with something like torchelastic.\n\nTest Plan: See D22054298 for verification of correctness and performance\n\nReviewed By: jiayisuse\n\nDifferential Revision: D21916637\n\nfbshipit-source-id: f8cadaab0071aaad1c4e31f9b089aa23cba0cfbe", "pr_number": "41050", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": []}, "4e5c55ef69": {"title": "[NCCL] Use cudaEventQuery to Poll for GPU operation errors (#41051)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41051\n\n**This Commit:**\nIn the workCleanupThread, we process completion and exception handling for workNCCL objects corresponding to collective calls that have either completed GPU Execution, or have already thrown an exception. This way, we throw an exception from the workCleanupThread for failed GPU operations. This approach replaces the previous (and lower performance) approach of enqueuing a callback on the CUDA stream to process failures.\n\n**This Stack:**\nThe purpose of this stack is to fix the hanging behavior observed in when using PyTorch DDP training with NCCL. In various situations (desynchronization, high GPU utilization, etc.), NCCL collectives may hang due to waiting on an unresponsive worker. This stack detects such hanging behavior and aborts timed-out collectives by throwing a user-visible exception, all with minimal perf regression. Training can then be restarted from a previous checkpoint with something like torchelastic.\n\nghstack-source-id: 111614319\n\nTest Plan: See D22054298 for verification of correctness and performance\n\nReviewed By: jiayisuse\n\nDifferential Revision: D21938498\n\nfbshipit-source-id: df598365031ff210afba57e0c7be865e3323ca07", "pr_number": "41051", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": []}, "f8f7b7840d": {"title": "[NCCL] Abort Errored and Timed Out NCCL Communicators from Watchdog Thread (#41052)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41052\n\n**This Commit:**\nWatchdog Thread checks for error-ed or timed out `WorkNCCL` objects and aborts all associated NCCL Communicators. For now, we  also process these aborted communicators as with the existing Watchdog logic (by adding them to abortedCommIds and writing aborted communicator ids to the store.)\n\n**This Stack:**\nThe purpose of this stack is to fix the hanging behavior observed in when using PyTorch DDP training with NCCL. In various situations (desynchronization, high GPU utilization, etc.), NCCL collectives may hang due to waiting on an unresponsive worker. This stack detects such hanging behavior and aborts timed-out collectives by throwing a user-visible exception, all with minimal perf regression. Training can then be restarted from a previous checkpoint with something like torchelastic.\n\nghstack-source-id: 111614313\n\nTest Plan: See D22054298 for verification of correctness and performance\n\nReviewed By: jiayisuse\n\nDifferential Revision: D21943151\n\nfbshipit-source-id: 337bfcb8af7542c451f1e4b3dcdfc5870bdec453", "pr_number": "41052", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": []}, "afbf2f140b": {"title": "[NCCL] WorkNCCL Helper Functions (#41053)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41053\n\n**This Commit:**\nSome minor refactoring - added helper to check if `WorkNCCL` objects have timed out. Adding a new finish function to ProcessGroupNCCL::WorkNCCL that avoids notifying CV and uses `lock_guard`. Also renaming the timeoutCVMutex mutex to be more descriptive.\n\n**This Stack:**\nThe purpose of this stack is to fix the hanging behavior observed in when using PyTorch DDP training with NCCL. In various situations (desynchronization, high GPU utilization, etc.), NCCL collectives may hang due to waiting on an unresponsive worker. This stack detects such hanging behavior and aborts timed-out collectives by throwing a user-visible exception, all with minimal perf regression. Training can then be restarted from a previous checkpoint with something like torchelastic.\n\nghstack-source-id: 111614315\n\nTest Plan: See D22054298 for verification of correctness and performance\n\nReviewed By: jiayisuse\n\nDifferential Revision: D21943520\n\nfbshipit-source-id: b27ee329f0da6465857204ee9d87953ed6072cbb", "pr_number": "41053", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": []}, "211ece7267": {"title": "[NCCL] ProcessGroupNCCL Destructor Blocks on WorkNCCL Completion (#41054)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41054\n\n**This Commit:**\nProcessGroupNCCL destructor now blocks until all WorkNCCL objects have either been aborted or completed and removed from the work vector.\n\n**This Stack:**\nThe purpose of this stack is to fix the hanging behavior observed in when using PyTorch DDP training with NCCL. In various situations (desynchronization, high GPU utilization, etc.), NCCL collectives may hang due to waiting on an unresponsive worker. This stack detects such hanging behavior and aborts timed-out collectives by throwing a user-visible exception, all with minimal perf regression. Training can then be restarted from a previous checkpoint with something like torchelastic.\n\nghstack-source-id: 111614314\n\nTest Plan:\n1. **DDP Sanity Check**: First we have a sanity check based on the PyTorch DDP benchmark. This verifies that the baseline DDP training with NCCL for  standard CU workloads works well (esp. with standard models like Resnet50 and BERT). Here is a sample Flow: f213293473\n\n1. **HPC Performance Benchmarks**: This stack has undergone thorough testing and profiling on the Training Cluster with varying number of nodes. This introduces 1-1.5% QPS regression only (~200-400 QPS regression for 8-64 GPUs).\n\n1. **HPC Accuracy Benchmarks**: We've confirmed NE parity with the existing NCCL/DDP stack without this change.\n\n1. **Kernel-Specific Benchmarks**: We have profiled other approaches for this system (such as cudaStreamAddCallback) and performed microbenchmarks to confirm the current solution is optimal.\n\n1. **Sandcastle/CI**: Apart from the recently fixed ProcessGroupNCCL tests, we will also introduce a new test for desynchronization scenarios.\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22054298\n\nfbshipit-source-id: 2b95a4430a4c9e9348611fd9cbcb476096183c06", "pr_number": "41054", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": []}, "48c47db8fe": {"title": "[NCCL] Add Environment Variable to guard Async Error Handling feature (#44163)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44163\n\nIn this PR, we introduce a new environment variable\n(NCCL_ASYNC_ERROR_HANDLING), which guards the asynchronous error handling\nfeature. We intend to eventually turn this feature on by default for all users,\nbut this is a temporary solution so the change in behavior from hanging to\ncrashing is not the default for users all of a sudden.\nghstack-source-id: 111637788\n\nTest Plan:\nCI/Sandcastle. We will turn on this env var by default in\ntorchelastic and HPC trainer soon.\n\nReviewed By: jiayisuse\n\nDifferential Revision: D23517895\n\nfbshipit-source-id: e7cd244b2ddf2dc0800ff7df33c73a6f00b63dcc", "pr_number": "44163", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": []}, "24efd29d19": {"title": "Check commutativity for computed dispatch table and add a test to check entries. (#44088)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44088\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23492793\n\nPulled By: ailzhang\n\nfbshipit-source-id: 37502f2a8a4d755219b400fcbb029e49d6cdb6e9", "pr_number": "44088", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h", "test/test_dispatch.py", "torch/csrc/utils/python_dispatch.cpp"], "labels": []}, "a00d36b0e7": {"title": "[PyTorch][Mobile] Insert the module name as `name()` to metadata dict if metadata doesn't contain \"model_name\" (#44400)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44400\n\nThis diff does the identical thing as D23549149 (https://github.com/pytorch/pytorch/commit/398409f07284cad25c500cc252518bbbf78f672e) does. A fix included for OSS CI: pytorch_windows_vs2019_py36_cuda10.1_test1\nghstack-source-id: 111679745\n\nTest Plan:\n- CI\n- OSS CI\n\nReviewed By: xcheng16\n\nDifferential Revision: D23601050\n\nfbshipit-source-id: 8ebdcd8fdc5865078889b54b0baeb397a90ddc40", "pr_number": "44400", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/module.cpp"], "labels": ["oncall: jit"]}, "032480d365": {"title": "fix typo in embedding_bag_non_contiguous_weight test (#44382)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44382\n\nThis is to fix a typo that introduced in #44032.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D23601316\n\nPulled By: glaringlee\n\nfbshipit-source-id: 17d6de5900443ea46c7a6ee9c7614fe6f2d92890", "pr_number": "44382", "files_changed": ["test/test_nn.py"], "labels": []}, "3674264947": {"title": "[quant] quantized path for ConstantPadNd (#43304)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43304\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23231946\n\nPulled By: z-a-f\n\nfbshipit-source-id: 8c77f9a81f5a36c268467a190b5b954df0a8f5a4", "pr_number": "43304", "files_changed": ["aten/src/ATen/native/ConstantPadNd.cpp", "test/quantization/test_quantized_op.py"], "labels": []}, "b0bcdbb1ab": {"title": "[JIT] Support partially specified sizes/strides in IRParser (#44113)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44113\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23508149\n\nPulled By: Lilyjjo\n\nfbshipit-source-id: b6b2d32109fae599bc5347dae742b67a2e4a0a49", "pr_number": "44113", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/cpp/jit/test_irparser.cpp", "torch/csrc/jit/frontend/schema_type_parser.cpp"], "labels": ["oncall: jit"]}, "fc51047af5": {"title": "Small fixes in Dependency.cmake and run_test.py (#44414)", "body": "Summary:\nDo not add gencode flags to NVCC_FLAGS twice: First time they are added in `cmake/public/cuda.cmake` no need to do it again in `cmake/Dependencies.cmake`\nCopy `additional_unittest_args` before appending local options to it in `run_test()` method\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44414\n\nReviewed By: seemethere\n\nDifferential Revision: D23605733\n\nPulled By: malfet\n\nfbshipit-source-id: 782a0da61650356a978a892fb03c66cb1a1ea26b", "pr_number": "44414", "files_changed": ["cmake/Dependencies.cmake", "test/run_test.py"], "labels": []}, "e0c65abd38": {"title": "Revert D23568330: [pytorch][PR] Moves some of TestTorchMathOps to OpInfos", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23568330 (https://github.com/pytorch/pytorch/commit/a953a825cc976858861b0b3f192bec205ab1d470)\n\nOriginal commit changeset: 03e69fccdbfd\n\nfbshipit-source-id: 04ec6843c5eb3c84ddf226dad0088172d9bed84d", "pr_number": null, "files_changed": ["test/test_ops.py", "test/test_torch.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": []}, "ba6ddaf04c": {"title": "[pyper] export caffe2 bucketize GPU operator to pytorch", "body": "Summary: Exporting the Bucketize operator on CUDA. Also adding unit test.\n\nTest Plan: buck test mode/dev-nosan caffe2/torch/fb/sparsenn:gpu_test -- test_bucketize\n\nDifferential Revision: D23581321\n\nfbshipit-source-id: 7f21862984c04d840410b8718db93006f526938a", "pr_number": null, "files_changed": ["caffe2/operators/bucketize_op.cu", "caffe2/operators/bucketize_op.h"], "labels": []}, "c010ef7f0c": {"title": "use non-overflowing divide in cuda kernel util GET_BLOCKS (#44391)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43476.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44391\n\nReviewed By: mrshenli\n\nDifferential Revision: D23602424\n\nPulled By: walterddr\n\nfbshipit-source-id: 40ed81547f933194ce5bf4a5bcebdb3434298bc1", "pr_number": "44391", "files_changed": ["aten/src/ATen/cuda/detail/KernelUtils.h"], "labels": []}, "b69c28d02c": {"title": "Improving ModuleList indexing error msg (#43361)", "body": "Summary:\nFollow up to https://github.com/pytorch/pytorch/pull/41946/, to suggest enumerating a module as an alternative if a user tries indexing into a modulelist/sequential with a non-integer literal\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43361\n\nReviewed By: mrshenli\n\nDifferential Revision: D23602388\n\nPulled By: eellison\n\nfbshipit-source-id: 51fa28d5bc45720529b3d45e92d367ee6c9e3316", "pr_number": "43361", "files_changed": ["test/jit/test_models.py", "test/jit/test_module_containers.py", "test/test_jit.py", "torch/csrc/jit/frontend/sugared_value.h"], "labels": ["oncall: jit"]}, "a7fba7de22": {"title": "Convert StoreTestUtils to Gtest (#43382)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43382\n\nStoreTestCommon defines standard helper functions that are used by all of our Store tests. These helpers currently throw exceptions upon failure, this PR changes them to use gtest assertions instead.\nghstack-source-id: 111690833\n\nTest Plan: Tested the 2 PR's above this on devvm\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22828156\n\nfbshipit-source-id: 9e116cf2904e05ac0342a441e483501e00aad3dd", "pr_number": "43382", "files_changed": ["torch/lib/c10d/test/CMakeLists.txt", "torch/lib/c10d/test/StoreTestCommon.hpp"], "labels": []}, "69a3ff005d": {"title": "Modularize FileStoreTest and move to Gtest (#43383)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43383\n\nFileStore Test currently has a large blob of tests that throw\nexceptions upon failure. This PR modularizes each test so they can run\nindependently, and migrates the framework to gtest.\nghstack-source-id: 111690831\n\nTest Plan: Confirmed tests pass on devvm\n\nReviewed By: jiayisuse\n\nDifferential Revision: D22879473\n\nfbshipit-source-id: 6fa5468e594a53c9a6b972757068dfc41645703e", "pr_number": "43383", "files_changed": ["torch/lib/c10d/test/FileStoreTest.cpp"], "labels": []}, "e028ad0762": {"title": "Fix HashStoreTests and move to Gtest (#43384)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43384\n\nMuch like the FileStoreTests, the HashStoreTests were also run in a single blob and threw exceptions upon failure. This modularizes the test by separating each function into separate gtest test cases.\nghstack-source-id: 111690834\n\nTest Plan: Confirmed that the tests pass on devvm.\n\nReviewed By: jiayisuse\n\nDifferential Revision: D23257579\n\nfbshipit-source-id: 7e821f0e9ee74c8b815f06facddfdb7dc2724294", "pr_number": "43384", "files_changed": ["torch/lib/c10d/test/HashStoreTest.cpp"], "labels": []}, "5ee31308e6": {"title": "[caffe2] exposes Net cancellation through pybind state (#44043)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44043\n\nTo invoke `cancel` from the net instance in Python, we expose it through pybind state.\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23249660\n\nfbshipit-source-id: 45a1e9062dca811746fcf2e5e42199da8f76bb54", "pr_number": "44043", "files_changed": ["caffe2/python/pybind_state.cc"], "labels": ["fb-exported"]}, "058d7228ec": {"title": "Expose the interface of nesterov of SGD Optimizer from caffe2 to dper", "body": "Summary:\nExpose the interface of `nesterov` of SGD Optimizer from caffe2 to dper.\n\ndper sgd optimizer (https://fburl.com/diffusion/chpobg0h) has referred to NAG sgdoptimizer in caffe2: https://fburl.com/diffusion/uat2lnan. So just need to add the parameter 'nesterov' in dper sgd optimizer.\n\nAnalysis of run resutls: N345540.\n\n- train_ne increases as momentum (m) decreases.\n- for m=0.95, 0.9: eval_ne is lower with NAG than production (no NAG, m = 0.95).\n- for m=0.99: eval_ne with or without NAG is higher than production. It indicates larger variance in validation and overfit in training (lower train_ne).\n\nTest Plan:\n1. unit tests:\n`buck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test -- test_sgd_without_nesterov`\n`buck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test -- test_sgd_with_nesterov`\n.\n1. build dper front end package: `flow-cli canary   ads.dper3.workflows.sparse_nn.train --mode opt --entitlement      ads_global --run-as-secure-group      team_ads_ml_ranking`. The build result (refreshed) is here https://www.internalfb.com/intern/buck/build/2a368b55-d94b-45c1-8617-2753fbce994b. Flow package version is ads_dper3.canary:856b545cc6b249c0bd328f845adeb0d2.\n.\n2. To build dper back end package: `flow-cli canary  dper.workflows.dper3.train --mode opt --entitlement      ads_global --run-as-secure-group      team_ads_ml_ranking`. The build result (refreshed) is here: https://www.internalfb.com/intern/buck/build/70fa91cd-bf6e-4a08-8a4d-41e41a77fb52. Flow package version is aml.dper2.canary:84123a34be914dfe86b1ffd9925869de.\n.\n3. Compare prod with NAG-enabled runs:\na) refreshed prod run (m=0.95): f213877098\nNAG enabled run (m=0.95): f213887113\n.\nb) prod run (m=0.9): f214065288\nNAG enabled run (m=0.9): f214066319\n.\nc) prod run (m=0.99): f214065804\nNAG enabled run (m=0.99): f214066725\n.\nd) change date type of nestrov to `bool` and launched a validation run\nNAG enabled (m=0.95): f214500597\n\nReviewed By: ustctf\n\nDifferential Revision: D23152229\n\nfbshipit-source-id: 61703ef6b4e72277f4c73171640fb8afc6d31f3c", "pr_number": null, "files_changed": ["caffe2/python/optimizer.py", "caffe2/sgd/momentum_sgd_op.h"], "labels": []}, "d3b6d5caf1": {"title": "[JIT] Add support for del to TS classes (#44352)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44352\n\n**Summary**\nThis commit adds support for `del` with class instances. If a class\nimplements `__delitem__`, then `del class_instance[key]` is syntactic\nsugar for `class_instance.__delitem__[key]`.\n\n**Test Plan**\nThis commit adds a unit test to TestClassTypes to test this feature.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23603102\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 28ad26ddc9a693a58a6c48a0e853a1c7cf5c9fd6", "pr_number": "44352", "files_changed": ["test/jit/test_class_type.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["oncall: jit"]}, "89ac30afb8": {"title": "[JIT] Propagate type sharing setting to submodule compilation (#44226)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44226\n\n**Summary**\nAt present, the `share_types` argument to `create_script_module` is used\nto decide whether to reuse a previously created type for a top-level\nmodule that has not yet been compiled. However, that setting does not apply\nto the compilation of submodules of the top-level module; types are\nstill reused if possible.\n\nThis commit modifies `create_script_module` so that the `share_types`\nflag is honoured during submodule compilation as well.\n\n**Test Plan**\nThis commit adds a unit test to `TestTypeSharing` that checks that\nsubmodule types are not shared or reused when `share_types` is set to\n`False`.\n\n**Fixes**\nThis commit fixes #43605.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23602371\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: b909b8b6abbe3b4cb9be8319ac263ade90e83bd3", "pr_number": "44226", "files_changed": ["test/jit/test_type_sharing.py", "torch/jit/_recursive.py"], "labels": ["oncall: jit"]}, "6324ef4ced": {"title": "[caffe2] Speed up compilation of aten-op.cc (#44440)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44440\n\n`aten-op.cc` takes a long time to compile due to the large generated constructor. For each case, the `std::function` constructor and the initialization functions are inlined, producing a huge amount of intermediate code that takes a long time to optimize, given that many compiler optimization passes are superlinear in the function size.\n\nThis diff moves each case to a separate function, so that each one is cheap to optimize, and the constructor is just a large jump table, which is easy to optimize.\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23593741\n\nfbshipit-source-id: 1ce7a31cda10d9b0c9d799716ea312a291dc0d36", "pr_number": "44440", "files_changed": ["c10/macros/Macros.h", "caffe2/contrib/aten/aten_op_template.h", "caffe2/contrib/aten/gen_op.py"], "labels": ["fb-exported"]}, "c515881137": {"title": "Add reset_grad() function (#44423)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44423\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42754\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23010859\n\nPulled By: ngimel\n\nfbshipit-source-id: 56eec43eba88b98cbf714841813977c68f983564", "pr_number": "44423", "files_changed": ["test/test_nn.py", "torch/nn/modules/module.py", "torch/optim/optimizer.py", "torch/optim/optimizer.pyi"], "labels": ["fb-exported"]}, "28bd4929bd": {"title": "[NNC] Make it able to normalize loop with variable start (#44133)", "body": "Summary:\nLoops with variable start can also be normalized.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44133\n\nTest Plan: updated testNormalizeStartVariable.\n\nReviewed By: navahgar\n\nDifferential Revision: D23507097\n\nPulled By: cheng-chang\n\nfbshipit-source-id: 4e9aad1cd4f4a839f59a00bf8ddf97637a1a6648", "pr_number": "44133", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["oncall: jit"]}, "65d4a6b7c0": {"title": "[ROCm] fix cub hipify mappings (#44431)", "body": "Summary:\nFixes ROCm-specific workarounds introduced by https://github.com/pytorch/pytorch/issues/44259.  This adds new hipify mappings that properly handle cub outside of caffe2 sources.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44431\n\nReviewed By: mrshenli\n\nDifferential Revision: D23617417\n\nPulled By: ngimel\n\nfbshipit-source-id: 5d16afb6b8e6ec5ed049c51571866b0878d534ca", "pr_number": "44431", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["module: rocm", "open source"]}, "7b547f086f": {"title": "To fix extra memory allocation when using circular padding (#39273)", "body": "Summary:\nFor fixing https://github.com/pytorch/pytorch/issues/39256\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39273\n\nReviewed By: anjali411\n\nDifferential Revision: D23471811\n\nPulled By: mruberry\n\nfbshipit-source-id: fb324b51baea765311715cdf14642b334f335733", "pr_number": "39273", "files_changed": ["test/test_nn.py", "torch/nn/functional.py"], "labels": ["open source", "triaged"]}, "28a23fce4c": {"title": "Deprecate torch.norm and torch.functional.norm (#44321)", "body": "Summary:\nPart of https://github.com/pytorch/pytorch/issues/24802\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44321\n\nReviewed By: mrshenli\n\nDifferential Revision: D23617273\n\nPulled By: mruberry\n\nfbshipit-source-id: 6f88b5cb097fd0acb9cf0e415172c5a86f94e9f2", "pr_number": "44321", "files_changed": ["test/test_linalg.py", "test/test_torch.py", "torch/functional.py"], "labels": ["open source"]}, "6c98d904c0": {"title": "handle the case of -0.0 on tanh quantization (#44406)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44406\n\nthis fix makes fakelowp identical to hw\n\n- mask out the floating point number with 0x7fff so we are always dealing\nwith positive numbers\n- dsp implementation is correct, ice-ref suffers from this same problem\n\nTest Plan: - tested with test_fusions.py, can't enable the test until the fix in ice-ref appears\n\nReviewed By: venkatacrc\n\nDifferential Revision: D23603878\n\nfbshipit-source-id: a72d93a4bc811f98d1b5e82ddb204be028addfeb", "pr_number": "44406", "files_changed": ["caffe2/contrib/fakelowp/quant_lut_fp16_fake_op.h"], "labels": ["fb-exported"]}, "208ad45b4b": {"title": "fix scripts (#44464)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44464\n\nReviewed By: agolynski\n\nDifferential Revision: D23624921\n\nPulled By: colesbury\n\nfbshipit-source-id: 72bed69edcf467a99eda9a3b97e894015c992dce", "pr_number": "44464", "files_changed": ["scripts/build_pytorch_android.sh"], "labels": ["open source"]}, "af9cad761a": {"title": "Stop ignoring NotImplementedErrors in cuda CriterionTests. (#44381)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44381\n\nPerhaps this was necessary when the test was originally introduced, but it's difficult to figure out what is actually tested.  And I don't think we actually use NotImplementedErorrs.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23598646\n\nPulled By: gchanan\n\nfbshipit-source-id: aa18154bfc4969cca22323e61683a301198823be", "pr_number": "44381", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "fa158c4ca6": {"title": "Combine criterion and new criterion tests in test_jit. (#43958)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43958\n\nThere is not any difference between these tests (I'm merging them), so let's merge them in the JIT as well.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23452337\n\nPulled By: gchanan\n\nfbshipit-source-id: e6d13cdb164205eec3dbb7cdcd0052b02c961778", "pr_number": "43958", "files_changed": ["test/test_jit.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "c8914afdfa": {"title": "Merge criterion_tests and new_criterion_tests. (#44398)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44398\n\nThese end up executing the same tests, so no reason to have them separate.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23600855\n\nPulled By: gchanan\n\nfbshipit-source-id: 0952492771498bf813f1bf8e1d7c8dce574ec965", "pr_number": "44398", "files_changed": ["test/test_cpp_api_parity.py", "test/test_jit.py", "test/test_nn.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "f9a0d0c21e": {"title": "Allow Tensor-likes in torch.autograd.gradcheck (#43877)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42942\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43877\n\nReviewed By: zou3519\n\nDifferential Revision: D23493257\n\nPulled By: ezyang\n\nfbshipit-source-id: 6cdaabe17157b484e9491189706ccc15420ac239", "pr_number": "43877", "files_changed": ["test/test_overrides.py", "torch/autograd/__init__.py", "torch/autograd/gradcheck.py", "torch/overrides.py"], "labels": ["open source", "triaged"]}, "cb90fef770": {"title": "Fix return value of PyErr_WarnEx ignored (SystemError) (#44371)", "body": "Summary:\nThis PR fixes unexpected `SystemError` when warnings are emitted and warning filters are set.\n\n## Current behavior\n\n```\n$ python -Werror\n>>> import torch\n>>> torch.range(1, 3)\nUserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nSystemError: <built-in method range of type object at 0x7f38c7703a60> returned a result with an error set\n```\n\n## Expected behavior\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nUserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n```\n\n## Note\n\nPython exception must be raised if `PyErr_WarnEx` returns `-1` ([python docs](https://docs.python.org/3/c-api/exceptions.html#issuing-warnings)). This PR fixes warnings raised in the following code:\n```py\nimport torch\n\ntorch.range(1, 3)\ntorch.autograd.Variable().volatile\ntorch.autograd.Variable().volatile = True\ntorch.tensor(torch.tensor([]))\ntorch.tensor([]).new_tensor(torch.tensor([]))\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44371\n\nReviewed By: mrshenli\n\nDifferential Revision: D23598410\n\nPulled By: albanD\n\nfbshipit-source-id: 2fbcb13fe4025dbebaf1fd837d4c8e0944e05010", "pr_number": "44371", "files_changed": ["tools/autograd/templates/python_torch_functions.cpp", "torch/csrc/autograd/python_legacy_variable.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/utils/tensor_new.cpp"], "labels": ["open source"]}, "38c10b4f30": {"title": "[NCCL] Fix the initialization of futureNCCLCallbackStreams (#44347)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44347\n\nCloned from Pull Request resolved: https://github.com/pytorch/pytorch/pull/44097, because the original author Sinan has completed the internship and now is unable to submit this diff.\n\nAs johnsonpaul mentioned in D23277575 (https://github.com/pytorch/pytorch/commit/7d517cf96f6d53bbe472cf1404e12b9b75230bb6). It looks like all processes were allocating memory on GPU-ID=0.\n\nI was able to reproduce it by running `test_ddp_comm_hook_allreduce_with_then_hook_nccl` unit test of `test_c10d.py` and running `nvidia-smi` while test was running. The issue was reproduced as:\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0   3132563      C   python                                       777MiB |\n|    0   3132564      C   python                                       775MiB |\n|    4   3132564      C   python                                       473MiB |\n+-----------------------------------------------------------------------------+\n```\nI realized that as we initialize ProcessGroupNCCL both processes were initially allocating memory on GPU 0.\n\nWe later also realized that I forgot `isHighPriority` input of `getStreamFromPool` and `futureNCCLCallbackStreams_.push_back(std::make_shared<at::cuda::CUDAStream>(at::cuda::getStreamFromPool(device_index)));` was just creating a vector of GPU 0 streams. As i changed `at::cuda::getStreamFromPool(device_index)` to `at::cuda::getStreamFromPool(false, device_index)`. `nvidia-smi` looked like:\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0    673925      C   python                                       771MiB |\n|    0    673926      C   python                                       771MiB |\n|    1    673925      C   python                                       771MiB |\n|    1    673926      C   python                                       771MiB |\n|    2    673925      C   python                                       771MiB |\n|    2    673926      C   python                                       771MiB |\n|    3    673925      C   python                                       771MiB |\n|    3    673926      C   python                                       771MiB |\n|    4    673925      C   python                                       771MiB |\n|    4    673926      C   python                                       771MiB |\n|    5    673925      C   python                                       771MiB |\n|    5    673926      C   python                                       771MiB |\n|    6    673925      C   python                                       771MiB |\n|    6    673926      C   python                                       771MiB |\n|    7    673925      C   python                                       707MiB |\n|    7    673926      C   python                                       623MiB |\n+-----------------------------------------------------------------------------+\n```\nThis confirms that we were just getting GPU 0 streams for the callback. I think this does not explain the `fp16_compress` stability issue, because we were able to reproduce that even without any then callback and just calling copy from fp32 to fp16 before allreduce. However, this can explain other issues where `allreduce` was not on par with `no_hook`. I'll run some additional simulations with this diff.\n\nI tried to to replace `getStreamFromPool` by `getDefaultCUDAStream(deviceIndex)` and it wasn't causing additional memory usage. In this diff, I temporarily solved the issue by just initializing null pointers for each device in the constructor and setting the callback stream for corresponding devices inside `ProcessGroupNCCL::getNCCLComm`. After the fix it looks like the memory issue was resolved:\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0   2513142      C   python                                       745MiB |\n|    4   2513144      C   python                                       747MiB |\n+-----------------------------------------------------------------------------+\n```\nI could use a dictionary instead of a vector for `futureNCCLCallbackStreams_`, but since number of devices is fixed, I think it isn't necessary. Please let me know what you think in the comments.\nghstack-source-id: 111485483\n\nTest Plan:\n`test_c10d.py` and some perf tests. Also check `nvidia-smi` while running tests to validate memory looks okay.\n\nThis diff also fixes the regression in HPC tests as we register a hook:\n\n{F322730175}\n\nSee https://fb.quip.com/IGuaAbD8 (https://github.com/pytorch/pytorch/commit/474fdd7e2d268270587bb11a052265bbdccf96a0)bnvy for details.\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23495436\n\nfbshipit-source-id: ad08e1d94343252224595d7c8a279fe75e244822", "pr_number": "44347", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["fb-exported"]}, "d232fec1f1": {"title": "Partly fix cuda builds of dper broken by caffe2 c++", "body": "Summary:\ncuda builds using clang error out when building caffe2 due to an incorrect std::move\n\nThis does not fix all known errors, but it's a step in the right direction.\n\nDifferential Revision: D23626667\n\nfbshipit-source-id: 7d9df886129f671ec430a166dd22e4af470afe1e", "pr_number": null, "files_changed": ["aten/src/ATen/native/cuda/ForeachBinaryOpList.cu", "aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu", "aten/src/ATen/native/cuda/ForeachPointwiseOp.cu", "aten/src/ATen/native/cuda/ForeachUnaryOp.cu"], "labels": []}, "b3f0297a94": {"title": "ConvPackedParams: remove legacy format (#43651)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43651\n\nThis is a forward compatibility follow-up to\nhttps://github.com/pytorch/pytorch/pull/43086/. We switch the\nconv serialization to output the v2 format instead of the v1 format.\n\nThe plan is to land this 1 - 2 weeks after the base PR.\n\nTest Plan:\n```\npython test/test_quantization.py TestSerialization.test_conv2d_graph_v2\npython test/test_quantization.py TestSerialization.test_conv2d_nobias_graph_v2\n```\n\nImported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23355480\n\nfbshipit-source-id: 4cb04ed8b90a0e3e452297a411d641a15f6e625f", "pr_number": "43651", "files_changed": ["aten/src/ATen/native/quantized/cpu/conv_serialization.h", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp", "test/backward_compatibility/check_backward_compatibility.py"], "labels": []}, "cc5a1cf616": {"title": "[JIT] Erase shapes before fallback graph (#44434)", "body": "Summary:\nPreviously the specialized types were copied over to the fallback function, although the tensors in the fallback type were not of that type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44434\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23611943\n\nPulled By: eellison\n\nfbshipit-source-id: 2ea88a97529409f6c5c4c1f59a14b623524933de", "pr_number": "44434", "files_changed": ["test/jit/test_profiler.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["oncall: jit"]}, "4bead6438a": {"title": "Enable torch.autograd typechecks (#44451)", "body": "Summary:\nTo help with further typing, move dynamically added native contributions from `torch.autograd` to `torch._C._autograd`\nFix invalid error handling pattern in\nhttps://github.com/pytorch/pytorch/blob/89ac30afb88451efc66ed8077b7bbb222afd3428/torch/csrc/autograd/init.cpp#L13-L15\n`PyImport_ImportModule` already raises Python exception and nullptr should be returned to properly propagate the to Python runtime.\n\nAnd all native methods/types in `torch/autograd/__init.py` after `torch._C._init_autograd()` has been called\nUse f-strings instead of `.format` in test_type_hints.py\nFixes https://github.com/pytorch/pytorch/issues/44450\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44451\n\nReviewed By: ezyang\n\nDifferential Revision: D23618261\n\nPulled By: malfet\n\nfbshipit-source-id: fa5f739d7cff8410641128b55b810318c5f636ae", "pr_number": "44451", "files_changed": ["mypy.ini", "test/test_type_hints.py", "torch/_C/_autograd.pyi", "torch/autograd/__init__.py", "torch/csrc/autograd/init.cpp"], "labels": ["module: typing", "triaged"]}, "a2a81e1335": {"title": "Add a CONTRIBUTING.md for the distributed package. (#44224)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44224\n\nThe purpose of this file is to help developers on PT distributed get\nupto speed on the code structure and layout for PT Distributed.\nghstack-source-id: 111644842\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23548377\n\nfbshipit-source-id: 561d5b8e257642de172def8fdcc1311fae20690b", "pr_number": "44224", "files_changed": ["docs/source/_static/img/pt_distributed_arch.png", "docs/source/_static/img/rpc_arch.png", "torch/distributed/CONTRIBUTING.md"], "labels": []}, "1dd3fae3d2": {"title": "[pytorch] Add logging to mobile Method run (#44234)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44234\n\nChanges mobile Method to point to a mobile Module directly instead of the Module ivalue in order to access metadata for logging/debugging, and then adds said logging.\nghstack-source-id: 111775806\n\nTest Plan:\nCI/existing unit tests to test BC\nTesting fb4a logging:\nBuilt fb4a on D23436351 (because usage of run_method isn't replaced yet in this diff), and then checked the Scuba logs to see that the appropriate ad clicks were logged (one ad for Buzzfeed shopping and another about Netflix from Bustle)\n\n{F328510687}\n{F328511201}\n[Scuba sample of QPL metrics](https://www.internalfb.com/intern/scuba/query/?dataset=qpl_metrics%2Fpytorch_employee&pool=uber&view=samples_client&drillstate=%7B%22sampleCols%22%3A[%22device_model%22%2C%22instance_id_sampled%22%2C%22method%22%2C%22ios_device_class%22%2C%22points_path%22%2C%22userid_sampled%22%2C%22client_sample_rate%22%2C%22browser_name%22%2C%22ios_device_name%22%2C%22points%22%2C%22is_employee%22%2C%22is_test_user%22%2C%22network_only_queries%22%2C%22annotations%22%2C%22oncall_shortname%22%2C%22environment_tags%22%2C%22revoked_queries%22%2C%22annotations_bool%22%2C%22points_data%22%2C%22annotations_double_array%22%2C%22annotations_string_array%22%2C%22revoked_steps%22%2C%22points_set%22%2C%22device_os_version%22%2C%22ota_version_rollout%22%2C%22steps%22%2C%22vadar_calculation_result%22%2C%22app_name%22%2C%22client_push_phase%22%2C%22vadar%22%2C%22release_channel%22%2C%22interaction_class%22%2C%22exposures%22%2C%22annotations_double%22%2C%22deviceid_sampled%22%2C%22is_logged_in%22%2C%22device_os%22%2C%22time%22%2C%22major_os_ver%22%2C%22annotations_int_array%22%2C%22duration_ns%22%2C%22app_build%22%2C%22bucket_id%22%2C%22cache_and_network_queries%22%2C%22value%22%2C%22vadar_v2%22%2C%22quicklog_event%22%2C%22unixname%22%2C%22vadar_calculation_result_v2%22%2C%22trace_tags%22%2C%22annotations_int%22%2C%22quicklog_module%22%2C%22push_phase%22%2C%22year_class%22%2C%22country%22%2C%22capped_duration%22%2C%22ram_class%22%2C%22weight%22%2C%22carrier%22%2C%22app_id%22%2C%22app_version%22%2C%22react_bundle_version%22%2C%22logging_source%22%2C%22is_unsampled_for_scuba%22%2C%22instrumentation_errors%22%2C%22android_cpu_abi_list%22%2C%22days_after_release%22%2C%22cpu_cores%22%2C%22user_bucket%22%2C%22quicklog_action%22%2C%22server_scuba_sample_rate%22%2C%22points_vector%22%2C%22annotations_bool_array%22%2C%22android_device_class%22%2C%22browser_full_version%22%2C%22major_app_ver%22]%2C%22derivedCols%22%3A[]%2C%22mappedCols%22%3A[]%2C%22enumCols%22%3A[]%2C%22hideEmptyColumns%22%3Afalse%2C%22focused_event%22%3A%22%22%2C%22show_metadata%22%3A%22false%22%2C%22start%22%3A%222020-09-08%2011%3A27%3A00%22%2C%22end%22%3A%22start%20%2B%201%20minute%22%2C%22timezone%22%3A%22America%2FLos_Angeles%22%2C%22samplingRatio%22%3A%221%22%2C%22num_samples%22%3A%22100%22%2C%22aggregateList%22%3A[]%2C%22param_dimensions%22%3A[]%2C%22modifiers%22%3A[]%2C%22order%22%3A%22none%22%2C%22order_desc%22%3Atrue%2C%22filterMode%22%3A%22DEFAULT%22%2C%22constraints%22%3A[[%7B%22column%22%3A%22quicklog_event%22%2C%22op%22%3A%22eq%22%2C%22value%22%3A[%22[%5C%22MOBILE_MODULE_STATS%5C%22]%22]%7D%2C%7B%22column%22%3A%22userid_sampled%22%2C%22op%22%3A%22eq%22%2C%22value%22%3A[%22[%5C%22100013484978975%5C%22]%22]%7D]]%2C%22c_constraints%22%3A[[]]%2C%22b_constraints%22%3A[[]]%2C%22metrik_view_params%22%3A%7B%22should_use_legacy_colors%22%3Afalse%2C%22columns_skip_formatting%22%3A[]%2C%22view%22%3A%22samples_client%22%2C%22width%22%3A%221358%22%2C%22height%22%3A%22912%22%2C%22tableID%22%3A%22qpl_metrics%2Fpytorch_employee%22%2C%22fitToContent%22%3Afalse%2C%22format_tooltip_in_percent%22%3Afalse%2C%22use_y_axis_hints_as_limits%22%3Atrue%2C%22has_dynamic_context_menu%22%3Atrue%2C%22has_context_menu%22%3Afalse%2C%22legend_mode%22%3A%22nongrid%22%2C%22connect_nulls%22%3Atrue%2C%22timezone_offset%22%3A420%2C%22timezone%22%3A%22America%2FLos_Angeles%22%2C%22y_min_hint%22%3A0%2C%22should_render_plugins_menu%22%3Afalse%7D%7D&normalized=1599581160)\n[Scuba sample showing ad source; just the bottom two results](https://www.internalfb.com/intern/scuba/query/?dataset=business_integrity_webpage_semantic&pool=uber&drillstate=%7B%22sampleCols%22%3A[%22from_custom_sampling%22%2C%22data_version%22%2C%22scribe_category_type%22%2C%22page_id%22%2C%22name%22%2C%22source_url%22%2C%22time%22%2C%22title_semantic%22%2C%22major_version%22%2C%22server_protocol%22%2C%22custom_sampling_enabled%22%2C%22ad_id%22%2C%22appversion%22%2C%22clienttime%22%2C%22isemployee%22%2C%22title%22%2C%22images%22%2C%22weight%22%2C%22carrier%22%2C%22is_ad%22%2C%22locale%22%2C%22appid%22%2C%22ip_country%22%2C%22iab_models%22]%2C%22derivedCols%22%3A[]%2C%22mappedCols%22%3A[]%2C%22enumCols%22%3A[]%2C%22return_remainder%22%3Afalse%2C%22should_pivot%22%3Afalse%2C%22is_timeseries%22%3Afalse%2C%22hideEmptyColumns%22%3Afalse%2C%22main_dimension%22%3A%22time%22%2C%22start%22%3A%22-5%20minutes%22%2C%22samplingRatio%22%3A%221%22%2C%22compare%22%3A%22none%22%2C%22axes%22%3A%22linked%22%2C%22overlay_types%22%3A[]%2C%22minBucketSamples%22%3A%22%22%2C%22dimensions%22%3A[]%2C%22scale_type%22%3A%22absolute%22%2C%22num_samples%22%3A%22100%22%2C%22metric%22%3A%22avg%22%2C%22fill_missing_buckets%22%3A%22connect%22%2C%22smoothing_bucket%22%3A%221%22%2C%22top%22%3A%227%22%2C%22markers%22%3A%22%22%2C%22timezone%22%3A%22America%2FLos_Angeles%22%2C%22end%22%3A%22now%22%2C%22show_p95_ci%22%3Afalse%2C%22time_bucket%22%3A%22auto%22%2C%22compare_mode%22%3A%22normal%22%2C%22aggregateList%22%3A[]%2C%22param_dimensions%22%3A[]%2C%22modifiers%22%3A[]%2C%22order%22%3A%22none%22%2C%22order_desc%22%3Atrue%2C%22filterMode%22%3A%22DEFAULT%22%2C%22constraints%22%3A[[%7B%22column%22%3A%22major_version%22%2C%22op%22%3A%22eq%22%2C%22value%22%3A[%22[%5C%22288%5C%22]%22]%7D]]%2C%22c_constraints%22%3A[[]]%2C%22b_constraints%22%3A[[]]%2C%22metrik_view_params%22%3A%7B%22should_use_legacy_colors%22%3Afalse%2C%22columns_skip_formatting%22%3A[]%2C%22view%22%3A%22time_view%22%2C%22width%22%3A%221358%22%2C%22height%22%3A%22912%22%2C%22tableID%22%3A%22business_integrity_webpage_semantic%22%2C%22fitToContent%22%3Afalse%2C%22format_tooltip_in_percent%22%3Afalse%2C%22use_y_axis_hints_as_limits%22%3Atrue%2C%22has_dynamic_context_menu%22%3Atrue%2C%22has_context_menu%22%3Afalse%2C%22legend_mode%22%3A%22nongrid%22%2C%22connect_nulls%22%3Atrue%2C%22timezone_offset%22%3A420%2C%22timezone%22%3A%22America%2FLos_Angeles%22%2C%22y_min_hint%22%3A0%2C%22should_render_plugins_menu%22%3Afalse%7D%7D&view=samples_client&normalized=1599587280)\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23548687\n\nfbshipit-source-id: 3e63085663f5fd8de90a4c7dbad0a17947aee973", "pr_number": "44234", "files_changed": ["torch/csrc/jit/mobile/method.h", "torch/csrc/jit/mobile/module.cpp"], "labels": ["oncall: jit"]}, "91b16bff1e": {"title": "Disable PyTorch iOS ARM64 builds until cert problem is fixed (#44499)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44499\n\nReviewed By: seemethere, xta0\n\nDifferential Revision: D23634961\n\nPulled By: malfet\n\nfbshipit-source-id: e32ae29c42c351bcb4f48bc52d4082ae56545e5b", "pr_number": "44499", "files_changed": [".circleci/cimodel/data/simple/ios_definitions.py", ".circleci/config.yml"], "labels": []}, "2e744b1820": {"title": "Support work.result() to get result tensors for allreduce for Gloo, NCCL backends (#43970)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43970\n\nIt is resubmition of #43386\n\nOriginal commit changeset: 27fbeb161706\nghstack-source-id: 111775070\n\nTest Plan:\nAdded checks to existing unit test and ran it on gpu devserver.\nVerified the test that was failing in original diff also passes: https://app.circleci.com/pipelines/github/pytorch/pytorch/210229/workflows/86bde47b-f2da-48e3-a618-566ae2713102/jobs/7253683\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23455047\n\nfbshipit-source-id: b8dc4a30b95570d68a482c19131674fff2a3bc7c", "pr_number": "43970", "files_changed": ["torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["ci/all"]}, "c48f511c7e": {"title": "Moves some of TestTorchMathOps to OpInfos (#44277)", "body": "Summary:\nThis PR fixes three OpInfo-related bugs and moves some functions from TestTorchMathOps to be tested using the OpInfo pattern. The bugs are:\n\n- A skip test path in test_ops.py incorrectly formatted its string argument\n- Decorating the tests in common_device_type.py was incorrectly always applying decorators to the original test, not the op-specific variant of the test. This could cause the same decorator to be applied multiple times, overriding past applications.\n- make_tensor was incorrectly constructing tensors in some cases\n\nThe functions moved are:\n\n- asin\n- asinh\n- sinh\n- acosh\n- tan\n- atan\n- atanh\n- tanh\n- log\n- log10\n- log1p\n- log2\n\nIn a follow-up PR more or all of the remaining functions in TestTorchMathOps will be refactored as OpInfo-based tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44277\n\nReviewed By: mrshenli, ngimel\n\nDifferential Revision: D23617361\n\nPulled By: mruberry\n\nfbshipit-source-id: edb292947769967de9383f6a84eb327f027509e0", "pr_number": "44277", "files_changed": ["test/test_ops.py", "test/test_torch.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["module: tests"]}, "129d52aef2": {"title": "Fix uniqueness check in movedim (#44307)", "body": "Summary:\nNoticed this bug in `torch.movedim` (https://github.com/pytorch/pytorch/issues/41480). [`std::unique`](https://en.cppreference.com/w/cpp/algorithm/unique) only guarantees uniqueness for _sorted_ inputs. The current check lets through non-unique values when they aren't adjacent to each other in the list, e.g. `(0, 1, 0)` wouldn't raise an exception and instead the algorithm fails later with an internal assert.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44307\n\nReviewed By: mrshenli\n\nDifferential Revision: D23598311\n\nPulled By: zou3519\n\nfbshipit-source-id: fd6cc43877c42bb243cfa85341c564b6c758a1bf", "pr_number": "44307", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "test/test_torch.py"], "labels": ["open source"]}, "41f62b17e7": {"title": "Fix DDP join() API in the case of model.no_sync() (#44427)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44427\n\nCloses https://github.com/pytorch/pytorch/issues/44425\n\nDDP join API currently does not work properly with `model.no_sync()`, see https://github.com/pytorch/pytorch/issues/44425 for details. This PR fixes the problem via the approach mentioned in the issue, namely scheduling an allreduce that tells joined ranks whether to sync in the backwards pass or not. Tests are added for skipping gradient synchronization for various `sync_interval`s.\nghstack-source-id: 111786479\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23609070\n\nfbshipit-source-id: e8716b7881f8eee95e3e3499283e716bd3d7fe76", "pr_number": "44427", "files_changed": ["torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": []}, "08b431f54c": {"title": "Add trace_backward, masked_select_backward, and take_backward as ops (#44408)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44408\n\nSee #44052 for context.\n\nTest Plan: - `pytest test/test_autograd.py -v`\n\nReviewed By: mrshenli\n\nDifferential Revision: D23605504\n\nPulled By: zou3519\n\nfbshipit-source-id: b9b1646d13caa6e536d08669c29bfc2ad8ff89a3", "pr_number": "44408", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TriangularOps.cpp", "aten/src/ATen/native/native_functions.yaml", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h"], "labels": []}, "7ff7e6cfc8": {"title": "Register cummaxmin_backward, cumprod_backward as operators (#44410)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44410\n\nSee #44052 for context. One of the cumprod_backward overloads was unused\nso I just deleted it.\n\nTest Plan: - `pytest test/test_autograd.py -v`\n\nReviewed By: mrshenli\n\nDifferential Revision: D23605503\n\nPulled By: zou3519\n\nfbshipit-source-id: f9c5b595e62d2d6e71f26580ba96df15cc9de4f7", "pr_number": "44410", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h"], "labels": []}, "69f6d94caa": {"title": "Register diag_backward, diagonal_backward, infinitetely...gelu_backward as operators (#44422)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44422\n\nSee #44052 for context.\n\nTest Plan:\n- `pytest test/test_autograd.py -v`\n- `pytest test/test_nn.py -v`\n\nReviewed By: mrshenli\n\nDifferential Revision: D23607691\n\nPulled By: zou3519\n\nfbshipit-source-id: 09fbcd66b877af4fa85fd9b2f851ed3912ce84d6", "pr_number": "44422", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h"], "labels": []}, "6ee41974e3": {"title": "Speedup Linux nightly builds (#44532)", "body": "Summary:\n`stdbuf` affects not only the process it launches, but all of its subprocessed, which have a very negative effects on the IPC communication between nvcc and c++ preprocessor, which results in 2x slowdown, for example:\n\n```\n$ time /usr/local/cuda/bin/nvcc /pytorch/aten/src/THC/generated/THCTensorMathPointwiseByte.cu -c ...\nreal\t0m34.623s\nuser\t0m31.736s\nsys\t0m2.825s\n```\nbut\n```\ntime stdbuf -i0 -o0 -e0 /usr/local/cuda/bin/nvcc /pytorch/aten/src/THC/generated/THCTensorMathPointwiseByte.cu -c ...\nreal\t1m14.113s\nuser\t0m37.989s\nsys\t0m36.104s\n```\nbecause OS spends lots of time transferring preprocessed source back to nvcc byte by byte, as requested via stdbuf call\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44532\n\nReviewed By: ngimel\n\nDifferential Revision: D23643411\n\nPulled By: malfet\n\nfbshipit-source-id: 9fdaf8b8a49574e6b281f68a5dd9ba9d33464dff", "pr_number": "44532", "files_changed": [".circleci/scripts/binary_linux_build.sh"], "labels": []}, "f7278473d3": {"title": "[NCCL] Fix NCCL_BLOCKING_WAIT functionality with Async Error Handling (#44411)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44411\n\nThis basically aborts errored NCCL communicators if either blocking\nwait or async error handling is enabled. Otherwise we may abort nccl\ncommunicators where neither are enabled, and this may result in subsequent GPU\noperations using corrupted data.\nghstack-source-id: 111839264\n\nTest Plan: Succesful Flow run: f217591683\n\nReviewed By: jiayisuse\n\nDifferential Revision: D23605382\n\nfbshipit-source-id: 6c16f9626362be3b0ce2feaf0979b2dff97ce61b", "pr_number": "44411", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": []}, "0c58a017bd": {"title": "[quant][eagermode][refactor] Add set/get method for quantization and fusion mappings (#43990)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43990\n\nAllow user to register custom quantization and fusion patterns\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23485344\n\nfbshipit-source-id: 4f0174ee6d8000d83de0f73cb370e9a1941d54aa", "pr_number": "43990", "files_changed": ["mypy.ini", "torch/quantization/__init__.py", "torch/quantization/_numeric_suite.py", "torch/quantization/fuse_modules.py", "torch/quantization/fuser_method_mappings.py", "torch/quantization/fx/fusion_patterns.py", "torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py", "torch/quantization/quantization_mappings.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "b5d75dddd9": {"title": "Enable lerp on half type; fix output memory format (#43541)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43541\n\nReviewed By: zou3519\n\nDifferential Revision: D23499592\n\nPulled By: ezyang\n\nfbshipit-source-id: 9efdd6cbf0a334ec035ddd467667ba874b892549", "pr_number": "43541", "files_changed": ["aten/src/ATen/native/cuda/Lerp.cu", "test/test_torch.py"], "labels": ["module: cuda", "module: half", "open source", "triaged"]}, "51ed31269e": {"title": "Replace FutureMessage with c10::ivalue::Future in DistEngine. (#44239)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44239\n\nAs part of https://github.com/pytorch/pytorch/issues/41574, use\nc10::ivalue::Future everywhere in DistEngine.\nghstack-source-id: 111645070\n\nTest Plan: waitforbuildbot\n\nReviewed By: mrshenli\n\nDifferential Revision: D23553507\n\nfbshipit-source-id: 1b51ba13d1ebfa6c5c70b12028e9e96ce8ba51ff", "pr_number": "44239", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/rpc/request_callback_no_python.cpp"], "labels": []}, "c6febc6480": {"title": "[JIT] Add a python hook for a function to interpret JIT graphs. (#44493)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44493\n\nThis function allows to execute a graph exactly as it is, without going\nthrough a graph executor which would run passes on the graph before\ninterpreting it. I found this feature extremely helpful when I worked on\na stress-testing script to shake out bugs from the TE fuser: I needed to\nexecute a very specific set of passes on a graph and nothing else, and\nthen execute exactly it.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23632505\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: ea81fc838933743e2057312d3156b77284d832ef", "pr_number": "44493", "files_changed": ["test/test_jit.py", "torch/csrc/jit/python/init.cpp"], "labels": ["oncall: jit"]}, "8b8986662f": {"title": "[JIT] Remove profiling nodes in autodiff forward graph (#44420)", "body": "Summary:\nPreviously we were not removing profiling nodes in graphs that required grad and contained diff graphs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44420\n\nReviewed By: bertmaher\n\nDifferential Revision: D23607482\n\nPulled By: eellison\n\nfbshipit-source-id: af095f3ed8bb3c5d09610f38cc7d1481cbbd2613", "pr_number": "44420", "files_changed": ["torch/csrc/jit/passes/insert_guards.cpp", "torch/csrc/jit/passes/insert_guards.h", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["oncall: jit"]}, "c967e7724e": {"title": "[quant] conv_transpose1d_prepack / conv_transpose1d_unpack (#40360)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40360\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22158982\n\nPulled By: z-a-f\n\nfbshipit-source-id: 844d02806554aaa68b521283703e630cc544d419", "pr_number": "40360", "files_changed": ["aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qconv_unpack.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py", "torch/testing/_internal/hypothesis_utils.py"], "labels": []}, "30fccc53a9": {"title": "[NNC] Don't attempt to refactor conditional scalars (#44223)", "body": "Summary:\nFixes a bug in the NNC registerizer for Cuda where it would hoist reads out of a conditional context when trying to cache them. As a quick fix, prevent scalar replacement if a usage is within a condition.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44223\n\nReviewed By: gchanan\n\nDifferential Revision: D23551247\n\nPulled By: nickgg\n\nfbshipit-source-id: 17a7bf2be4c8c3dd8a9ab7997dce9aea200c3685", "pr_number": "44223", "files_changed": ["test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/test_registerizer.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/registerizer.cpp", "torch/csrc/jit/tensorexpr/registerizer.h"], "labels": ["oncall: jit"]}, "09892de815": {"title": "Clarify track_running_stats docs; Make SyncBatchNorm track_running_stats behavior consistent (#44445)", "body": "Summary:\ncontext: https://github.com/pytorch/pytorch/pull/38084\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44445\n\nReviewed By: colesbury\n\nDifferential Revision: D23634216\n\nPulled By: mrshenli\n\nfbshipit-source-id: d1242c694dec0e7794651f8031327625eb9989ee", "pr_number": "44445", "files_changed": ["torch/nn/modules/batchnorm.py"], "labels": ["module: docs", "open source", "triaged"]}, "77cc7d1ecd": {"title": "C++ APIs Transformer NN Module Top Layer (#44333)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44333\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23584010\n\nPulled By: glaringlee\n\nfbshipit-source-id: 990026e3f1b5ae276776e344ea981386cb7528fe", "pr_number": "44333", "files_changed": ["test/cpp/api/transformer.cpp", "torch/csrc/api/include/torch/nn/modules.h", "torch/csrc/api/include/torch/nn/modules/transformer.h", "torch/csrc/api/include/torch/nn/modules/transformercoder.h", "torch/csrc/api/include/torch/nn/options.h", "torch/csrc/api/include/torch/nn/options/transformer.h", "torch/csrc/api/include/torch/nn/options/transformerlayer.h", "torch/csrc/api/include/torch/nn/pimpl.h", "torch/csrc/api/src/nn/modules/transformer.cpp", "torch/csrc/api/src/nn/options/transformer.cpp"], "labels": []}, "f44de7cdc3": {"title": "Add missing rpc.shutdown() (#44417)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44417\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D23626208\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4ff8cad0e1193f99518804c21c9dd26ae718f4eb", "pr_number": "44417", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "a9754fb860": {"title": "Use TP Tensor.metadata to carry device info (#44396)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44396\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D23602576\n\nPulled By: mrshenli\n\nfbshipit-source-id: c639789979b2b71fc165efbcf70f37b4c39469df", "pr_number": "44396", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h"], "labels": []}, "b6b1c01adf": {"title": "torch.view_as_complex fails with segfault for a zero dimensional tensor (#44175)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44061\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44175\n\nReviewed By: colesbury\n\nDifferential Revision: D23628103\n\nPulled By: anjali411\n\nfbshipit-source-id: 6f70b5824150121a1617c0757499832923ae02b5", "pr_number": "44175", "files_changed": ["aten/src/ATen/native/ComplexHelper.h", "test/test_torch.py"], "labels": ["module: complex", "open source"]}, "9a3b83cbf2": {"title": "Update submodule gloo to have latest commits to enable it can work on Windows (#44529)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44529\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23650123\n\nPulled By: mrshenli\n\nfbshipit-source-id: b5b891cbcec51a14379d6604af63c714c32d93e7", "pr_number": "44529", "files_changed": ["third_party/gloo"], "labels": ["open source"]}, "d07d25a8c5": {"title": "Fix MSELoss when target.requires_grad is True. (#44437)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44437\n\nMSELoss had a completely different (and incorrect, see https://github.com/pytorch/pytorch/issues/43228) path when target.requires_grad was True.\n\nThis PR does the following:\n1) adds derivative support for target via the normal derivatives.yaml route\n2) kill the different (and incorrect) path for when target.requires_grad was True\n3) modify the MSELoss CriterionTests to verify that the target derivative is checked.\n\nTODO:\n1) do we still need check_criterion_jacobian when we run grad/gradgrad checks?\n2) ensure the Module tests check when target.requires_grad\n3) do we actually test when reduction='none' and reduction='mean'?\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23612166\n\nPulled By: gchanan\n\nfbshipit-source-id: 4f74d38d8a81063c74e002e07fbb7837b2172a10", "pr_number": "44437", "files_changed": ["tools/autograd/derivatives.yaml", "torch/csrc/api/include/torch/nn/functional/loss.h", "torch/nn/functional.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "567c51cce9": {"title": "In common_distributed, fix TEST_SKIPS multiprocessing manager (#44525)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44525\n\nSince `TEST_SKIPS` is a global multiprocessing.manager, this was causing\nissues when one test would fail and make the rest of the tests fail during\nsetup due to networking errors.\n\nSee the failed CI job: https://app.circleci.com/pipelines/github/pytorch/pytorch/212491/workflows/0450151d-ca09-4cf6-863d-272de6ed917f/jobs/7389065 for an example, where `test_ddp_backward` failed but then caused the rest of the tests to fail at the line `test_skips.update(TEST_SKIPS)`.\n\nTo fix this issue, at the end of every test we revert `TEST_SKIPS` back to a regular dict, and redo the conversion to a `mulitiprocessing.Manager` in the next test, which prevents these errors.\nghstack-source-id: 111844724\n\nTest Plan: CI\n\nReviewed By: malfet\n\nDifferential Revision: D23641618\n\nfbshipit-source-id: 27ce823968ece9804bb4dda898ffac43ef732b89", "pr_number": "44525", "files_changed": ["torch/testing/_internal/common_distributed.py"], "labels": []}, "b73b44f976": {"title": "[PyTorch Mobile] Move some string ops to register_prim_ops.cpp and make them selective (#44500)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44500\n\nSome user models are using those operators. Unblock them while keep the ops selective.\n\nTest Plan: CI\n\nReviewed By: linbinyu\n\nDifferential Revision: D23634769\n\nfbshipit-source-id: 55841d1b07136b6a27b6a39342f321638dc508cd", "pr_number": "44500", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_string_ops.cpp"], "labels": ["fb-exported", "oncall: jit"]}, "ea55820606": {"title": "[dper3] Export PackSegments and UnpackSegments to Pytorch", "body": "Summary: As title.\n\nTest Plan:\n```\nbuck test //caffe2/caffe2/python/operator_test/:torch_integration_test -- test_pack_segments\n```\n\nReviewed By: yf225\n\nDifferential Revision: D23610495\n\nfbshipit-source-id: bd8cb61f2284a08a54091a4f982f01fcf681f215", "pr_number": null, "files_changed": ["caffe2/operators/pack_segments.cc", "caffe2/operators/pack_segments.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": []}, "3de2c0b42f": {"title": "Fix L1Loss when target.requires_grad is True. (#44471)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44471\n\nL1Loss had a completely different (and incorrect, see #43228) path when target.requires_grad was True.\n\nThis PR does the following:\n\n1) adds derivative support for target via the normal derivatives.yaml route\n2) kill the different (and incorrect) path for when target.requires_grad was True\n3) modify the L1Loss CriterionTests to verify that the target derivative is checked.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23626008\n\nPulled By: gchanan\n\nfbshipit-source-id: 2828be16b56b8dabe114962223d71b0e9a85f0f5", "pr_number": "44471", "files_changed": ["tools/autograd/derivatives.yaml", "torch/nn/functional.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "df6ea62526": {"title": "Add nondeterministic check to new upsample overloads", "body": "Summary: I think these were missed due to a code landing race condition.\n\nTest Plan: Fixes CUDA tests with PR 43025 applied.\n\nReviewed By: iseeyuan, AshkanAliabadi\n\nDifferential Revision: D23639566\n\nfbshipit-source-id: 1322d7708e246b075a66588e7e54f4e12092477f", "pr_number": null, "files_changed": ["aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu"], "labels": []}, "7d78a6fcdd": {"title": "Update interpolate to use new upsample overloads (#43025)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43025\n\n- Use new overloads that better reflect the arguments to interpolate.\n- More uniform interface for upsample ops allows simplifying the Python code.\n- Also reorder overloads in native_functions.yaml to give them priority.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37177\n\nghstack-source-id: 106938111\n\nTest Plan:\ntest_nn has pretty good coverage.\n\nRelying on CI for ONNX, etc.\n\nDidn't test FC because this change is *not* forward compatible.\n\nTo ensure backwards compatibility, I ran this code before this change\n\n```python\ndef test_func(arg):\n    interp = torch.nn.functional.interpolate\n    with_size = interp(arg, size=(16,16))\n    with_scale = interp(arg, scale_factor=[2.1, 2.2], recompute_scale_factor=False)\n    with_compute = interp(arg, scale_factor=[2.1, 2.2])\n    return (with_size, with_scale, with_compute)\n\ntraced_func = torch.jit.trace(test_func, torch.randn(1,1,1,1))\n\nsample = torch.randn(1, 3, 7, 7)\noutput = traced_func(sample)\n\nassert not torch.allclose(output[1], output[2])\n\ntorch.jit.save(traced_func, \"model.pt\")\ntorch.save((sample, output), \"data.pt\")\n```\n\nthen this code after this change\n\n```python\nmodel = torch.jit.load(\"model.pt\")\nsample, golden = torch.load(\"data.pt\")\nresult = model(sample)\nfor r, g in zip(result, golden):\n    assert torch.allclose(r, g)\n```\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D21209991\n\nfbshipit-source-id: 5b2ebb7c3ed76947361fe532d1dbdd6faa3544c8", "pr_number": "43025", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "test/onnx/expect/TestOperators.test_upsample_nearest_scale.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale_default_scale_factor.expect", "torch/nn/functional.py", "torch/onnx/symbolic_helper.py"], "labels": ["fb-exported"]}, "cdf5e2ae86": {"title": "add typing annotations for a few torch.utils.* modules (#43806)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43431. Depends on [gh-43862](https://github.com/pytorch/pytorch/pull/43862) (EDIT: now merged)\n\nModules:\n- torch.utils.mkldnn\n- torch.utils.mobile_optimizer\n- torch.utils.bundled_inputs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43806\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23635151\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: a85b75a7927dde6cc55bcb361f8ff601ffb0b2a1", "pr_number": "43806", "files_changed": ["mypy.ini", "tools/pyi/gen_pyi.py", "torch/_C/__init__.pyi.in", "torch/_C/_nn.pyi.in", "torch/jit/_serialization.py", "torch/utils/bundled_inputs.py", "torch/utils/tensorboard/writer.py"], "labels": ["module: typing", "open source", "triaged"]}, "a61318a535": {"title": "[pytorch] Replace mobile run_method with get_method and operator() (#44202)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44202\n\nIn preparation for changing mobile run_method() to be variadic, this diff:\n\n* Implements get_method() for mobile Module, which is similar to find_method but expects the method to exist.\n* Replaces calls to the current nonvariadic implementation of run_method() by calling get_method() and then invoking the operator() overload on Method objects.\nghstack-source-id: 111848222\n\nTest Plan: CI, and all the unit tests which currently contain run_method that are being changed.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23436351\n\nfbshipit-source-id: 4655ed7182d8b6f111645d69798465879b67a577", "pr_number": "44202", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_lite.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/python/script_init.cpp"], "labels": ["oncall: jit"]}, "442957d8b6": {"title": "[pytorch] Remove mobile nonvariadic run_method (#44235)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44235\n\nRemoves nonvariadic run_method() from mobile Module entirely (to be later replaced by a variadic version). All use cases should have been migrated to use get_method() and Method::operator() in D23436351\nghstack-source-id: 111848220\n\nTest Plan: CI\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23484577\n\nfbshipit-source-id: 602fcde61e13047a34915b509da048b9550103b1", "pr_number": "44235", "files_changed": ["torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h"], "labels": ["oncall: jit"]}, "11fb51d093": {"title": "[quant][graphmode][fx][fix] Support dictionary output (#44508)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44508\n\nBug fix for dictionary output\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23636182\n\nfbshipit-source-id: 0c00cd6b9747fa3f8702d7f7a0d5edb31265f466", "pr_number": "44508", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "39bb455e36": {"title": "Update fallback kernel for Autograd keys. (#44349)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44349\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23589807\n\nPulled By: ailzhang\n\nfbshipit-source-id: 0e4b0bf3e07bb4e35cbf1bda22f7b03193eb3dc4", "pr_number": "44349", "files_changed": ["aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "test/test_dispatch.py"], "labels": []}, "b7ef4eec46": {"title": "[NNC] Add loop slicing transforms (#43854)", "body": "Summary:\nAdd new transforms `sliceHead` and `sliceTail` to `LoopNest`, for example:\n\nBefore transformation:\n```\nfor x in 0..10:\n  A[x] = x*2\n```\n\nAfter `sliceHead(x, 4)`:\n\n```\nfor x in 0..4:\n  A[x] = x*2\nfor x in 4..10:\n  A[x] = x*2\n```\n\nAfter `sliceTail(x, 1)`:\n```\nfor x in 0..4:\n  A[x] = x*2\nfor x in 4..9:\n  A[x] = x*2\nfor x in 9..10:\n  A[x] = x*2\n```\n\n`sliceHead(x, 10)` and `sliceTail(x, 10)` is no-op.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43854\n\nTest Plan: Tests are added in `test_loopnest.cpp`, the tests cover the basic transformations, and also tests the combination with other transformations such as `splitWithTail`.\n\nReviewed By: nickgg\n\nDifferential Revision: D23417366\n\nPulled By: cheng-chang\n\nfbshipit-source-id: 06c6348285f2bafb4be3286d1642bfbe1ea499bf", "pr_number": "43854", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/DesignOverview.md", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.h", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["oncall: jit"]}, "5579b53a7f": {"title": "Fix SmoothL1Loss when target.requires_grad is True. (#44486)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44486\n\nSmoothL1Loss had a completely different (and incorrect, see #43228) path when target.requires_grad was True.\n\nThis PR does the following:\n\n1) adds derivative support for target via the normal derivatives.yaml route\n2) kill the different (and incorrect) path for when target.requires_grad was True\n3) modify the SmoothL1Loss CriterionTests to verify that the target derivative is checked.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23630699\n\nPulled By: gchanan\n\nfbshipit-source-id: 0f94d1a928002122d6b6875182867618e713a917", "pr_number": "44486", "files_changed": ["tools/autograd/derivatives.yaml", "torch/csrc/api/include/torch/nn/functional/loss.h", "torch/nn/functional.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "2b8f0b2023": {"title": "[caffe2] adds Cancel to OperatorBase and NetBase (#44145)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44145\n\n## Motivation\n\n* To be able to make C2 ops cancellable so we can safely exit.\n* Some C2 operators are now blocking thus being non-cancellable. If an error\n  occurs we need to be able to safely stop all net execution so we can throw\n  the exception to the caller.\n\n## Summary\n*  Adds `NetBase::Cancel()` to NetBase which iterates over the entire list of\n   operators and call Cancel.\n* Cancel on all ops was added to Net since there's nothing Asyc specific about it.\n* `AsyncSchedulingNet` calls parent Cancel.\n* To preserve backwards compatibility, `AsyncSchedulingNet`'s Cancel still calls\n   `CancelAndFinishAsyncTasks` .\n* Adds `Cancel()` to `OperatorBase`.\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23279202\n\nfbshipit-source-id: e1bb0ff04a4e1393f935dbcac7c78c0baf728550", "pr_number": "44145", "files_changed": ["caffe2/core/net.cc", "caffe2/core/net.h", "caffe2/core/net_async_scheduling.cc", "caffe2/core/operator.h", "caffe2/core/plan_executor.cc"], "labels": ["fb-exported"]}, "8a574c7104": {"title": "[Cmake] Drop quotation marks around `$ENV{MAX_JOBS}` (#44557)", "body": "Summary:\nSolves `the '-j' option requires a positive integer argument` error on some systems when MAX_JOBS is not defined\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44557\n\nReviewed By: vkuzo\n\nDifferential Revision: D23653511\n\nPulled By: malfet\n\nfbshipit-source-id: 7d86fb7fb6c946c34afdc81bf2c3168a74d00a1f", "pr_number": "44557", "files_changed": ["cmake/External/nccl.cmake"], "labels": []}, "192c4111a3": {"title": "Simplify target handling in nn gradcheck. (#44507)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44507\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23635799\n\nPulled By: gchanan\n\nfbshipit-source-id: 75090d6a48771e5c92e737a0829fbfa949f7c8a7", "pr_number": "44507", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "70dfeb44bd": {"title": "MinMax based observers: respect device affinity for state_dict (#44537)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44537\n\nOriginally, the `min_val`, `max_val`, `min_vals`, `max_vals`\nattributes of observers were Tensors but not buffers.  They had custom\nstate_dict save/load code to ensure their state was saved.\n\nAt some point, these attributes became buffers, and the custom\nsave/load code remained. This introduced a subtle bug:\n* create model A, move it to a device (cpu/cuda) and save its state_dict\n* create model B, load its state dict.\n* `min_val|min_vals|max_val|max_vals` would always be loaded to model A's device, even if the rest of model B was on a different device\n* the above is inconsistent with how save/load on different devices is expected to work (see https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices)\n\nIn practice, the case people would sometimes hit is:\n* model A is on CPU, state dict is saved\n* model B is created and moved to GPU, state_dict from model A is loaded\n* assertions throw when operations are attempted across different devices\n\nThis PR fixes the behavior by removing the custom save/load where\npossible and letting the default `nn.Module` save/load code handle\ndevice assignment.  We special case `PerChannelMinMaxObserver` and its\nchildren to allow for loading buffers or different size, which is\nnormal.\n\nThere are some followups to also enable this for HistogramObserver\nand FakeQuantize, which can be done in separate PRs due to higher\ncomplexity.\n\nTest Plan:\n```\npython test/test_quantization.py TestObserver.test_state_dict_respects_device_affinity\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23644493\n\nfbshipit-source-id: 0dbb6aa309ad569a91a663b9ee7e44644080032e", "pr_number": "44537", "files_changed": ["test/quantization/test_workflow_module.py", "torch/quantization/observer.py"], "labels": []}, "8bec7cfa91": {"title": "[rpc] rename some functions (#43042)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43042\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D23228894\n\nPulled By: wanchaol\n\nfbshipit-source-id: 3702b7826ecb455073fabb9dc5dca804c0e092b2", "pr_number": "43042", "files_changed": ["torch/testing/_internal/distributed/rpc/jit/rpc_test.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py"], "labels": ["oncall: jit"]}, "3e5df5f216": {"title": "[rpc][jit] support rpc_sync in TorchScript (#43043)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43043\n\nThis add the support for rpc_sync in TorchScript in a way similar to\nrpc_async\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D23252039\n\nPulled By: wanchaol\n\nfbshipit-source-id: 8a05329cb8a24079b2863178b73087d47273914c", "pr_number": "43043", "files_changed": ["aten/src/ATen/core/interned_strings.h", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/csrc/jit/serialization/python_print.cpp", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["oncall: jit"]}, "ab6126b50e": {"title": "[rpc][jit] support remote call in TorchScript (#43046)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43046\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D23621108\n\nPulled By: wanchaol\n\nfbshipit-source-id: e8152c6cdd3831f32d72d46ac86ce22f3f13c651", "pr_number": "43046", "files_changed": ["aten/src/ATen/core/interned_strings.h", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/csrc/jit/serialization/python_print.cpp", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["oncall: jit"]}, "7632484000": {"title": "Add some batched gradient tests (#44494)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44494\n\nThese tests check (most) operations that are useful for bayesian logistic\nregression (BLR) models. Said operators are basically those found in the\nlog_prob functions of Distributions objects. This PR is not a general,\nstructured solution for testing batched gradients (see \"Alternative\nsolution\" for that), but I wanted to test a small subset of operations\nto confirm that the BLR use case works.\n\nThere will be follow-up PRs implementing support for some missing\noperations for the BLR use case.\n\nAlternative solution\n=====================\n\nIdeally, and in the future, I want to autogenerate tests from\ncommon_method_invocations and delete all of the manual tests\nintroduced by this PR. However, if we were to do this now,\nwe would need to store the following additional metadata somewhere:\n- operator name, supports_batched_grad, allow_vmap_fallback_usage\n\nWe could store that metadata as a separate table from\ncommon_method_invocations, or add two columns to\ncommon_method_invocations. Either way that seems like a lot of work and\nthe situation will get better once vmap supports batched gradients for\nall operators (on the fallback path).\n\nI am neutral between performing the alternative approach now v.s. just\nmanually writing out some tests for these operations, so I picked the\neasier approach. Please let me know if you think it would be better to\npursue the alternative approach now.\n\nTest Plan: - `pytest test/test_vmap.py -v -k \"BatchedGrad\"`\n\nReviewed By: anjali411\n\nDifferential Revision: D23650408\n\nPulled By: zou3519\n\nfbshipit-source-id: 2f26c7ad4655318a020bdaab5c767cd3956ea5eb", "pr_number": "44494", "files_changed": ["test/test_vmap.py"], "labels": []}, "e2bb34e860": {"title": "Batched grad support for: slice, select, diagonal (#44505)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44505\n\nAdded batching rules for slice_backward, select_backward, and\ndiagonal_backward.\n\nTest Plan: - new tests: `pytest test/test_vmap.y -v -k \"BatchedGrad\"`\n\nReviewed By: agolynski, anjali411\n\nDifferential Revision: D23650409\n\nPulled By: zou3519\n\nfbshipit-source-id: e317609d068c88ee7bc07fab88b2b3acb8fad7e1", "pr_number": "44505", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "aten/src/ATen/VmapTransforms.h", "test/test_autograd.py", "test/test_vmap.py"], "labels": []}, "dd4bbe1a79": {"title": "Add iterator like functionality for DispatchKeySet (#44066)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44066\n\nAdd STL Input iterator to DispatchKeySet:\n* Iterator is able to iterate from first not undefined DispatchKey\nto NumDispatchKeys.\n* Iterator is invalidated once underlying DispatchKeySet is invalidated\n\nNote see http://www.cplusplus.com/reference/iterator/ for comparisons of\ndifferent iterators.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23611405\n\nPulled By: linux-jedi\n\nfbshipit-source-id: 131b287d60226a1d67a6ee0f88571f8c4d29f9c3", "pr_number": "44066", "files_changed": ["c10/core/DispatchKeySet.h", "c10/test/core/DispatchKeySet_test.cpp"], "labels": []}, "1fb5883072": {"title": "removing conv filters from conv pattern matching (#44512)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44512\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23637409\n\nPulled By: z-a-f\n\nfbshipit-source-id: ad5be0fa6accfbcceaae9171bf529772d87b4098", "pr_number": "44512", "files_changed": ["torch/csrc/jit/passes/graph_rewrite_helper.cpp"], "labels": ["oncall: jit"]}, "a82ea6a91f": {"title": "[quant][graphmode][fx][fix] Support None qconfig in convert (#44524)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44524\n\nNone qconfig is not handled previously\ncloses: https://github.com/pytorch/pytorch/issues/44438\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23640269\n\nfbshipit-source-id: 8bfa88c8c78d4530338d9d7fa9669876c386d91f", "pr_number": "44524", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "c2b40b056a": {"title": "Filter default tests for `clang` coverage in oss", "body": "Summary: Some tests like `test_dataloader.py` are not able to run under `clang` in oss, because it generates too large intermediate files (~40G) that can't be merged by `llvm`. Skip them when user doesn't specify the `--run-only` option\n\nTest Plan: Test locally. But still, not recomend user to run `clang` coverage in default mode, because it takes too much space.\n\nReviewed By: malfet\n\nDifferential Revision: D23549829\n\nfbshipit-source-id: 0737e6e9dcbe3f38de00580ee6007906e743e52f", "pr_number": null, "files_changed": ["tools/code_coverage/README.md", "tools/code_coverage/package/oss/init.py"], "labels": []}, "f3a79b881f": {"title": "add `lcov` to oss for beautiful html report (#44568)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44568\n\nBy `lcov`, we can generate beautiful html. It's better than current file report and line report. Therefore in oss gcc, remove `export` code and `file/line level report` code, only use the html report.\n\nBut in clang, since such tool is not available, we will still use file-report and line-report generated by ourself.\n\nTest Plan:\nTest in docker ubuntu machine.\n## Mesurement\n1. After running `atest`, it takes about 15 mins to collect code coverage and genrate the report.\n```\n# gcc code coverage\npython oss_coverage.py --run-only=atest\n```\n\n## Presentation\n**The html result looks like:**\n\n*Top Level:*\n\n{F328330856}\n\n*File Level:*\n\n{F328336709}\n\nReviewed By: malfet\n\nDifferential Revision: D23550784\n\nfbshipit-source-id: 1fff050e7f7d1cc8e86a6a200fd8db04b47f5f3e", "pr_number": "44568", "files_changed": ["tools/code_coverage/README.md", "tools/code_coverage/oss_coverage.py", "tools/code_coverage/package/oss/cov_json.py", "tools/code_coverage/package/tool/print_report.py", "tools/code_coverage/package/tool/summarize_jsons.py"], "labels": ["fb-exported"]}, "42f9f2f38f": {"title": "[fix] ReduceOps throw error if dim is repeated (#44281)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44273\n\nTODO\n\n* [x] Add test\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44281\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23569004\n\nPulled By: ezyang\n\nfbshipit-source-id: 1ca6523fef168c8ce252aeb7ca418be346b297bf", "pr_number": "44281", "files_changed": ["aten/src/ATen/native/ReduceOpsUtils.h", "test/test_torch.py"], "labels": ["module: operators (deprecated)", "module: xla", "open source", "triaged"]}, "b6f0ea0c71": {"title": "[quant][graphmode][fx][fix] Remove qconfig in convert (#44526)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44526\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23641960\n\nfbshipit-source-id: 546da1c16694d1e1dfb72629085acaae2165e759", "pr_number": "44526", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "db24c5c582": {"title": "Change code coverage option name (#43999)", "body": "Summary:\nAccording to [documentation](https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/cmake.py#L265), only options starts with `BUILD_` / `USE_` / `CMAKE_` in `CMakeLists.txt` can be imported by environment variables.\n\n ---\nThis diff is originally intended to enable  `c++` source coverage with `CircleCI` and `codecov.io`, but we will finish it in the future. You can find the related information in the diff history. Following is the originally procedur:\n\nBased on [this pull request](https://github.com/pytorch/pytorch/commit/1bda5e480c0fe01dae8db718cd457befa1b46ac6), life becomes much easier for this time.\n1.in `build.sh`\n- Enable coverage builld option for c++\n- `apt-get install lcov`\n\n2.in `test.sh`\n- run `lcov`\n\n3.in `pytorch-job-specs.yml`\n- copy coverage.info to `test/` folder and upload it to codecov.io\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43999\n\nTest Plan: Test on github\n\nReviewed By: malfet\n\nDifferential Revision: D23464656\n\nPulled By: scintiller\n\nfbshipit-source-id: b2365691f04681d25ba5c00293fbcafe8e8e0745", "pr_number": "43999", "files_changed": ["CMakeLists.txt", "cmake/Summary.cmake"], "labels": ["fb-exported"]}, "566b8d0650": {"title": "handle missing NEON vst1_*_x2 intrinsics (#44198) (#44199)", "body": "Summary:\nCentOS 8 on AArch64 has vld1_* intrinsics but lacks vst1q_f32_x2 one.\n\nThis patch checks for it and handle it separately to vld1_* ones.\n\nFixes https://github.com/pytorch/pytorch/issues/44198\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44199\n\nReviewed By: seemethere\n\nDifferential Revision: D23641273\n\nPulled By: malfet\n\nfbshipit-source-id: c2053c8e0427705eaeeeb82ec030925bff22623a", "pr_number": "44199", "files_changed": ["CMakeLists.txt", "aten/src/ATen/cpu/vec256/intrinsics.h", "aten/src/ATen/cpu/vec256/missing_vst1_neon.h"], "labels": ["open source"]}, "2ae74c0632": {"title": "Compile less legacy code when BUILD_CAFFE2 is set to False (take 2) (#44453)", "body": "Summary:\n2nd attempt to land https://github.com/pytorch/pytorch/pull/44079\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44453\n\nReviewed By: walterddr, seemethere\n\nDifferential Revision: D23619528\n\nPulled By: malfet\n\nfbshipit-source-id: c7c206ebd327dcf3994789bd47008b05ff862fe7", "pr_number": "44453", "files_changed": ["aten/src/ATen/test/CMakeLists.txt", "caffe2/CMakeLists.txt", "caffe2/core/CMakeLists.txt", "caffe2/proto/CMakeLists.txt", "caffe2/utils/CMakeLists.txt", "torch/csrc/jit/serialization/import.cpp"], "labels": ["oncall: jit"]}, "64b4307d47": {"title": "[NNC] Cuda Codegen - mask loops bound to block/thread dimensions (#44325)", "body": "Summary:\nFix an issue where loops of different sizes are bound to the same Cuda dimension / metavar.\n\nComing soon more info and tests...\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44325\n\nReviewed By: colesbury\n\nDifferential Revision: D23628859\n\nPulled By: nickgg\n\nfbshipit-source-id: 3621850a4cc38a790b62ad168d32e7a0e2462fad", "pr_number": "44325", "files_changed": ["test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/codegen.h", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.h"], "labels": ["oncall: jit"]}, "d729e2965e": {"title": "[TensorExpr] Do not inline autodiff graphs if they contain prim::TypeCheck nodes. (#44564)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44564\n\nBefore this change we sometimes inlined autodiff subgraph containing\nfusion groups. This happened because we didn't look for 'unsupported'\nnodes recursively (maybe we should), but fusion groups were inside\nif-nodes.\n\nThe problem was detected by bertmaher in 'LearningToPaint' benchmark\ninvestigation where this bug caused us to keep constantly hitting\nfallback paths of the graph.\n\nTest Plan: Imported from OSS\n\nReviewed By: bwasti\n\nDifferential Revision: D23657049\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 7c853424f6dce4b5c344d6cd9c467ee04a8f167e", "pr_number": "44564", "files_changed": ["torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp"], "labels": ["oncall: jit"]}, "1f0dcf39fc": {"title": "[JIT] dont optimize device dtype on inline (#43363)", "body": "Summary:\nFollow up to https://github.com/pytorch/pytorch/pull/36404\n\nAdding prim::device and prim::dtype to list of skipped peepholes when we run inlining. In the long term another fix may not be to encode shape / dtype info on the traced graph, because it is not guaranteed to be correct. This is blocked by ONNX currently.\n\nPartial fix for https://github.com/pytorch/pytorch/issues/43134\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43363\n\nReviewed By: glaringlee\n\nDifferential Revision: D23383987\n\nPulled By: eellison\n\nfbshipit-source-id: 2e9c5160d39d690046bd9904be979d58af8d3a20", "pr_number": "43363", "files_changed": ["test/jit/test_tracer.py", "torch/csrc/jit/passes/peephole.cpp", "torch/onnx/utils.py"], "labels": ["oncall: jit"]}, "ab5fee2784": {"title": "Move the inline implementations of GradBucket class to the header. (#44339)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44339\n\nMoved the inline implementations of GradBucket class to the header for\nsuccinctness and readability. This coding style is also consistent with\nreducer.h under the same directory.\n\nTest Plan: buck test caffe2/torch/lib/c10d:ProcessGroupGlooTest\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23569701\n\nfbshipit-source-id: 237d9e2c5f63a6bcac829d0fcb4a5ba3bede75e5", "pr_number": "44339", "files_changed": ["torch/csrc/distributed/c10d/comm.cpp", "torch/csrc/distributed/c10d/comm.h"], "labels": ["fb-exported"]}, "82b4477948": {"title": "Pass the input tensor vector by const reference. (#44340)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44340\n\nChanged the constructor of GradBucket to pass the input by const\nreference and hence avoided unnecessary explicit move semantics. Since\npreviously the declaration and definition are separated, passing the input\ntensor vector by value looks quite bizarre.\n\nTest Plan: buck test caffe2/torch/lib/c10d:ProcessGroupGlooTest\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23569939\n\nfbshipit-source-id: db761d42e76bf938089a0b38e98e76a05bcf4162", "pr_number": "44340", "files_changed": ["torch/csrc/distributed/c10d/comm.h"], "labels": ["fb-exported"]}, "8641b55158": {"title": "fix dangling ptr in embedding_bag (#44571)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44571\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet, ngimel\n\nDifferential Revision: D23661007\n\nPulled By: glaringlee\n\nfbshipit-source-id: e4a54acd0de55f275828c1d1289a1f069de07291", "pr_number": "44571", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp"], "labels": []}, "d191caa3e7": {"title": "Cleanup workarounds for compiler bug of ROCm (#44579)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44579\n\nReviewed By: mruberry\n\nDifferential Revision: D23664481\n\nPulled By: ngimel\n\nfbshipit-source-id: ef698f26455e5827c5b5c0e5d42a1c95bcac8af4", "pr_number": "44579", "files_changed": ["aten/src/ATen/native/cuda/Reduce.cuh"], "labels": ["module: rocm", "open source"]}, "05c1f1d974": {"title": "[ROCm] remove thrust workaround in ScanKernels (#44553)", "body": "Summary:\nRemove ROCm workaround added in https://github.com/pytorch/pytorch/issues/39180.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44553\n\nReviewed By: mruberry\n\nDifferential Revision: D23663988\n\nPulled By: ngimel\n\nfbshipit-source-id: 71b2fd7db006d9d3459b908a996c4d96838ba742", "pr_number": "44553", "files_changed": ["aten/src/ATen/native/cuda/ScanKernels.cu"], "labels": ["module: rocm", "open source"]}, "6f2c3c39d2": {"title": "Add SNPE deps for caffe2 benchmark android binary", "body": "Summary:\nAdding snpe dependencies to caffe2_benchmark so that this can benchmark SNPE models on portal devices.\n\nAlso need to change ndk_libcxx to gnustl till snpe is updated to work with ndk.\n\nTest Plan: Tested on top of the stack.\n\nReviewed By: linbinyu\n\nDifferential Revision: D23569397\n\nfbshipit-source-id: a6281832804ed4fbb5a8406f436caeae1ff4fd2b", "pr_number": null, "files_changed": ["mode/aibench_caffe2_android"], "labels": []}, "0743d013a6": {"title": "fuse layernorm + quantize (#44232)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44232\n\nenhance layernorm to optionally quantize its output\nadd fusion code to replace instances of layernorm +quantization\n\nTest Plan:\ntested layernorm\nnet_runner\n\nP141557987\n\nReviewed By: venkatacrc\n\nDifferential Revision: D23510893\n\nfbshipit-source-id: 32f57ba2090d35d86dcc951e0f3f6a8901ab3153", "pr_number": "44232", "files_changed": ["caffe2/contrib/fakelowp/layernorm_fp16_fake_op.cc", "caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h", "caffe2/contrib/fakelowp/test/test_layernorm_nnpi_fp16.py", "caffe2/opt/fakefp16_transform.cc", "caffe2/opt/onnxifi_transformer.cc"], "labels": ["fb-exported"]}, "a309355be3": {"title": "[dper3] Create dper LearningRate low-level module", "body": "Summary: As title; this will unblock migration of several modules that need learning rate functionality.\n\nTest Plan:\n```\nbuck test //dper3/dper3/modules/low_level_modules/tests:learning_rate_test\n```\n\nWIP: need to add more learning rate tests for the different policies\n\nReviewed By: yf225\n\nDifferential Revision: D23584071\n\nfbshipit-source-id: f6656531b1caba38c3e3a7d6e16d9591563391e2", "pr_number": null, "files_changed": ["caffe2/sgd/learning_rate_op.cc", "caffe2/sgd/learning_rate_op.h"], "labels": []}, "e703c17967": {"title": "Revert D23584071: [dper3] Create dper LearningRate low-level module", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23584071 (https://github.com/pytorch/pytorch/commit/a309355be3d2c12dfdca17ee34a8259654c5b823)\n\nOriginal commit changeset: f6656531b1ca\n\nfbshipit-source-id: b0a93f4286053fb8576a70278edca3a7d89c722b", "pr_number": null, "files_changed": ["caffe2/sgd/learning_rate_op.cc", "caffe2/sgd/learning_rate_op.h"], "labels": []}, "7e91728f68": {"title": "Deprecates calling linspace and logspace without setting steps explicitly (#43860)", "body": "Summary:\n**BC-breaking note**\n\nThis change is BC-breaking for C++ callers of linspace and logspace if they were providing a steps argument that could not be converted to an optional.\n\n**PR note**\n\nThis PR deprecates calling linspace and logspace wihout setting steps explicitly by:\n\n- updating the documentation to warn that not setting steps is deprecated\n- warning (once) when linspace and logspace are called without steps being specified\n\nA test for this behavior is added to test_tensor_creation_ops. The warning only appears once per process, however, so the test would pass even if no warning were thrown. Ideally there would be a mechanism to force all warnings, include those from TORCH_WARN_ONCE, to trigger.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43860\n\nReviewed By: izdeby\n\nDifferential Revision: D23498980\n\nPulled By: mruberry\n\nfbshipit-source-id: c48d7a58896714d184cb6ff2a48e964243fafc90", "pr_number": "43860", "files_changed": ["aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/cuda/RangeFactories.cu", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_tensor_creation_ops.py", "torch/_torch_docs.py"], "labels": ["topic: bc-breaking"]}, "6d4a605ce9": {"title": "Fix bug simplifying if-then-else when it can be removed (#44462)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44462\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23671157\n\nPulled By: bertmaher\n\nfbshipit-source-id: b9b92ad0de1a7bd9bc1fcac390b542d885d0ca58", "pr_number": "44462", "files_changed": ["test/cpp/tensorexpr/test_simplify.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.h"], "labels": ["oncall: jit"]}, "82da6b3702": {"title": "[JIT] Fix jit-log verbosity selection logic. (#44587)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44587\n\nCurrently it's skewed by one.\n\nThe following test demonstrates it:\n```\n$ cat test.py\n\nimport torch\ndef foo(a,b):\n    return a*a*b\ntorch._C._jit_set_profiling_executor(True)\ntorch._C._jit_set_profiling_mode(True)\ntorch._C._jit_override_can_fuse_on_cpu(True)\ntorch._C._jit_set_texpr_fuser_enabled(True)\nf = torch.jit.script(foo)\nfor _ in range(10):\n    f(torch.rand(10), torch.rand(10))\n\n$ cat test_logging_levels.sh\n\nPYTORCH_JIT_LOG_LEVEL=\"tensorexpr_fuser\"    python test.py 2>&1 | grep DUMP   >& /dev/null && echo OK || echo FAIL\nPYTORCH_JIT_LOG_LEVEL=\"tensorexpr_fuser\"    python test.py 2>&1 | grep UPDATE >& /dev/null && echo FAIL || echo OK\nPYTORCH_JIT_LOG_LEVEL=\"tensorexpr_fuser\"    python test.py 2>&1 | grep DEBUG  >& /dev/null && echo FAIL || echo OK\n\nPYTORCH_JIT_LOG_LEVEL=\">tensorexpr_fuser\"   python test.py 2>&1 | grep DUMP   >& /dev/null && echo OK || echo FAIL\nPYTORCH_JIT_LOG_LEVEL=\">tensorexpr_fuser\"   python test.py 2>&1 | grep UPDATE >& /dev/null && echo OK || echo FAIL\nPYTORCH_JIT_LOG_LEVEL=\">tensorexpr_fuser\"   python test.py 2>&1 | grep DEBUG  >& /dev/null && echo FAIL || echo OK\n\nPYTORCH_JIT_LOG_LEVEL=\">>tensorexpr_fuser\"  python test.py 2>&1 | grep DUMP   >& /dev/null && echo OK || echo FAIL\nPYTORCH_JIT_LOG_LEVEL=\">>tensorexpr_fuser\"  python test.py 2>&1 | grep UPDATE >& /dev/null && echo OK || echo FAIL\nPYTORCH_JIT_LOG_LEVEL=\">>tensorexpr_fuser\"  python test.py 2>&1 | grep DEBUG  >& /dev/null && echo OK || echo FAIL\n```\n\nBefore this change:\n```\nOK\nFAIL\nOK\nOK\nOK\nFAIL\nOK\nOK\nOK\n```\n\nWith this change everthing passes.\n\nDifferential Revision: D23666813\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 4adaa5a3d06deadf54eae014a0d76588cdc5e20a", "pr_number": "44587", "files_changed": ["torch/csrc/jit/jit_log.cpp"], "labels": ["oncall: jit"]}, "bcf97b8986": {"title": "[JIT] Cleanup some places where we log graphs in executors. (#44588)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44588\n\n1) SOURCE_DUMP crashes when invoked on a backward graph since\n   `prim::GradOf` nodes can't be printed as sources (they don't have\n   schema).\n2) Dumping graph each time we execute an optimized plan produces lots of\n   output in tests where we run the graph multiple times (e.g.\n   benchmarks). Outputting that on the least level of verbosity seems\n   like an overkill.\n3) Duplicated log statement is removed.\n\nDifferential Revision: D23666812\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: b9a30e34fd39c85f3e13c3f1e3594e157e1c130f", "pr_number": "44588", "files_changed": ["torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["oncall: jit"]}, "7862827269": {"title": "[pytorch] Add variadic run_method for lite intepreter (#44337)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44337\n\nAdd a new run_method to mobile Module which is variadic (takes any number of arguments) to match full jit.\nghstack-source-id: 111909068\n\nTest Plan: Added new unit test to test_jit test suite\n\nReviewed By: linbinyu, ann-ss\n\nDifferential Revision: D23585763\n\nfbshipit-source-id: 007cf852290f03615b78c35aa6f7a21287ccff9e", "pr_number": "44337", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "torch/csrc/jit/mobile/module.h"], "labels": ["oncall: jit", "open source"]}, "fe26102a0e": {"title": "Enable TE in test_jit.py (#44200)", "body": "Summary:\nEnable TE in test_jit.py and adjust/fix tests accordingly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44200\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23673624\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 5999725c7aacc6ee77885eb855a41ddfb4d9a8d8", "pr_number": "44200", "files_changed": ["test/jit/test_autodiff_subgraph_slicing.py", "test/test_jit.py", "test/test_jit_cuda_fuser.py"], "labels": ["oncall: jit"]}, "8daaa3bc7e": {"title": "Fix latex error in heaviside docs (#44481)", "body": "Summary:\nThis fixes a `katex` error I was getting trying to build the docs:\n```\nParseError: KaTeX parse error: Undefined control sequence: \\0 at position 55: \u2026gin{cases}\n```\n\nThis failure was introduced in https://github.com/pytorch/pytorch/issues/42523.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44481\n\nReviewed By: colesbury\n\nDifferential Revision: D23627700\n\nPulled By: mruberry\n\nfbshipit-source-id: 9cc09c687a7d9349da79a0ac87d6c962c9cfbe2d", "pr_number": "44481", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source"]}, "68a5c361ae": {"title": "Adding Adapative Autorange to benchmark utils. (#44607)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44219\n\nRebasing https://github.com/pytorch/pytorch/pull/44288 and fixing the git history.\n\nThis allows users to bencmark code without having to specify how long to run the benchmark. It runs the benchmark until the variance (IQR / Median) is low enough that we can be confident in the measurement.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44607\n\nTest Plan: There are unit tests, and we manually tested using Examples posted in git.\n\nReviewed By: robieta\n\nDifferential Revision: D23671208\n\nPulled By: bitfort\n\nfbshipit-source-id: d63184290b88b26fb81c2452e1ae701c7d513d12", "pr_number": "44607", "files_changed": ["test/test_utils.py", "torch/utils/_benchmark/utils/common.py", "torch/utils/_benchmark/utils/timer.py"], "labels": []}, "c68a99bd61": {"title": "[numpy] Add `torch.exp2` (#44184)", "body": "Summary:\nReference https://github.com/pytorch/pytorch/issues/42515\n\nTODO\n* [x] Add tests\n* [x] Add docs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44184\n\nReviewed By: ngimel\n\nDifferential Revision: D23674237\n\nPulled By: mruberry\n\nfbshipit-source-id: 7f4fb1900fad3051cd7fc9d3d7f6d985c5fb093c", "pr_number": "44184", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "c10/util/math_compat.h", "docs/source/torch.rst", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: numpy", "open source"]}, "7040a070e3": {"title": "[torch] Minor: Avoid ostreamstring in Operator's canonicalSchemaString() (#44442)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44442\n\nI noticed lock contention on startup as lookupByLiteral() was\ncalling registerPendingOperators() - some calls were holding the\nlock for 10+ ms, as operators were being registered.\n\ncanonicalSchemaString() was using ostreamstring, which isn't typically\nparticularly fast (partly because of c++ spec locale requirements).\nIf we repalce with regular c++ string appends, it's somewhat faster\n(which isn't hard when comparing with stringstream; albeit a bit\nmore codegen)\n\nOver the first minute or so, this cuts out 1.4 seconds under the\nOperatorRegistry lock (as part of registerPendingOperators) in the\nfirst couple minutes of run time (mostly front-loaded) when running\nsync sgd.\n\nAs an example, before:\n   registerPendingOperators 12688 usec for 2449 operators\nAfter:\n   registerPendingOperators 6853 usec for 2449 operators\nghstack-source-id: 111862971\n\nTest Plan: buck test mode/dev-nosan caffe2/test/cpp/...\n\nReviewed By: ailzhang\n\nDifferential Revision: D23614515\n\nfbshipit-source-id: e712f9dac5bca0b1876e11fb8f0850402f03873a", "pr_number": "44442", "files_changed": ["torch/csrc/jit/runtime/operator.cpp"], "labels": ["oncall: jit"]}, "bd257a17a1": {"title": "Add HIP/ROCm version to collect_env.py (#44106)", "body": "Summary:\nThis adds HIP version info to the `collect_env.py` output.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44106\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D23652341\n\nPulled By: zou3519\n\nfbshipit-source-id: a1f5bce8da7ad27a1277a95885934293d0fd43c5", "pr_number": "44106", "files_changed": ["torch/utils/collect_env.py"], "labels": ["open source", "triaged"]}, "105132b891": {"title": "Move ONNX circle ci build to torch and remove all caffe2 CI job/workflows (#44595)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44595\n\nReviewed By: seemethere\n\nDifferential Revision: D23670280\n\nPulled By: walterddr\n\nfbshipit-source-id: b32633912f6c8b4606be36b90f901e636567b355", "pr_number": "44595", "files_changed": [".circleci/cimodel/data/caffe2_build_data.py", ".circleci/cimodel/data/caffe2_build_definitions.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/generate_config_yml.py", ".circleci/verbatim-sources/build-parameters/caffe2-build-params.yml", ".circleci/verbatim-sources/job-specs/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".jenkins/caffe2/test.sh", "scripts/onnx/test.sh", "test/onnx/test_pytorch_helper.py"], "labels": []}, "95a69a7d09": {"title": "adds list_gpu_processes function (#44616)", "body": "Summary:\nper title, to make it easier to track the creation of stray contexts:\n```\npython -c \"import torch; a=torch.randn(1, device='cuda'); print(torch.cuda.memory.list_gpu_processes(0)); print(torch.cuda.memory.list_gpu_processes(1))\"\nGPU:0\nprocess      79749 uses      601.000 MB GPU memory\nGPU:1\nno processes are running\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44616\n\nReviewed By: mruberry\n\nDifferential Revision: D23675739\n\nPulled By: ngimel\n\nfbshipit-source-id: ffa14cad9d7144e883de13b1c2c6817bd432f53a", "pr_number": "44616", "files_changed": ["torch/cuda/memory.py"], "labels": []}, "21a09ba94d": {"title": "Fix lerp.cu bug when given discontiguous out tensor (#44559)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44559\n\nPlease refer to the discussion at the bottom of https://github.com/pytorch/pytorch/pull/43541 about the bug.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23655403\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 10e4ce5c2fe7bf6e95bcfac4033202430292b03f", "pr_number": "44559", "files_changed": ["aten/src/ATen/native/cuda/Lerp.cu"], "labels": []}, "ace81b6794": {"title": "Remove an extra empty line in the warning comments. (#44622)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44622\n\nRemove an extra empty line in the warning comments.Remove an extra empty line.\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23674070\n\nfbshipit-source-id: 4ee570590c66a72fb808e9ee034fb773b833efcd", "pr_number": "44622", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["fb-exported"]}, "e261e0953e": {"title": "Fix centos8 gcc (#44644)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44198 properly this time\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44644\n\nReviewed By: albanD\n\nDifferential Revision: D23684909\n\nPulled By: malfet\n\nfbshipit-source-id: cea6f6e2ae28138f6b93a6513d1abd36d14ae573", "pr_number": "44644", "files_changed": ["CMakeLists.txt"], "labels": ["open source"]}, "856510c96d": {"title": "[JIT] Dont optimize shape info in batch_mm (#44565)", "body": "Summary:\nWe run remove profile nodes and specialize types before batch_mm, so we cannot run peepholes on the type information of tensors since these properties have not been guarded to be guaranteed to be correct.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44565\n\nReviewed By: albanD\n\nDifferential Revision: D23661538\n\nPulled By: eellison\n\nfbshipit-source-id: 0dd23a65714f047f49b4db4ec582b21870925fe1", "pr_number": "44565", "files_changed": ["test/jit/test_profiler.py", "torch/csrc/jit/passes/batch_mm.cpp"], "labels": ["oncall: jit"]}, "a475613d1d": {"title": "[static runtime] Swap to out-variant compatible nodes (#44127)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44127\n\nTest Plan: Imported from OSS\n\nReviewed By: hlu1\n\nDifferential Revision: D23604306\n\nPulled By: bwasti\n\nfbshipit-source-id: 18ccfb9b466b822e28130be3d5c4fae36c76820b", "pr_number": "44127", "files_changed": ["test/test_static_runtime.py", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h"], "labels": ["oncall: jit"]}, "199435af90": {"title": "Update median doc to note return value of even-sized input (#44562)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44562\n\nAdd a note that torch.median returns the smaller of the two middle elements for even-sized input and refer user to torch.quantile for the mean of the middle values.\n\nfixes https://github.com/pytorch/pytorch/issues/39520\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23657208\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 2747aa652d1e7f10229d9299b089295aeae092c2", "pr_number": "44562", "files_changed": ["torch/_torch_docs.py"], "labels": []}, "ad7a2eb1c9": {"title": "Simplify nested Min and Max patterns. (#44142)", "body": "Summary:\nImprove simplification of nested Min and Max patterns.\n\nSpecifically, handles the following pattern simplications:\n  * `Max(A, Max(A, Const)) => Max(A, Const)`\n  * `Max(Min(A, B), Min(A, C)) => Min(A, Max(B, C))`\n  * `Max(Const, Max(A, OtherConst) => Max(A, Max(Const, OtherConst))`\n     - This case can have an arbitrarily long chain of Max ops. For example: `Max(5, Max(x, Max(y, Max(z, 8)))) => Max(Max(Max(x, 8), y), z)`\n\nSimilarly, for the case of Min as well.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44142\n\nReviewed By: albanD\n\nDifferential Revision: D23644486\n\nPulled By: navahgar\n\nfbshipit-source-id: 42bd241e6c2af820566744c8494e5dee172107f4", "pr_number": "44142", "files_changed": ["test/cpp/tensorexpr/test_boundsinference.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/hash_provider.cpp", "torch/csrc/jit/tensorexpr/hash_provider.h", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.h", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/ir_printer.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.h", "torch/csrc/jit/tensorexpr/ir_visitor.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.h"], "labels": ["oncall: jit"]}, "ecac8294a6": {"title": "enable type checking for torch._classes (#44576)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/42980\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44576\n\nReviewed By: malfet\n\nDifferential Revision: D23668741\n\nPulled By: walterddr\n\nfbshipit-source-id: 4201ea3187a40051ebff53d28c8e571ea1a61126", "pr_number": "44576", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in"], "labels": []}, "9d4943daaf": {"title": "[quant] conv_transpose1d / conv_transpose2d (#40370)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40370\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22158979\n\nPulled By: z-a-f\n\nfbshipit-source-id: f5cb812c9953efa7608f06cf0188de447f73f358", "pr_number": "40370", "files_changed": ["aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp"], "labels": []}, "cfba33bde3": {"title": "Fix the ELU formula in the docs (#43764)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43389.\n\nThis PR replaces the old ELU formula from the docs that yields wrong results for negative alphas with the new one that fixes the issue and relies on the cases notation which makes the formula more straightforward.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43764\n\nReviewed By: ailzhang\n\nDifferential Revision: D23425532\n\nPulled By: albanD\n\nfbshipit-source-id: d0931996e5667897d926ba4fc7a8cc66e8a66837", "pr_number": "43764", "files_changed": ["torch/nn/modules/activation.py"], "labels": ["open source"]}, "b5dd6e3e61": {"title": "split torch.testing._internal.* and add type checking for torch.testing._internal.common_cuda (#44575)", "body": "Summary:\nFirst step to fix https://github.com/pytorch/pytorch/issues/42969.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44575\n\nReviewed By: malfet\n\nDifferential Revision: D23668740\n\nPulled By: walterddr\n\nfbshipit-source-id: eeb3650b1780aaa5727b525b4e6182e1bc47a83f", "pr_number": "44575", "files_changed": ["mypy.ini", "torch/testing/_internal/common_cuda.py"], "labels": []}, "a188dbdf3f": {"title": "Check for index-rank consistency in FunctionInliner (#44561)", "body": "Summary:\nWhen caller / callee pairs are\tinserted into the mapping, verify that\nthe arity of the buffer access is consistent with its declared rank.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44561\n\nTest Plan: CI, test_tensorexpr --gtest_filter=TensorExprTest.DetectInlineRankMismatch\n\nReviewed By: albanD\n\nDifferential Revision: D23684342\n\nPulled By: asuhan\n\nfbshipit-source-id: dd3a0cdd4c2492853fa68381468e0ec037136cab", "pr_number": "44561", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["oncall: jit"]}, "84949672bf": {"title": "Fix exception chaining in `test/` (#44193)", "body": "Summary:\n## Motivation\nThis PR fixes https://github.com/pytorch/pytorch/issues/43770 and is the continuation of https://github.com/pytorch/pytorch/issues/43836.\n\n## Description of the change\nThis PR fixes exception chaining only in files under `test/` where appropriate.\nTo fix exception chaining, I used either:\n1. `raise new_exception from old_exception` where `new_exception` itself seems not descriptive enough to debug or `old_exception` delivers valuable information.\n2. `raise new_exception from None` where raising both of `new_exception` and `old_exception` seems a bit noisy and redundant.\n\n## List of lines containing `raise` in `except` clause:\nI wrote [this simple script](https://gist.github.com/akihironitta/4223c1b32404b36c1b349d70c4c93b4d) using [ast](https://docs.python.org/3.8/library/ast.html#module-ast) to list lines where `raise`ing in `except` clause.\n\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_cpp_extensions_aot.py#L16\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit.py#L2503\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/onnx/model_defs/word_language_model.py#L22\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/onnx/verify.py#L73\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/onnx/verify.py#L110\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/onnx/test_verify.py#L31\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_c10d.py#L255\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_c10d.py#L2992\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_c10d.py#L3025\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_c10d.py#L3712\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_distributed.py#L3180\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_distributed.py#L3198\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_data_parallel.py#L752\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_data_parallel.py#L776\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_type_hints.py#L151\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit_fuser.py#L771\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit_fuser.py#L773\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dispatch.py#L105\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_distributions.py#L4738\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_nn.py#L9824\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_namedtensor.py#L843\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit_fuser_te.py#L875\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit_fuser_te.py#L877\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dataloader.py#L31\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dataloader.py#L43\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dataloader.py#L365\n- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dataloader.py#L391\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44193\n\nReviewed By: albanD\n\nDifferential Revision: D23681529\n\nPulled By: malfet\n\nfbshipit-source-id: 7c2256ff17334625081137b35baeb816c1e53e0b", "pr_number": "44193", "files_changed": ["test/onnx/model_defs/word_language_model.py", "test/test_cpp_extensions_aot.py", "test/test_dataloader.py", "test/test_distributions.py", "test/test_jit.py", "test/test_jit_fuser.py", "test/test_jit_fuser_te.py", "test/test_namedtensor.py", "test/test_type_hints.py"], "labels": ["open source"]}, "742654d1b6": {"title": "[quant] ConvTranspose1d / ConvTranspose2d (#40371)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40371\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22158981\n\nPulled By: z-a-f\n\nfbshipit-source-id: defbf6fbe730a58d5b155dcb2460dd969797215c", "pr_number": "40371", "files_changed": ["test/quantization/test_quantized_op.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/conv.py", "torch/quantization/default_mappings.py"], "labels": []}, "d0a56cab07": {"title": "[quant] Fixing the output shape for the linear (#44513)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44513\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23637508\n\nPulled By: z-a-f\n\nfbshipit-source-id: d19d4c1b234b05e8d9813e864863d937b6c35bf5", "pr_number": "44513", "files_changed": ["aten/src/ATen/native/quantized/cpu/qlinear.cpp", "test/quantization/test_quantized_op.py"], "labels": []}, "62ebad4ff9": {"title": "[ONNX] Export new_empty and new_zeros (#43506)", "body": "Summary:\nAdding symbolic to export new_empty and new_zeros\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43506\n\nReviewed By: houseroad\n\nDifferential Revision: D23674574\n\nPulled By: bzinodev\n\nfbshipit-source-id: ecfcdbd4845fd3a3c6618a060129fbeee4df5dd7", "pr_number": "43506", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["open source", "triaged"]}, "da11d932bc": {"title": "[ONNX] Update arange op to support out argument (#43777)", "body": "Summary:\nUpdate arange op to support out argument\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43777\n\nReviewed By: albanD\n\nDifferential Revision: D23674583\n\nPulled By: bzinodev\n\nfbshipit-source-id: 6fb65e048c6b1a551569d4d2a33223522d2a960c", "pr_number": "43777", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["open source", "triaged"]}, "f7cfbac89b": {"title": "[ONNX] Update len symbolic (#43824)", "body": "Summary:\nUpdate len symbolic.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43824\n\nReviewed By: izdeby\n\nDifferential Revision: D23575765\n\nPulled By: bzinodev\n\nfbshipit-source-id: 0e5c8c8d4a5297f65e2dc43168993350f784c776", "pr_number": "43824", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["open source", "triaged"]}, "8f327cd6c5": {"title": "[vulkan][op] add.Scalar, mul.Scalar (#42674)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42674\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22978763\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 9fd97d394205e3fa51992ee99d5bfafc33f75efa", "pr_number": "42674", "files_changed": ["aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/VulkanOps.h", "aten/src/ATen/native/vulkan/glsl/add_scalar.glsl", "aten/src/ATen/native/vulkan/glsl/mul_scalar.glsl", "aten/src/ATen/test/vulkan_test.cpp"], "labels": []}, "89aed1a933": {"title": "[vulkan][op] avg_pool2d (#42675)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42675\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D22978765\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 64938d8965aeeb408dd5c40d688eca13fb7ebb8a", "pr_number": "42675", "files_changed": ["aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/VulkanOps.h", "aten/src/ATen/native/vulkan/glsl/avg_pool2d.glsl", "aten/src/ATen/native/vulkan/glsl/permute.glsl", "aten/src/ATen/test/vulkan_test.cpp"], "labels": []}, "43406e218a": {"title": "[ONNX] Update ONNX shape inference (#43929)", "body": "Summary:\n* Support sequence type (de)serialization, enables onnx shape inference on sequence nodes.\n* Fix shape inference with block input/output: e.g. Loop and If nodes.\n* Fix bugs in symbolic discovered by coverage of onnx shape inference.\n* Improve debuggability: added more jit logs. For simplicity, the default log level, when jit log is enabled, will not dump ir graphs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43929\n\nReviewed By: albanD\n\nDifferential Revision: D23674604\n\nPulled By: bzinodev\n\nfbshipit-source-id: ab6aacb16d0e3b9a4708845bce27c6d65e567ba7", "pr_number": "43929", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/passes/onnx/shape_type_inference.cpp", "torch/csrc/jit/passes/onnx/shape_type_inference.h", "torch/csrc/jit/serialization/export.cpp", "torch/csrc/jit/serialization/onnx.cpp", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py"], "labels": ["module: onnx", "oncall: jit", "open source", "triaged"]}, "e594c30bc2": {"title": "[quant][graphmode][fx] Support fp16 dynamic quantization for linear (#44582)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44582\n\nTest Plan:\ntest_quantize_fx.py\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23665974\n\nfbshipit-source-id: 19ba6c61a9c77ef570b00614016506e9a2729f7c", "pr_number": "44582", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "686e281bcf": {"title": "Updates div to perform true division (#42907)", "body": "Summary:\nThis PR:\n\n- updates div to perform true division\n- makes torch.true_divide an alias of torch.div\n\nThis follows on work in previous PyTorch releases that first deprecated div performing \"integer\" or \"floor\" division, then prevented it by throwing a runtime error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42907\n\nReviewed By: ngimel\n\nDifferential Revision: D23622114\n\nPulled By: mruberry\n\nfbshipit-source-id: 414c7e3c1a662a6c3c731ad99cc942507d843927", "pr_number": "42907", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "test/jit/test_save_load.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_foreach.py", "test/test_jit.py", "test/test_op_aliases.py", "test/test_sparse.py", "test/test_torch.py", "test/test_type_promotion.py", "tools/autograd/derivatives.yaml", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["oncall: jit"]}, "2254e5d976": {"title": "Add note comments to enforce nondeterministic alert documentation (#44140)", "body": "Summary:\nThis PR fulfills Ed's request (https://github.com/pytorch/pytorch/pull/41692#discussion_r473122076) for a strategy to keep the functions that have nondeterministic alerts fully documented.\n\nPart of https://github.com/pytorch/pytorch/issues/15359\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44140\n\nReviewed By: colesbury\n\nDifferential Revision: D23644469\n\nPulled By: ezyang\n\nfbshipit-source-id: 60936ccced13f071c620f7d25ef6dcbca338de7f", "pr_number": "44140", "files_changed": ["aten/src/ATen/Context.h", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/GridSampler.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/ReflectionPad.cu", "aten/src/ATen/native/cuda/ReplicationPadding.cu", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/THC/THCBlas.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu"], "labels": ["module: docs", "open source", "triaged"]}, "551494b01d": {"title": "[JIT] Fix torch.tensor for empty multidimensional-typed lists (#44652)", "body": "Summary:\nWe were hitting an assert error when you passed in an empty `List[List[int]]` - this fixes that error by not recursing into 0-element tensors.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44652\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23688247\n\nPulled By: eellison\n\nfbshipit-source-id: d48ea24893044fae96bc39f76c0f1f9726eaf4c7", "pr_number": "44652", "files_changed": ["test/test_jit.py", "torch/csrc/jit/runtime/register_special_ops.cpp"], "labels": ["oncall: jit"]}, "e107ef5ca2": {"title": "Add type annotations for torch.nn.utils.* (#43080)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43013\n\nRedo of gh-42954\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43080\n\nReviewed By: albanD\n\nDifferential Revision: D23681334\n\nPulled By: malfet\n\nfbshipit-source-id: 20ec78aa3bfecb7acffc12eb89d3ad833024394c", "pr_number": "43080", "files_changed": ["mypy.ini", "tools/pyi/gen_pyi.py", "torch/nn/utils/prune.py"], "labels": ["module: typing", "open source", "triaged"]}, "71673b31f9": {"title": "DPP Async Tracing (#44252)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44252\n\nAdd tracing to DPP client. Because DPP requests are async, we need to be able to start a trace event in one thread and potentially end in a different thread. RecordFunction and LibgpumonObserver previously assume each trace event starts and finishes in the same thread. So they use a thread local context to track enter and exit call backs. Async events breaks this assumption. This change attaches the event context to the RecordFunction object so we do not need to use thread local context.\n\nTest Plan:\nTested with dpp perf test and able to collect trace.\n\n{F307824044}\n\nReviewed By: ilia-cher\n\nDifferential Revision: D23323486\n\nfbshipit-source-id: 4b6ca6c0e32028fb38a476cd1f44c17a001fc03b", "pr_number": "44252", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h", "c10/macros/Macros.h", "c10/util/SmallVector.h", "test/cpp/jit/test_misc.cpp"], "labels": ["fb-exported"]}, "63105fd5b1": {"title": "Refactor CallbackManager as a nested class of RecordFunction. (#44645)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44645\n\nMoved CallbackManager as a nested class of RecordFunction to allow private access to the call handles and context without exposing them publicly. It still hides the singleton instance of the CallbackManager inside record_function.cpp.\n\nTest Plan: Unit tests.\n\nReviewed By: ilia-cher\n\nDifferential Revision: D23494065\n\nfbshipit-source-id: 416d5bf6c9426e112877fbd233a6f4dff7bef455", "pr_number": "44645", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h"], "labels": ["fb-exported", "oncall: jit"]}, "e7d782e724": {"title": "[JIT] Add property support for ScriptModules (#42390)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42390\n\n**Summary**\nThis commit extends support for properties to include\nScriptModules.\n\n**Test Plan**\nThis commit adds a unit test that has a ScriptModule with\na user-defined property.\n\n`python test/test_jit_py3.py TestScriptPy3.test_module_properties`\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison, mannatsingh\n\nDifferential Revision: D22880298\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 74f6cb80f716084339e2151ca25092b6341a1560", "pr_number": "42390", "files_changed": ["test/jit/test_recursive_script.py", "test/test_jit_py3.py", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_tree_views.cpp", "torch/csrc/jit/python/script_init.cpp", "torch/jit/_recursive.py", "torch/jit/_script.py", "torch/jit/frontend.py", "torch/nn/modules/rnn.py"], "labels": ["oncall: jit"]}, "2c4b4aa81b": {"title": "Revert D23494065: Refactor CallbackManager as a nested class of RecordFunction.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23494065 (https://github.com/pytorch/pytorch/commit/63105fd5b192e2fe9310dd0445d73af79e3880b3)\n\nOriginal commit changeset: 416d5bf6c942\n\nfbshipit-source-id: 3b1ec928e3db0cc203bb63ec4db3da1584b9b884", "pr_number": null, "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h"], "labels": []}, "ed862d3682": {"title": "Split CUDA_NVCC_FLAGS by space (#44603)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44599\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44603\n\nReviewed By: albanD\n\nDifferential Revision: D23692320\n\nPulled By: ezyang\n\nfbshipit-source-id: 6a63d94ab8b88e7a82f9d65f03523d6ef639c754", "pr_number": "44603", "files_changed": ["CMakeLists.txt", "aten/CMakeLists.txt", "cmake/Dependencies.cmake"], "labels": ["open source"]}, "c71ce10cfc": {"title": "add dilation to transposeconv's _output_padding method (#43793)", "body": "Summary:\nThis PR adds dilation to _ConvTransposeNd._output_padding method and tests using a bunch of different sized inputs.\n\nFixes https://github.com/pytorch/pytorch/issues/14272\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43793\n\nReviewed By: zou3519\n\nDifferential Revision: D23493313\n\nPulled By: ezyang\n\nfbshipit-source-id: bca605c428cbf3a97d3d24316d8d7fde4bddb307", "pr_number": "43793", "files_changed": ["test/test_nn.py", "torch/nn/modules/conv.py"], "labels": ["open source", "topic: bc-breaking", "triaged"]}, "aedce773ed": {"title": "Deleted docker images for rocm 3.3 and rocm 3.5 (#44672)", "body": "Summary:\njeffdaily\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44672\n\nReviewed By: malfet\n\nDifferential Revision: D23694924\n\nPulled By: xw285cornell\n\nfbshipit-source-id: 0066dc4b36c366588e1f309c82e7e1dc2ce8eec1", "pr_number": "44672", "files_changed": [".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh"], "labels": ["module: rocm", "open source"]}, "2fd142a2ef": {"title": "Small clarification to amp gradient penalty example (#44667)", "body": "Summary:\nrequested by https://discuss.pytorch.org/t/what-is-the-correct-way-of-computing-a-grad-penalty-using-amp/95827/3\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44667\n\nReviewed By: mruberry\n\nDifferential Revision: D23692768\n\nPulled By: ngimel\n\nfbshipit-source-id: 83c61b94e79ef9f86abed2cc066f188dce0c8456", "pr_number": "44667", "files_changed": ["docs/source/notes/amp_examples.rst"], "labels": ["open source"]}, "2435d941b1": {"title": "Fix FP16 fastAtomicAdd for one case where tensor start address is not 32 bit aligned (#44642)", "body": "Summary:\nFor https://github.com/pytorch/pytorch/issues/44206 and https://github.com/pytorch/pytorch/issues/42218, I'd like to update trilinear interpolate backward and grid_sample backward to use `fastAtomicAdd`.\n\nAs a prelude, I spotted a UB risk in `fastAtomicAdd`.  I think existing code incurs a misaligned `__half2` atomicAdd when `index` is odd and `tensor` is not 32-bit aligned (`index % 2 == 1` and `(reinterpret_cast<std::uintptr_t>(tensor) % sizeof(__half2) == 1`). In this case we think we're `!low_bit` and go down the `!low_bit` code path, but in fact we are `low_bit`.  It appears the original [fastAtomicAdd PR](https://github.com/pytorch/pytorch/pull/21879#discussion_r295040377)'s discussion did not consider that case explicitly.\n\nI wanted to push my tentative fix for discussion ASAP. jjsjann123 and mkolod as original authors of `fastAtomicAdd`. (I'm also curious why we need to `reinterpret_cast<std::uintptr_t>(tensor...` for the address modding, but that's minor.)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44642\n\nReviewed By: mruberry\n\nDifferential Revision: D23699820\n\nPulled By: ngimel\n\nfbshipit-source-id: 0db57150715ebb45e6a1fb36897e46f00d61defd", "pr_number": "44642", "files_changed": ["aten/src/ATen/native/cuda/KernelUtils.cuh"], "labels": ["open source"]}, "7036e91abd": {"title": "Revert D23323486: DPP Async Tracing", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23323486 (https://github.com/pytorch/pytorch/commit/71673b31f932b671ace0b4d44a559891f770396e)\n\nOriginal commit changeset: 4b6ca6c0e320\n\nfbshipit-source-id: c6bd6d277aca070bef2de3522c2a60e23b4395ad", "pr_number": null, "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h", "c10/macros/Macros.h", "c10/util/SmallVector.h", "test/cpp/jit/test_misc.cpp"], "labels": []}, "72b5665c4f": {"title": "Upgrade oneDNN (mkl-dnn) to v1.6 (#44706)", "body": "Summary:\n- Bump oneDNN (mkl-dnn) to 1.6 for bug fixes\n    - Fixes https://github.com/pytorch/pytorch/issues/42446. RuntimeError: label is redefined for convolutions with large filter size on Intel AVX512\n    - Implemented workaround for internal compiler error when building oneDNN with Microsoft Visual Studio 2019 (https://github.com/pytorch/pytorch/pull/43169)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44706\n\nReviewed By: ngimel\n\nDifferential Revision: D23705967\n\nPulled By: albanD\n\nfbshipit-source-id: 65e8fecc52a76c9f3324403a8b60ffa8a8948bc6", "pr_number": "44706", "files_changed": ["third_party/ideep"], "labels": ["open source"]}, "5f692a67db": {"title": "qat conv_fused.py: one more patch for forward compatibility (#44671)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44671\n\nSee comments inline - the FC between\nhttps://github.com/pytorch/pytorch/pull/38478 and\nhttps://github.com/pytorch/pytorch/pull/38820 was broken,\npatching it.\n\nTest Plan: Verified with customer hitting the issue that this fixes their issue.\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23694029\n\nfbshipit-source-id: a5e1733334e22305a111df750b190776889705d0", "pr_number": "44671", "files_changed": ["torch/nn/intrinsic/qat/modules/conv_fused.py"], "labels": ["fb-exported"]}, "f5d231d593": {"title": "move rebuild buckets from end of first iteration to beginning of second iteration (#44326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44326\n\nPart of relanding PR #41954, this refactoring is to move rebuild_buckets call from end of first iteration to beginning of second iteration\nghstack-source-id: 112011490\n\nTest Plan: unit tests\n\nReviewed By: mrshenli\n\nDifferential Revision: D23583017\n\nfbshipit-source-id: ef67f79437a820d9b5699b651803622418499a83", "pr_number": "44326", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/nn/parallel/distributed.py"], "labels": []}, "9c364da9b9": {"title": "Fix doc builds for bool kwargs (#44686)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43669\n\nThe bool will still link to https://docs.python.org/3/library/functions.html#bool.\nTested using bmm:\n![image](https://user-images.githubusercontent.com/16063114/93156438-2ad11080-f6d6-11ea-9b81-96e02ee68d90.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44686\n\nReviewed By: ngimel\n\nDifferential Revision: D23703823\n\nPulled By: mruberry\n\nfbshipit-source-id: 7286afad084f5ab24a1254ad84e5d01907781c85", "pr_number": "44686", "files_changed": ["docs/source/conf.py", "torch/_torch_docs.py"], "labels": ["module: docs", "open source", "triaged"]}, "6bc77f4d35": {"title": "Use amax/maximum instead of max in optimizers (#43797)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43797\n\nReviewed By: malfet\n\nDifferential Revision: D23406641\n\nPulled By: mruberry\n\nfbshipit-source-id: 0cd075124aa6533b21375fe2c90c44a5d05ad6e6", "pr_number": "43797", "files_changed": ["torch/optim/adam.py", "torch/optim/adamax.py", "torch/optim/adamw.py"], "labels": ["open source"]}, "1d733d660d": {"title": "[docs] torch.min/max: remove incorrect warning from docs (#44615)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44195\n\ncc: mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44615\n\nReviewed By: ngimel\n\nDifferential Revision: D23703525\n\nPulled By: mruberry\n\nfbshipit-source-id: 471ebd764be667e29c03a30f3ef341440adc54d2", "pr_number": "44615", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source", "triaged"]}, "d62994a94d": {"title": "ci: Add anaconda pruning to CI pipeline (#44651)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44651\n\nAdds pruning for our anaconda channels (pytorch-nightly, pytorch-test)\ninto our CI pipeline so that it gets run on a more consistent basis.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D23692851\n\nPulled By: seemethere\n\nfbshipit-source-id: fa69b506b73805bf2ffbde75d221aef1ee3f753e", "pr_number": "44651", "files_changed": [".circleci/cimodel/data/simple/anaconda_prune_defintions.py", ".circleci/config.yml", ".circleci/generate_config_yml.py", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml", "scripts/release/anaconda-prune/prune.sh", "scripts/release/anaconda-prune/run.sh"], "labels": ["module: ci"]}, "07cba8b1fc": {"title": "Run vmap tests in CI (#44656)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44656\n\nAll this time, test_vmap wasn't running in the CI. Fortunately all the\ntests pass locally for me. h/t to anjali411 for pointing this out.\n\nTest Plan: - Wait for CI\n\nReviewed By: anjali411\n\nDifferential Revision: D23689355\n\nPulled By: zou3519\n\nfbshipit-source-id: 543c3e6aed0af77bfd6ea7a7549337f8230e3d32", "pr_number": "44656", "files_changed": ["test/run_test.py", "test/test_vmap.py"], "labels": []}, "4ce6af35c4": {"title": "Enable fp16 for CUDA SparseLengthsSum/Mean (#44089)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44089\n\nAdd support of fp16 as input type in SparseLengthSum/Mean caffe2 operator\n\nReviewed By: xianjiec\n\nDifferential Revision: D23436877\n\nfbshipit-source-id: 02fbef2fde17d4b0abea9ca5d17a36aa989f98a0", "pr_number": "44089", "files_changed": ["caffe2/operators/segment_reduction_op_gpu.cu", "caffe2/operators/segment_reduction_op_gpu.cuh", "caffe2/python/operator_test/segment_ops_test.py"], "labels": ["fb-exported"]}, "8df0400a50": {"title": "Fix fallback graph in specialize autogradzero (#44654)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44654\n\nPreviously we weren't creating a fallback graph as intended in specialize autograd zero, so if a Tensor failed one of our undefinedness checks we would run the backward normally without reprofiling & optimizing.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23691764\n\nPulled By: eellison\n\nfbshipit-source-id: 10c6fa79518c84a6f5ef2bfbd9ea10843af751eb", "pr_number": "44654", "files_changed": ["test/jit/test_profiler.py", "torch/csrc/jit/passes/specialize_autogradzero.cpp"], "labels": ["oncall: jit"]}, "69839ea3f6": {"title": "[NNC] make inlining immediate (take 3) (#44231)", "body": "Summary:\nThis is a reup https://github.com/pytorch/pytorch/issues/43885 with an extra commit which should fix the bugs that caused it to be reverted. Read that for general context.\n\nThe issue here was that we were still using the side maps `tensor_to_stmt_` and `stmt_to_tensor_` which get invalidated by any transform of the IR (rather than just any transform that isn't computeInline). I added a comment about this but didn't actually address our usages of it.\n\nI've removed these maps and changed the `getLoopBodyFor` and `getLoopStatementsFor` helpers to search the root stmt directly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44231\n\nReviewed By: albanD\n\nDifferential Revision: D23689688\n\nPulled By: nickgg\n\nfbshipit-source-id: 1c6009a880f8c0cebf2300fd06b5cc9322bffbf9", "pr_number": "44231", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/analysis.h", "torch/csrc/jit/tensorexpr/function.cpp", "torch/csrc/jit/tensorexpr/hash_provider.cpp", "torch/csrc/jit/tensorexpr/hash_provider.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.h", "torch/csrc/jit/tensorexpr/tensor.h"], "labels": ["oncall: jit"]}, "285ba0d068": {"title": "Enable fp16 for UniformFill (#44540)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44540\n\nSupport output type to be fp16 for UniformFill\n\nReviewed By: jianyuh\n\nDifferential Revision: D23558030\n\nfbshipit-source-id: 53a5b2c92cfe78cd11f55e6ee498e1bd682fe4a1", "pr_number": "44540", "files_changed": ["caffe2/operators/half_float_ops.cc", "caffe2/operators/half_float_ops.cu", "caffe2/operators/half_float_ops.h", "caffe2/python/operator_test/filler_ops_test.py"], "labels": ["fb-exported"]}, "2f4c31ce3a": {"title": "[jit] Speed up saving in case of many classes (#44589)", "body": "Summary:\nThere's an annoying O(N^2) in module export logic that makes saving some of the models (if they have many classes) take eternity.\n\nI'm not super familiar with this code to properly untangle the deps and make it a pure hash lookup. So I just added a side lookup table for raw pointers. It's still quadratic, but it's O(num_classes^2) instead of O(num_classes * num_references) which already gives huge savings.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44589\n\nTest Plan:\nTested with one of the offending models - just loading a saving a Torchscript file:\n\n```\nBefore:\nload 1.9239683151245117\nsave 165.74712467193604\n\nAfter:\nload 1.9409027099609375\nsave 1.4711427688598633\n```\n\nReviewed By: suo\n\nDifferential Revision: D23675278\n\nPulled By: dzhulgakov\n\nfbshipit-source-id: 8f3fa7730941085ea20d9255b49a149ac1bf64fe", "pr_number": "44589", "files_changed": ["torch/csrc/jit/jit_log.cpp", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/python_print.cpp", "torch/csrc/jit/serialization/python_print.h"], "labels": ["oncall: jit"]}, "26a91a9f04": {"title": "[WIP][JIT] Add benchmarking support of NV Fuser with FP16 dtype support (#44101)", "body": "Summary:\nModified files in `benchmarks/tensorexpr` to add support for NVIDIA's Fuser for the jit compiler.\n\nThis support has some modifications besides adding an option to support the NVIDIA fuser:\n\n* Adds FP16 Datatype support\n* Fixes SOL/Algo calculations to generally use the data type instead of being fixed to 4 bytes\n* Adds IR printing and kernel printing knobs\n* Adds a knob `input_iter` to create ranges of inputs currently only for reductions\n* Adds further reduction support for Inner and Outer dimension reductions that are compatible with the `input_iter` knob.\n* Added `simple_element`, `reduce2d_inner`, and `reduce2d_outer` to isolate performance on elementwise  and reduction operations in the most minimal fashion.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44101\n\nReviewed By: ngimel\n\nDifferential Revision: D23713658\n\nPulled By: bertmaher\n\nfbshipit-source-id: d6b83cfab559aefe107c23b3c0f2df9923b3adc1", "pr_number": "44101", "files_changed": ["benchmarks/tensorexpr/__main__.py", "benchmarks/tensorexpr/attention.py", "benchmarks/tensorexpr/benchmark.py", "benchmarks/tensorexpr/broadcast.py", "benchmarks/tensorexpr/conv.py", "benchmarks/tensorexpr/elementwise.py", "benchmarks/tensorexpr/matmul.py", "benchmarks/tensorexpr/normalization.py", "benchmarks/tensorexpr/pooling.py", "benchmarks/tensorexpr/pt_engine.py", "benchmarks/tensorexpr/reduction.py", "benchmarks/tensorexpr/rnn_eltwise.py", "benchmarks/tensorexpr/softmax.py", "benchmarks/tensorexpr/swish.py"], "labels": ["open source"]}, "fb085d90e3": {"title": "Revert D23583017: move rebuild buckets from end of first iteration to beginning of second iteration", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23583017 (https://github.com/pytorch/pytorch/commit/f5d231d5933e0ca8feb59f9fe76913d8aa720457)\n\nOriginal commit changeset: ef67f79437a8\n\nfbshipit-source-id: fd914b7565aba6a5574a32b31403525abb80ff07", "pr_number": null, "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/nn/parallel/distributed.py"], "labels": []}, "993b4651fd": {"title": "Convert num_kernels to int64 before calling into CUDA GET_BLOCKS (#44688)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44688\n\nthis fixes https://github.com/pytorch/pytorch/issues/44472\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D23699819\n\nPulled By: soulitzer\n\nfbshipit-source-id: 7ecfe78d09344178d1e6c7e1503417feb6beff6c", "pr_number": "44688", "files_changed": ["aten/src/ATen/native/cuda/MaxUnpooling.cu", "aten/src/ATen/native/cuda/vol2col.cuh", "aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu"], "labels": []}, "2c1b215b48": {"title": "[fx] remove delegate, replace with tracer (#44566)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44566\n\nThe Delegate objects were confusing. They were suppose to be a way to\nconfigure how tracing works, but in some cases they appeared necessary\nfor consturcting graphs, which was not true. This makes the organization\nclearer by removing Delgate and moving its functionality into a Tracer class,\nsimilar to how pickle has a Pickler class.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23683177\n\nPulled By: zdevito\n\nfbshipit-source-id: 7605a34e65dfac9a487c0bada39a23ca1327ab00", "pr_number": "44566", "files_changed": ["test/fx/quantization.py", "test/test_fx.py", "torch/fx/__init__.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/proxy.py", "torch/fx/symbolic_trace.py"], "labels": ["fx"]}, "2efc618f19": {"title": "lr_schedule.py redundant code (#44613)", "body": "Summary:\nThe subclass sets \"self.last_epoch\" when this is set in the parent class's init function. Why would we need to set last_epoch twice? I think calling \"super\" resets last_epoch anyway, so I am not sure why we would want to include this in the subclass. Am I missing something?\n\nFor the record, I am just a Pytorch enthusiast. I hope my question isn't totally silly.\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44613\n\nReviewed By: albanD\n\nDifferential Revision: D23691770\n\nPulled By: mrshenli\n\nfbshipit-source-id: 080d9acda86e1a2bfaafe2c6fcb8fc1544f8cf8a", "pr_number": "44613", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["open source"]}, "d66520ba08": {"title": "[TensorExpr] Fuser: try merging adjacent fusion groups. (#43671)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43671\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23360796\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 60ec318fe77ae9f2c821d9c4d106281845266e0f", "pr_number": "43671", "files_changed": ["test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/cpp/tensorexpr/tests.h", "test/test_jit_fuser_te.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["oncall: jit"]}, "b85568a54a": {"title": "[CI] Add profiling-te benchmarks. (#44756)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44756\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23719728\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 739940e02a6697fbed2a43a13682a6e5268f710b", "pr_number": "44756", "files_changed": [".jenkins/pytorch/test.sh"], "labels": []}, "3f512b0de2": {"title": "[quant][qat] Ensure observers and fq modules are scriptable (#44749)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44749\n\nEnsure fx module is scriptable after calling prepare_qat on it\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFx.test_qat_and_script\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23718380\n\nfbshipit-source-id: abf63ffb21e707f7def8f6c88246877f5aded58c", "pr_number": "44749", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fake_quantize.py", "torch/quantization/observer.py"], "labels": []}, "63469da3bb": {"title": "Add a test to ensure DDP join works with RPC (#44439)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44439\n\nAdds a test to ddp_under_dist_autograd_test to enusre that that uneven\ninputs join() API works properly when DDP + RPC is combined. We test that when\nrunning in outside DDP mode (DDP applied to whole hybrid module) we can\ncorrectly process uneven inputs across different trainers.\nghstack-source-id: 112156980\n\nTest Plan: CI\n\nReviewed By: albanD\n\nDifferential Revision: D23612409\n\nfbshipit-source-id: f1e328c096822042daaba263aa8747a9c7e89de7", "pr_number": "44439", "files_changed": ["torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py"], "labels": []}, "06036f76b6": {"title": "CUDA BFloat16 pow (#44760)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44760\n\nReviewed By: ngimel\n\nDifferential Revision: D23727936\n\nPulled By: mruberry\n\nfbshipit-source-id: 8aa89e989294347d7f593b1a63ce4a1dbfdf783e", "pr_number": "44760", "files_changed": ["aten/src/ATen/native/cuda/PowKernel.cu", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "dbf17a1d4c": {"title": "Fixing a few links in distributed CONTRIBUTING.md (#44753)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44753\n\nghstack-source-id: 112132781\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23719077\n\nfbshipit-source-id: 3d943dfde100d175f417554fc7fca1fdb295129f", "pr_number": "44753", "files_changed": ["torch/distributed/CONTRIBUTING.md"], "labels": []}, "b63b684394": {"title": "Consolidate CODEOWNERS file for distributed package. (#44763)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44763\n\nThe file had separate rules for RPC and DDP/c10d, consolidated all of\nit together and placed all the distributed rules together.\nghstack-source-id: 112140871\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23721162\n\nfbshipit-source-id: d41c757eb1615376d442bd6b2802909624bd1d3f", "pr_number": "44763", "files_changed": ["CODEOWNERS"], "labels": []}, "a5cc151b8c": {"title": "Build EigenBlas as static library (#44747)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43709\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44747\n\nReviewed By: ezyang\n\nDifferential Revision: D23717927\n\nPulled By: malfet\n\nfbshipit-source-id: c46fbcf5a55895cb984dd4c5301fbcb784fc17d5", "pr_number": "44747", "files_changed": ["cmake/External/EigenBLAS.cmake"], "labels": ["triaged"]}, "5e717f0d5e": {"title": "delete the space for the docs rendering (#44740)", "body": "Summary:\nsee the docs rendering of `jacobian` and `hessian` at https://pytorch.org/docs/stable/autograd.html\n\n![image](https://user-images.githubusercontent.com/20907377/93268949-f0618500-f762-11ea-9ec6-ddd062540c59.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44740\n\nReviewed By: ngimel\n\nDifferential Revision: D23724899\n\nPulled By: mrshenli\n\nfbshipit-source-id: f7558ff53989e5dc7e678706207be2ac7ce22c66", "pr_number": "44740", "files_changed": ["torch/autograd/functional.py"], "labels": ["open source"]}, "ced8727d88": {"title": "Fix a broken link in CONTRIBUTING.md (#44701)", "body": "Summary:\nas the title says :)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44701\n\nReviewed By: ngimel\n\nDifferential Revision: D23724919\n\nPulled By: mrshenli\n\nfbshipit-source-id: 5ca5ea974ee6a94ed132dbe7892a9b4b9c3dd9be", "pr_number": "44701", "files_changed": ["CONTRIBUTING.md"], "labels": ["module: docs", "open source"]}, "eb75cfb9c0": {"title": "Back out \"Revert D23323486: DPP Async Tracing\" plus windows build fix. (#44702)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44702\n\nOriginal commit changeset: c6bd6d277aca\n\nThis diff caused windows build to fail due to a compiler bug in VS2019 (lambda capture constant int value). This back out works around the issue with explicit capture of const int value.\n\nTest Plan: Tested and previously landed.\n\nReviewed By: mruberry\n\nDifferential Revision: D23703215\n\nfbshipit-source-id: f9ef23be97540bc9cf78a855295fb8c69f360459", "pr_number": "44702", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h", "c10/macros/Macros.h", "c10/util/SmallVector.h", "test/cpp/jit/test_misc.cpp"], "labels": ["fb-exported", "oncall: jit"]}, "ee493e1a91": {"title": "CUDA bfloat compare ops (#44748)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44748\n\nReviewed By: mruberry\n\nDifferential Revision: D23725997\n\nPulled By: ngimel\n\nfbshipit-source-id: 4f89dce3a8b8f1295ced522011b59e60d756e749", "pr_number": "44748", "files_changed": ["aten/src/ATen/native/cuda/CompareEQKernel.cu", "aten/src/ATen/native/cuda/CompareGEKernel.cu", "aten/src/ATen/native/cuda/CompareGTKernel.cu", "aten/src/ATen/native/cuda/CompareLEKernel.cu", "aten/src/ATen/native/cuda/CompareLTKernel.cu", "aten/src/ATen/native/cuda/CompareNEKernel.cu", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "a011b86115": {"title": "change self.generator to generator (#44461)", "body": "Summary:\nbug fix\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44461\n\nReviewed By: mruberry\n\nDifferential Revision: D23725053\n\nPulled By: ngimel\n\nfbshipit-source-id: 89706313013d9eae96aaaf144924867457efd2c0", "pr_number": "44461", "files_changed": ["torch/utils/data/sampler.py"], "labels": ["open source"]}, "3e6bb5233f": {"title": "Reference amp tutorial (recipe) from core amp docs (#44725)", "body": "Summary:\nhttps://pytorch.org/tutorials/recipes/recipes/amp_recipe.html is live.  Core amp docs should reference it.\n\nAlso i fixed some typos in the `zero_grad` docs we ignored when git was behaving weirdly during ngimel 's merge of https://github.com/pytorch/pytorch/pull/44423.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44725\n\nReviewed By: mruberry\n\nDifferential Revision: D23723807\n\nPulled By: ngimel\n\nfbshipit-source-id: ca0b76365f8ca908bd978e3b38bf81857fa6c2a3", "pr_number": "44725", "files_changed": ["docs/source/amp.rst", "docs/source/notes/amp_examples.rst", "torch/nn/modules/module.py", "torch/optim/optimizer.py"], "labels": ["open source", "triaged"]}, "07d07e3c6c": {"title": "Remove EXPERIMENTAL_ENUM_SUPPORT feature guard (#44243)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41095\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44243\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23605979\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 098ae69049c4664ad5d1521c45b8a7dd22e72f6c", "pr_number": "44243", "files_changed": ["test/jit/test_enum.py", "test/test_jit.py", "torch/jit/annotations.py"], "labels": ["oncall: jit"]}, "570102ce85": {"title": "Remove many unused THC pointwise math operators (#44230)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44230\n\nReviewed By: albanD\n\nDifferential Revision: D23701185\n\nPulled By: ngimel\n\nfbshipit-source-id: caf7b7a815b37d50232448d6965e591508546bd7", "pr_number": "44230", "files_changed": ["aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPointwise.cu"], "labels": ["open source"]}, "e6101f5507": {"title": "fixes lda condition for blas functions, fixes bug with beta=0 in addmv slow path (#44681)", "body": "Summary:\nper title. If `beta=0` and slow path was taken, `nan` and `inf` in the result were not masked as is the case with other linear algebra functions. Similarly, since `mv` is implemented as `addmv` with `beta=0`, wrong results were sometimes produced for `mv` slow path.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44681\n\nReviewed By: mruberry\n\nDifferential Revision: D23708653\n\nPulled By: ngimel\n\nfbshipit-source-id: e2d5d3e6f69b194eb29b327e1c6f70035f3b231c", "pr_number": "44681", "files_changed": ["aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/BlasKernel.cpp", "test/test_torch.py"], "labels": []}, "6954ae1278": {"title": "Vec256 Test cases (#42685)", "body": "Summary:\n[Tests for Vec256 classes https://github.com/pytorch/pytorch/issues/15676](https://github.com/pytorch/pytorch/issues/15676)\n\nTesting\nCurrent list:\n\n- [x] Blends\n- [x] Memory: UnAlignedLoadStore\n- [x] Arithmetics: Plus,Minu,Multiplication,Division\n- [x] Bitwise: BitAnd, BitOr, BitXor\n- [x] Comparison: Equal, NotEqual, Greater, Less, GreaterEqual, LessEqual\n- [x] MinMax: Minimum, Maximum, ClampMin, ClampMax, Clamp\n- [x] SignManipulation: Absolute, Negate\n- [x] Interleave: Interleave, DeInterleave\n- [x] Rounding: Round, Ceil, Floor, Trunc\n- [x] Mask: ZeroMask\n- [x] SqrtAndReciprocal: Sqrt, RSqrt, Reciprocal\n- [x] Trigonometric: Sin, Cos, Tan\n- [x] Hyperbolic: Tanh, Sinh, Cosh\n- [x] InverseTrigonometric: Asin, ACos, ATan, ATan2\n- [x] Logarithm: Log, Log2, Log10, Log1p\n- [x] Exponents: Exp, Expm1\n- [x] ErrorFunctions: Erf, Erfc, Erfinv\n- [x] Pow: Pow\n- [x] LGamma: LGamma\n- [x] Quantization: quantize, dequantize, requantize_from_int\n- [x] Quantization: widening_subtract, relu, relu6\nMissing:\n- [ ] Constructors, initializations\n- [ ] Conversion , Cast\n- [ ] Additional: imag, conj, angle (note: imag and conj only checked for float complex)\n\n#### Notes on tests and testing framework\n- some math functions are tested within domain range\n- mostly testing framework randomly tests against std implementation within the domain or within the implementation domain for some math functions.\n- some functions are tested against the local version. ~~For example, std::round and vector version of round differs. so it was tested against the local version~~\n- round was tested against pytorch at::native::round_impl. ~~for double type on **Vsx  vec_round failed  for  (even)+0 .5 values**~~ . it was solved by using vec_rint\n- ~~**complex types are not tested**~~  **After enabling complex testing due to precision and domain some of the complex functions failed for vsx and x86 avx as well. I will either test it against local implementation or check within the accepted domain**\n- ~~quantizations are not tested~~  Added tests for quantizing, dequantize, requantize_from_int, relu, relu6, widening_subtract functions\n- the testing framework should be improved further\n- ~~For now  `-DBUILD_MOBILE_TEST=ON `will be used for Vec256Test too~~\nVec256 Test cases will be built for each CPU_CAPABILITY\n\nFixes: https://github.com/pytorch/pytorch/issues/15676\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42685\n\nReviewed By: malfet\n\nDifferential Revision: D23034406\n\nPulled By: glaringlee\n\nfbshipit-source-id: d1bf03acdfa271c88744c5d0235eeb8b77288ef8", "pr_number": "42685", "files_changed": ["aten/CMakeLists.txt", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/cpu/vec256/vec256.h", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/vec256_test_all_types.cpp", "aten/src/ATen/test/vec256_test_all_types.h", "caffe2/CMakeLists.txt", "cmake/Codegen.cmake"], "labels": ["module: vectorization", "open source", "triaged"]}, "924717bf51": {"title": "Add _get_type() API to RRef (#44663)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44663\n\nThe new API returns the type of the data object referenced by this\n`RRef`. On the owner, this is same as `type(rref.local_value())`.\nOn a user, this will trigger an RPC to fetch the `type` object from\nthe owner. After this function is run once, the `type` object is\ncached by the `RRef`, and subsequent invocations no longer trigger\nRPC.\n\ncloses #33210\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23691990\n\nPulled By: mrshenli\n\nfbshipit-source-id: a2d87cd601a691dd75164b6bcd7315245e9cf6bd", "pr_number": "44663", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "257c6d0fde": {"title": "Make async_execution compatible with RRef helpers (#44666)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44666\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23691989\n\nPulled By: mrshenli\n\nfbshipit-source-id: b36f4b1c9d7782797a0220434a8272610a23e83e", "pr_number": "44666", "files_changed": ["torch/distributed/rpc/rref_proxy.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "cce7680a23": {"title": "Add bound method tests for async_execution with RRef helper (#44716)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44716\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23707326\n\nPulled By: mrshenli\n\nfbshipit-source-id: a2f8db17447e9f82c9f6ed941ff1f8cb9090ad74", "pr_number": "44716", "files_changed": ["torch/distributed/rpc/functions.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "1cd5ba49c6": {"title": "Add batching rule for \"is_complex\", \"conj\" (#44649)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44649\n\nTo unblock #43208, which adds \"is_complex\" checks to backward formulas\nthat are being tested for batched gradient support with vmap.\n\nTest Plan: - `pytest test/test_vmap.py -v`\n\nReviewed By: anjali411\n\nDifferential Revision: D23685356\n\nPulled By: zou3519\n\nfbshipit-source-id: 29e41a9296336f6d1008e3040cade4c643bf5ebf", "pr_number": "44649", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_vmap.py"], "labels": ["module: complex"]}, "c44e4878ae": {"title": "Enable torch.backends.quantized typechecks (#44794)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44793\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44794\n\nReviewed By: walterddr\n\nDifferential Revision: D23734353\n\nPulled By: malfet\n\nfbshipit-source-id: 491bd7c8f147759715eb296d7537a172685aa066", "pr_number": "44794", "files_changed": ["mypy.ini", "torch/backends/quantized/__init__.py"], "labels": []}, "1718b16d15": {"title": "[Caffe2] gcs_cuda_only is trivial if CUDA not available (#44578)", "body": "Summary:\nMake `gcs_cuda_only` and `gcs_gpu_only` return empty device lists if CUDA/GPU(CUDA or RocM) not available\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44578\n\nReviewed By: walterddr\n\nDifferential Revision: D23664227\n\nPulled By: malfet\n\nfbshipit-source-id: 176b5d964c0b02b8379777cd9a38698c11818690", "pr_number": "44578", "files_changed": ["caffe2/python/hypothesis_test_util.py"], "labels": []}, "e9c6449b46": {"title": "[FX][EZ] Allow constructing GraphModule with dict for root (#44679)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44679\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23696766\n\nPulled By: jamesr66a\n\nfbshipit-source-id: fe18b7b579c1728d00589bd5fd5e54c917cc61fe", "pr_number": "44679", "files_changed": ["test/test_fx.py", "torch/fx/graph_module.py", "torch/fx/node.py"], "labels": ["fx"]}, "6debe825be": {"title": "[vulkan] glsl shaders relaxed precision mode to cmake option (#43076)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43076\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D23143354\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 7b3ead1e63cf8acf6e8e547080a8ead7a2db994b", "pr_number": "43076", "files_changed": ["CMakeLists.txt", "aten/src/ATen/gen_vulkan_glsl.py", "aten/src/ATen/gen_vulkan_spv.py", "aten/src/ATen/native/vulkan/Vulkan.cpp", "aten/src/ATen/native/vulkan/Vulkan.h", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/gen_glsl.py", "aten/src/ATen/native/vulkan/gen_spv.py", "aten/src/ATen/native/vulkan/glsl/KO4C4HW_to_image.glsl", "aten/src/ATen/native/vulkan/glsl/adaptive_avg_pool2d.glsl", "aten/src/ATen/native/vulkan/glsl/add.glsl", "aten/src/ATen/native/vulkan/glsl/addmm.glsl", "aten/src/ATen/native/vulkan/glsl/clamp.glsl", "aten/src/ATen/native/vulkan/glsl/conv2d_dw_clamp.glsl", "aten/src/ATen/native/vulkan/glsl/conv2d_nogroup_clamp.glsl", "aten/src/ATen/native/vulkan/glsl/image_to_nchw.glsl", "aten/src/ATen/native/vulkan/glsl/max_pool2d.glsl", "aten/src/ATen/native/vulkan/glsl/mean.glsl", "aten/src/ATen/native/vulkan/glsl/mm.glsl", "aten/src/ATen/native/vulkan/glsl/nchw_to_image.glsl", "aten/src/ATen/native/vulkan/glsl/upsampleNearest2d.glsl", "cmake/VulkanCodegen.cmake"], "labels": []}, "20ac736200": {"title": "Remove py2 compatible future imports (#44735)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44735\n\nReviewed By: mruberry\n\nDifferential Revision: D23731306\n\nPulled By: ezyang\n\nfbshipit-source-id: 0ba009a99e475ddbe22981be8ac636f8a1c8b02f", "pr_number": "44735", "files_changed": [".jenkins/pytorch/win-test-helpers/run_python_nn_smoketests.py", "aten/src/ATen/native/quantized/cpu/qnnpack/generate-wrapper.py", "benchmarks/fastrnns/bench.py", "benchmarks/fastrnns/test_bench.py", "benchmarks/framework_overhead_benchmark/C2Module.py", "benchmarks/framework_overhead_benchmark/SimpleAddModule.py", "benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py", "benchmarks/framework_overhead_benchmark/pt_wrapper_module.py", "benchmarks/framework_overhead_benchmark/utils.py", "benchmarks/operator_benchmark/benchmark_all_other_test.py", "benchmarks/operator_benchmark/benchmark_all_quantized_test.py", "benchmarks/operator_benchmark/benchmark_all_test.py", "benchmarks/operator_benchmark/benchmark_caffe2.py", "benchmarks/operator_benchmark/benchmark_core.py", "benchmarks/operator_benchmark/benchmark_pytorch.py", "benchmarks/operator_benchmark/benchmark_runner.py", "benchmarks/operator_benchmark/benchmark_test_generator.py", "benchmarks/operator_benchmark/benchmark_utils.py", "benchmarks/operator_benchmark/c2/add_test.py", "benchmarks/operator_benchmark/c2/matmul_test.py", "benchmarks/operator_benchmark/common/repeat_benchmark.py", "benchmarks/operator_benchmark/common/tests/add_ops_list_test.py", "benchmarks/operator_benchmark/common/tests/c2_cpu_gpu_forward_backward_test.py", "benchmarks/operator_benchmark/common/tests/jit_forward_test.py", "benchmarks/operator_benchmark/common/tests/pt_backward_test.py", "benchmarks/operator_benchmark/common/tests/pt_configs_list_test.py", "benchmarks/operator_benchmark/common/tests/pt_cpu_gpu_forward_backward_test.py", "benchmarks/operator_benchmark/common/tests/random_sample_test.py", "benchmarks/operator_benchmark/operator_benchmark.py", "benchmarks/operator_benchmark/pt/add_test.py", "benchmarks/operator_benchmark/pt/as_strided_test.py", "benchmarks/operator_benchmark/pt/batchnorm_test.py", "benchmarks/operator_benchmark/pt/cat_test.py", "benchmarks/operator_benchmark/pt/channel_shuffle_test.py", "benchmarks/operator_benchmark/pt/chunk_test.py", "benchmarks/operator_benchmark/pt/conv_test.py", "benchmarks/operator_benchmark/pt/diag_test.py", "benchmarks/operator_benchmark/pt/gather_test.py", "benchmarks/operator_benchmark/pt/groupnorm_test.py", "benchmarks/operator_benchmark/pt/hardsigmoid_test.py", "benchmarks/operator_benchmark/pt/hardswish_test.py", "benchmarks/operator_benchmark/pt/instancenorm_test.py", "benchmarks/operator_benchmark/pt/layernorm_test.py", "benchmarks/operator_benchmark/pt/linear_test.py", "benchmarks/operator_benchmark/pt/matmul_test.py", "benchmarks/operator_benchmark/pt/pool_test.py", "benchmarks/operator_benchmark/pt/qactivation_test.py", "benchmarks/operator_benchmark/pt/qarithmetic_test.py", "benchmarks/operator_benchmark/pt/qbatchnorm_test.py", "benchmarks/operator_benchmark/pt/qcat_test.py", "benchmarks/operator_benchmark/pt/qcomparators_test.py", "benchmarks/operator_benchmark/pt/qconv_test.py", "benchmarks/operator_benchmark/pt/qembedding_pack_test.py", "benchmarks/operator_benchmark/pt/qembeddingbag_test.py", "benchmarks/operator_benchmark/pt/qgroupnorm_test.py", "benchmarks/operator_benchmark/pt/qinstancenorm_test.py", "benchmarks/operator_benchmark/pt/qinterpolate_test.py", "benchmarks/operator_benchmark/pt/qlayernorm_test.py", "benchmarks/operator_benchmark/pt/qlinear_test.py", "benchmarks/operator_benchmark/pt/qobserver_test.py", "benchmarks/operator_benchmark/pt/qpool_test.py", "benchmarks/operator_benchmark/pt/qrnn_test.py", "benchmarks/operator_benchmark/pt/qtensor_method_test.py", "benchmarks/operator_benchmark/pt/quantization_test.py", "benchmarks/operator_benchmark/pt/qunary_test.py", "benchmarks/operator_benchmark/pt/softmax_test.py", "benchmarks/operator_benchmark/pt/split_test.py", "benchmarks/operator_benchmark/pt/unary_test.py", "benchmarks/operator_benchmark/pt_extension/cpp_extension_test.py", "binaries/bench_gen/bench_gen.py", "caffe2/python/examples/imagenet_trainer.py", "modules/detectron/upsample_nearest_op_test.py", "scripts/diagnose_protobuf.py", "scripts/model_zoo/update-caffe2-models.py", "scripts/model_zoo/update-models-from-caffe2.py", "test/backward_compatibility/check_backward_compatibility.py", "test/backward_compatibility/dump_all_function_schemas.py", "test/distributed/algorithms/ddp_comm_hooks/test_ddp_hooks.py", "test/distributed/test_c10d.py", "test/distributed/test_distributed_fork.py", "test/distributed/test_distributed_spawn.py", "test/jit/test_class_type.py", "test/onnx/debug_embed_params.py", "test/onnx/export_onnx_tests_filter.py", "test/onnx/export_onnx_tests_generator.py", "test/onnx/test_caffe2_common.py", "test/onnx/test_models_onnxruntime.py", "test/onnx/test_onnx_common.py", "test/onnx/test_onnx_opset.py", "test/onnx/test_operators.py", "test/onnx/test_pytorch_common.py", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_utility_funs.py", "test/onnx/test_verify.py", "test/quantization/test_qat_module.py", "test/quantization/test_quantized_op.py", "test/simulate_nccl_errors.py", "test/test_function_schema.py", "test/test_jit.py", "test/test_jit_cuda_fuser.py", "test/test_jit_fuser.py", "test/test_jit_fuser_te.py", "test/test_mkldnn.py", "test/test_multiprocessing_spawn.py", "test/test_pruning_op.py", "test/test_tensorboard.py", "test/test_throughput_benchmark.py", "test/test_type_hints.py", "test/test_utils.py", "test/test_xnnpack_integration.py", "torch/_utils_internal.py", "torch/backends/quantized/__init__.py", "torch/backends/xnnpack/__init__.py", "torch/distributed/__init__.py", "torch/distributed/autograd/__init__.py", "torch/distributed/launch.py", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/_testing/__init__.py", "torch/distributed/rpc/backend_registry.py", "torch/distributions/von_mises.py", "torch/hub.py", "torch/multiprocessing/_atfork.py", "torch/multiprocessing/spawn.py", "torch/nn/functional.py", "torch/nn/init.py", "torch/nn/intrinsic/modules/fused.py", "torch/nn/intrinsic/qat/__init__.py", "torch/nn/intrinsic/qat/modules/__init__.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/intrinsic/qat/modules/linear_relu.py", "torch/nn/intrinsic/quantized/__init__.py", "torch/nn/intrinsic/quantized/modules/bn_relu.py", "torch/nn/intrinsic/quantized/modules/conv_relu.py", "torch/nn/intrinsic/quantized/modules/linear_relu.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/pooling.py", "torch/nn/quantized/modules/embedding_ops.py", "torch/nn/utils/fusion.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset12.py", "torch/onnx/symbolic_opset8.py", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py", "torch/overrides.py", "torch/quantization/__init__.py", "torch/quantization/_correct_bias.py", "torch/quantization/_learnable_fake_quantize.py", "torch/quantization/_numeric_suite.py", "torch/quantization/fake_quantize.py", "torch/quantization/fuse_modules.py", "torch/quantization/fx/__init__.py", "torch/quantization/observer.py", "torch/quantization/qconfig.py", "torch/quantization/quant_type.py", "torch/quantization/quantize.py", "torch/quantization/quantize_jit.py", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_quantization.py", "torch/testing/_internal/common_quantized.py", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/distributed_test.py", "torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "torch/testing/_internal/te_utils.py", "torch/utils/__init__.py", "torch/utils/_cpp_extension_versioner.py", "torch/utils/checkpoint.py", "torch/utils/collect_env.py", "torch/utils/cpp_extension.py", "torch/utils/dlpack.py", "torch/utils/file_baton.py", "torch/utils/hipify/hipify_python.py", "torch/utils/hooks.py", "torch/utils/mkldnn.py", "torch/utils/tensorboard/_caffe2_graph.py", "torch/utils/tensorboard/_convert_np.py", "torch/utils/tensorboard/summary.py", "torch/utils/tensorboard/writer.py", "torch/utils/throughput_benchmark.py"], "labels": ["fx", "oncall: jit", "open source", "triaged"]}, "f3bd984e44": {"title": "Move the description comment of compute_bucket_assignment_by_size from cpp to the header file. (#44703)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44703\n\nThe description of this public function should be in the header file.\n\nAlso fix some typos.\n\nTest Plan: N/A.\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23703661\n\nfbshipit-source-id: 24ae63de9498e321b31dfb2efadb44183c6370df", "pr_number": "44703", "files_changed": ["torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h"], "labels": ["fb-exported"]}, "07d9cc80a4": {"title": "Fix error code checks for triangular_solve (CPU) (#44720)", "body": "Summary:\nAdded missing error checks for the CPU version of `triangular_solve`.\nFixes https://github.com/pytorch/pytorch/issues/43141.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44720\n\nReviewed By: mruberry\n\nDifferential Revision: D23733400\n\nPulled By: ngimel\n\nfbshipit-source-id: 9837e01b04a6bfd9181e08d46bf96329f292cae0", "pr_number": "44720", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "test/test_torch.py"], "labels": ["open source"]}, "a3835179a1": {"title": "[FakeLowP] Addressing FakeLowP OSS issues. (#44819)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44819\n\n[12:39 AM] Cherckez, Tal\nplease review the following patch.\nshould address these issues that our validation team found:\nA) test_op_nnpi_fp16: hypothesis to trigger max_example*max_example.\nB) batchnorm: batchNorm has derived from unit test which doesnt have setting required for hypothesis. hence default value as 100 getting set.\n\nTest Plan:\nbuck test //caffe2/caffe2/contrib/fakelowp/test/...\nhttps://our.intern.facebook.com/intern/testinfra/testrun/5910974543950859\n\nReviewed By: hyuen\n\nDifferential Revision: D23740970\n\nfbshipit-source-id: 16fcc49f7bf84a5d7342786f671cd0b4e0fc87d3", "pr_number": "44819", "files_changed": ["caffe2/contrib/fakelowp/test/test_batchnorm_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py"], "labels": ["fb-exported"]}, "82ab167cce": {"title": "[NNC] Fix masking for all block and thread dimensions in CudaCodeGen (#44733)", "body": "Summary:\nUnifies a number of partial solutions to the thread and block dimension extent masking, including the NoThreadIdxWriter and my last fix https://github.com/pytorch/pytorch/issues/44325. The NoThreadIdxWriter is gone in favour of tracking the current loop extents and masking any statements that have a lower rank than the launch parameters in any Block or Thread dimension, which handles both the \"no\" and \"smaller\" axis binding cases.\n\nFor example it will transform the following:\n```\nfor i in 0..10 // blockIdx.x\n  for j in 0..10 // threadIdx.x\n    do thing(i, j);\n  for k in 0..5 // threadIdx.x\n    do other thing(i, k);\n```\n\nInto:\n```\ndo thing(blockIdx.x, threadIdx.x);\nif (threadIdx.x < 5) {\n  do other thing(blockIdx.x, threadIdx.x);\n}\n```\n\nAnd handle the case where statements are not bound by any axis, eg.\n```\ndo outer thing;\nfor i in 0..10 // blockIdx.x\n  for j in 0..10 // threadIdx.x\n    do thing(i, j);\n  do other thing(i);\n```\n\nwill become:\n\n```\nif (blockIdx.x < 1) {\n  if (threadIdx.x < 1) {\n    do outer thing;\n  }\n}\nsyncthreads();\ndo thing(blockIdx.x, threadIdx.x);\nsyncthreads();\nif (threadIdx.x < 1) {\n  do other thing(blockIdx.x);\n}\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44733\n\nReviewed By: mruberry\n\nDifferential Revision: D23736878\n\nPulled By: nickgg\n\nfbshipit-source-id: 52d08626ae8043d53eb937843466874d479a6768", "pr_number": "44733", "files_changed": ["test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.h", "torch/csrc/jit/tensorexpr/ir.h"], "labels": ["oncall: jit"]}, "5027c161a9": {"title": "Add TORCH_SELECTIVE_NAME to AMP definitions (#44711)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44711\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23711425\n\nPulled By: ezyang\n\nfbshipit-source-id: d4b0ef77893af80fe9b74791e66825e223ae221d", "pr_number": "44711", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "torch/library.h"], "labels": []}, "cb3b8a33f1": {"title": "[JIT] Disallow plain Dict type annotation without arg (#44334)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44334\n\n**Summary**\nThis commit detects and prohibits the case in which `typing.Dict` is\nused as an annotation without type arguments (i.e. `typing.Dict[K, V]`).\nAt present, `typing.Dict` is always assumed to have two arguments, and\nwhen it is used without them, `typing.Dict.__args__` is nonempty and\ncontains some `typing.TypeVar` instances, which have no JIT type equivalent.\nConsequently, trying to convert `typing.Dict` to a JIT type results in\na `c10::DictType` with `nullptr` for its key and value types, which can cause\na segmentation fault.\n\nThis is fixed by returning a `DictType` from\n`jit.annotations.try_ann_to_type` only if the key and value types are converted\nsuccessfully to a JIT type and returning `None` otherwise.\n\n**Test Plan**\nThis commit adds a unit test to `TestDict` that tests the plain `Dict`\nannotations throw an error.\n\n**Fixes**\nThis commit closes #43530.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23610766\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 036b10eff6e3206e0da3131cfb4997d8189c4fec", "pr_number": "44334", "files_changed": ["test/jit/test_list_dict.py", "torch/_jit_internal.py"], "labels": ["oncall: jit"]}, "78b806ab4a": {"title": "[JIT] Disallow plain List type annotation without arg (#44584)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44584\n\n**Summary**\nThis commit extends the work done in #38130 and disallows plain\nPython3-style `List` type annotations.\n\n**Test Plan**\nThis commit extends `TestList.test_no_element_type_annotation` to the\nPython3-style type annotation.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23721514\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 48957868286f44ab6d5bf5e1bf97f0a4ebf955df", "pr_number": "44584", "files_changed": ["test/jit/test_list_dict.py", "torch/_jit_internal.py"], "labels": ["oncall: jit"]}, "ac0d13cc88": {"title": "Vectorize complex copy. (#44722)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44722\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D23731276\n\nPulled By: ezyang\n\nfbshipit-source-id: 4902c4b79577ae3c70aca94828006b12914ab7f9", "pr_number": "44722", "files_changed": ["aten/src/ATen/native/cpu/CopyKernel.cpp"], "labels": ["module: complex"]}, "7b3432caff": {"title": "[TensorExpr] Support boolean in simplifier (#44659)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44659\n\nTest Plan: test_tensorexpr --gtest_filter=TensorExprTest.ConstantFoldCastToBool\n\nReviewed By: ngimel\n\nDifferential Revision: D23714675\n\nPulled By: asuhan\n\nfbshipit-source-id: 4c18d972b628d5ad55bad58eddd5f6974e043d9c", "pr_number": "44659", "files_changed": ["test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/ir_simplifier.h"], "labels": ["oncall: jit"]}, "3f5bb2bade": {"title": "[quant] Support clone for per channel affine quantized tensor (#44573)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44573\n\nfixes: https://github.com/pytorch/pytorch/issues/33309\n\nTest Plan: Imported from OSS\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D23663828\n\nfbshipit-source-id: 9a021a22b6075b1e94b3f91c0c101fbb9246ec0e", "pr_number": "44573", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/QTensor.cpp", "aten/src/ATen/native/quantized/TensorFactories.cpp", "test/quantization/test_quantized_tensor.py"], "labels": []}, "09a84071a3": {"title": "enable mypy check for jit_metaprogramming_utils (#44752)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42969\nenable mypy check for jit_metaprogramming_utils.py and fixed all errors.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44752\n\nReviewed By: walterddr\n\nDifferential Revision: D23741285\n\nPulled By: qxu-fb\n\nfbshipit-source-id: 21e36ca5d25c8682fb93b806e416b9e1db76f71e", "pr_number": "44752", "files_changed": ["mypy.ini", "torch/testing/_internal/jit_metaprogramming_utils.py"], "labels": []}, "ffe127e4f1": {"title": "[JIT] Disallow plain Tuple type annotation without arg (#44585)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44585\n\n**Summary**\nThis commit disallows plain `Tuple` type annotations without any\ncontained types both in type comments and in-line as Python3-style\ntype annotations.\n\n**Test Plan**\nThis commit adds a unit test for these two situations.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23721515\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: e11c77a4fac0b81cd535c37a31b9f4129c276592", "pr_number": "44585", "files_changed": ["test/test_jit_py3.py", "torch/_jit_internal.py"], "labels": []}, "161490d441": {"title": "Move `torch/version.py` generation to cmake (#44577)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44577\n\nI would like to to move this to cmake so that I can depend on it\nhappening from other parts of the build.\n\nThis PR pulls out the logic for determining the version string and\nwriting the version file into its own module. `setup.py` still receives\nthe version string and uses it as before, but now the code for writing\nout `torch/version.py` lives in a custom command in torch/CMakeLists.txt\n\nI noticed a small inconsistency in how version info is populated.\n`TORCH_BUILD_VERSION` is populated from `setup.py` at configuration\ntime, while `torch/version.py` is written at build time. So if, e.g. you\nconfigured cmake on a certain git rev, then built it in on another, the\ntwo versions would be inconsistent.\n\nThis does not appear to matter, so I opted to preserve the existing\nbehavior.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23734781\n\nPulled By: suo\n\nfbshipit-source-id: 4002c9ec8058503dc0550f8eece2256bc98c03a4", "pr_number": "44577", "files_changed": ["setup.py", "tools/generate_torch_version.py", "torch/CMakeLists.txt"], "labels": []}, "574f9af160": {"title": "[NCCL] Add option to run NCCL on high priority cuda stream (#43796)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43796\n\nThis diff adds an option for the process group NCCL backend to pick high priority cuda streams.\n\nTest Plan: waitforsandcastle\n\nReviewed By: jiayisuse\n\nDifferential Revision: D23404286\n\nfbshipit-source-id: b79ae097b7cd945a26e8ba1dd13ad3147ac790eb", "pr_number": "43796", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["fb-exported"]}, "43fe034514": {"title": "[JIT] Disallow plain Optional type annotation without arg (#44586)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44586\n\n**Summary**\nThis commit disallows plain `Optional` type annotations without\nany contained types both in type comments and in-line as\nPython3-style type annotations.\n\n**Test Plan**\nThis commit adds a unit test for these two situations.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23721517\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: ead411e94aa0ccce227af74eb0341e2a5331370a", "pr_number": "44586", "files_changed": ["test/test_jit_py3.py", "torch/_jit_internal.py"], "labels": []}, "6befc09465": {"title": "Fix misuse of PyObject_IsSubclass (#44769)", "body": "Summary:\nPyObject_IsSubclass may set python live exception bit if given object is not a class. `IsNamedTuple` is currently using it incorrectly, which may trip all following python operations in debug-build python. Normal release-build python is not affected because `assert` is no-op in release-build.\n\nFixes https://github.com/pytorch/pytorch/issues/43577\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44769\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23725584\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 2dabd4f8667a045d5bf75813500876c6fd81542b", "pr_number": "44769", "files_changed": ["torch/csrc/jit/python/python_sugared_value.cpp"], "labels": ["oncall: jit"]}, "0ccc38b773": {"title": "[caffe2] adds Cancel to SafeDequeueBlobsOp and SafeEnqueueBlobsOp (#44495)", "body": "Summary:\n## Motivation\n\n* To be able to make C2 ops cancellable so we can safely exit.\n* Some C2 operators are now blocking thus being non-cancellable. If an error\n  occurs we need to be able to safely stop all net execution so we can throw\n  the exception to the caller.\n\n* When an error occurs in a net or it got cancelled, running ops will have the\n `Cancel` method called.\n\n* This diff adds `Cancel` method to the `SafeEnqueueBlobsOp`\nand `SafeDequeueBlobsOp` to have the call queue->close() to force all the\n blocking ops to return.\n* Adds unit test that verified the error propagation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44495\n\nTest Plan:\n## Unit Test added to verify that queue ops propagate errors\n```\nbuck test caffe2/caffe2/python:hypothesis_test\n```\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23236088\n\nPulled By: dahsh\n\nfbshipit-source-id: daa90d9ee32483fb51195e269a52cf5987bb0a5a", "pr_number": "44495", "files_changed": ["caffe2/python/hypothesis_test.py", "caffe2/queue/queue_ops.h"], "labels": ["fb-exported"]}, "8ec6bc7292": {"title": "[pytorch][vulkan][jni] LiteModuleLoader load argument to use vulkan device", "body": "Summary:\n### Java, CPP\nIntroducing additional parameter `device` to LiteModuleLoader to specify device on which the `forward` will work.\n\nOn the java side this is enum that contains CPU and VULKAN, passing as jint to jni side and storing it as a member field on the same level as module.\n\nOn pytorch_jni_lite.cpp - for all input tensors converting them to vulkan.\n\nOn pytorch_jni_common.cpp (also goes to OSS) - if result Tensor is not cpu - call cpu. (Not Cpu at the moment is only Vulkan).\n\n### BUCK\nIntroducing `pytorch_jni_lite_with_vulkan` target, that depends on `pytorch_jni_lite_with_vulkan` and adds `aten_vulkan`\n\nIn that case `pytorch_jni_lite_with_vulkan` can be used along with `pytorch_jni_lite_with_vulkan`.\n\nTest Plan:\nAfter the following diff with aidemo segmentation:\n```\nbuck install -r aidemos-android\n```\n{F296224521}\n\nReviewed By: dreiss\n\nDifferential Revision: D23198335\n\nfbshipit-source-id: 95328924e398901d76718c4d828f96e112dfa1b0", "pr_number": null, "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_lite.cpp", "android/pytorch_android/src/main/java/org/pytorch/Device.java", "android/pytorch_android/src/main/java/org/pytorch/LiteModuleLoader.java", "android/pytorch_android/src/main/java/org/pytorch/LiteNativePeer.java"], "labels": []}, "204f985fc3": {"title": "[NNC] Add simplification of Loop + Condition patterns. (#44764)", "body": "Summary:\nAdds a new optimization to the IRSimplifier which changes this pattern:\n```\nfor ...\n  if ...\n   do thing;\n```\ninto:\n```\nif ...\n  for ...\n    do thing;\n```\n\nWhich should be almost strictly better.\n\nThere are many cases where this isn't safe to do, hence tests. Most  obviously when the condition depends on something modified within the loop.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44764\n\nReviewed By: mruberry\n\nDifferential Revision: D23734463\n\nPulled By: nickgg\n\nfbshipit-source-id: 51617e837de96b354fb702d0090ac65ddc523d36", "pr_number": "44764", "files_changed": ["test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/analysis.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.cpp", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["oncall: jit"]}, "29664e6aa3": {"title": "[FX] Further sanitize generated names (#44808)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44808\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D23739413\n\nPulled By: jamesr66a\n\nfbshipit-source-id: b759c3ea613dfa717fb23977b72ff4773d9dcc99", "pr_number": "44808", "files_changed": ["test/test_fx.py", "torch/fx/graph.py"], "labels": ["fx"]}, "79108fc16c": {"title": "[JIT] Improve Future subtype checking (#44570)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44570\n\n**Summary**\nThis commit improves subtype checking for futures so that\n`Future[T]` is considered to be a subtype of `Future[U]` if `U` is a\nsubtype of `V`.\n\n**Test Plan**\nThis commit adds a test case to `test_async.py` that tests this.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23660588\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: b606137c91379debab91b9f41057f7b1605757c5", "pr_number": "44570", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/jit/test_async.py"], "labels": ["oncall: jit"]}, "e48201c5cf": {"title": "Mention TF32 on related docs (#44690)", "body": "Summary:\ncc: ptrblck\n\n![image](https://user-images.githubusercontent.com/1032377/93168022-cbbfcb80-f6d6-11ea-8f6e-f2c8a15c5bea.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44690\n\nReviewed By: ngimel\n\nDifferential Revision: D23727921\n\nPulled By: mruberry\n\nfbshipit-source-id: db7cc8e74cde09c13d6a57683129fd839863b914", "pr_number": "44690", "files_changed": ["docs/source/backends.rst", "docs/source/index.rst", "torch/_torch_docs.py", "torch/backends/cudnn/__init__.py", "torch/nn/functional.py", "torch/nn/modules/conv.py", "torch/nn/modules/linear.py"], "labels": ["open source", "triaged"]}, "ba6534ae2b": {"title": "enable type check common_distributed (#44821)", "body": "Summary:\nEnabled type checking in common_distributed by using tensors of ints\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44821\n\nTest Plan: Run python test/test_type_hints.py, errors are no longer ingnored by mypy.ini\n\nReviewed By: walterddr\n\nDifferential Revision: D23747466\n\nPulled By: alanadakotashine\n\nfbshipit-source-id: 820fd502d7ff715728470fbef0be90ae7f128dd6", "pr_number": "44821", "files_changed": ["mypy.ini", "torch/testing/_internal/common_distributed.py"], "labels": []}, "34331b0e0f": {"title": "CUDA BFloat16 and other improvements on abs (#44804)", "body": "Summary:\nNot sure if ROCm supports `std::abs` today, let's see the CI\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44804\n\nReviewed By: mruberry\n\nDifferential Revision: D23748837\n\nPulled By: ngimel\n\nfbshipit-source-id: ccf4e63279f3e5927a85d8d8f70ba4b8c334156b", "pr_number": "44804", "files_changed": ["aten/src/ATen/native/cuda/AbsKernel.cu", "test/test_torch.py"], "labels": ["open source"]}, "b61d3d8be8": {"title": "Implement torch.kaiser_window (#44271)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/38349\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44271\n\nReviewed By: ngimel\n\nDifferential Revision: D23727972\n\nPulled By: mruberry\n\nfbshipit-source-id: b4c931b2eb3a536231ad6d6c3cb66e52a13286ac", "pr_number": "44271", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorFactories.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/torch.rst", "test/test_tensor_creation_ops.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["module: numpy", "module: operators (deprecated)", "open source", "triaged"]}, "fdeee74590": {"title": "[pytorch][vulkan] Fix downcast warnings-errors, aten_vulkan buck target", "body": "Summary:\nbuck build has -Wall for downcasts - need to add safe_downcast<int32_t> everywhere\n\nBUCK build changes for aten_vulkan to include vulkan_wrapper lib\n\nTest Plan: The next diff with segmentation demo works fine\n\nReviewed By: dreiss\n\nDifferential Revision: D23739445\n\nfbshipit-source-id: b22a30e1493c4174c35075a68586defb0fccd2af", "pr_number": null, "files_changed": ["aten/src/ATen/native/vulkan/VulkanOps.cpp"], "labels": []}, "e18a2219dd": {"title": "Implement scatter reductions (CUDA), remove divide/subtract (#41977)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33394 .\n\nThis PR does two things:\n1. Implement CUDA scatter reductions with revamped GPU atomic operations.\n2. Remove support for divide and subtract for CPU reduction as was discussed with ngimel .\n\nI've also updated the docs to reflect the existence of only multiply and add.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41977\n\nReviewed By: mruberry\n\nDifferential Revision: D23748888\n\nPulled By: ngimel\n\nfbshipit-source-id: ea643c0da03c9058e433de96db02b503514c4e9c", "pr_number": "41977", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/ScatterGatherKernel.cpp", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/cuda_atomic_ops_test.cu", "aten/src/THC/THCAtomics.cuh", "aten/src/THC/THCNumerics.cuh", "test/test_torch.py", "third_party/gloo", "third_party/ideep", "torch/_tensor_docs.py"], "labels": ["open source", "triaged"]}, "b6f4bb0a70": {"title": "Revert D23236088: [pytorch][PR] [caffe2] adds Cancel to SafeDequeueBlobsOp and SafeEnqueueBlobsOp", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23236088 (https://github.com/pytorch/pytorch/commit/0ccc38b77390f34bb173885ff039363d2931c356)\n\nOriginal commit changeset: daa90d9ee324\n\nfbshipit-source-id: 933c7deab177250075683a9bea143ac37f16a598", "pr_number": null, "files_changed": ["caffe2/python/hypothesis_test.py", "caffe2/queue/queue_ops.h"], "labels": []}, "99093277c0": {"title": "Support Python Slice class in TorchScript (#44335)", "body": "Summary:\nImplements support for[ Python Slice class](https://docs.python.org/3/c-api/slice.html) (not slice expression, which is already supported)\n\nSlice object can be used in any place that supports slice expression, including multi-dim tensor slicing.\n\nFixes https://github.com/pytorch/pytorch/issues/43511\nFixes https://github.com/pytorch/pytorch/issues/43125\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44335\n\nReviewed By: suo, jamesr66a\n\nDifferential Revision: D23682213\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: f74fe25370e89fbfd2b3727d95ce4e1c4ba8dec4", "pr_number": "44335", "files_changed": ["test/jit/test_slice.py", "test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h"], "labels": ["oncall: jit"]}, "28085cbd39": {"title": "Fixed quantile nan propagation and implemented nanquantile (#44393)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44393\n\ntorch.quantile now correctly propagates nan and implemented torch.nanquantile similar to numpy.nanquantile.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23649613\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 5201d076745ae1237cedc7631c28cf446be99936", "pr_number": "44393", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "503c74888f": {"title": "Always use NewModuleTest instead of ModuleTest. (#44745)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44745\n\nMuch like CriterionTest, NewCriterionTest these are outdated formulations and we should just use the new one.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23717808\n\nPulled By: gchanan\n\nfbshipit-source-id: eb91982eef23452456044381334bfc9a5bbd837e", "pr_number": "44745", "files_changed": ["test/test_cpp_api_parity.py"], "labels": []}, "a40ef25e30": {"title": "[te] Disable flaky test CudaSharedMemReduce_1 (#44862)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44862\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23753831\n\nPulled By: bertmaher\n\nfbshipit-source-id: d7d524ac34e4ca208df022a5730c2d11b3068f12", "pr_number": "44862", "files_changed": ["test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/tests.h"], "labels": []}, "4affbbd9f8": {"title": "minor style edits to torch/testing/_internal/common_quantized.py (#44807)", "body": "Summary:\nstyle nits\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44807\n\nReviewed By: malfet\n\nDifferential Revision: D23742537\n\nPulled By: janeyx99\n\nfbshipit-source-id: 446343822d61f8fd9ef6dfcb8e5da4feff6522b6", "pr_number": "44807", "files_changed": ["torch/testing/_internal/common_quantized.py"], "labels": []}, "f605d7581e": {"title": "Implement better caching allocator for segmentation usecase. (#44618)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44618\n\nThis diff refactors caching allocator to allow for overriding behavior by\nmaking it a virtual class.\n\nTest Plan: https://www.internalfb.com/intern/fblearner/details/218419618?tab=Experiment%20Results\n\nReviewed By: dreiss\n\nDifferential Revision: D23672902\n\nfbshipit-source-id: 976f02922178695fab1c87f453fcb59142c258ec", "pr_number": "44618", "files_changed": ["c10/core/CPUCachingAllocator.h"], "labels": ["fb-exported"]}, "6006e45028": {"title": ".circleci: Switch to dynamic MAX_JOBS (#44729)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44729\n\nSwitches our MAX_JOBS from a hardcoded value to a more dynamic value so\nthat we can always utilize all of the core that are available to us\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D23759643\n\nPulled By: seemethere\n\nfbshipit-source-id: ad26480cb0359c988ae6f994e26a09f601b728e3", "pr_number": "44729", "files_changed": [".circleci/scripts/binary_linux_build.sh"], "labels": ["ci/binaries"]}, "d2b4534d4d": {"title": "refactor intialize bucket views (#44330)", "body": "Summary:\n[test all]\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44330\n\nPart of relanding PR #41954, this refactor is to seperate intialize_bucket_views and populate_bucket_views_out, as they are doing different things and called by different callsites as well\nghstack-source-id: 112257271\n\nTest Plan: unit tests\n\nReviewed By: mrshenli\n\nDifferential Revision: D23583347\n\nfbshipit-source-id: a5f2041b2c4f2c2b5faba1af834c7143eaade938", "pr_number": "44330", "files_changed": ["torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h"], "labels": []}, "74c3dcd1d2": {"title": "Revert D23725053: [pytorch][PR] change self.generator to generator", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23725053 (https://github.com/pytorch/pytorch/commit/a011b86115541365ebd55598f85ff9a42a6875d8)\n\nOriginal commit changeset: 89706313013d\n\nfbshipit-source-id: 035214f0d4298d29a52f8032d364b52dfd956fe8", "pr_number": null, "files_changed": ["torch/utils/data/sampler.py"], "labels": []}, "361b38da19": {"title": "[quant][fx] Add node name as prefix to observer module name (#44765)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44765\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFx.test_save_observer_state_dict\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23741355\n\nfbshipit-source-id: 7185ceae5b3b520ac0beebb627c44eab7ae7d231", "pr_number": "44765", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "1fde54d531": {"title": "[quant][qat] Ensure fake_quant and observer can be disabled on scriptmodule (#44773)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44773\n\nThe model is created and prepared using fx APIs and then scripted for training.\nIn order to test QAT on scriptmodel we need to be able to disable/enable fake_quant\nand observer modules on it.\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFx.test_qat_and_script\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23741354\n\nfbshipit-source-id: 3fee7aa9b049d9901313b977710f4dc1c4501532", "pr_number": "44773", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fake_quantize.py"], "labels": []}, "24df3b7373": {"title": "torch.empty_like and torch.zeros_like raise error if any memory format is provided with sparse input (#43699) (#44058)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43699\n\n- Changed the order of `TORCH_CHECK` and `if (options.layout() == kSparse && self.is_sparse())`\ninside `empty_like` method.\n\n- [x] Added tests\n\nEDIT:\n\nMore details on that and why we can not take zeros_like  approach.\nPython code :\n```python\nres = torch.zeros_like(input_coalesced, memory_format=torch.preserve_format)\n```\nis routed to\n```c++\n// TensorFactories.cpp\nTensor zeros_like(\n    const Tensor& self,\n    const TensorOptions& options,\n    c10::optional<c10::MemoryFormat> optional_memory_format) {\n  if (options.layout() == kSparse && self.is_sparse()) {\n    auto res = at::empty({0}, options); // to be resized\n    res.sparse_resize_and_clear_(\n        self.sizes(), self.sparse_dim(), self.dense_dim());\n    return res;\n  }\n  auto result = at::empty_like(self, options, optional_memory_format);\n  return result.zero_();\n}\n```\nand passed to `if (options.layout() == kSparse && self.is_sparse())`\n\nWhen we call in Python\n```python\nres = torch.empty_like(input_coalesced, memory_format=torch.preserve_format)\n```\nit is routed to\n```c++\nTensor empty_like(\n    const Tensor& self,\n    const TensorOptions& options_,\n    c10::optional<c10::MemoryFormat> optional_memory_format) {\n  TORCH_CHECK(\n    !(options_.has_memory_format() && optional_memory_format.has_value()),\n    \"Cannot set memory_format both in TensorOptions and explicit argument; please delete \"\n    \"the redundant setter.\");\n  TensorOptions options =\n      self.options()\n          .merge_in(options_)\n          .merge_in(TensorOptions().memory_format(optional_memory_format));\n  TORCH_CHECK(\n      !(options.layout() != kStrided &&\n          optional_memory_format.has_value()),\n      \"memory format option is only supported by strided tensors\");\n  if (options.layout() == kSparse && self.is_sparse()) {\n    auto result = at::empty({0}, options); // to be resized\n    result.sparse_resize_and_clear_(\n        self.sizes(), self.sparse_dim(), self.dense_dim());\n    return result;\n  }\n```\n\ncc pearu\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44058\n\nReviewed By: albanD\n\nDifferential Revision: D23672494\n\nPulled By: mruberry\n\nfbshipit-source-id: af232274dd2b516dd6e875fc986e3090fa285658", "pr_number": "44058", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "test/test_sparse.py", "test/test_torch.py", "tools/autograd/derivatives.yaml"], "labels": ["module: sparse", "open source", "triaged"]}, "c1fa42497b": {"title": "fix legacy GET_BLOCKS code from THCUNN/common.h (#44789)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44472\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44789\n\nReviewed By: malfet\n\nDifferential Revision: D23732762\n\nPulled By: walterddr\n\nfbshipit-source-id: c3748e365e9a1d009b00140ab0ef892da905d09b", "pr_number": "44789", "files_changed": ["aten/src/THCUNN/common.h"], "labels": []}, "c189328e5d": {"title": "CUDA BFloat16 unary ops part 2 (#44824)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44824\n\nReviewed By: mruberry\n\nDifferential Revision: D23752360\n\nPulled By: ngimel\n\nfbshipit-source-id: 3aadaf9db9d4e4937aa38671e8589ecbeece709d", "pr_number": "44824", "files_changed": ["aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "test/test_torch.py"], "labels": ["module: bfloat16", "module: cuda", "open source", "triaged"]}, "2558e5769d": {"title": "Implement sort for list of tuples (#43448)", "body": "Summary:\n* Implement tuple sort by traversing contained IValue types and generate a lambda function as comparator for sort.\n* Tuple, class objects can now arbitrarily nest within each other and still be sortable\n\nFixes https://github.com/pytorch/pytorch/issues/43219\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43448\n\nReviewed By: eellison\n\nDifferential Revision: D23352273\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: b6efa8d00e112178de8256da3deebdba7d06c0e1", "pr_number": "43448", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "test/jit/test_class_type.py", "test/test_jit.py", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["oncall: jit"]}, "bee97d5be0": {"title": "Document the default behavior for dist.new_group() when ranks=None (#44000)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44000\n\nThis wasn't documented, so add a doc saying all ranks are used when\nranks=None\nghstack-source-id: 111206308\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D23465034\n\nfbshipit-source-id: 4c51f37ffcba3d58ffa5a0adcd5457e0c5676a5d", "pr_number": "44000", "files_changed": ["torch/distributed/distributed_c10d.py"], "labels": []}, "f5440a448a": {"title": "CUDA BFloat16 i0 support (#44750)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44750\n\nReviewed By: glaringlee\n\nDifferential Revision: D23764383\n\nPulled By: ngimel\n\nfbshipit-source-id: d0e784d89241e8028f97766fdac51fe1ab4c188c", "pr_number": "44750", "files_changed": ["aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "test/test_torch.py"], "labels": ["open source"]}, "5d57025206": {"title": "[TensorExpr] Add log1p support to the LLVM backend (#44839)", "body": "Summary:\nAlso corrected Sleef_log1p registrations, float versions had a redundant f.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44839\n\nTest Plan: test_tensorexpr --gtest_filter=TensorExprTest.LLVMElemwiseLog1pFloat_LLVM\n\nReviewed By: glaringlee\n\nDifferential Revision: D23762113\n\nPulled By: asuhan\n\nfbshipit-source-id: b5cf003b5c0c1ad549c7f04470352231929ac459", "pr_number": "44839", "files_changed": ["test/cpp/tensorexpr/test_base.h", "test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/llvm_jit.cpp"], "labels": ["oncall: jit"]}, "4066022146": {"title": "Do not use `PRId64` in torch/csrc (#44767)", "body": "Summary:\nInstead use `fmt::format()` or `%lld` and cast argument to `(long long)`\nFix typos and add helper `PyErr_SetString()` method in torch/csrc/Exceptions.h\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44767\n\nReviewed By: ezyang\n\nDifferential Revision: D23723671\n\nPulled By: malfet\n\nfbshipit-source-id: c0101aed222184aa436b1e8768480d1531dff232", "pr_number": "44767", "files_changed": ["torch/csrc/DataLoader.cpp", "torch/csrc/Exceptions.h", "torch/csrc/Storage.cpp", "torch/csrc/cuda/Storage.cpp", "torch/csrc/generic/Storage.cpp", "torch/csrc/generic/StorageMethods.cpp"], "labels": []}, "086a2e7a4e": {"title": "[caffe2] add cost inference for FusedFakeQuantFC and FusedFakeQuantFCGradient (#44840)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44840\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44762\n\nMove CostInferenceForFCGradient to fc_inference.cc/h to be used in multiple .cc files.\n\nTest Plan: CI\n\nReviewed By: qizzzh\n\nDifferential Revision: D23714877\n\nfbshipit-source-id: d27f33e270a93b0e053f2af592dc4a24e35526cd", "pr_number": "44840", "files_changed": ["caffe2/operators/fc_inference.cc", "caffe2/operators/fc_inference.h", "caffe2/operators/fully_connected_op.cc"], "labels": ["fb-exported"]}, "40e44c5f0a": {"title": "Make nuclear and frobenius norm non-out depend on out variants (#44095)", "body": "Summary:\nPart of https://github.com/pytorch/pytorch/issues/24802\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44095\n\nReviewed By: ngimel\n\nDifferential Revision: D23735893\n\nPulled By: mruberry\n\nfbshipit-source-id: bd1264b6a8e7f9220033982b0118aa962991ca88", "pr_number": "44095", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp"], "labels": ["open source", "triaged"]}, "a153eafab7": {"title": "Let logspace support bfloat16 on both CPU and CUDA (#44675)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44675\n\nReviewed By: ngimel\n\nDifferential Revision: D23710801\n\nPulled By: mruberry\n\nfbshipit-source-id: 12d8e56f41bb635b500e89aaaf5df86a1795eb72", "pr_number": "44675", "files_changed": ["aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/cuda/RangeFactories.cu", "test/test_tensor_creation_ops.py"], "labels": ["open source", "triaged"]}, "f5b92332c1": {"title": "[TensorExpr] Fix order comparisons for unsigned types (#44857)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44857\n\nTest Plan: test_tensorexpr --gtest_filter=TensorExprTest.LLVMCompareSelectByte*_LLVM\n\nReviewed By: glaringlee\n\nDifferential Revision: D23762162\n\nPulled By: asuhan\n\nfbshipit-source-id: 1553429bd2d5292ccda57910326b8c70e4e6ab88", "pr_number": "44857", "files_changed": ["test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp"], "labels": ["oncall: jit"]}, "1c996b7170": {"title": "Enable typechecking for torch.testing._internal.common_quantized.* (#44805)", "body": "Summary:\nAddresses a subproblem of [Issue 42969](https://github.com/pytorch/pytorch/issues/42969)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44805\n\nReviewed By: malfet\n\nDifferential Revision: D23742754\n\nPulled By: janeyx99\n\nfbshipit-source-id: e916a6a0c049cac318549a485d47f19363087d15", "pr_number": "44805", "files_changed": ["mypy.ini", "torch/backends/quantized/__init__.py"], "labels": []}, "e535fb3f7d": {"title": "[ONNX] Enable true_divide scripting export with ONNX shape inference (#43991)", "body": "Summary:\nFixes the `true_divide` symbolic to cast tensors correctly.\nThe logic depends on knowing input types at export time, which is a known gap for exporting scripted modules. On that end we are improving exporter by enabling ONNX shape inference https://github.com/pytorch/pytorch/issues/40628, and starting to increase coverage for scripting support.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43991\n\nReviewed By: mruberry\n\nDifferential Revision: D23674614\n\nPulled By: bzinodev\n\nfbshipit-source-id: 1b1b85340eef641f664a14c4888781389c886a8b", "pr_number": "43991", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py"], "labels": ["module: onnx", "open source", "triaged"]}, "18b77d7d17": {"title": "[TensorExpr] Add Mod support to the LLVM backend (#44823)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44823\n\nTest Plan: test_tensorexpr --gtest_filter=TensorExprTest.LLVMElemwiseMod_LLVM\n\nReviewed By: glaringlee\n\nDifferential Revision: D23761996\n\nPulled By: asuhan\n\nfbshipit-source-id: c3c5b2fe0d989dec04f0152ce47c5cae35ed19c9", "pr_number": "44823", "files_changed": ["test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp"], "labels": ["oncall: jit"]}, "2043fbdfb6": {"title": "Enable torch.backends.cuda typechecking in CI (#44916)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44916\n\nReviewed By: walterddr\n\nDifferential Revision: D23769844\n\nPulled By: malfet\n\nfbshipit-source-id: 3be3616fba9e2f9c6d89cc71d5f0d24ffcc45cf2", "pr_number": "44916", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in"], "labels": ["module: typing", "triaged"]}, "e14b2080be": {"title": "[reland] move rebuild buckets from end of first iteration to beginning of second iteration (#44798)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44798\n\n[test all]\n\nUpdate for relanding: in ddp.join(), moved _rebuild_buckets from end of backward to beginning of forward as well.\n\nPart of relanding PR #41954, this refactoring is to move rebuild_buckets call from end of first iteration to beginning of second iteration\nghstack-source-id: 112279261\nghstack-source-id: 112279261\n\nTest Plan: unit tests\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23735185\n\nfbshipit-source-id: c26e0efeecb3511640120faa1122a2c856cd694e", "pr_number": "44798", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/nn/parallel/distributed.py"], "labels": []}, "60ae6c9c18": {"title": "[FX] Fix GraphModule copy methods not regenerating forward (#44806)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44806\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23738732\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 14e13551c6568c562f3f789b6274b6c86afefd0b", "pr_number": "44806", "files_changed": ["test/test_fx.py", "torch/fx/graph_module.py"], "labels": ["fx"]}, "9a007ba4cb": {"title": "[jit] stop parsing the block after seeing exit statements (#44870)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44870\n\nfix https://github.com/pytorch/pytorch/issues/44864\n\nTest Plan: buck test mode/dev-nosan //caffe2/test:jit -- 'test_assert_is_script'\n\nReviewed By: eellison\n\nDifferential Revision: D23755094\n\nfbshipit-source-id: ca3f8b27dc6f9dc9364a22a1bce0e2f588ed4308", "pr_number": "44870", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["fb-exported", "oncall: jit"]}, "5dbcbea265": {"title": "TorchScript with record_function (#44345)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44345\n\nAs part of enhancing profiler support for RPC, when executing TorchScript functions over RPC, we would like to be able to support user-defined profiling scopes created by `with record_function(...)`.\n\nSince after https://github.com/pytorch/pytorch/pull/34705, we support `with` statements in TorchScript, this PR adds support for `with torch.autograd.profiler.record_function` to be used within TorchScript.\n\nThis can be accomplished via the following without this PR:\n```\ntorch.opts.profiler._record_function_enter(...)\n# Script code, such as forward pass\ntorch.opts.profiler._record_function_exit(....)\n```\n\nThis is a bit hacky and it would be much cleaner to use the context manager now that we support `with` statements. Also, `_record_function_` type operators are internal operators that are subject to change, this change will help avoid BC issues in the future.\n\nTested with `python test/test_jit.py TestWith.test_with_record_function -v`\nghstack-source-id: 112320645\n\nTest Plan:\nRepro instructions:\n1) Change `def script_add_ones_return_any(x) -> Any` to `def script_add_ones_return_any(x) -> Tensor` in `jit/rpc_test.py`\n2) `buck test mode/dev-nosan //caffe2/test/distributed/rpc:process_group_agent -- test_record_function_on_caller_rpc_async --print-passing-details`\n3) The function which ideally should accept `Future[Any]` is `def _call_end_callbacks_on_future` in `autograd/profiler.py`.\n\npython test/test_jit.py TestWith.test_with_foo -v\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23332074\n\nfbshipit-source-id: 61b0078578e8b23bfad5eeec3b0b146b6b35a870", "pr_number": "44345", "files_changed": ["test/jit/test_with.py", "test/test_autograd.py", "torch/autograd/profiler.py", "torch/csrc/distributed/rpc/torchscript_functions.cpp", "torch/distributed/rpc/internal.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["oncall: jit"]}, "1694fde7eb": {"title": "Fix a GroupNorm cuda bug when input does not require_grad (#44863)", "body": "Summary:\nFix https://discuss.pytorch.org/t/illegal-memory-access-when-i-use-groupnorm/95800\n\n`dX` is a Tensor, comparing `dX` with `nullptr` was wrong.\n\ncc BIT-silence who wrote the kernel.\n\nThe test couldn't pass with `rtol=0` and `x.requires_grad=True`, so I have to update that to `1e-5`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44863\n\nReviewed By: mruberry\n\nDifferential Revision: D23754101\n\nPulled By: BIT-silence\n\nfbshipit-source-id: 2eb0134dd489480e5ae7113a7d7b84629104cd49", "pr_number": "44863", "files_changed": ["aten/src/ATen/native/cuda/group_norm_kernel.cu", "test/test_nn.py"], "labels": ["open source"]}, "f2b3480795": {"title": "CUDA BFloat softmax (#44837)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44837\n\nReviewed By: glaringlee\n\nDifferential Revision: D23767981\n\nPulled By: ngimel\n\nfbshipit-source-id: be92c25a1b66ed50a52e090db167079def6f6b39", "pr_number": "44837", "files_changed": ["aten/src/ATen/native/cuda/SoftMax.cu", "test/test_nn.py"], "labels": ["module: bfloat16", "module: cuda", "open source", "triaged"]}, "e400150c3b": {"title": "Fixed for caffe2/opt/tvm_transformer.cc (#44249)", "body": "Summary:\nFixes #https://github.com/pytorch/pytorch/issues/41706\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44249\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23752331\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 1d7297e080bc1e065129259e406af7216f3f0665", "pr_number": "44249", "files_changed": ["caffe2/opt/tvm_transformer.cc", "caffe2/opt/tvm_transformer.h"], "labels": ["better-engineering", "open source", "triaged"]}, "caea1adc35": {"title": "Complex support for stft and istft (#43886)", "body": "Summary:\nRef https://github.com/pytorch/pytorch/issues/42175, fixes https://github.com/pytorch/pytorch/issues/34797\n\nThis adds complex support to `torch.stft` and `torch.istft`. Note that there are really two issues with complex here: complex signals, and returning complex tensors.\n\n## Complex signals and windows\n`stft` currently assumes all signals are real and uses `rfft` with `onesided=True` by default. Similarly, `istft` always takes a complex fourier series and uses `irfft` to return real signals.\n\nFor `stft`, I now allow complex inputs and windows by calling the full `fft` if either are complex. If the user gives `onesided=True` and the signal is complex, then this doesn't work and raises an error instead. For `istft`, there's no way to automatically know what to do when `onesided=False` because that could either be a redundant representation of a real signal or a complex signal. So there, the user needs to pass the argument `return_complex=True` in order to use `ifft` and get a complex result back.\n\n## stft returning complex tensors\nThe other issue is that `stft` returns a complex result, represented as a `(... X 2)` real tensor. I think ideally we want this to return proper complex tensors but to preserver BC I've had to add a `return_complex` argument to manage this transition. `return_complex` defaults to false for real inputs to preserve BC but defaults to True for complex inputs where there is no BC to consider.\n\nIn order to `return_complex` by default everywhere without a sudden BC-breaking change, a simple transition plan could be:\n1. introduce `return_complex`, defaulted to false when BC is an issue but giving a warning. (this PR)\n2. raise an error in cases where `return_complex` defaults to false, making it a required argument.\n3. change `return_complex` default to true in all cases.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43886\n\nReviewed By: glaringlee\n\nDifferential Revision: D23760174\n\nPulled By: mruberry\n\nfbshipit-source-id: 2fec4404f5d980ddd6bdd941a63852a555eb9147", "pr_number": "43886", "files_changed": ["aten/src/ATen/native/Col2Im.cpp", "aten/src/ATen/native/Im2Col.cpp", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/cuda/Col2Im.cu", "aten/src/ATen/native/cuda/Im2Col.cu", "aten/src/ATen/native/im2col.h", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_spectral_ops.py", "torch/functional.py", "torch/overrides.py", "torch/tensor.py"], "labels": ["open source", "triaged"]}, "df39c40054": {"title": "Cleanup tracer handling of optional arguments (#43009)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43009\n\n* **#43009 Cleanup tracer handling of optional arguments**\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23766621\n\nPulled By: mruberry\n\nfbshipit-source-id: c1b46cd23b58b18ef4c03021b2514d7e692badb6", "pr_number": "43009", "files_changed": ["torch/csrc/jit/frontend/tracer.cpp"], "labels": ["oncall: jit", "open source", "triaged"]}, "6d178f6b8e": {"title": "Stop ignoring errors in cuda nn module tests. (#44783)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44783\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23731778\n\nPulled By: gchanan\n\nfbshipit-source-id: 32df903a9e36bbf3f66645ee2d77efa5ed6ee429", "pr_number": "44783", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "07b7e44ed1": {"title": "Stop using check_criterion_jacobian. (#44786)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44786\n\nThis predates gradcheck and gradcheck does the same and more.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23731902\n\nPulled By: gchanan\n\nfbshipit-source-id: 425fd30e943194f63a663708bada8960265b8f05", "pr_number": "44786", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "cff0e57c31": {"title": "Remove Incorrect Comment in tools/build_libtorch and remove Python2 support in the module import (#44888)", "body": "Summary:\nFixes #{44293} and removes Python2 imports from MNIST download module as Python2 is not being supported.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44888\n\nReviewed By: agolynski\n\nDifferential Revision: D23785579\n\nPulled By: bugra\n\nfbshipit-source-id: d9380502380876282008dd2d5feb92a446648982", "pr_number": "44888", "files_changed": ["tools/build_libtorch.py", "tools/download_mnist.py"], "labels": []}, "c68cc78299": {"title": "Add a device parameter to RemoteModule (#44254)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44254\n\nAdd a device parameter to RemoteModule, so it can be placed on any device\nand not just CPU.\n\nOriginal PR issue: RemoteModule enhancements #40550\n\nTest Plan: buck test test/distributed/rpc:process_group_agent -- RemoteModule\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23483803\n\nfbshipit-source-id: 4918583c15c6a38a255ccbf12c9168660ab7f6db", "pr_number": "44254", "files_changed": ["torch/distributed/nn/api/remote_module.py", "torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["fb-exported"]}, "0063512a4b": {"title": "[ONNX] Updates to diagnostic tool to find missing ops (#44124)", "body": "Summary:\nMoved description of tool and changes in function name\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44124\n\nReviewed By: albanD\n\nDifferential Revision: D23674618\n\nPulled By: bzinodev\n\nfbshipit-source-id: 5db0bb14fc106fc96358b1e0590f08e975388c6d", "pr_number": "44124", "files_changed": ["test/onnx/test_utility_funs.py", "torch/onnx/__init__.py", "torch/onnx/utils.py"], "labels": ["open source"]}, "174cbff00a": {"title": "Improve sugared value's error message (#42889)", "body": "Summary:\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* **https://github.com/pytorch/pytorch/issues/42889 Improve sugared value's error message**\n\nI think most (if not all) cases where this code path is reached can be attributed to closing over a global variable.\nImproving error message to make this clearer to users.\n\nclose https://github.com/pytorch/pytorch/issues/41288\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42889\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23779347\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: ced702a96234040f79eb16ad998d202e360d6654", "pr_number": "42889", "files_changed": ["test/test_jit.py", "torch/csrc/jit/python/python_sugared_value.h"], "labels": ["oncall: jit"]}, "09f2c6a94c": {"title": "Back out \"Revert D23494065: Refactor CallbackManager as a friend class of RecordFunction.\" (#44699)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44699\n\nOriginal commit changeset: 3b1ec928e3db\n\nPrevious revert (D23698861) was on the wrong diff stack. Backing out the revert.\n\nTest Plan: Passed unit tests and previously landed.\n\nReviewed By: mruberry\n\nDifferential Revision: D23702258\n\nfbshipit-source-id: 5c3e197bca412f454db5a7e86251ec85faf621c1", "pr_number": "44699", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h"], "labels": ["fb-exported"]}, "f175830558": {"title": "[NNC] Fuse identical conditions in simplifier (#44886)", "body": "Summary:\nAdds a pass to the IR Simplifier which fuses together the bodies of Cond statements which have identical conditions. e.g.\n\n```\nif (i < 10) {\n  do_thing_1;\n} else {\n  do_thing_2;\n}\nif (i < 10) {\n  do_thing_3;\n}\n```\n\nis transformed into:\n\n```\nif (i < 10) {\n  do_thing_1;\n  do_thing_3;\n} else {\n  do_thing_2;\n}\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44886\n\nReviewed By: glaringlee\n\nDifferential Revision: D23768565\n\nPulled By: nickgg\n\nfbshipit-source-id: 3fe40d91e82bdfff8dcb8c56a02a4fd579c070df", "pr_number": "44886", "files_changed": ["test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.h"], "labels": ["oncall: jit"]}, "7bd8a6913d": {"title": "CUDA BFloat div, addcdiv, addcmul, mean, var (#44758)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44758\n\nReviewed By: mruberry\n\nDifferential Revision: D23752317\n\nPulled By: ngimel\n\nfbshipit-source-id: 77992cf991f4e2b4b6839de73ea7e6ce2e1061c6", "pr_number": "44758", "files_changed": ["aten/src/ATen/native/cuda/PointwiseOpsKernel.cu", "aten/src/ATen/native/cuda/ReduceMomentKernel.cu", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "c2cf6efd96": {"title": "Enable type check for torch.testing._internal.dist_utils.* (#44832)", "body": "Summary:\nAddresses a sub-task of https://github.com/pytorch/pytorch/issues/44752.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44832\n\nReviewed By: malfet\n\nDifferential Revision: D23744260\n\nPulled By: samestep\n\nfbshipit-source-id: 46aede57b4fa66a770d5df382b0aea2bd6772b9b", "pr_number": "44832", "files_changed": ["mypy.ini", "torch/testing/_internal/dist_utils.py"], "labels": ["module: typing"]}, "6d312132e1": {"title": "Beef up vmap docs and expose to master documentation (#44825)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44825\n\nTest Plan: - build and view docs locally.\n\nReviewed By: ezyang\n\nDifferential Revision: D23742727\n\nPulled By: zou3519\n\nfbshipit-source-id: f62b7a76b5505d3387b7816c514c086c01089de0", "pr_number": "44825", "files_changed": ["docs/source/torch.rst", "torch/_vmap_internals.py"], "labels": []}, "a47e3697ab": {"title": "Use iterator of DispatchKeySet. (#44682)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44682\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23698387\n\nPulled By: ailzhang\n\nfbshipit-source-id: 4fa140db9254c2c9c342bf1c8dfd952469b0b779", "pr_number": "44682", "files_changed": ["aten/src/ATen/core/LegacyTypeDispatch.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/library.cpp", "aten/src/ATen/templates/TensorBody.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/TensorImpl.h", "c10/core/impl/LocalDispatchKeySet.cpp", "c10/test/core/DispatchKeySet_test.cpp"], "labels": []}, "d22dd80128": {"title": "Enable type check for torch.testing._internal.common_device_type. (#44911)", "body": "Summary:\nThis PR intends to fix the type exceptions in common_device_type.py.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44911\n\nReviewed By: walterddr\n\nDifferential Revision: D23768397\n\nPulled By: wuyangzhang\n\nfbshipit-source-id: 053692583b4d6169b0eb5ffe0c3d30635c0db699", "pr_number": "44911", "files_changed": ["mypy.ini", "torch/testing/_internal/common_device_type.py"], "labels": []}, "af3fc9725d": {"title": "Extract rpc/tensorpipe_utils.{cpp,h} from rpc/utils.{cpp,h} (#44803)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44803\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D23732022\n\nfbshipit-source-id: 5b839c7997bbee162a14d03414ee32baabbc8ece", "pr_number": "44803", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/rpc/test_tensorpipe_serialization.cpp", "tools/build_variables.bzl", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/csrc/jit/serialization/pickler.h", "torch/csrc/jit/serialization/unpickler.h"], "labels": ["ci/all", "ci/binaries", "fb-exported"]}, "374e9373b5": {"title": "[jit] Pull (most) tests out of libtorch_python (#44795)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44795\n\nToday, we build our cpp tests twice, once as a standalone gtest binary,\nand once linked in `libtorch_python` so we can call them from\n`test_jit.py`.\n\nThis is convenient (it means that `test_jit.py` is a single entry point\nfor all our tests), but has a few drawbacks:\n1. We can't actually use the gtest APIs, since we don't link gtest into\n`libtorch_python`. We're stuck with the subset that we want to write\npolyfills for, and an awkward registration scheme where you have to\nwrite a test then include it in `tests.h`).\n2. More seriously, we register custom operators and classes in these\ntests. In a world where we may be linking many `libtorch_python`s, this\nhas a tendency to cause errors with `libtorch`.\n\nSo now, only tests that explicitly require cooperation with Python are\nbuilt into `libtorch_python`. The rest are built into\n`build/bin/test_jit`.\n\nThere are tests which require that we define custom classes and\noperators. In these cases, I've built thm into separate `.so`s that we\ncall `torch.ops.load_library()` on.\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity, ZolotukhinM\n\nDifferential Revision: D23735520\n\nPulled By: suo\n\nfbshipit-source-id: d146bf4e7eb908afa6f96b394e4d395d63ad72ff", "pr_number": "44795", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/torch_python_test.cpp", "test/cpp/tensorexpr/CMakeLists.txt", "test/jit/test_backends.py", "test/jit/test_torchbind.py", "test/test_autograd.py", "test/test_fx.py", "test/test_jit.py", "torch/CMakeLists.txt", "torch/csrc/jit/python/init.cpp"], "labels": ["oncall: jit"]}, "2d884f2263": {"title": "Optimize Scale function (#44913)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44913\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18322\n\nOptimize Scale function\n\ni-am-not-moving-c2-to-c10\n\nTest Plan: buck test mode/dbg caffe2/caffe2/python/operator_test:weighted_sum_test\n\nReviewed By: BIT-silence\n\nDifferential Revision: D14575780\n\nfbshipit-source-id: db333a7964581dcaff6e432ff1d6b517ba1a075f", "pr_number": "44913", "files_changed": ["caffe2/utils/math/elementwise.cc", "caffe2/utils/math/elementwise.cu", "caffe2/utils/math/elementwise.h"], "labels": ["fb-exported"]}, "fd4e21c91e": {"title": "Add optional string support to native_functions schema (#43010)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43010\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23751851\n\nPulled By: mruberry\n\nfbshipit-source-id: 648f7430e1b7311eff28421f38e01f52d998fcbd", "pr_number": "43010", "files_changed": ["tools/autograd/gen_python_functions.py", "tools/jit/gen_unboxing_wrappers.py", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tracer.h", "torch/csrc/utils/python_arg_parser.h"], "labels": ["oncall: jit", "open source", "triaged"]}, "76a109c930": {"title": "[caffe2/aten] Fix clang build (#44934)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44934\n\nFix build errors when using clang to build cuda sources:\n\n```\nIn file included from aten/src/ATen/native/cuda/DistributionBernoulli.cu:4:\nIn file included from aten/src/ATen/cuda/CUDAApplyUtils.cuh:5:\ncaffe2/aten/src/THC/THCAtomics.cuh:321:1: error: control reaches end of non-void function [-Werror,-Wreturn-type]\n}\n^\n1 error generated when compiling for sm_70.\n\nIn file included from aten/src/ATen/native/cuda/DistributionBernoulli.cu:4:\nIn file included from aten/src/ATen/cuda/CUDAApplyUtils.cuh:5:\ncaffe2/aten/src/THC/THCAtomics.cuh:321:1: error: control reaches end of non-void function [-Werror,-Wreturn-type]\n}\n^\n1 error generated when compiling for sm_60.\n\nIn file included from aten/src/ATen/native/cuda/DistributionBernoulli.cu:4:\nIn file included from aten/src/ATen/cuda/CUDAApplyUtils.cuh:5:\ncaffe2/aten/src/THC/THCAtomics.cuh:321:1: error: control reaches end of non-void function [-Werror,-Wreturn-type]\n}\n^\n1 error generated when compiling for sm_52.\n```\n\nTest Plan: CI\n\nReviewed By: ngimel\n\nDifferential Revision: D23775266\n\nfbshipit-source-id: 141e6624e2da870a8c50ff9f71fcf0717222fb17", "pr_number": "44934", "files_changed": ["aten/src/THC/THCAtomics.cuh"], "labels": ["fb-exported"]}, "06389406bb": {"title": "CUDA BFloat activations 1 (#44834)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44834\n\nReviewed By: mruberry\n\nDifferential Revision: D23752660\n\nPulled By: ngimel\n\nfbshipit-source-id: 209a937e8a9afe12b7dd86ecfa493c9417fd22fb", "pr_number": "44834", "files_changed": ["aten/src/ATen/native/cuda/Activation.cu", "test/test_nn.py"], "labels": ["module: bfloat16", "module: cuda", "open source", "triaged"]}, "e255a4e1fd": {"title": "Enable bfloat16 random kernels on Windows (#44918)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33793\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44918\n\nReviewed By: pbelevich\n\nDifferential Revision: D23777548\n\nPulled By: ngimel\n\nfbshipit-source-id: 9cf13166d7deba17bc72e402b82ed0afe347cb9b", "pr_number": "44918", "files_changed": ["aten/src/ATen/native/cuda/DistributionTemplates.h", "test/test_tensor_creation_ops.py", "test/test_torch.py"], "labels": ["open source"]}, "40c09cfe14": {"title": "[CircleCI] Fix CUDA test setup (#44982)", "body": "Summary:\nCircle updated windows-nvidia-2019:canary image to exclude VC++ 14.26\nUpdate the config to use 14.27\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44982\n\nReviewed By: seemethere\n\nDifferential Revision: D23794116\n\nPulled By: malfet\n\nfbshipit-source-id: f3281f7d51acae4a4d06cecff01100fa77bd81ff", "pr_number": "44982", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml"], "labels": []}, "043466f978": {"title": "[FX] Pass module's qualname to is_leaf_module (#44966)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44966\n\nTest Plan: Imported from OSS\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23790360\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 7ef569fd93646584b27af7a615fa69c8d8bbdd3b", "pr_number": "44966", "files_changed": ["test/test_fx.py", "torch/fx/graph_module.py", "torch/fx/symbolic_trace.py"], "labels": ["fx"]}, "572f7e069c": {"title": "Enable type check for torch.testing._internal.te_utils.* (#44927)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44927\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D23776842\n\nPulled By: sshawnwu\n\nfbshipit-source-id: 65c028169a37e1f2f7d9fdce8a958234ee1caa26", "pr_number": "44927", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in", "torch/csrc/jit/python/init.cpp"], "labels": ["oncall: jit"]}, "e9941a5dd4": {"title": "[vulkan][py] torch.utils.optimize_for_vulkan (#44903)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44903\n\nTest Plan: Imported from OSS\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23766039\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: dbdf484ee7d3a7719aab105efba51b92ebc51568", "pr_number": "44903", "files_changed": ["torch/_C/__init__.pyi.in", "torch/csrc/jit/passes/vulkan_rewrite.cpp", "torch/csrc/jit/passes/vulkan_rewrite.h", "torch/csrc/jit/python/init.cpp", "torch/utils/mobile_optimizer.py"], "labels": []}, "1c15452703": {"title": "Update Windows builders to latest VS2019 (#44746)", "body": "Summary:\nRestore https://github.com/pytorch/pytorch/issues/44706, which should workaround VC compiler crash, which was reverted by https://github.com/pytorch/pytorch/issues/41977\nUpdate configs to use \":stable\" Windows images\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44746\n\nReviewed By: walterddr\n\nDifferential Revision: D23793682\n\nPulled By: malfet\n\nfbshipit-source-id: bfdc36c35b920f58798a18c15642ec7efc68f00e", "pr_number": "44746", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/header-section.yml", "third_party/ideep"], "labels": []}, "21a1b9c7cf": {"title": "skip more nccl tests that causes flaky timeouts on rocm build (#44996)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44996\n\nReviewed By: malfet\n\nDifferential Revision: D23797564\n\nPulled By: walterddr\n\nfbshipit-source-id: 4d60f76bb8ae54bb04a9f4143a68623933461b2a", "pr_number": "44996", "files_changed": ["test/distributed/test_c10d.py"], "labels": []}, "620c999979": {"title": "update gloo submodule (#45008)", "body": "Summary:\nRevert accidental gloo submodule changes in https://github.com/pytorch/pytorch/issues/41977\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45008\n\nReviewed By: malfet\n\nDifferential Revision: D23799892\n\nPulled By: ngimel\n\nfbshipit-source-id: e8dab244c6abad32ed60efe3c26cab40837e57c8", "pr_number": "45008", "files_changed": ["third_party/gloo"], "labels": []}, "d75c402755": {"title": "Add cusolver to build, rewrite MAGMA inverse with cusolver (#42403)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42265\n\nThis PR adds cusolver to the pytorch build, and enables the use of cusolver/cublas library functions on GPU `torch.inverse` on certain tensor shapes.\n\nSpecifically, when\n\n* the tensor is two dimensional (single batch), or\n* has >2 dimensions (multiple batches) and `batch_size <= 2`, or\n* magma is not linked,\n\ncusolver/cublas will be used. In other conditions, the current implementation of MAGMA will still be used.\n\nhttps://github.com/pytorch/pytorch/blob/8c0949ae454b1d2c1b626a5ea19ba5ea6487d305/aten/src/ATen/native/cuda/BatchLinearAlgebra.cu#L742-L752\n\nThe reason for this is that for tensors with large batch_size, `cublasXgetrfBatched` and `cublasXgetriBatched` doesn't perform very well. For `batch_size > 1`, we launch cusolver functions in multiple streams. This lets cusolver functions run in parallel, and can greatly increase the performance. When `batch_size > 2`, the parallel launched cusolver functions are slightly slower than the current magma implementation, so we still use the current magma impl.\n\nOn CUDA 9.2, there were some numerical issues detected, so cusolver impl will not be used. The cusolver impl will also not be used on platforms other than Nvidia CUDA.\n\nhttps://github.com/pytorch/pytorch/blob/060769feaf02db56ac79e0c73dab1105828ece69/aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h#L10-L13\n\nNote that there is a new heuristic used before cusolver/cublas calls here:\n\nhttps://github.com/pytorch/pytorch/blob/8c0949ae454b1d2c1b626a5ea19ba5ea6487d305/aten/src/ATen/native/cuda/MiscUtils.h#L113-L121\n\nwhere `use_loop_launch = true` means launch single batch cusolver functions in parallel, and `use_loop_launch = false` means use cublas_X_batched functions. When magma is enabled (only `batch_size <= 2` will be dispatched to cusolver/cublas), the heuristic will always return `true` and the cusolver calls are faster than small batch_size magma calls. When magma is disabled, this adds the functionality of `torch.inverse`, which was disabled before for all shapes (though large batch_size cublas performance may not be as well as magma).\n\nChecklist:\n- [X] Add benchmark, cpu, gpu-before (magma), gpu-after (cusolver)\n- [X] Rewrite single inverse (ndim == 2) with cusolver\n- [X] Rewrite batched inverse (ndim > 2) with cublas\n- [X] Add cusolver to build\n- [x] Clean up functions related to `USE_MAGMA` define guard\n- [x] Workaround for non-cuda platform\n- [x] Workaround for cuda 9.2\n- [x] Add zero size check\n- [x] Add tests\n\nNext step:\n\nIf cusolver doesn't cause any problem in pytorch build, and there are no major performance regressions reported after this PR being merged, I will start porting other cusolver/cublas functions for linear algebra to improve the performance.\n\n<details>\n<summary> benchmark 73499c6 </summary>\n\nbenchmark code: https://github.com/xwang233/code-snippet/blob/master/torch.inverse/inverse-cusolver.ipynb\n\nshape meaning:\n\n* `[] 2 torch.float32 -> torch.randn(2, 2, dtype=torch.float32)`\n* `[2] 4 torch.float32 -> torch.randn(2, 4, 4, dtype=torch.float32)`\n\n| shape | cpu_time (ms) | gpu_time_before (magma) (ms) | gpu_time_after (ms) |\n| --- | --- | --- | --- |\n| [] 2 torch.float32 |  0.095 |  7.534 |  0.129  |\n| [] 4 torch.float32 |  0.009 |  7.522 |  0.129  |\n| [] 8 torch.float32 |  0.011 |  7.647 |  0.138  |\n| [] 16 torch.float32 |  0.075 |  7.582 |  0.135  |\n| [] 32 torch.float32 |  0.073 |  7.573 |  0.191  |\n| [] 64 torch.float32 |  0.134 |  7.694 |  0.288  |\n| [] 128 torch.float32 |  0.398 |  8.073 |  0.491  |\n| [] 256 torch.float32 |  1.054 |  11.860 |  1.074  |\n| [] 512 torch.float32 |  5.218 |  14.130 |  2.582  |\n| [] 1024 torch.float32 |  19.010 |  18.780 |  6.936  |\n| [1] 2 torch.float32 |  0.009 |  0.113 |  0.128 ***regressed |\n| [1] 4 torch.float32 |  0.009 |  0.113 |  0.131 ***regressed |\n| [1] 8 torch.float32 |  0.011 |  0.116 |  0.129 ***regressed |\n| [1] 16 torch.float32 |  0.015 |  0.122 |  0.135 ***regressed |\n| [1] 32 torch.float32 |  0.032 |  0.177 |  0.178 ***regressed |\n| [1] 64 torch.float32 |  0.070 |  0.420 |  0.281  |\n| [1] 128 torch.float32 |  0.328 |  0.816 |  0.490  |\n| [1] 256 torch.float32 |  1.125 |  1.690 |  1.084  |\n| [1] 512 torch.float32 |  4.344 |  4.305 |  2.576  |\n| [1] 1024 torch.float32 |  16.510 |  16.340 |  6.928  |\n| [2] 2 torch.float32 |  0.009 |  0.113 |  0.186 ***regressed |\n| [2] 4 torch.float32 |  0.011 |  0.115 |  0.184 ***regressed |\n| [2] 8 torch.float32 |  0.012 |  0.114 |  0.184 ***regressed |\n| [2] 16 torch.float32 |  0.019 |  0.119 |  0.173 ***regressed |\n| [2] 32 torch.float32 |  0.050 |  0.170 |  0.240 ***regressed |\n| [2] 64 torch.float32 |  0.120 |  0.429 |  0.375  |\n| [2] 128 torch.float32 |  0.576 |  0.830 |  0.675  |\n| [2] 256 torch.float32 |  2.021 |  1.748 |  1.451  |\n| [2] 512 torch.float32 |  9.070 |  4.749 |  3.539  |\n| [2] 1024 torch.float32 |  33.655 |  18.240 |  12.220  |\n| [4] 2 torch.float32 |  0.009 |  0.112 |  0.318 ***regressed |\n| [4] 4 torch.float32 |  0.010 |  0.115 |  0.319 ***regressed |\n| [4] 8 torch.float32 |  0.013 |  0.115 |  0.320 ***regressed |\n| [4] 16 torch.float32 |  0.027 |  0.120 |  0.331 ***regressed |\n| [4] 32 torch.float32 |  0.085 |  0.173 |  0.385 ***regressed |\n| [4] 64 torch.float32 |  0.221 |  0.431 |  0.646 ***regressed |\n| [4] 128 torch.float32 |  1.102 |  0.834 |  1.055 ***regressed |\n| [4] 256 torch.float32 |  4.042 |  1.811 |  2.054 ***regressed |\n| [4] 512 torch.float32 |  18.390 |  4.884 |  5.087 ***regressed |\n| [4] 1024 torch.float32 |  69.025 |  19.840 |  20.000 ***regressed |\n\n</details>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42403\n\nReviewed By: ailzhang, mruberry\n\nDifferential Revision: D23717984\n\nPulled By: ngimel\n\nfbshipit-source-id: 54cbd9ea72a97989cff4127089938e8a8e29a72b", "pr_number": "42403", "files_changed": ["BUILD.bazel", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/cuda/CUDAContext.h", "aten/src/ATen/cuda/CUDASolver.cpp", "aten/src/ATen/cuda/CUDASolver.h", "aten/src/ATen/cuda/CusolverDnHandlePool.cpp", "aten/src/ATen/cuda/Exceptions.h", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/LinearAlgebraUtils.h", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h", "aten/src/ATen/native/cuda/MiscUtils.h", "test/test_cuda.py", "test/test_sparse.py", "test/test_torch.py", "torch/testing/_internal/common_cuda.py"], "labels": ["module: build", "open source", "topic: linear algebra", "triaged"]}, "9e5045e978": {"title": "[pytorch] clean up normalized_dynamic_type() hack (#44889)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44889\n\nThis HACK doesn't seem to be necessary any more - there is no 'real'\ntype in generated Declarations.yaml file.\nVerified by comparing generated code before/after.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23761624\n\nPulled By: ljk53\n\nfbshipit-source-id: de996f04d77eebea3fb9297dd90a8ebeb07647bb", "pr_number": "44889", "files_changed": ["tools/autograd/gen_python_functions.py"], "labels": []}, "0714c003ee": {"title": "[pytorch][tensorexpr] Make gtest-style macros in tests match actual gtest signatures (#44861)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44861\n\nWe were redefining things like ASSERT_EQ to take a _VA_ARGS_ parameter, so compiling these files with gtest (instead of pytorch's custom python-based cpp test infra) fails.\n\nTest Plan: buck build //caffe2/test/cpp/tensorexpr\n\nReviewed By: asuhan\n\nDifferential Revision: D23711293\n\nfbshipit-source-id: 8af14fa7c1f1e8169d14bb64515771f7bc3089e5", "pr_number": "44861", "files_changed": ["test/cpp/tensorexpr/padded_buffer.h", "test/cpp/tensorexpr/test_aten.cpp", "test/cpp/tensorexpr/test_base.h", "test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/test_expr.cpp"], "labels": ["fb-exported"]}, "2163d31016": {"title": "histogram observer: ensure buffer shape consistency (#44956)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44956\n\nMakes buffer shapes for HistogramObserver have the\nsame shapes in uninitialized versus initialized states.\n\nThis is useful because the detectron2 checkpointer assumes\nthat these states will stay the same, so it removes the\nneed for manual hacks around the shapes changing.\n\nTest Plan:\n```\npython test/test_quantization.py TestObserver.test_histogram_observer_consistent_buffer_shape\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23785382\n\nfbshipit-source-id: 1a83fd4f39b244b00747c368d5d305a07d877c92", "pr_number": "44956", "files_changed": ["test/quantization/test_workflow_module.py", "torch/quantization/observer.py"], "labels": []}, "7ecfaef7ec": {"title": "CUDA BFloat16 layernorm (#45002)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45002\n\nReviewed By: mruberry\n\nDifferential Revision: D23800931\n\nPulled By: ngimel\n\nfbshipit-source-id: cc213d02352907a3e945cd9fffd1de29e355a16c", "pr_number": "45002", "files_changed": ["aten/src/ATen/native/cuda/layer_norm_kernel.cu", "test/test_nn.py"], "labels": ["open source"]}, "faef89c89f": {"title": "CUDA BFloat Pooling (#44836)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44836\n\nReviewed By: mruberry\n\nDifferential Revision: D23800992\n\nPulled By: ngimel\n\nfbshipit-source-id: 2945a27874345197cbd1d8a4fbd20816afc02c86", "pr_number": "44836", "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/AveragePool2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "test/test_nn.py"], "labels": ["module: bfloat16", "module: cuda", "open source", "triaged"]}, "60709ad1bf": {"title": "Adds multiply and divide aliases (#44463)", "body": "Summary:\nThese alias are consistent with NumPy. Note that C++'s naming would be different (std::multiplies and std::divides), and that PyTorch's existing names (mul and div) are consistent with Python's dunders.\n\nThis also improves the instructions for adding an alias to clarify that dispatch keys should be removed when copying native_function.yaml entries to create the alias entries.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44463\n\nReviewed By: ngimel\n\nDifferential Revision: D23670782\n\nPulled By: mruberry\n\nfbshipit-source-id: 9f1bdf8ff447abc624ff9e9be7ac600f98340ac4", "pr_number": "44463", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_op_aliases.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py"], "labels": ["oncall: jit"]}, "49db7b59e0": {"title": "For logical tests, use the dtypes decorator (#42483)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42483\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23684424\n\nPulled By: mruberry\n\nfbshipit-source-id: ba7ab5c3a6eaa0c16975728200f27d164ed4f852", "pr_number": "42483", "files_changed": ["test/test_torch.py"], "labels": ["merge-this-please", "open source", "triaged"]}, "da7863f46b": {"title": "Add one dimensional FFTs to torch.fft namespace (#43011)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43011\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23751850\n\nPulled By: mruberry\n\nfbshipit-source-id: 8dc5fec75102d8809eeb85a3d347ba1b5de45b33", "pr_number": "43011", "files_changed": ["aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/SpectralOpsUtils.h", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/native/mkl/SpectralOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/fft.rst", "test/cpp/api/fft.cpp", "test/test_autograd.py", "test/test_spectral_ops.py", "tools/autograd/derivatives.yaml", "torch/csrc/api/include/torch/fft.h", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/fft/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "triaged"]}, "9f67176b82": {"title": "Complex gradcheck logic (#43208)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43208\n\nThis PR adds gradcheck for complex. The logic used for complex gradcheck is described in Section 3.5.3 here: https://arxiv.org/pdf/1701.00392.pdf\n\nMore concretely, this PR introduces the following changes:\n1. Updates get_numerical_jacobian to take as input a scalar value for vector (v). Adds gradcheck logic for C -> C, C-> R, R -> C. For R -> C functions, only the real value of gradient is propagated.\n2. Adds backward definition for `torch.complex` and also adds a test to verify the definition added.\n3. Updates backward for `mul`, `sin`, `cos`, `sinh`, `cosh`.\n4. Adds tests for all `torch.real`, `torch.imag`, `torch.view_as_real`, `torch.view_as_complex`, `torch.conj`.\n\nFollow up tasks:\n1. Add more thorough tests for R -> C cases. Specifically, add R->C test variants for functions. for e.g., `torch.mul(complex_tensor, real_tensor)`\n2. Add back commented test in `common_methods_invocation.py`.\n3. Add more special case checking for complex gradcheck to make debugging easier.\n4. Update complex autograd note.\n5. disable complex autograd for operators not tested for complex.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23655088\n\nPulled By: anjali411\n\nfbshipit-source-id: caa75e09864b5f6ead0f988f6368dce64cf15deb", "pr_number": "43208", "files_changed": ["test/test_autograd.py", "test/test_jit.py", "test/test_ops.py", "test/test_overrides.py", "tools/autograd/derivatives.yaml", "torch/autograd/gradcheck.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: complex"]}, "4810365576": {"title": "Enabled torch.testing._internal.jit_utils.* typechecking. (#44985)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44985\n\nReviewed By: malfet\n\nDifferential Revision: D23794444\n\nPulled By: kauterry\n\nfbshipit-source-id: 9893cc91780338a8223904fb574efa77fa3ab2b9", "pr_number": "44985", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in", "torch/testing/_internal/jit_utils.py"], "labels": ["module: typing", "triaged"]}, "a6895d43b6": {"title": "Turn on gradgrad check for BCELoss Criterion Tests. (#44894)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44894\n\nLooks like we added double backwards support but only turned on the ModuleTests.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23762544\n\nPulled By: gchanan\n\nfbshipit-source-id: b5cef579608dd71f3de245c4ba92e49216ce8a5e", "pr_number": "44894", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "e2f49c8437": {"title": "skip im2col & vol2col in cpu/cuda convolution methods (#44600)", "body": "Summary:\nthis fixes https://github.com/pytorch/pytorch/issues/44482.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44600\n\nReviewed By: ngimel\n\nDifferential Revision: D23733483\n\nPulled By: walterddr\n\nfbshipit-source-id: 90e188027ef6bb08588619b6629110b5f73d63e3", "pr_number": "44600", "files_changed": ["aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/THCUNN/generic/SpatialConvolutionMM.cu"], "labels": []}, "4bbb6adff5": {"title": "[NNC] fix SyncThreads insertion and reenable CudaSharedMem test (#44909)", "body": "Summary:\nA previous fix for masking Cuda dimensions (https://github.com/pytorch/pytorch/issues/44733) changed the behaviour of inserting thread synchronization barriers in the Cuda CodeGen, causing the CudaSharedMemReduce_1 to be flaky and ultimately disabled.\n\nThe issue is working out where these barriers must be inserted - solving this optimally is very hard, and I think not possible without dependency analysis we don't have, so I've changed our logic to be quite pessimistic. We'll insert barriers before and after any blocks that have thread dimensions masked (even between blocks that have no data dependencies). This should be correct, but it's an area we could improve performance. To address this somewhat I've added a simplifier pass that removes obviously unnecessary syncThreads.\n\nTo avoid this test being flaky again, I've added a check against the generated code to ensure there is a syncThread in the right place.\n\nAlso fixed a couple of non-functional but clarity issues in the generated code: fixed the missing newline after Stores in the CudaPrinter, and prevented the PrioritizeLoad mutator from pulling out loads contained within simple Let statements (such as those produced by the Registerizer).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44909\n\nReviewed By: agolynski\n\nDifferential Revision: D23800565\n\nPulled By: nickgg\n\nfbshipit-source-id: bddef1f40d8d461da965685f01d00b468d8a2c2f", "pr_number": "44909", "files_changed": ["test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.h", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["oncall: jit"]}, "ac8c7c4e9f": {"title": "Make Channel API accept buffer structs rather than raw pointers. (#45014)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45014\n\nPull Request resolved: https://github.com/pytorch/tensorpipe/pull/219\n\nPull Request resolved: https://github.com/pytorch/tensorpipe/pull/212\n\n+ Introduce buffer.h defining the buffer struct(s). The `CpuBuffer`\nstruct is always defined, while the `CudaBuffer` struct is defined\nonly when `TENSORPIPE_SUPPORTS_CUDA` is true.\n+ Update all channels to take a `CpuBuffer` or `CudaBuffer` for\n`send`/`recv` rather than a raw pointer and a length.\n+ Make the base `Channel`/`Context` classes templated on `TBuffer`,\neffectively creating two channel hierarchies (one for CPU channels,\none for CUDA channels).\n+ Update the Pipe and the generic channel tests to use the new API. So\nfar, generic channel tests are CPU only, and tests for the CUDA IPC\nchannel are (temporarily) disabled. A subsequent PR will take care of\nrefactoring tests so that generic tests work for CUDA channels. An\nother PR will add support for CUDA tensors in the Pipe.\n\nDifferential Revision: D23598033\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nPulled By: beauby\n\nfbshipit-source-id: 1d6c3f91e288420858835cd5e7962e8da051b44b", "pr_number": "45014", "files_changed": ["BUILD.bazel", "test/cpp/rpc/test_tensorpipe_serialization.cpp", "third_party/tensorpipe", "third_party/tensorpipe.BUILD", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp"], "labels": ["fb-exported", "module: tensorpipe"]}, "a4aba1d465": {"title": "fix compile error (#45052)", "body": "Summary:\nUpdate vulkanOptimizeForMobile function invoking in optimize_for_mobile.cc to align latest call contract in PR https://github.com/pytorch/pytorch/pull/44903.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45052\n\nReviewed By: malfet\n\nDifferential Revision: D23814953\n\nPulled By: mrshenli\n\nfbshipit-source-id: 0fa844a8291e952715b9de35cdec0e411c42b7f9", "pr_number": "45052", "files_changed": ["binaries/optimize_for_mobile.cc"], "labels": ["open source"]}, "acc2a1e5fa": {"title": "Update submodule gloo (#45025)", "body": "Summary:\nIncluding commits to fix Windows CI failure of enable distributed training on Windows PR\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45025\n\nReviewed By: beauby\n\nDifferential Revision: D23807995\n\nPulled By: mrshenli\n\nfbshipit-source-id: a2f4c1684927ca66d7d3e9920ecb588fb4386f7c", "pr_number": "45025", "files_changed": ["third_party/gloo", "third_party/gloo.BUILD"], "labels": ["open source"]}, "92f8f75c59": {"title": "Add alias dispatch key Math. (#44354)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44354\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23591481\n\nPulled By: ailzhang\n\nfbshipit-source-id: 6e93c4ec99a07f3fc920ba2d09dc222e6ced5adf", "pr_number": "44354", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.cpp", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "test/test_dispatch.py", "torch/csrc/utils/python_dispatch.cpp"], "labels": []}, "42af2c7923": {"title": "[jit] gtest-ify test_alias_analysis.cpp (#45018)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45018\n\nNow that https://github.com/pytorch/pytorch/pull/44795 has landed, we\ncan convert the bulk of our cpp tests to use gtest APIs. Eventually\nwe'll want to get rid of our weird harness for cpp tests entirely in\nfavor of using regular gtest everywhere. This PR demonstrates some of\nthe benefits of this approach:\n1. You don't need to register your test twice (once to define it, once\nin tests.h).\n2. Consequently, it's easier to have many individual test cases.\nFailures can be reported independently (rather than having huge\nfunctions to test entire modules.\n3. Some nicer testing APIs, notably test fixtures.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23802297\n\nPulled By: suo\n\nfbshipit-source-id: 774255da7716294ac573747dcd5e106e5fe3ac8f", "pr_number": "45018", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_interpreter.cpp", "test/cpp/jit/test_memory_dag.cpp", "test/cpp/jit/tests.h"], "labels": ["oncall: jit"]}, "7de512ced8": {"title": "nightly robustness fixes for linking across devices (#43771)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43761\n\nCC rgommers ezyang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43771\n\nReviewed By: glaringlee\n\nDifferential Revision: D23819835\n\nPulled By: malfet\n\nfbshipit-source-id: a3be2780c4b8bdbf347d456c4d14df863c2ff8c2", "pr_number": "43771", "files_changed": ["tools/nightly.py"], "labels": ["open source", "triaged"]}, "1a580c1021": {"title": "Adding test to quantized copy for 'from float' (#43681)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43681\n\nTest Plan: Imported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23364507\n\nPulled By: z-a-f\n\nfbshipit-source-id: ef1b00937b012b0647d9b9afa054437f2bce032a", "pr_number": "43681", "files_changed": ["test/quantization/test_quantized_tensor.py"], "labels": []}, "7118d53711": {"title": "add .cache to gitignore (#45017)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45017\n\nthis is the default indexing folder for clangd 11.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23817619\n\nPulled By: suo\n\nfbshipit-source-id: 6a60136e591b2fec3d432ac5343cb76ac0934502", "pr_number": "45017", "files_changed": [".gitignore"], "labels": []}, "9dc2bcdc07": {"title": "Introducing (Const)StridedRandomAccessor + CompositeRandomAccessor + migrate `sort` to ATen (CPU) (#39744)", "body": "Summary:\nThis PR introduces a (Const)StridedRandomAccessor, a [random access iterator](https://en.cppreference.com/w/cpp/named_req/RandomAccessIterator) over a strided array, and a CompositeRandomAccessor, a random access iterator over two random access iterators.\n\nThe main motivation is to be able to use a handful of operations from STL and thrust in numerous dim-apply types of algorithms and eliminate unnecessary buffer allocations. Plus more advanced algorithms are going to be available with C++17.\n\nPorting `sort` provides a hands-on example of how these iterators could be used.\n\nFixes [https://github.com/pytorch/pytorch/issues/24770](https://github.com/pytorch/pytorch/issues/24770).\n\nSome benchmarks:\n```python\nfrom IPython import get_ipython\n\ntorch.manual_seed(13)\n\nipython = get_ipython()\n\nsizes = [\n        [10000, 10000],\n        [1000, 1000, 100]\n        ]\nfor size in sizes:\n    t = torch.randn(*size)\n    dims = len(size)\n\n    print(f\"Tensor of size {size}\")\n    for dim in range(dims):\n        print(f\"sort for dim={dim}\")\n        print(\"float:\")\n        ipython.magic(\"timeit t.sort(dim)\")\n    print()\n\n```\n#### Master\n```\nTensor of size [10000, 10000]\nsort for dim=0\nfloat:\n10.7 s \u00b1 201 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\nsort for dim=1\nfloat:\n6.27 s \u00b1 50.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nTensor of size [1000, 1000, 100]\nsort for dim=0\nfloat:\n7.21 s \u00b1 23.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\nsort for dim=1\nfloat:\n6.1 s \u00b1 21.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\nsort for dim=2\nfloat:\n3.58 s \u00b1 27 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n```\n#### This PR\n```\nTensor of size [10000, 10000]\nsort for dim=0\nfloat:\n10.5 s \u00b1 209 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\nsort for dim=1\nfloat:\n6.16 s \u00b1 28.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nTensor of size [1000, 1000, 100]\nsort for dim=0\nfloat:\n5.94 s \u00b1 60.6 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\nsort for dim=1\nfloat:\n5.1 s \u00b1 11.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\nsort for dim=2\nfloat:\n3.43 s \u00b1 8.52 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n```\nAs you can see, the legacy sorting routine is actually quite efficient. The performance gain is likely due to the improved reduction with TensorIterator.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39744\n\nReviewed By: malfet\n\nDifferential Revision: D23796486\n\nPulled By: glaringlee\n\nfbshipit-source-id: 7bddad10dfbc0a0e5cad7ced155d6c7964e8702c", "pr_number": "39744", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/native/CompositeRandomAccessor.h", "aten/src/ATen/native/CompositeRandomAccessorCommon.h", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/Sorting.h", "aten/src/ATen/native/StridedRandomAccessor.h", "aten/src/ATen/native/cpu/SortingKernel.cpp", "aten/src/ATen/native/cuda/CompositeRandomAccessor.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp"], "labels": ["open source", "triaged"]}, "c941dd3492": {"title": "[FX] s/get_param/get_attr/ (#45000)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45000\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D23798016\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 1d2f3db1994a62b95d0ced03bf958e54d30c35dd", "pr_number": "45000", "files_changed": ["test/fx/quantization.py", "test/test_fx.py", "torch/fx/__init__.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/symbolic_trace.py", "torch/quantization/fx/quantize.py", "torch/quantization/fx/utils.py"], "labels": ["fx"]}, "1cab27d485": {"title": "Add a torch.hub.load_local() function that can load models from any local directory with a hubconf.py (#44204)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43622\n\n- Moves the model loading part of `torch.hub.load()` into a new `torch.hub.load_local()` function that takes in a path to a local directory that contains a `hubconf.py` instead of a repo name.\n- Refactors `torch.hub.load()` so that it now calls `torch.hub.load_local()` after downloading and extracting the repo.\n- Updates `torch.hub` docs to include the new function + minor fixes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44204\n\nReviewed By: malfet\n\nDifferential Revision: D23817429\n\nPulled By: ailzhang\n\nfbshipit-source-id: 788fd83c87a94f487b558715b2809d346ead02b2", "pr_number": "44204", "files_changed": ["docs/source/hub.rst", "test/test_utils.py", "torch/hub.py"], "labels": ["open source", "triaged"]}, "581a364437": {"title": "CUDA BFloat16 unary ops part 1 (#44813)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44813\n\nReviewed By: mruberry\n\nDifferential Revision: D23805816\n\nPulled By: ngimel\n\nfbshipit-source-id: 28c645dc31f094c8b6c3d3803f0b4152f0475a64", "pr_number": "44813", "files_changed": ["aten/src/ATen/native/cuda/UnaryGeometricKernels.cu", "aten/src/ATen/native/cuda/UnaryLogKernels.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: bfloat16", "module: cuda", "open source", "triaged"]}, "dfb8f2d51f": {"title": "CUDA BFloat16 addmm, addmv (#44986)", "body": "Summary:\nThis PR was originally authored by slayton58. I steal his implementation and added some tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44986\n\nReviewed By: mruberry\n\nDifferential Revision: D23806039\n\nPulled By: ngimel\n\nfbshipit-source-id: 305d66029b426d8039fab3c3e011faf2bf87aead", "pr_number": "44986", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "test/test_torch.py"], "labels": ["open source"]}, "9a31eee107": {"title": "[vulkan] Remove duplication of op registration and clean unused vars (#44932)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44932\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D23778203\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: d1bc0a5c2cdd711d8a4cd983154a4f6774987674", "pr_number": "44932", "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.cpp", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanCommon.h"], "labels": []}, "81bb19c9f0": {"title": "[JIT] Prohibit subscripted assignments for tuple types (#44929)", "body": "Summary:\nThis would force jit.script to raise an error if someone tries to mutate tuple\n```\nTuple[int, int] does not support subscripted assignment:\n  File \"/home/nshulga/test/tupleassignment.py\", line 9\ntorch.jit.script\ndef foo(x: Tuple[int, int]) -> int:\n    x[-1] = x[0] + 1\n    ~~~~~ <--- HERE\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44929\n\nReviewed By: suo\n\nDifferential Revision: D23777668\n\nPulled By: malfet\n\nfbshipit-source-id: 8efaa4167354ffb4930ccb3e702736a3209151b6", "pr_number": "44929", "files_changed": ["aten/src/ATen/core/qualified_name.h", "test/test_jit_py3.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["oncall: jit", "triaged"]}, "20f52cdd76": {"title": "[hpc]optimize the torch.cat cuda kernel (#44833)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44833\n\nCurrent cat cuda kernel employs the pin memory to pass the tensor data. 1) It is much slower than passing through argument using constant memory 2) the H2D sometimes overlaps with other H2D in training, and thus generates some random delay and leads to desync issue.\n\nFor small N, we actually saw 2X improvements.\n\nTest Plan:\nbenchmark\n```\n./buck-out/opt/gen/caffe2/benchmarks/operator_benchmark/pt/cat_test.par --tag_filter all --device cuda\n```\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : all\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1,1,1)_N2_dim0_cuda\n# Input: sizes: (1, 1, 1), N: 2, dim: 0, device: cuda\nForward Execution Time (us) : 38.825\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 45.440\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(128,1024,2)_N2_dim1_cuda\n# Input: sizes: (128, 1024, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 38.765\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1024,1024,2)_N2_dim0_cuda\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 0, device: cuda\nForward Execution Time (us) : 60.075\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1025,1023,2)_N2_dim1_cuda\n# Input: sizes: (1025, 1023, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 65.203\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1024,1024,2)_N2_dim2_cuda\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 2, device: cuda\nForward Execution Time (us) : 83.941\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f0d50fc2440>,111,65]_N5_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f0d50fc2440>, 111, 65], N: 5, dim: 0, device: cuda\nForward Execution Time (us) : 51.059\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[96,<function<lambda>at0x7f0d50fc2b90>,64]_N5_dim1_cuda\n# Input: sizes: [96, <function <lambda> at 0x7f0d50fc2b90>, 64], N: 5, dim: 1, device: cuda\nForward Execution Time (us) : 42.134\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[128,64,<function<lambda>at0x7f0b22b7e3b0>]_N5_dim2_cuda\n# Input: sizes: [128, 64, <function <lambda> at 0x7f0b22b7e3b0>], N: 5, dim: 2, device: cuda\nForward Execution Time (us) : 78.333\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f0b22b7e5f0>,32,64]_N50_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f0b22b7e5f0>, 32, 64], N: 50, dim: 0, device: cuda\nForward Execution Time (us) : 77.065\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[32,<function<lambda>at0x7f0b22b7e680>,64]_N50_dim1_cuda\n# Input: sizes: [32, <function <lambda> at 0x7f0b22b7e680>, 64], N: 50, dim: 1, device: cuda\nForward Execution Time (us) : 74.632\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[33,65,<function<lambda>at0x7f0b22b7e710>]_N50_dim2_cuda\n# Input: sizes: [33, 65, <function <lambda> at 0x7f0b22b7e710>], N: 50, dim: 2, device: cuda\nForward Execution Time (us) : 81.846\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(64,32,4,16,32)_N2_dim2_cuda\n# Input: sizes: (64, 32, 4, 16, 32), N: 2, dim: 2, device: cuda\nForward Execution Time (us) : 99.291\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(16,32,4,16,32)_N8_dim2_cuda\n# Input: sizes: (16, 32, 4, 16, 32), N: 8, dim: 2, device: cuda\nForward Execution Time (us) : 114.060\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(9,31,5,15,33)_N17_dim4_cuda\n# Input: sizes: (9, 31, 5, 15, 33), N: 17, dim: 4, device: cuda\nForward Execution Time (us) : 478.777\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f0b22b7e7a0>]_N100_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f0b22b7e7a0>], N: 100, dim: 0, device: cuda\nForward Execution Time (us) : 80.165\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f0b22b7e830>]_N1000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f0b22b7e830>], N: 1000, dim: 0, device: cuda\nForward Execution Time (us) : 491.983\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f0b22b7e8c0>]_N2000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f0b22b7e8c0>], N: 2000, dim: 0, device: cuda\nForward Execution Time (us) : 966.613\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f0b22b7e950>]_N3000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f0b22b7e950>], N: 3000, dim: 0, device: cuda\nForward Execution Time (us) : 1500.133\n```\n\nAfter optimization\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : all\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1,1,1)_N2_dim0_cuda\n# Input: sizes: (1, 1, 1), N: 2, dim: 0, device: cuda\nForward Execution Time (us) : 22.168\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 33.430\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(128,1024,2)_N2_dim1_cuda\n# Input: sizes: (128, 1024, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 19.884\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1024,1024,2)_N2_dim0_cuda\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 0, device: cuda\nForward Execution Time (us) : 48.082\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1025,1023,2)_N2_dim1_cuda\n# Input: sizes: (1025, 1023, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 53.261\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1024,1024,2)_N2_dim2_cuda\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 2, device: cuda\nForward Execution Time (us) : 71.294\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f837a135200>,111,65]_N5_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f837a135200>, 111, 65], N: 5, dim: 0, device: cuda\nForward Execution Time (us) : 40.165\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[96,<function<lambda>at0x7f837a135950>,64]_N5_dim1_cuda\n# Input: sizes: [96, <function <lambda> at 0x7f837a135950>, 64], N: 5, dim: 1, device: cuda\nForward Execution Time (us) : 32.666\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[128,64,<function<lambda>at0x7f82e50e2440>]_N5_dim2_cuda\n# Input: sizes: [128, 64, <function <lambda> at 0x7f82e50e2440>], N: 5, dim: 2, device: cuda\nForward Execution Time (us) : 67.003\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f82e50e24d0>,32,64]_N50_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f82e50e24d0>, 32, 64], N: 50, dim: 0, device: cuda\nForward Execution Time (us) : 67.035\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[32,<function<lambda>at0x7f82e50e2560>,64]_N50_dim1_cuda\n# Input: sizes: [32, <function <lambda> at 0x7f82e50e2560>, 64], N: 50, dim: 1, device: cuda\nForward Execution Time (us) : 63.803\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[33,65,<function<lambda>at0x7f82e50e25f0>]_N50_dim2_cuda\n# Input: sizes: [33, 65, <function <lambda> at 0x7f82e50e25f0>], N: 50, dim: 2, device: cuda\nForward Execution Time (us) : 69.969\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(64,32,4,16,32)_N2_dim2_cuda\n# Input: sizes: (64, 32, 4, 16, 32), N: 2, dim: 2, device: cuda\nForward Execution Time (us) : 98.327\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(16,32,4,16,32)_N8_dim2_cuda\n# Input: sizes: (16, 32, 4, 16, 32), N: 8, dim: 2, device: cuda\nForward Execution Time (us) : 112.363\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(9,31,5,15,33)_N17_dim4_cuda\n# Input: sizes: (9, 31, 5, 15, 33), N: 17, dim: 4, device: cuda\nForward Execution Time (us) : 478.224\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f82e50e2680>]_N100_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f82e50e2680>], N: 100, dim: 0, device: cuda\nForward Execution Time (us) : 63.269\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f82e50e2710>]_N1000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f82e50e2710>], N: 1000, dim: 0, device: cuda\nForward Execution Time (us) : 470.141\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f82e50e27a0>]_N2000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f82e50e27a0>], N: 2000, dim: 0, device: cuda\nForward Execution Time (us) : 966.668\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f82e50e2830>]_N3000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f82e50e2830>], N: 3000, dim: 0, device: cuda\nForward Execution Time (us) : 1485.309\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D23727275\n\nfbshipit-source-id: 171275ac541c649f7aeab0a2f8f0fea9486d0180", "pr_number": "44833", "files_changed": ["aten/src/ATen/native/cuda/Shape.cu"], "labels": ["fb-exported"]}, "f77ba0e48c": {"title": "Change typo 'momemtum' to 'momentum' (#45045)", "body": "Summary:\nAs the title.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45045\n\nReviewed By: mruberry\n\nDifferential Revision: D23808563\n\nPulled By: mrshenli\n\nfbshipit-source-id: ca818377f4c23d67b037c146fef667ab8731961e", "pr_number": "45045", "files_changed": ["caffe2/python/data_parallel_model.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/instancenorm.py"], "labels": ["open source"]}, "4b3046ed28": {"title": "Vectorize int8_t on CPU (#44759)", "body": "Summary:\nint8_t is not vectorized in vec256_int.h. This PR adds vectorization for\nint8_t. As pointed out in https://github.com/pytorch/pytorch/issues/43033, this is an important type for vectorization because\na lot of images are loaded in this data type.\n\nRelated issue: https://github.com/pytorch/pytorch/issues/43033\n\nBenchmark (Debian Buster,  Intel(R) Xeon(R) E-2136 CPU @ 3.30GHz, Turbo off, Release build):\n\n```python\nimport timeit\ndtype = 'torch.int8'\nfor op in ('+', '-'):\n    for n, t in [(10_000, 200000),\n                (100_000, 20000)]:\n        print(f'a {op} b, numel() == {n} for {t} times, dtype={dtype}')\n        print(timeit.timeit(f'c = a {op} b', setup=f'import torch; a = torch.arange(1, {n}, dtype={dtype}); b = torch.arange({n}, 1, -1, dtype={dtype})', number=t))\n```\n\nResults:\n\nBefore:\n\n```\na + b, numel() == 10000 for 200000 times, dtype=torch.int8\n1.2223373489978258\na + b, numel() == 100000 for 20000 times, dtype=torch.int8\n0.6108450189931318\na - b, numel() == 10000 for 200000 times, dtype=torch.int8\n1.256775538000511\na - b, numel() == 100000 for 20000 times, dtype=torch.int8\n0.6101213909860235\n```\n\nAfter:\n\n```\na + b, numel() == 10000 for 200000 times, dtype=torch.int8\n0.5713336059998255\na + b, numel() == 100000 for 20000 times, dtype=torch.int8\n0.39169703199877404\na - b, numel() == 10000 for 200000 times, dtype=torch.int8\n0.5838428330025636\na - b, numel() == 100000 for 20000 times, dtype=torch.int8\n0.37486923701362684\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44759\n\nReviewed By: malfet\n\nDifferential Revision: D23786383\n\nPulled By: glaringlee\n\nfbshipit-source-id: 67f5bcd344c0b5014bacbc876143231fca156713", "pr_number": "44759", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_int.h"], "labels": ["open source", "triaged"]}, "8968030f19": {"title": "[WIP] Add vec256 test to linux CI (#44912)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44912\n\nThis is to add vec256 test into linux CI system.\nThe whole test will last 50 to 70 seconds.\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D23772923\n\nPulled By: glaringlee\n\nfbshipit-source-id: ef929b53f3ea7894abcd9510a8e0389979cab4a2", "pr_number": "44912", "files_changed": [".jenkins/pytorch/test.sh"], "labels": []}, "5621ba87a2": {"title": "[vulkan] reshape op to use infer_size to expand -1 (#45104)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45104\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D23834249\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 0e3699d6a4227788d1d634349c0bf259c0ad5e8d", "pr_number": "45104", "files_changed": ["aten/src/ATen/native/vulkan/VulkanOps.cpp"], "labels": []}, "dfc88d4fd0": {"title": "[vulkan] support dimensions negative indexing (#45068)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45068\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D23816081\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: bda753f3f216dac7c05b6f728a3bd6068e5d06a0", "pr_number": "45068", "files_changed": ["aten/src/ATen/native/vulkan/VulkanAten.cpp"], "labels": []}, "09e7f62ce2": {"title": "Fix RPC and ProcessGroup GIL deadlock (#45088)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45088\n\nFixes #45082\n\nFound a few problems while working on #44983\n\n1. We deliberately swallow RPC timeouts during shutdown, as we haven't\nfound a good way to handle those. When we convert `_wait_all_workers`\ninto `_all_gather`, the same logic was inherited. However, as\n`_all_gather` meant to be used in more general scenarios, we should\nno longer keep silent about errors. This commit let the error throw\nin `_all_gather` and also let `shutdown()` to catch them and log.\n2. After fixing (1), I found that `UnpickledPythonCall` needs to\nacquire GIL on destruction, and this can lead to deadlock when used\nin conjuction with `ProcessGroup`. Because `ProcessGroup` ctor is a\nsynchronization point which holds GIL. In `init_rpc`, followers\n(`rank != 0`) can exit before the leader (`rank == 0`). If the two\nhappens together, we could get a) on a follower, it exits `init_rpc`\nafter running `_broadcast_to_followers` and before the reaching dtor\nof `UnpickledPythonCall`. Then it runs the ctor of `ProcessGroup`,\nwhich holds the GIL and wait for the leader to join. However, the\nleader is waiting for the response from `_broadcast_to_followers`,\nwhich is blocked by the dtor of `UnpickledPythonCall`. And hence\nthe deadlock. This commit drops the GIL in `ProcessGroup` ctor.\n3. After fixing (2), I found that `TensorPipe` backend\nnondeterministically fails with `test_local_shutdown`, due to a\nsimilar reason as (2), but this time it is that `shutdown()` on a\nfollower runs before the leader finishes `init_rpc`. This commit\nadds a join for `TensorPipe` backend `init_rpc` after `_all_gather`.\n\nThe 3rd one should be able to solve the 2nd one as well. But since\nI didn't see a reason to hold GIL during `ProcessGroup` ctor, I\nmade that change too.\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23825592\n\nPulled By: mrshenli\n\nfbshipit-source-id: 94920f2ad357746a6b8e4ffaa380dd56a7310976", "pr_number": "45088", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/distributed/rpc/api.py", "torch/distributed/rpc/backend_registry.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "0dda65ac77": {"title": "[ONNX] add jit pass for lists (#43820)", "body": "Summary:\nAdd jit preprocessing pass for adding int lists.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43820\n\nReviewed By: albanD\n\nDifferential Revision: D23674598\n\nPulled By: bzinodev\n\nfbshipit-source-id: 35766403a073e202563bba5251c07efb7cc5cfb1", "pr_number": "43820", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp", "torch/onnx/symbolic_opset9.py", "torch/onnx/symbolic_registry.py"], "labels": ["oncall: jit", "open source", "triaged"]}, "32c1a8c79f": {"title": "adjust shape inference in sls tests (#44936)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44936\n\nneed to provide max sequence size and max element size instead of\ntotal\nadded a check that onnxifi was succesful\n\nTest Plan: sls tests\n\nReviewed By: yinghai\n\nDifferential Revision: D23779437\n\nfbshipit-source-id: 5048d6536ca00f0a3b0b057c4e2cf6584b1329d6", "pr_number": "44936", "files_changed": ["caffe2/contrib/fakelowp/test/test_sls_4bit_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp32.py"], "labels": ["fb-exported"]}, "2111ec3bf3": {"title": "CUDA BFloat16 losses (#45011)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45011\n\nReviewed By: mruberry\n\nDifferential Revision: D23805840\n\nPulled By: ngimel\n\nfbshipit-source-id: 3eb60d4367c727100763879e20e9df9d58bf5ad6", "pr_number": "45011", "files_changed": ["aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/Loss.cu", "aten/src/ATen/native/cuda/PointwiseOpsKernel.cu", "aten/src/ATen/native/cuda/ReduceSumProdKernel.cu", "aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "torch/testing/_internal/common_nn.py"], "labels": ["open source"]}, "5aed75b21b": {"title": "[quant][graphmode][jit] Try to support append (#44641)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44641\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23682356\n\nfbshipit-source-id: 09a03dfde0b1346a5764e8e28ba56e32b343d239", "pr_number": "44641", "files_changed": ["torch/csrc/jit/passes/quantization/helper.cpp"], "labels": ["oncall: jit"]}, "d126a0d4fd": {"title": "[iOS] Disable the iOS nightly build until the cert issue has resolved (#45094)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45094\n\nTest Plan: Imported from OSS\n\nReviewed By: husthyc\n\nDifferential Revision: D23831152\n\nPulled By: xta0\n\nfbshipit-source-id: 6327edba01e4d5abad63ac35680eefb22276423f", "pr_number": "45094", "files_changed": [".circleci/cimodel/data/simple/nightly_ios.py", ".circleci/config.yml"], "labels": []}, "c947ab0bb9": {"title": "Added sparse support for asin and neg functions, updated log1p (#44028)", "body": "Summary:\nDescription:\n\n- [x] added C++ code for sparse `asin` and `neg` ops similarly to `log1p` op\n- [x] added tests\n  - [x] coalesced input CPU/CUDA\n  - [x] uncoalesced input CPU/CUDA\n- [x] added tests for `negative`  and `arcsin`\n\nBackprop will be addressed in another PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44028\n\nReviewed By: agolynski\n\nDifferential Revision: D23793027\n\nPulled By: mruberry\n\nfbshipit-source-id: 5fd642808da8e528cf6acd608ca0dcd720c4ccc3", "pr_number": "44028", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "test/test_sparse.py"], "labels": ["module: sparse", "open source", "triaged"]}, "339961187a": {"title": "[pytorch] refine dispatch keys in native_functions.yaml (1/N) (#45010)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45010\n\nThe motivation of this change is to differentiate \"backend specific\" ops\nand \"generic\" ops.\n\n\"backend specific\" ops are those invoking backend specific kernels thus\nonly able to run on certain backends, e.g.: CPU, CUDA.\n\n\"generic\" ops are those not *directly* invoking backend specific kernels.\nThey are usually calling other \"backend specific\" ops to get things\ndone. Thus, they are also referred to as \"composite\" ops, or \"math\" ops\n(because they are usually pure C++ code constructed from math formula).\n\nThe other way to see the difference is that: we have to implement new\nkernels for the \"backend specific\" ops if we want to run these ops on a\nnew backend. In contrast, \"generic\"/\"composite\" ops can run on the new\nbackend if we've added support for all the \"backend specific\" ops to\nwhich they delegate their work.\n\nHistorically we didn't make a deliberate effort to always populate\nsupported backends to the \"dispatch\" section for all the \"backend specific\"\nops in native_functions.yaml. So now there are many ops which don't have\n\"dispatch\" section but are actually \"backend specific\" ops. Majority\nof them are calling \"DispatchStub\" kernels, which usually only support\nCPU/CUDA (via TensorIterator) or QuantizedCPU/CUDA.\n\nThe ultimate goal is to be able to differentiate these two types of ops\nby looking at the \"dispatch\" section in native_functions.yaml.\n\nThis PR leveraged the analysis script on #44963 to populate missing\ndispatch keys for a set of \"backend specific\" ops. As the initial step,\nwe only deal with the simplest case:\n* These ops don't already have dispatch section in native_functions.yaml;\n* These ops call one or more DispatchStub (thus \"backend specific\");\n* These ops don't call any other aten ops - except for some common\n  ones almost every op calls via framework, e.g. calling aten::eq via\n  Dispatcher::checkSchemaCompatibility. Calling other nontrivial aten\n  ops is a sign of being \"composite\", so we don't want to deal with this\n  case now;\n* These ops don't call Tensor::is_quantized() / Tensor::is_sparse() / etc.\n  Some ops call thse Tensor::is_XXX() methods to dispatch to quantized /\n  sparse kernels internally. We don't deal with this case now.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23803951\n\nPulled By: ljk53\n\nfbshipit-source-id: aaced7c34427d1ede72380af4513508df366ea16", "pr_number": "45010", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": []}, "71aeb84ab4": {"title": "Revert D23803951: [pytorch] refine dispatch keys in native_functions.yaml (1/N)", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23803951 (https://github.com/pytorch/pytorch/commit/339961187a9750e8d5f10954ce78ea8cf819987c)\n\nOriginal commit changeset: aaced7c34427\n\nfbshipit-source-id: fcc4fb6a2c1d79b587f62347b43f8851fe1647fd", "pr_number": null, "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": []}, "1b059f2c6d": {"title": "Directly use work.result() to retrieve tensor rather than passing as a separate argument (#44914)", "body": "Summary:\nWe currently are fetching an allreduced tensor from Python in C++ in, where we are storing the resulting tensor in a struct's parameter. This PR removes extra tensor paratemeter in the function parameter and fetch from a single place.\n\nFixes https://github.com/pytorch/pytorch/issues/43960\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44914\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23798888\n\nPulled By: bugra\n\nfbshipit-source-id: ad1b8c31c15e3758a57b17218bbb9dc1f61f1577", "pr_number": "44914", "files_changed": ["torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/nn/parallel/distributed.py"], "labels": []}, "58b6ab69e5": {"title": "torch.sgn for complex tensors (#39955)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39955\n\nresolves https://github.com/pytorch/pytorch/issues/36323 by adding `torch.sgn` for complex tensors.\n`torch.sgn` returns `x/abs(x)` for `x != 0` and returns `0 + 0j` for `x==0`\n\nThis PR doesn't test the correctness of the gradients. It will be done as a part of auditing all the ops in future once we decide the autograd behavior (JAX vs TF) and add gradchek.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23460526\n\nPulled By: anjali411\n\nfbshipit-source-id: 70fc4e14e4d66196e27cf188e0422a335fc42f92", "pr_number": "39955", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_complex_float.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cpu/zmath.h", "aten/src/ATen/native/cuda/UnarySignKernels.cu", "aten/src/ATen/native/native_functions.yaml", "benchmarks/operator_benchmark/pt/unary_test.py", "docs/source/name_inference.rst", "docs/source/tensors.rst", "test/test_autograd.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "36ec8f8fb8": {"title": "[dper3] Create dper LearningRate low-level module (#44639)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44639\n\nAs title; this will unblock migration of several modules that need learning rate functionality.\n\nTest Plan:\n```\nbuck test //dper3/dper3/modules/low_level_modules/tests:learning_rate_test\n```\n\nReviewed By: yf225\n\nDifferential Revision: D23681733\n\nfbshipit-source-id: 1d98cb35bf6a4ff0718c9cb6abf22401980b523c", "pr_number": "44639", "files_changed": ["caffe2/sgd/learning_rate_op.cc", "caffe2/sgd/learning_rate_op.h", "test/backward_compatibility/check_backward_compatibility.py"], "labels": ["fb-exported"]}, "4a0aa69a66": {"title": "Fix undefined variable 'namedshape' in tensor.py (#45085)", "body": "Summary:\nHot Fix\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45085\n\nReviewed By: malfet, seemethere\n\nDifferential Revision: D23824444\n\nPulled By: walterddr\n\nfbshipit-source-id: c9f37b394d281b7ef44b14c30699bb7510a362a7", "pr_number": "45085", "files_changed": ["torch/tensor.py"], "labels": []}, "e155fbe915": {"title": "add warning when ParameterList/Dict is used with DataParallel (#44405)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44405\n\nTest Plan: Imported from OSS\n\nReviewed By: agolynski\n\nDifferential Revision: D23783987\n\nPulled By: albanD\n\nfbshipit-source-id: 5018b0d381cb09301d2f88a98a910854f740ace1", "pr_number": "44405", "files_changed": ["test/distributed/test_data_parallel.py", "test/test_nn.py", "torch/nn/modules/container.py"], "labels": []}, "63fd257879": {"title": "Add `Ellipsis` constant to the list of recognized tokens (#44959)", "body": "Summary:\nPer https://docs.python.org/3.6/library/constants.html\n> `Ellipsis` is the same as ellipsis literal `...`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44959\n\nReviewed By: suo\n\nDifferential Revision: D23785660\n\nPulled By: malfet\n\nfbshipit-source-id: f68461849e7d16ef68042eb96566f2c936c06b0f", "pr_number": "44959", "files_changed": ["torch/csrc/jit/frontend/lexer.h", "torch/csrc/jit/frontend/parser.cpp"], "labels": ["oncall: jit"]}, "9fc7a942f0": {"title": "Change from self to self.class() in _DecoratorManager to ensure a new object is every time a function is called recursively (#44633)", "body": "Summary:\nChange from self to self._class_() in _DecoratorManager to ensure a new object is every time a function is called recursively\n\nFixes https://github.com/pytorch/pytorch/issues/44531\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44633\n\nReviewed By: agolynski\n\nDifferential Revision: D23783601\n\nPulled By: albanD\n\nfbshipit-source-id: a818664dee7bdb061a40ede27ef99e9546fc80bb", "pr_number": "44633", "files_changed": ["test/test_autograd.py", "torch/autograd/grad_mode.py"], "labels": ["open source", "triaged"]}, "ae286d81e0": {"title": "[JIT] improve alias analysis for list constructs (#39111)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39111\n\nIn our present alias analysis, we consider any Value that enter another container as entering the heap, and thus aliasing all other heap values of the same type. There are a number of advantages to this approach:\n- it is not to hard to maintain the aliasDb implementation\n- it is much easier from an op schema perspective - there are many composite list ops registered internally and externally that would be tricky to register and get right if we did something more complicated\n- It limits the size of the AliasDb, because a container of size 10 only contains a single memory dag element instead of 10 elements.\n\nThe downside is that we have are unable to handle the simple and extremely common case of a list of tensors being used in an ATen op.\n\nIn an example like:\n\n```\n def foo(input):\n    x = torch.tensor([1, 2, 3, 4])\n    y = [x, x]\n    input.add_(1)\n    return torch.cat(y)\n```\n\nwe will consider x to be written to. any write to any wildcard element (an element that enters a tuple, an element that is taken from a list) will mark x as written to. This can be limiting for our ability to create a functional subset and fuse graphs - as a result, 4 of TorchVision classification models could not be functionalized.\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23828003\n\nPulled By: eellison\n\nfbshipit-source-id: 9109fcb6f2ca20ca897cae71683530285da9d537", "pr_number": "39111", "files_changed": ["test/cpp/jit/test_alias_analysis.cpp", "test/jit/test_remove_mutation.py", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/alias_analysis.h"], "labels": ["oncall: jit"]}, "4b42f0b613": {"title": "Support Math keyword in native_functions.yaml. (#44556)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44556\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D23698386\n\nPulled By: ailzhang\n\nfbshipit-source-id: f10ea839a2cfe7d16f5823a75b8b8c5f1ae22dde", "pr_number": "44556", "files_changed": ["aten/src/ATen/native/README.md", "aten/src/ATen/native/group_norm.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/TypeDefault.cpp", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/math_kernel_test.cpp", "tools/autograd/gen_variable_type.py", "tools/codegen/gen.py"], "labels": []}, "8501b89a87": {"title": "[ONNX] Update ort release (#45095)", "body": "Summary:\nUpdate ort release\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45095\n\nReviewed By: bwasti\n\nDifferential Revision: D23832041\n\nPulled By: malfet\n\nfbshipit-source-id: 39c47a87e451c4c43ba4d4e8be385cc195cc611a", "pr_number": "45095", "files_changed": [".jenkins/caffe2/test.sh"], "labels": ["module: ci", "open source", "triaged"]}, "1fd48a9d1f": {"title": "Revert D23798016: [FX] s/get_param/get_attr/", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23798016 (https://github.com/pytorch/pytorch/commit/c941dd3492535b3e09f4cb3f60c80b02f5e04c3f)\n\nOriginal commit changeset: 1d2f3db1994a\n\nfbshipit-source-id: 974d930064b37d396c5d66c905a63d45449813e5", "pr_number": null, "files_changed": ["test/fx/quantization.py", "test/test_fx.py", "torch/fx/__init__.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/symbolic_trace.py", "torch/quantization/fx/quantize.py", "torch/quantization/fx/utils.py"], "labels": []}, "10f287539f": {"title": "Align casing in test_dispatch with dispatch keys. (#44933)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44933\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23778247\n\nPulled By: ailzhang\n\nfbshipit-source-id: bc3725eae670b03543015afe763cb3bb16baf8f6", "pr_number": "44933", "files_changed": ["test/test_dispatch.py", "torch/csrc/utils/python_dispatch.cpp"], "labels": []}, "ef885c10d8": {"title": "[pytorch] Add triplet margin loss with custom distance (#43680)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43680\n\nAs discussed [here](https://github.com/pytorch/pytorch/issues/43342),\nadding in a Python-only implementation of the triplet-margin loss that takes a\ncustom distance function.  Still discussing whether this is necessary to add to\nPyTorch Core.\n\nTest Plan:\npython test/run_tests.py\n\nImported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23363898\n\nfbshipit-source-id: 1cafc05abecdbe7812b41deaa1e50ea11239d0cb", "pr_number": "43680", "files_changed": ["docs/source/nn.functional.rst", "docs/source/nn.rst", "test/test_nn.py", "torch/nn/functional.py", "torch/nn/functional.pyi.in", "torch/nn/modules/__init__.py", "torch/nn/modules/loss.py", "torch/overrides.py"], "labels": ["open source", "triaged"]}, "e2b40ce793": {"title": "Support BFloat16 for binary logical operators on CUDA (#42485)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42485\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23684423\n\nPulled By: mruberry\n\nfbshipit-source-id: edc2b46b726361d4c8bf8a4bf4e4a09197b20428", "pr_number": "42485", "files_changed": ["aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "09aee06e82": {"title": "[caffe2] Replace embedding conversion ops with fbgemm functions (#44843)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44843\n\nReplace perfkernels calls with fbgemm kernels to avoid code duplication\nghstack-source-id: 112496292\n\nTest Plan: CI\n\nReviewed By: radkris-git\n\nDifferential Revision: D23675519\n\nfbshipit-source-id: 05c285a9eeb9ea109a04a78cb442a24ee40a4aec", "pr_number": "44843", "files_changed": ["caffe2/perfkernels/fused_nbit_rowwise_conversion.cc", "caffe2/perfkernels/fused_nbit_rowwise_conversion_avx2.cc", "test/quantization/test_quantized_op.py"], "labels": []}, "2b1f25885e": {"title": "[quant] Fix ConvTranspose mapping (#44844)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44844\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23746466\n\nPulled By: z-a-f\n\nfbshipit-source-id: cb84e0fef5ab82e8ed8dd118d9fb21ee7b480ef7", "pr_number": "44844", "files_changed": ["torch/quantization/quantization_mappings.py"], "labels": []}, "c253b10154": {"title": "Fix incorrect EnumValue serialization issue (#44891)", "body": "Summary:\nPreviously, `prim::EnumValue` is serialized to `ops.prim.EnumValue`, which doesn't have the right implementation to refine return type. This diff correctly serializes it to enum.value, thus fixing the issue.\n\nFixes https://github.com/pytorch/pytorch/issues/44892\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44891\n\nReviewed By: malfet\n\nDifferential Revision: D23818962\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 6edfdf9c4b932176b08abc69284a916cab10081b", "pr_number": "44891", "files_changed": ["test/jit/test_enum.py", "torch/csrc/jit/serialization/python_print.cpp"], "labels": ["oncall: jit"]}, "a4ce3f4194": {"title": "Fix type hint warnings for common_methods_invocations.py (#44971)", "body": "Summary:\nFixes a subtask of https://github.com/pytorch/pytorch/issues/42969\n\nTested the following and no warnings were seen.\n\npython test/test_type_hints.py\n....\n----------------------------------------------------------------------\nRan 4 tests in 180.759s\n\nOK\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44971\n\nReviewed By: walterddr\n\nDifferential Revision: D23822274\n\nPulled By: visweshfb\n\nfbshipit-source-id: e3485021e348ee0a8508a9d128f04bad721795ef", "pr_number": "44971", "files_changed": ["mypy.ini", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "def433bbb6": {"title": ".circleci: Upgrade all xcode 9 workers to xcode 11 (#45153)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45153\n\nxcode 9 is being deprectated within circleci infra so we should get\neverything else on a more recent version of xcode\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D23852774\n\nPulled By: seemethere\n\nfbshipit-source-id: c02e162f1993d408de439fee21b340e9640e5a24", "pr_number": "45153", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": []}, "79fe794f87": {"title": "[FX] Make Graphs immutable and make GraphModule recompile after assigning graph (#44830)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44830\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23743850\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 501b92a89ff636c26abeff13105a75462384554c", "pr_number": "44830", "files_changed": ["test/test_fx.py", "torch/fx/graph.py", "torch/fx/graph_module.py"], "labels": ["fx"]}, "d1c68a7069": {"title": "Clarify that 5-D 'bilinear' grid_sample is actually trilinear (#45090)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41528\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45090\n\nReviewed By: ailzhang\n\nDifferential Revision: D23841046\n\nPulled By: zou3519\n\nfbshipit-source-id: 941770cd5b3e705608957739026e9113e5f0c616", "pr_number": "45090", "files_changed": ["torch/nn/functional.py"], "labels": ["open source"]}, "cddcfde81d": {"title": "[JIT] Fix WithTest.test_with_exceptions (#45106)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45106\n\n**Summary**\nThis commit fixes `WithTest.test_with_exceptions`. It's been running\nin regular Python this whole time; none of the functions created and\ninvoked for the test were scripted. Fortunately, the tests still pass\nafter being fixed.\n\n**Test Plan**\nRan unit tests + continuous integration.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23848206\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: fd975ee34db9441ef4e4a4abf2fb21298166bbaa", "pr_number": "45106", "files_changed": ["test/jit/test_with.py"], "labels": ["oncall: jit"]}, "35cdb01327": {"title": "[PyTorch] Enable type check for autocast_test_lists (#45107)", "body": "Summary:\nThis is a sub-task for addressing: https://github.com/pytorch/pytorch/issues/42969. We re-enable type check for `autocast_test_lists `.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45107\n\nTest Plan:\n`python test/test_type_hints.py` passed:\n```\n(pytorch) bash-5.0$ with-proxy python test/test_type_hints.py\n....\n----------------------------------------------------------------------\nRan 4 tests in 103.871s\n\nOK\n```\n\nReviewed By: walterddr\n\nDifferential Revision: D23842884\n\nPulled By: Hangjun\n\nfbshipit-source-id: a39f3810e3abebc6b4c1cb996b06312f6d42ffd6", "pr_number": "45107", "files_changed": ["mypy.ini", "torch/testing/_internal/autocast_test_lists.py"], "labels": []}, "7f4a27be3a": {"title": "[resubmit][FX] s/get_param/get_attr/ (#45147)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45147\n\nghstack-source-id: 112605923\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23845096\n\nfbshipit-source-id: 9ca209aa84cbaddd6e89c52b541e43b11197e2d5", "pr_number": "45147", "files_changed": ["test/fx/quantization.py", "test/test_fx.py", "torch/fx/__init__.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/symbolic_trace.py", "torch/quantization/fx/quantize.py", "torch/quantization/fx/utils.py"], "labels": ["fx"]}, "ccfbfe5eb5": {"title": "[quant][graphmode][fx] Custom module support (#44766)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44766\n\nThere might be modules that are not symbolically traceable, e.g. LSTM (since it has\ninput dependent control flows), to support quantization in these cases, user will provide\nthe corresponding observed and quantized version of the custom module, the observed\ncustom module with observers already inserted in the module and the quantized version will\nhave the corresponding ops quantized. And use\n```\nfrom torch.quantization import register_observed_custom_module_mapping\nfrom torch.quantization import register_quantized_custom_module_mapping\nregister_observed_custom_module_mapping(CustomModule, ObservedCustomModule)\nregister_quantized_custom_module_mapping(CustomModule, QuantizedCustomModule)\n```\nto register the custom module mappings, we'll also need to define a custom delegate class\nfor symbolic trace in order to prevent the custom module from being traced:\n```python\nclass CustomDelegate(DefaultDelegate):\n      def is_leaf_module(self, m):\n          return (m.__module__.startswith('torch.nn') and\n                    not isinstance(m, torch.nn.Sequential)) or \\\n                    isinstance(m, CustomModule)\nm = symbolic_trace(original_m, delegate_class=CustomDelegate)\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23723455\n\nfbshipit-source-id: 50d666e29b94cbcbea5fb6bcc73b00cff87eb77a", "pr_number": "44766", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/nn/quantized/modules/conv.py", "torch/quantization/__init__.py", "torch/quantization/custom_module_class_mappings.py", "torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "2a37f3fd2f": {"title": "Relax CUDA architecture check (#45130)", "body": "Summary:\nNVIDIA GPUs are binary compatible within major compute capability revision\n\nThis would prevent: \"GeForce RTX 3080 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\" messages from appearing, since CUDA-11 do not support code generation for sm_85.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45130\n\nReviewed By: ngimel\n\nDifferential Revision: D23841556\n\nPulled By: malfet\n\nfbshipit-source-id: bcfc9e8da63dfe62cdec06909b6c049aaed6a18a", "pr_number": "45130", "files_changed": ["torch/cuda/__init__.py"], "labels": []}, "b98ac20849": {"title": "install ATen/native/cuda and hip headers (#45097)", "body": "Summary:\nThe ATen/native/cuda headers were copied to torch/include, but then not included in the final package.  Further, add ATen/native/hip headers to the installation, as well.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45097\n\nReviewed By: mruberry\n\nDifferential Revision: D23831006\n\nPulled By: malfet\n\nfbshipit-source-id: ab527928185faaa912fd8cab208733a9b11a097b", "pr_number": "45097", "files_changed": ["aten/src/ATen/CMakeLists.txt", "setup.py"], "labels": ["module: build", "open source", "triaged"]}, "c0267c6845": {"title": "[caffe2] Support data types in shape hints (#45110)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45110\n\nA recent change in DSNN quantizes the ad embedding to 8 bits. Ad embeddings are part of the inputs to the DSNN merge net. To correctly pass shape hints of input tensors including quantized ad embeddings, we need to be able to annotate the data types in shape hints.\n\nA bit on the corner cases, if type is omitted or not a valid type, e.g., white spaces, instead of throwing an exception, I decided to return the default type, float.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/fb/opt:shape_info_utils_test\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D23834091\n\nfbshipit-source-id: 5e072144a7a7ff4b5126b618062dfc4041851dd3", "pr_number": "45110", "files_changed": ["caffe2/opt/shape_info.cc"], "labels": ["fb-exported"]}, "ebde5a80bb": {"title": "[tensorexpr] Add flag to fuse with unknown shapes (#44401)", "body": "Summary:\nThis flag simply allows users to get fusion groups that will *eventually* have shapes (such that `getOperation` is a valid).\n\nThis is useful for doing early analysis and compiling just in time.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44401\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23656140\n\nPulled By: bwasti\n\nfbshipit-source-id: 9a26c202752399d1932ad7d69f21c88081ffc1e5", "pr_number": "44401", "files_changed": ["test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.h"], "labels": ["oncall: jit"]}, "e045119956": {"title": "[JIT] Add default arguments for class types (#45098)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45098\n\n**Summary**\nThis commit adds support for default arguments in methods of class\ntypes. Similar to how default arguments are supported for regular\nscript functions and methods on scripted modules, default values are\nretrieved from the definition of a TorchScript class in Python as Python\nobjects, converted to IValues, and then attached to the schemas of\nalready compiled class methods.\n\n**Test Plan**\nThis commit adds a set of new tests to TestClassType to test default\narguments.\n\n**Fixes**\nThis commit fixes #42562.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23844769\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: ceedff7703bf9ede8bd07b3abcb44a0f654936bd", "pr_number": "45098", "files_changed": ["test/jit/test_class_type.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/python/script_init.cpp", "torch/jit/_script.py", "torch/jit/frontend.py"], "labels": ["oncall: jit"]}, "f575df201f": {"title": "[quant][graphmode][jit][api] Expose preserved_attrs from finalize to convert_jit (#44490)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44490\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23631142\n\nfbshipit-source-id: f0913f0cb4576067e2a7288326024942d12e0ae0", "pr_number": "44490", "files_changed": ["torch/csrc/jit/passes/quantization/finalize.cpp", "torch/csrc/jit/passes/quantization/finalize.h", "torch/csrc/jit/python/init.cpp", "torch/quantization/quantize_jit.py"], "labels": ["oncall: jit"]}, "666223df46": {"title": "[jit] gtestify test_argument_spec.cpp (#45019)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45019\n\nSee https://github.com/pytorch/pytorch/pull/45018 for context.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23802298\n\nPulled By: suo\n\nfbshipit-source-id: 0e36d095d4d81dcd5ebe6d56b3dc469d6d5482d0", "pr_number": "45019", "files_changed": ["test/cpp/jit/test_argument_spec.cpp", "test/cpp/jit/test_base.h", "test/cpp/jit/tests.h", "tools/build_variables.bzl"], "labels": ["oncall: jit"]}, "67a19fecef": {"title": "CUDA BFloat16 pooling (#45151)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45151\n\nReviewed By: ailzhang\n\nDifferential Revision: D23854056\n\nPulled By: ngimel\n\nfbshipit-source-id: 32f0835218c2602a09654a9ac2d161c4eb360f90", "pr_number": "45151", "files_changed": ["aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "test/test_nn.py"], "labels": ["open source", "triaged"]}, "1bd6533d60": {"title": "Remove thread_local RecordFunctionGuard from profiler. (#44646)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44646\n\nPer a discussion with ilia-cher, this is not needed anymore and\nremoving it would make some future changes to support async RPC profiling\neasier. Tested by ensuring profiling tests in `test_autograd.py` still pass.\nghstack-source-id: 112605618\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D23683998\n\nfbshipit-source-id: 4e49a439509884fe04d922553890ae353e3331ab", "pr_number": "44646", "files_changed": ["torch/csrc/autograd/profiler.cpp"], "labels": []}, "70d2e4d1f6": {"title": "[RPC profiling] Allow disableProfiler() to be called from another thread. (#44653)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44653\n\nThis changes the profiler per a discussion with ilia-cher offline that enables `disableProfiler()` event consolidation logic to be called from different threads (i.e. threads where the profiler was not explicitly enabled). This is needed to support the functionality enabled by D23638387 where we defer profiling event collection until executing an async callback that can execute on a different thread, to support RPC async function profiling.\n\nThis is done by introducing 2 flags `cleanupTLSState` and `consolidate` which controls whether we should clean up thread local settings (we don't do this when calling `disableProfiler()` on non-main threads) and whether we should consolidate all profiled events. Backwards compatiblity is ensured since both options are true by default.\n\nAdded a test in `test_misc.cpp` to test this.\nghstack-source-id: 112605620\n\nReviewed By: mrshenli\n\nDifferential Revision: D23638499\n\nfbshipit-source-id: f5bbb0d41ef883c5e5870bc27e086b8b8908f46b", "pr_number": "44653", "files_changed": ["c10/util/ThreadLocalDebugInfo.cpp", "c10/util/ThreadLocalDebugInfo.h", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/profiler.h"], "labels": ["oncall: jit"]}, "d4a634c209": {"title": "[RPC profiling] Don't wrap toHere() calls with profiling (#44655)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44655\n\nSince `toHere()` does not execute operations over RPC and simply\ntransfers the value to the local node, we don't need to enable the profiler\nremotely for this message. This causes unnecessary overhead and is not needed.\n\nSince `toHere` is a blocking call, we already profile the call on the local node using `RECORD_USER_SCOPE`, so this does not change the expected profiler results (validated by ensuring all remote profiling tests pass).\nghstack-source-id: 112605610\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D23641466\n\nfbshipit-source-id: 109d9eb10bd7fe76122b2026aaf1c7893ad10588", "pr_number": "44655", "files_changed": ["torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/autograd/utils.h", "torch/csrc/distributed/rpc/rref_impl.cpp"], "labels": []}, "cb75addee4": {"title": "torch.package - a way to package models and code (#45015)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45015\n\ntorch.package allows you to write packages of code, pickled python data, and\narbitrary binary and text resources into a self-contained package.\n\ntorch.package.PackageExporter writes the packages and\ntorch.package.PackageImporter reads them.\n\nThe importers can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.\n\nThe code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.\n\nThe importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external using :method:`extern_module`.\nThe file `extern_modules` in the zip archive lists all the modules that a package externally depends on.\nThis prevents \"implicit\" dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine.\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D23824337\n\nPulled By: zdevito\n\nfbshipit-source-id: 1247c34ba9b656f9db68a83e31f2a0fbe3bea6bd", "pr_number": "45015", "files_changed": ["test/module_a.py", "test/namespace_b/subpackage.py", "test/package_a/__init__.py", "test/package_a/subpackage.py", "test/run_test.py", "test/test_package.py", "torch/package/__init__.py", "torch/package/_custom_import_pickler.py", "torch/package/_importlib.py", "torch/package/_mock.py", "torch/package/_mock_zipreader.py", "torch/package/exporter.py", "torch/package/find_file_dependencies.py", "torch/package/importer.py", "torch/serialization.py"], "labels": []}, "25ed739ac9": {"title": "[packaging] rstrip fix (#45166)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45166\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D23852505\n\nPulled By: zdevito\n\nfbshipit-source-id: 6bb743b37333ae19fc24629686e8d06aef812c50", "pr_number": "45166", "files_changed": ["torch/package/importer.py"], "labels": []}, "0a9ac98bed": {"title": "[reland][pytorch] refine dispatch keys in native_functions.yaml (1/N) (#45137)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45137\n\nReland https://github.com/pytorch/pytorch/pull/45010 - which broke\nmaster due to merge conflict.\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D23843510\n\nPulled By: ljk53\n\nfbshipit-source-id: 28aabb9da533b6b806ab8779a0ee96b695e9e242", "pr_number": "45137", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": []}, "989d877c95": {"title": "[JIT] Do not allow creating generics with None types (#44958)", "body": "Summary:\nOtherwise, invoking something like  `python -c \"import torch._C;print(torch._C.ListType(None))\"` will result in SIGSEGV\n\nDiscovered while trying to create a torch script for function with the following type annotation `Tuple[int, Ellipsis] -> None`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44958\n\nReviewed By: suo\n\nDifferential Revision: D23799906\n\nPulled By: malfet\n\nfbshipit-source-id: 916a243007d13ed3e7a5b282dd712da3d66e3bf7", "pr_number": "44958", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/jit/test_list_dict.py"], "labels": ["topic: crash", "triaged"]}, "144dacd8d9": {"title": "CUDA BFloat16 batched gemm (#45167)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45167\n\nReviewed By: mruberry\n\nDifferential Revision: D23860458\n\nPulled By: ngimel\n\nfbshipit-source-id: 698de424a046963a30017b58d227fa510f85bf3f", "pr_number": "45167", "files_changed": ["aten/src/THC/THCBlas.cu", "aten/src/THC/THCBlas.h", "aten/src/THC/generic/THCTensorMathBlas.cu", "test/test_torch.py"], "labels": ["open source"]}, "7fba30c2be": {"title": "[quant][fx][bug] Fix error in convert step for QAT (#45050)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45050\n\nUpdate tests to actually test for QAT\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFxOps.test_linear\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23808022\n\nfbshipit-source-id: d749ab2d215fe19238ff9d539307ffce9ef0ca9b", "pr_number": "45050", "files_changed": ["torch/quantization/fx/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["fx"]}, "215679573e": {"title": "[TensorExpr] Fix operator order in combineMultilane (#45157)", "body": "Summary:\ncombineMultilane used the wrong order when ramp was on the left hand side,\nwhich matters for subtract.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45157\n\nTest Plan: test_tensorexpr --gtest_filter=TensorExprTest.SimplifyRampSubBroadcast\n\nReviewed By: ailzhang\n\nDifferential Revision: D23851751\n\nPulled By: asuhan\n\nfbshipit-source-id: 864d1611e88769fb43327ef226bb3310017bf858", "pr_number": "45157", "files_changed": ["test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp"], "labels": ["oncall: jit"]}, "76dc50e9c8": {"title": "[RPC] Infer backend type if only options are given (#45065)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45065\n\nTo preserve backwards compatibility with applications that were passing in some ProcessGroupRpcBackendOptions but were not explicitly setting backend=BackendType.PROCESS_GROUP, we're here now inferring the backend type from the options if only the latter ones are passed. If neither are passed, we'll default to TensorPipe, as before this change.\nghstack-source-id: 112586258\n\nTest Plan: Added new unit tests.\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23814289\n\nfbshipit-source-id: f4be7919e0817a4f539a50ab12216dc3178cb752", "pr_number": "45065", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/backend_registry.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "e5bade7b2c": {"title": "[PyTorch Mobile] Move string op registrations to prim and make them selective (#44960)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44960\n\nSince we have templated selective build, it should be safe to move the operators to prim so that they can be selectively built in mobile\n\nTest Plan: CI\n\nReviewed By: linbinyu\n\nDifferential Revision: D23772025\n\nfbshipit-source-id: 52cebae76e4df5a6b2b51f2cd82f06f75e2e45d0", "pr_number": "44960", "files_changed": ["aten/src/ATen/templates/TypeDefault.cpp", "tools/build_variables.bzl", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_string_ops.cpp"], "labels": ["fb-exported", "oncall: jit"]}, "94c3cdd994": {"title": "Let rpc._all_gather use default RPC timeout (#44983)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44983\n\n`_all_gather` was converted from `_wait_all_workers` and inherited its\n5 seconds fixed timeout. As `_all_gather` meant to support a broader\nset of use cases, the timeout configuration should be more flexible.\nThis PR makes `rpc._all_gather` use the global default RPC timeout.\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23794383\n\nPulled By: mrshenli\n\nfbshipit-source-id: 382f52c375f0f25c032c5abfc910f72baf4c5ad9", "pr_number": "44983", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/testing/faulty_process_group_agent.cpp", "torch/distributed/rpc/api.py", "torch/distributed/rpc/constants.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "5b20bf4fd9": {"title": "Added support for complex input for Cholesky decomposition (#44895)", "body": "Summary:\nCholesky decomposition now works for complex inputs.\n\nFixes https://github.com/pytorch/pytorch/issues/44637.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44895\n\nReviewed By: ailzhang\n\nDifferential Revision: D23841583\n\nPulled By: anjali411\n\nfbshipit-source-id: 3b1f34a7af17827884540696f8771a0d5b1df478", "pr_number": "44895", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "test/test_torch.py"], "labels": ["module: complex", "open source", "topic: linear algebra"]}, "9e30a76697": {"title": "Filter `strtod_l` is undeclared errors from sccache log (#45183)", "body": "Summary:\nThis prevents DrCI from misidentifying test failures for the compilation failures, such as:\n```\n/var/lib/jenkins/workspace/build/CMakeFiles/CMakeTmp/CheckSymbolExists.c:8:19: error: use of undeclared identifier \\'strtod_l\\'\n  return ((int*)(&strtod_l))[argc];\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45183\n\nReviewed By: ezyang\n\nDifferential Revision: D23859267\n\nPulled By: malfet\n\nfbshipit-source-id: 283d9bd2ab712f23239b72f3758d121e2d026fb0", "pr_number": "45183", "files_changed": [".jenkins/pytorch/print_sccache_log.py"], "labels": []}, "9db3871288": {"title": "Update true_divide_out to use at::. (#45079)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45079\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D23821701\n\nPulled By: ailzhang\n\nfbshipit-source-id: 562eac10faba7a503eda0029a0b026c1fb85fe1e", "pr_number": "45079", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "test/test_torch.py"], "labels": []}, "a5a4924c27": {"title": "Warn if `import torch` is called from the source root. (#39995)", "body": "Summary:\nThis is a small developer quality of life improvement. I commonly try to run some snippet of python as I'm working on a PR and forget that I've cd-d into the local clone to run some git commands, resulting in annoying failures like:\n`ImportError: cannot import name 'default_generator' from 'torch._C' (unknown location)`\n\nThis actually took a non-trivial amount of time to figure out the first time I hit it, and even now it's annoying because it happens just infrequently enough to not sit high in the mental cache.\n\nThis PR adds a check to `torch/__init__.py` and warns if `import torch` is likely resolving to the wrong thing:\n\n```\nWARNING:root:You appear to be importing PyTorch from a clone of the git repo:\n  /data/users/taylorrobie/repos/pytorch\n  This will prevent `import torch` from resolving to the PyTorch install\n  (instead it will try to load /data/users/taylorrobie/repos/pytorch/torch/__init__.py)\n  and will generally lead to other failures such as a failure to load C extensions.\n```\n\nso that the soon to follow internal import failure makes some sense. I elected to make this a warning rather than an exception because I'm not 100% sure that it's **always** wrong. (e.g. weird `PYTHONPATH` or `importlib` corner cases.)\n\nEDIT: There are now separate cases for `cwd` vs. `PYTHONPATH`, and failure is an `ImportError`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39995\n\nReviewed By: malfet\n\nDifferential Revision: D23817209\n\nPulled By: robieta\n\nfbshipit-source-id: d9ac567acb22d9c8c567a8565a7af65ac624dbf7", "pr_number": "39995", "files_changed": ["torch/__init__.py"], "labels": []}, "da4033d32a": {"title": "Make cudaHostRegister actually useful on cudart. (#45159)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45159\n\nBy default, pybind11 binds void* to be capsules.  After a lot of\nGoogling, I have concluded that this is not actually useful:\nyou can't actually create a capsule from Python land, and our\ndata_ptr() function returns an int, which means that the\nfunction is effectively unusable.  It didn't help that we had no\ntests exercising it.\n\nI've replaced the void* with uintptr_t, so that we now accept int\n(and you can pass data_ptr() in directly).  I'm not sure if we\nshould make these functions accept ctypes types; unfortunately,\npybind11 doesn't seem to have any easy way to do this.\n\nFixes #43006\n\nAlso added cudaHostUnregister which was requested.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D23849731\n\nPulled By: ezyang\n\nfbshipit-source-id: 8a79986f3aa9546abbd2a6a5828329ae90fd298f", "pr_number": "45159", "files_changed": ["test/test_cuda.py", "torch/csrc/cuda/shared/cudart.cpp"], "labels": []}, "4d80c8c648": {"title": "Fix inlining interface call in fork subgraph (#43790)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43790\n\nInterface calls were not handled properly when they are used in fork\nsubgraph. This PR fixes this issue.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23402039\n\nPulled By: bzinodev\n\nfbshipit-source-id: 41adc5ee7d942250e732e243ab30e356d78d9bf7", "pr_number": "43790", "files_changed": ["test/jit/test_freezing.py", "test/jit/test_module_interface.py", "torch/csrc/jit/passes/freeze_module.cpp"], "labels": ["oncall: jit"]}, "99242eca1d": {"title": "Dockerfile: Support CUDA 11 (#45071)", "body": "Summary:\nAlthough PyTorch already supports CUDA 11, the Dockerfile still relies on CUDA 10. This pull request upgrades all the necessary versions such that recent NVIDIA GPUs like A100 can be used.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45071\n\nReviewed By: ezyang\n\nDifferential Revision: D23873224\n\nPulled By: seemethere\n\nfbshipit-source-id: 822c25f183dcc3b4c5b780c00cd37744d34c6e00", "pr_number": "45071", "files_changed": ["Dockerfile", "docker.Makefile"], "labels": ["open source", "triaged"]}, "21fabae47a": {"title": "Remove expensive call to PyObject_GetAttrString in PyTorch_LookupSpecial (#44684)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44684\n\nThe ad-hoc quantization benchmarking script in D23689062 recently highlighted that quantized ops were surprisingly slow after the introduction of support for custom ops in torch.fx in D23203204 (https://github.com/pytorch/pytorch/commit/f15e27265ff76f49844b0ccc6ca387cb564824bf).\n\nUsing strobelight, it's immediately clear that up to 66% of samples were seen in `c10::get_backtrace`, which is descends from `torch::is_tensor_and_apppend_overloaded -> torch::check_has_torch_function ->  torch::PyTorch_LookupSpecial -> PyObject_HasAttrString ->  PyObject_GetAttrString`.\n\nI'm no expert by any means so please correct any/all misinterpretation, but it appears that:\n- `check_has_torch_function` only needs to return a bool\n- `PyTorch_LookupSpecial` should return `NULL` if a matching method is not found on the object\n- in the impl of `PyTorch_LookupSpecial` the return value from `PyObject_HasAttrString` only serves as a bool to return early, but ultimately ends up invoking `PyObject_GetAttrString`, which raises, spawning the generation of a backtrace\n- `PyObject_FastGetAttrString` returns `NULL` (stolen ref to an empty py::object if the if/else if isn't hit) if the method is not found, anyway, so it could be used singularly instead of invoking both `GetAttrString` and `FastGetAttrString`\n- D23203204 (https://github.com/pytorch/pytorch/commit/f15e27265ff76f49844b0ccc6ca387cb564824bf) compounded (but maybe not directly caused) the problem by increasing the number of invocations\n\nso, removing it in this diff and seeing how many things break :)\n\nbefore:\nstrobelight: see internal section\noutput from D23689062 script:\n```\n$ ./buck-out/gen/scripts/v/test_pt_quant_perf.par\nSequential(\n  (0): Quantize(scale=tensor([0.0241]), zero_point=tensor([60]), dtype=torch.quint8)\n  (1): QuantizedLinear(in_features=4, out_features=4, scale=0.017489388585090637, zero_point=68, qscheme=torch.per_tensor_affine)\n  (2): DeQuantize()\n)\nfp 0.010896682739257812\nq 0.11908197402954102\n```\n\nafter:\nstrobelight: see internal section\noutput from D23689062 script:\n```\n$ ./buck-out/gen/scripts/v/test_pt_quant_perf.par\nSequential(\n  (0): Quantize(scale=tensor([0.0247]), zero_point=tensor([46]), dtype=torch.quint8)\n  (1): QuantizedLinear(in_features=4, out_features=4, scale=0.012683945707976818, zero_point=41, qscheme=torch.per_tensor_affine)\n  (2): DeQuantize()\n)\nfp 0.011141300201416016\nq 0.022639036178588867\n```\n\nwhich roughly restores original performance seen in P142370729\n\nUPDATE: 9/22 mode/opt benchmarks\n```\nbuck run //scripts/x:test_pt_quant_perf mode/opt\nSequential(\n  (0): Quantize(scale=tensor([0.0263]), zero_point=tensor([82]), dtype=torch.quint8)\n  (1): QuantizedLinear(in_features=4, out_features=4, scale=0.021224206313490868, zero_point=50, qscheme=torch.per_tensor_affine)\n  (2): DeQuantize()\n)\nfp 0.002968311309814453\nq 0.5138928890228271\n```\n\nwith patch:\n```\nbuck run //scripts/x:test_pt_quant_perf mode/opt\nSequential(\n  (0): Quantize(scale=tensor([0.0323]), zero_point=tensor([70]), dtype=torch.quint8)\n  (1): QuantizedLinear(in_features=4, out_features=4, scale=0.017184294760227203, zero_point=61, qscheme=torch.per_tensor_affine)\n  (2): DeQuantize()\n)\nfp 0.0026655197143554688\nq 0.0064449310302734375\n```\n\nReviewed By: ezyang\n\nDifferential Revision: D23697334\n\nfbshipit-source-id: f756d744688615e01c94bf5c48c425747458fb33", "pr_number": "44684", "files_changed": ["torch/csrc/utils/python_arg_parser.h"], "labels": ["fb-exported"]}, "adb2b380ba": {"title": "[quant][graphmode][fx] qconfig_dict support more types of configurations (#44856)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44856\n\nSupport following format of qconfig_dict\n```python\nqconfig_dict = {\n    # optional, global config\n    \"\": qconfig?,\n\n    # optional, used for module and function types\n    # could also be split into module_types and function_types if we prefer\n    \"object_type\": [\n      (nn.Conv2d, qconfig?),\n      (F.add, qconfig?),\n      ...,\n    ],\n\n    # optional, used for module names\n    \"module_name\": [\n      (\"foo.bar\", qconfig?)\n      ...,\n    ],\n\n    # optional, matched in order, first match takes precedence\n    \"module_name_regex\": [\n      (\"foo.*bar.*conv[0-9]+\", qconfig?)\n      ...,\n    ]\n    # priority (in increasing order): global, object_type, module_name_regex, module_name\n    # qconfig == None means fusion and quantization should be skipped for anything\n    # matching the rule\n}\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23751304\n\nfbshipit-source-id: 5b98f4f823502b12ae2150c93019c7b229c49c50", "pr_number": "44856", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py", "torch/quantization/quantize_fx.py"], "labels": ["fx"]}, "9e206ee9f1": {"title": "[NNC] Fix a bug in SplitWithMask when splitting multiple times (#45141)", "body": "Summary:\nWhen doing a splitWithMask we only mask if the loop extent is not cleanly divide by the split factor. However, the logic does not simplify so any nontrivial loop extents will always cause a mask to be added, e.g. if the loop had been previously split. Unlike splitWithTail, the masks added by splitWithMask are always overhead and we don't have the analysis to optimize them out if they are unnecessary, so it's good to avoid inserting them if we can.\n\nThe fix is just to simplify the loop extents before doing the extent calculation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45141\n\nReviewed By: ezyang\n\nDifferential Revision: D23869170\n\nPulled By: nickgg\n\nfbshipit-source-id: 44686fd7b802965ca4f5097b0172a41cf837a1f5", "pr_number": "45141", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["oncall: jit"]}, "3f89b779c4": {"title": "[jit] allow submodule methods inference rule be different (#43872)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43872\n\nThis PR allows the recursive scripting to have a separate\nsubmodule_stubs_fn to create its submodule with specific user provided\nrules.\n\nFixes https://github.com/pytorch/pytorch/issues/43729\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D23430176\n\nPulled By: wanchaol\n\nfbshipit-source-id: 20530d7891ac3345b36f1ed813dc9c650b28d27a", "pr_number": "43872", "files_changed": ["test/jit/test_tracer.py", "torch/jit/_recursive.py", "torch/jit/_script.py", "torch/jit/_trace.py"], "labels": ["oncall: jit"]}, "d2b045030e": {"title": "gtest-ify JIT tests, through the letter c (#45020)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45020\n\nSee https://github.com/pytorch/pytorch/pull/45018 for context.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23802296\n\nPulled By: suo\n\nfbshipit-source-id: 20c9798a414e9ba30869a862012cbdee0613c8b1", "pr_number": "45020", "files_changed": ["test/cpp/jit/test_autodiff.cpp", "test/cpp/jit/test_class_import.cpp", "test/cpp/jit/test_class_parser.cpp", "test/cpp/jit/test_cleanup_passes.cpp", "test/cpp/jit/test_code_template.cpp", "test/cpp/jit/test_constant_pooling.cpp", "test/cpp/jit/test_create_autodiff_subgraphs.cpp", "test/cpp/jit/test_custom_class.cpp", "test/cpp/jit/test_custom_operators.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h"], "labels": ["oncall: jit"]}, "246bd9422a": {"title": "gtestify dce and fuser tests (#45055)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45055\n\nSee https://github.com/pytorch/pytorch/pull/45018 for context.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23811085\n\nPulled By: suo\n\nfbshipit-source-id: 45008e41f2394d2ba319745b0340392e1b3d3172", "pr_number": "45055", "files_changed": ["test/cpp/jit/test_dce.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/tests.h"], "labels": ["oncall: jit"]}, "2a1a51facb": {"title": "Fix typos. (#45195)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45195\n\nFix some typos in reducer class.\nghstack-source-id: 112673443\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23862399\n\nfbshipit-source-id: 0dc69e5ea1fa7d33c85d1909b2216bcd1f579f6a", "pr_number": "45195", "files_changed": ["torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h"], "labels": []}, "8e0fc711f4": {"title": "[TensorExpr] Remove unused EvalConstExpr function (#45180)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45180\n\nTest Plan: build\n\nReviewed By: ezyang\n\nDifferential Revision: D23877151\n\nPulled By: asuhan\n\nfbshipit-source-id: a5d4d211c1dc85e6f7045330606163a933b9474e", "pr_number": "45180", "files_changed": ["torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["oncall: jit"]}, "0495998862": {"title": "[TensorExpr] Disallow arithmetic binary operations on Bool (#44677)", "body": "Summary:\nArithmetic operations on Bool aren't fully supported in the evaluator. Moreover,\nsuch semantics can be implemented by the client code through insertion of\nexplicit casts to widen and narrow to the desired types.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44677\n\nTest Plan:\ntest_tensorexpr --gtest_filter=TensorExprTest.ExprDisallowBoolArithmetic\npython test/test_jit_fuser_te.py\n\nReviewed By: agolynski\n\nDifferential Revision: D23801412\n\nPulled By: asuhan\n\nfbshipit-source-id: fff5284e3a216655dbf5a9a64d1cb1efda271a36", "pr_number": "44677", "files_changed": ["test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/tests.h", "test/test_jit_fuser_te.py", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/types.h"], "labels": ["oncall: jit"]}, "f93ead6d37": {"title": "[quant][eagermode] Custom module support (#44835)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44835\n\nThis is for feature parity with fx graph mode quantization\n\nTest Plan: Imported from OSS\n\nReviewed By: z-a-f\n\nDifferential Revision: D23745086\n\nfbshipit-source-id: ae2fc86129f9896d5a9039b73006a4da15821307", "pr_number": "44835", "files_changed": ["test/quantization/test_quantize.py", "torch/quantization/__init__.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["fx"]}, "76c185dcca": {"title": "[TensorExpr] When lanes differ, insert Broadcast instead of Cast (#45179)", "body": "Summary:\nWe need to check if dtypes differ in scalar type or lanes to decide between\nCast and Broadcast.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45179\n\nTest Plan: test_tensorexpr --gtest_filter=TensorExprTest.SimplifyBroadcastTermExpander\n\nReviewed By: bwasti\n\nDifferential Revision: D23873316\n\nPulled By: asuhan\n\nfbshipit-source-id: ca141be67e10c2b6c5f2ff9c11e42dcfc62ac620", "pr_number": "45179", "files_changed": ["test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp"], "labels": ["oncall: jit"]}, "89c570ed0a": {"title": "Revert D23811085: gtestify dce and fuser tests", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23811085 (https://github.com/pytorch/pytorch/commit/246bd9422a1f64965ad9082798c8b17f96bc2924)\n\nOriginal commit changeset: 45008e41f239\n\nfbshipit-source-id: 94c981f565cab9b710fe52a55bbe8dbf9c179c23", "pr_number": null, "files_changed": ["test/cpp/jit/test_dce.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/tests.h"], "labels": []}, "e9aa6898ab": {"title": "Revert D23802296: gtest-ify JIT tests, through the letter c", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23802296 (https://github.com/pytorch/pytorch/commit/d2b045030eb60283b8aeeb2956c7ebe91628fece)\n\nOriginal commit changeset: 20c9798a414e\n\nfbshipit-source-id: a28d56039ca404fe94ed7572f1febd1673e3e788", "pr_number": null, "files_changed": ["test/cpp/jit/test_autodiff.cpp", "test/cpp/jit/test_class_import.cpp", "test/cpp/jit/test_class_parser.cpp", "test/cpp/jit/test_cleanup_passes.cpp", "test/cpp/jit/test_code_template.cpp", "test/cpp/jit/test_constant_pooling.cpp", "test/cpp/jit/test_create_autodiff_subgraphs.cpp", "test/cpp/jit/test_custom_class.cpp", "test/cpp/jit/test_custom_operators.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h"], "labels": []}, "27c7158166": {"title": "Remove __future__ imports for legacy Python2 supports (#45033)", "body": "Summary:\nThere is a module called `2to3` which you can target for future specifically to remove these, the directory of `caffe2` has the most redundant imports:\n\n```2to3 -f future -w caffe2```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45033\n\nReviewed By: seemethere\n\nDifferential Revision: D23808648\n\nPulled By: bugra\n\nfbshipit-source-id: 38971900f0fe43ab44a9168e57f2307580d36a38", "pr_number": "45033", "files_changed": ["caffe2/contrib/aten/aten_test.py", "caffe2/contrib/fakelowp/test/test_batchmatmul_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_batchnorm_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_deq_swish_quant_nnpi.py", "caffe2/contrib/fakelowp/test/test_fc_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_fusions.py", "caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py", "caffe2/contrib/fakelowp/test/test_int8_quant.py", "caffe2/contrib/fakelowp/test/test_layernorm_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_4bit_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp32.py", "caffe2/contrib/gloo/gloo_test.py", "caffe2/contrib/nccl/nccl_ops_test.py", "caffe2/contrib/nnpack/nnpack_ops_test.py", "caffe2/contrib/playground/AnyExp.py", "caffe2/contrib/playground/AnyExpOnTerm.py", "caffe2/contrib/playground/ModuleRegister.py", "caffe2/contrib/playground/checkpoint.py", "caffe2/contrib/playground/compute_loss.py", "caffe2/contrib/playground/compute_topk_accuracy.py", "caffe2/contrib/playground/meter.py", "caffe2/contrib/playground/module_map.py", "caffe2/contrib/playground/output_generator.py", "caffe2/contrib/playground/resnetdemo/IN1k_resnet.py", "caffe2/contrib/playground/resnetdemo/IN1k_resnet_no_test_model.py", "caffe2/contrib/playground/resnetdemo/caffe2_resnet50_default_forward.py", "caffe2/contrib/playground/resnetdemo/caffe2_resnet50_default_param_update.py", "caffe2/contrib/playground/resnetdemo/explicit_resnet_forward.py", "caffe2/contrib/playground/resnetdemo/explicit_resnet_param_update.py", "caffe2/contrib/playground/resnetdemo/gfs_IN1k.py", "caffe2/contrib/playground/resnetdemo/override_no_test_model_no_checkpoint.py", "caffe2/contrib/playground/resnetdemo/rendezvous_filestore.py", "caffe2/contrib/prof/cuda_profile_ops_test.py", "caffe2/contrib/tensorboard/tensorboard.py", "caffe2/contrib/tensorboard/tensorboard_exporter.py", "caffe2/contrib/tensorboard/tensorboard_exporter_test.py", "caffe2/contrib/tensorboard/tensorboard_test.py", "caffe2/contrib/warpctc/ctc_ops_test.py", "caffe2/core/nomnigraph/op_gen.py", "caffe2/distributed/file_store_handler_op_test.py", "caffe2/distributed/redis_store_handler_op_test.py", "caffe2/distributed/store_ops_test_util.py", "caffe2/experiments/python/SparseTransformer.py", "caffe2/experiments/python/convnet_benchmarks.py", "caffe2/experiments/python/device_reduce_sum_bench.py", "caffe2/experiments/python/funhash_op_test.py", "caffe2/experiments/python/net_construct_bench.py", "caffe2/experiments/python/sparse_funhash_op_test.py", "caffe2/experiments/python/sparse_reshape_op_test.py", "caffe2/experiments/python/tt_contraction_op_test.py", "caffe2/experiments/python/tt_pad_op_test.py", "caffe2/perfkernels/hp_emblookup_codegen.py", "caffe2/python/__init__.py", "caffe2/python/allcompare_test.py", "caffe2/python/attention.py", "caffe2/python/benchmark_generator.py", "caffe2/python/benchmarks/fused_rowwise_nbit_conversion_bench.py", "caffe2/python/benchmarks/sparse_lengths_sum_nbit_benchmark.py", "caffe2/python/binarysize.py", "caffe2/python/brew.py", "caffe2/python/brew_test.py", "caffe2/python/build.py", "caffe2/python/cached_reader.py", "caffe2/python/checkpoint.py", "caffe2/python/checkpoint_test.py", "caffe2/python/cnn.py", "caffe2/python/context.py", "caffe2/python/context_test.py", "caffe2/python/control.py", "caffe2/python/control_ops_grad.py", "caffe2/python/control_ops_grad_test.py", "caffe2/python/control_ops_util.py", "caffe2/python/control_test.py", "caffe2/python/convert.py", "caffe2/python/convert_test.py", "caffe2/python/core.py", "caffe2/python/core_gradients_test.py", "caffe2/python/core_test.py", "caffe2/python/crf.py", "caffe2/python/crf_predict.py", "caffe2/python/crf_viterbi_test.py", "caffe2/python/data_parallel_model.py", "caffe2/python/data_parallel_model_test.py", "caffe2/python/data_workers.py", "caffe2/python/data_workers_test.py", "caffe2/python/dataio.py", "caffe2/python/dataio_test.py", "caffe2/python/dataset.py", "caffe2/python/db_file_reader.py", "caffe2/python/db_test.py", "caffe2/python/docs/formatter.py", "caffe2/python/docs/generator.py", "caffe2/python/docs/github.py", "caffe2/python/docs/parser.py", "caffe2/python/dyndep.py", "caffe2/python/embedding_generation_benchmark.py", "caffe2/python/examples/char_rnn.py", "caffe2/python/examples/lmdb_create_example.py", "caffe2/python/experiment_util.py", "caffe2/python/extension_loader.py", "caffe2/python/fakefp16_transform_lib.py", "caffe2/python/fakelowp/init_shared_libs.py", "caffe2/python/fakelowp/test_utils.py", "caffe2/python/filler_test.py", "caffe2/python/functional.py", "caffe2/python/functional_test.py", "caffe2/python/fused_8bit_rowwise_conversion_ops_test.py", "caffe2/python/gradient_check_test.py", "caffe2/python/gradient_checker.py", "caffe2/python/gru_cell.py", "caffe2/python/helpers/algebra.py", "caffe2/python/helpers/arg_scope.py", "caffe2/python/helpers/array_helpers.py", "caffe2/python/helpers/control_ops.py", "caffe2/python/helpers/conv.py", "caffe2/python/helpers/db_input.py", "caffe2/python/helpers/dropout.py", "caffe2/python/helpers/elementwise_linear.py", "caffe2/python/helpers/fc.py", "caffe2/python/helpers/nonlinearity.py", "caffe2/python/helpers/normalization.py", "caffe2/python/helpers/pooling.py", "caffe2/python/helpers/tools.py", "caffe2/python/helpers/train.py", "caffe2/python/hip_test_util.py", "caffe2/python/hsm_util.py", "caffe2/python/hypothesis_test.py", "caffe2/python/hypothesis_test_util.py", "caffe2/python/ideep/LRN_op_test.py", "caffe2/python/ideep/adam_op_test.py", "caffe2/python/ideep/blobs_queue_db_test.py", "caffe2/python/ideep/channel_shuffle_op_test.py", "caffe2/python/ideep/concat_split_op_test.py", "caffe2/python/ideep/conv_op_test.py", "caffe2/python/ideep/conv_transpose_test.py", "caffe2/python/ideep/convfusion_op_test.py", "caffe2/python/ideep/copy_op_test.py", "caffe2/python/ideep/dropout_op_test.py", "caffe2/python/ideep/elementwise_sum_op_test.py", "caffe2/python/ideep/expanddims_squeeze_op_test.py", "caffe2/python/ideep/fc_op_test.py", "caffe2/python/ideep/leaky_relu_op_test.py", "caffe2/python/ideep/moment_sgd_op_test.py", "caffe2/python/ideep/operator_fallback_op_test.py", "caffe2/python/ideep/order_switch_op_test.py", "caffe2/python/ideep/pool_op_test.py", "caffe2/python/ideep/pre_convert_test.py", "caffe2/python/ideep/relu_op_test.py", "caffe2/python/ideep/reshape_op_test.py", "caffe2/python/ideep/shape_op_test.py", "caffe2/python/ideep/sigmoid_op_test.py", "caffe2/python/ideep/softmax_op_test.py", "caffe2/python/ideep/spatial_bn_op_test.py", "caffe2/python/ideep/test_ideep_net.py", "caffe2/python/ideep/transform_ideep_net.py", "caffe2/python/ideep/transpose_op_test.py", "caffe2/python/ideep/weightedsum_op_test.py", "caffe2/python/ideep_test_util.py", "caffe2/python/layer_model_helper.py", "caffe2/python/layer_model_instantiator.py", "caffe2/python/layer_parameter_sharing_test.py", "caffe2/python/layer_test_util.py", "caffe2/python/layers/__init__.py", "caffe2/python/layers/adaptive_weight.py", "caffe2/python/layers/add_bias.py", "caffe2/python/layers/arc_cosine_feature_map.py", "caffe2/python/layers/batch_huber_loss.py", "caffe2/python/layers/batch_lr_loss.py", "caffe2/python/layers/batch_mse_loss.py", "caffe2/python/layers/batch_normalization.py", "caffe2/python/layers/batch_sigmoid_cross_entropy_loss.py", "caffe2/python/layers/batch_softmax_loss.py", "caffe2/python/layers/blob_weighted_sum.py", "caffe2/python/layers/bpr_loss.py", "caffe2/python/layers/bucket_weighted.py", "caffe2/python/layers/build_index.py", "caffe2/python/layers/concat.py", "caffe2/python/layers/constant_weight.py", "caffe2/python/layers/conv.py", "caffe2/python/layers/dropout.py", "caffe2/python/layers/fc.py", "caffe2/python/layers/fc_with_bootstrap.py", "caffe2/python/layers/fc_without_bias.py", "caffe2/python/layers/feature_sparse_to_dense.py", "caffe2/python/layers/functional.py", "caffe2/python/layers/gather_record.py", "caffe2/python/layers/homotopy_weight.py", "caffe2/python/layers/label_smooth.py", "caffe2/python/layers/last_n_window_collector.py", "caffe2/python/layers/layer_normalization.py", "caffe2/python/layers/layers.py", "caffe2/python/layers/margin_rank_loss.py", "caffe2/python/layers/merge_id_lists.py", "caffe2/python/layers/pairwise_similarity.py", "caffe2/python/layers/position_weighted.py", "caffe2/python/layers/random_fourier_features.py", "caffe2/python/layers/reservoir_sampling.py", "caffe2/python/layers/sampling_train.py", "caffe2/python/layers/sampling_trainable_mixin.py", "caffe2/python/layers/select_record_by_context.py", "caffe2/python/layers/semi_random_features.py", "caffe2/python/layers/sparse_dropout_with_replacement.py", "caffe2/python/layers/sparse_feature_hash.py", "caffe2/python/layers/sparse_lookup.py", "caffe2/python/layers/split.py", "caffe2/python/layers/tags.py", "caffe2/python/layers/uniform_sampling.py", "caffe2/python/layers_test.py", "caffe2/python/lazy_dyndep.py", "caffe2/python/lazy_dyndep_test.py", "caffe2/python/lengths_reducer_fused_8bit_rowwise_ops_test.py", "caffe2/python/lengths_reducer_rowwise_8bit_ops_test.py", "caffe2/python/lstm_benchmark.py", "caffe2/python/memonger.py", "caffe2/python/memonger_test.py", "caffe2/python/mkl/mkl_LRN_op_test.py", "caffe2/python/mkl/mkl_LRN_speed_test.py", "caffe2/python/mkl/mkl_concat_op_test.py", "caffe2/python/mkl/mkl_conv_op_test.py", "caffe2/python/mkl/mkl_copy_op_test.py", "caffe2/python/mkl/mkl_elementwise_add_op_test.py", "caffe2/python/mkl/mkl_elementwise_sum_op_test.py", "caffe2/python/mkl/mkl_fc_op_test.py", "caffe2/python/mkl/mkl_fc_speed_test.py", "caffe2/python/mkl/mkl_fill_op_test.py", "caffe2/python/mkl/mkl_pool_op_test.py", "caffe2/python/mkl/mkl_pool_speed_test.py", "caffe2/python/mkl/mkl_relu_op_test.py", "caffe2/python/mkl/mkl_sbn_op_test.py", "caffe2/python/mkl/mkl_sbn_speed_test.py", "caffe2/python/mkl/mkl_sigmoid_op_test.py", "caffe2/python/mkl/mkl_speed_test.py", "caffe2/python/mkl/mkl_squeeze_op_test.py", "caffe2/python/mkl/rewrite_graph.py", "caffe2/python/mkl/rewrite_graph_test.py", "caffe2/python/mkl_test_util.py", "caffe2/python/model_helper.py", "caffe2/python/model_helper_test.py", "caffe2/python/modeling/compute_histogram_for_blobs.py", "caffe2/python/modeling/compute_histogram_for_blobs_test.py", "caffe2/python/modeling/compute_norm_for_blobs.py", "caffe2/python/modeling/compute_norm_for_blobs_test.py", "caffe2/python/modeling/compute_statistics_for_blobs.py", "caffe2/python/modeling/compute_statistics_for_blobs_test.py", "caffe2/python/modeling/get_entry_from_blobs.py", "caffe2/python/modeling/get_entry_from_blobs_test.py", "caffe2/python/modeling/gradient_clipping.py", "caffe2/python/modeling/gradient_clipping_test.py", "caffe2/python/modeling/initializers.py", "caffe2/python/modeling/initializers_test.py", "caffe2/python/modeling/net_modifier.py", "caffe2/python/modeling/parameter_info.py", "caffe2/python/modeling/parameter_sharing.py", "caffe2/python/modeling/parameter_sharing_test.py", "caffe2/python/models/__sym_init__.py", "caffe2/python/models/download.py", "caffe2/python/models/imagenet_trainer_test_utils.py", "caffe2/python/models/resnet.py", "caffe2/python/models/resnet_test.py", "caffe2/python/models/seq2seq/beam_search.py", "caffe2/python/models/seq2seq/seq2seq_beam_search_test.py", "caffe2/python/models/seq2seq/seq2seq_model_helper.py", "caffe2/python/models/seq2seq/seq2seq_model_helper_test.py", "caffe2/python/models/seq2seq/seq2seq_util.py", "caffe2/python/models/seq2seq/train.py", "caffe2/python/models/seq2seq/translate.py", "caffe2/python/models/shufflenet.py", "caffe2/python/models/shufflenet_test.py", "caffe2/python/modifier_context.py", "caffe2/python/net_builder.py", "caffe2/python/net_builder_test.py", "caffe2/python/net_drawer.py", "caffe2/python/net_printer.py", "caffe2/python/net_printer_test.py", "caffe2/python/nomnigraph.py", "caffe2/python/nomnigraph_test.py", "caffe2/python/nomnigraph_transformations.py", "caffe2/python/nomnigraph_transformations_test.py", "caffe2/python/normalizer.py", "caffe2/python/normalizer_context.py", "caffe2/python/normalizer_test.py", "caffe2/python/numa_benchmark.py", "caffe2/python/numa_test.py", "caffe2/python/observer_test.py", "caffe2/python/onnx/backend.py", "caffe2/python/onnx/backend_cpp_rep.py", "caffe2/python/onnx/backend_rep.py", "caffe2/python/onnx/bin/conversion.py", "caffe2/python/onnx/error.py", "caffe2/python/onnx/frontend.py", "caffe2/python/onnx/helper.py", "caffe2/python/onnx/onnxifi.py", "caffe2/python/onnx/test_onnxifi.py", "caffe2/python/onnx/tests/__init__.py", "caffe2/python/onnx/tests/c2_ref_test.py", "caffe2/python/onnx/tests/conversion_test.py", "caffe2/python/onnx/tests/helper_test.py", "caffe2/python/onnx/tests/onnx_backend_test.py", "caffe2/python/onnx/tests/ssa_test.py", "caffe2/python/onnx/tests/test_utils.py", "caffe2/python/onnx/workspace.py", "caffe2/python/operator_fp_exceptions_test.py", "caffe2/python/operator_test/activation_ops_test.py", "caffe2/python/operator_test/adadelta_test.py", "caffe2/python/operator_test/adagrad_test.py", "caffe2/python/operator_test/adagrad_test_helper.py", "caffe2/python/operator_test/adam_test.py", "caffe2/python/operator_test/affine_channel_op_test.py", "caffe2/python/operator_test/apmeter_test.py", "caffe2/python/operator_test/arg_ops_test.py", "caffe2/python/operator_test/assert_test.py", "caffe2/python/operator_test/atomic_ops_test.py", "caffe2/python/operator_test/basic_rnn_test.py", "caffe2/python/operator_test/batch_box_cox_test.py", "caffe2/python/operator_test/batch_bucketize_op_test.py", "caffe2/python/operator_test/batch_moments_op_test.py", "caffe2/python/operator_test/batch_sparse_to_dense_op_test.py", "caffe2/python/operator_test/bbox_transform_test.py", "caffe2/python/operator_test/bisect_percentile_op_test.py", "caffe2/python/operator_test/blobs_queue_db_test.py", "caffe2/python/operator_test/boolean_mask_test.py", "caffe2/python/operator_test/boolean_unmask_test.py", "caffe2/python/operator_test/box_with_nms_limit_op_test.py", "caffe2/python/operator_test/bucketize_op_test.py", "caffe2/python/operator_test/cast_op_test.py", "caffe2/python/operator_test/ceil_op_test.py", "caffe2/python/operator_test/channel_backprop_stats_op_test.py", "caffe2/python/operator_test/channel_shuffle_test.py", "caffe2/python/operator_test/channel_stats_op_test.py", "caffe2/python/operator_test/checkpoint_test.py", "caffe2/python/operator_test/clip_op_test.py", "caffe2/python/operator_test/clip_tensor_op_test.py", "caffe2/python/operator_test/collect_and_distribute_fpn_rpn_proposals_op_test.py", "caffe2/python/operator_test/concat_split_op_test.py", "caffe2/python/operator_test/conditional_test.py", "caffe2/python/operator_test/conftest.py", "caffe2/python/operator_test/conv_test.py", "caffe2/python/operator_test/conv_transpose_test.py", "caffe2/python/operator_test/copy_ops_test.py", "caffe2/python/operator_test/copy_rows_to_tensor_op_test.py", "caffe2/python/operator_test/cosine_embedding_criterion_op_test.py", "caffe2/python/operator_test/counter_ops_test.py", "caffe2/python/operator_test/crf_test.py", "caffe2/python/operator_test/cross_entropy_ops_test.py", "caffe2/python/operator_test/ctc_beam_search_decoder_op_test.py", "caffe2/python/operator_test/ctc_greedy_decoder_op_test.py", "caffe2/python/operator_test/cudnn_recurrent_test.py", "caffe2/python/operator_test/data_couple_op_test.py", "caffe2/python/operator_test/dataset_ops_test.py", "caffe2/python/operator_test/deform_conv_test.py", "caffe2/python/operator_test/dense_vector_to_id_list_op_test.py", "caffe2/python/operator_test/depthwise_3x3_conv_test.py", "caffe2/python/operator_test/detectron_keypoints.py", "caffe2/python/operator_test/distance_op_test.py", "caffe2/python/operator_test/dropout_op_test.py", "caffe2/python/operator_test/duplicate_operands_test.py", "caffe2/python/operator_test/elementwise_linear_op_test.py", "caffe2/python/operator_test/elementwise_logical_ops_test.py", "caffe2/python/operator_test/elementwise_op_broadcast_test.py", "caffe2/python/operator_test/elementwise_ops_test.py", "caffe2/python/operator_test/emptysample_ops_test.py", "caffe2/python/operator_test/enforce_finite_op_test.py", "caffe2/python/operator_test/ensure_clipped_test.py", "caffe2/python/operator_test/ensure_cpu_output_op_test.py", "caffe2/python/operator_test/erf_op_test.py", "caffe2/python/operator_test/expand_op_test.py", "caffe2/python/operator_test/fc_operator_test.py", "caffe2/python/operator_test/feature_maps_ops_test.py", "caffe2/python/operator_test/filler_ops_test.py", "caffe2/python/operator_test/find_op_test.py", "caffe2/python/operator_test/flatten_op_test.py", "caffe2/python/operator_test/flexible_top_k_test.py", "caffe2/python/operator_test/floor_op_test.py", "caffe2/python/operator_test/fused_nbit_rowwise_conversion_ops_test.py", "caffe2/python/operator_test/fused_nbit_rowwise_test_helper.py", "caffe2/python/operator_test/gather_ops_test.py", "caffe2/python/operator_test/gather_ranges_op_test.py", "caffe2/python/operator_test/given_tensor_byte_string_to_uint8_fill_op_test.py", "caffe2/python/operator_test/given_tensor_fill_op_test.py", "caffe2/python/operator_test/glu_op_test.py", "caffe2/python/operator_test/group_conv_test.py", "caffe2/python/operator_test/group_norm_op_test.py", "caffe2/python/operator_test/gru_test.py", "caffe2/python/operator_test/heatmap_max_keypoint_op_test.py", "caffe2/python/operator_test/hsm_test.py", "caffe2/python/operator_test/hyperbolic_ops_test.py", "caffe2/python/operator_test/im2col_col2im_test.py", "caffe2/python/operator_test/image_input_op_test.py", "caffe2/python/operator_test/index_hash_ops_test.py", "caffe2/python/operator_test/index_ops_test.py", "caffe2/python/operator_test/instance_norm_test.py", "caffe2/python/operator_test/integral_image_ops_test.py", "caffe2/python/operator_test/jsd_ops_test.py", "caffe2/python/operator_test/key_split_ops_test.py", "caffe2/python/operator_test/lars_test.py", "caffe2/python/operator_test/layer_norm_op_test.py", "caffe2/python/operator_test/leaky_relu_test.py", "caffe2/python/operator_test/learning_rate_adaption_op_test.py", "caffe2/python/operator_test/learning_rate_op_test.py", "caffe2/python/operator_test/length_split_op_test.py", "caffe2/python/operator_test/lengths_pad_op_test.py", "caffe2/python/operator_test/lengths_reducer_fused_nbit_rowwise_ops_test.py", "caffe2/python/operator_test/lengths_tile_op_test.py", "caffe2/python/operator_test/lengths_top_k_ops_test.py", "caffe2/python/operator_test/listwise_l2r_operator_test.py", "caffe2/python/operator_test/load_save_test.py", "caffe2/python/operator_test/locally_connected_op_test.py", "caffe2/python/operator_test/loss_ops_test.py", "caffe2/python/operator_test/lpnorm_op_test.py", "caffe2/python/operator_test/map_ops_test.py", "caffe2/python/operator_test/margin_ranking_criterion_op_test.py", "caffe2/python/operator_test/math_ops_test.py", "caffe2/python/operator_test/matmul_op_test.py", "caffe2/python/operator_test/mean_op_test.py", "caffe2/python/operator_test/merge_id_lists_op_test.py", "caffe2/python/operator_test/mkl_conv_op_test.py", "caffe2/python/operator_test/mkl_packed_fc_op_test.py", "caffe2/python/operator_test/mod_op_test.py", "caffe2/python/operator_test/moments_op_test.py", "caffe2/python/operator_test/momentum_sgd_test.py", "caffe2/python/operator_test/mpi_test.py", "caffe2/python/operator_test/mul_gradient_benchmark.py", "caffe2/python/operator_test/negate_gradient_op_test.py", "caffe2/python/operator_test/ngram_ops_test.py", "caffe2/python/operator_test/normalize_op_test.py", "caffe2/python/operator_test/numpy_tile_op_test.py", "caffe2/python/operator_test/one_hot_ops_test.py", "caffe2/python/operator_test/onnx_while_test.py", "caffe2/python/operator_test/order_switch_test.py", "caffe2/python/operator_test/pack_ops_test.py", "caffe2/python/operator_test/pack_rnn_sequence_op_test.py", "caffe2/python/operator_test/pad_test.py", "caffe2/python/operator_test/partition_ops_test.py", "caffe2/python/operator_test/percentile_op_test.py", "caffe2/python/operator_test/piecewise_linear_transform_test.py", "caffe2/python/operator_test/pooling_test.py", "caffe2/python/operator_test/prepend_dim_test.py", "caffe2/python/operator_test/python_op_test.py", "caffe2/python/operator_test/quantile_test.py", "caffe2/python/operator_test/rand_quantization_op_speed_test.py", "caffe2/python/operator_test/rand_quantization_op_test.py", "caffe2/python/operator_test/rank_loss_operator_test.py", "caffe2/python/operator_test/rebatching_queue_test.py", "caffe2/python/operator_test/record_queue_test.py", "caffe2/python/operator_test/recurrent_net_executor_test.py", "caffe2/python/operator_test/recurrent_network_test.py", "caffe2/python/operator_test/reduce_ops_test.py", "caffe2/python/operator_test/reduction_ops_test.py", "caffe2/python/operator_test/reshape_ops_test.py", "caffe2/python/operator_test/resize_op_test.py", "caffe2/python/operator_test/rmac_regions_op_test.py", "caffe2/python/operator_test/rms_norm_op_test.py", "caffe2/python/operator_test/rnn_cell_test.py", "caffe2/python/operator_test/roi_align_rotated_op_test.py", "caffe2/python/operator_test/rowwise_counter_test.py", "caffe2/python/operator_test/scale_op_test.py", "caffe2/python/operator_test/segment_ops_test.py", "caffe2/python/operator_test/selu_op_test.py", "caffe2/python/operator_test/sequence_ops_test.py", "caffe2/python/operator_test/shape_inference_test.py", "caffe2/python/operator_test/sinusoid_position_encoding_op_test.py", "caffe2/python/operator_test/softmax_ops_test.py", "caffe2/python/operator_test/softplus_op_test.py", "caffe2/python/operator_test/sparse_dropout_with_replacement_op_test.py", "caffe2/python/operator_test/sparse_gradient_checker_test.py", "caffe2/python/operator_test/sparse_lengths_sum_benchmark.py", "caffe2/python/operator_test/sparse_lp_regularizer_test.py", "caffe2/python/operator_test/sparse_normalize_test.py", "caffe2/python/operator_test/sparse_ops_test.py", "caffe2/python/operator_test/sparse_to_dense_mask_op_test.py", "caffe2/python/operator_test/spatial_bn_op_test.py", "caffe2/python/operator_test/specialized_segment_ops_test.py", "caffe2/python/operator_test/square_root_divide_op_test.py", "caffe2/python/operator_test/stats_ops_test.py", "caffe2/python/operator_test/stats_put_ops_test.py", "caffe2/python/operator_test/storm_test.py", "caffe2/python/operator_test/string_ops_test.py", "caffe2/python/operator_test/text_file_reader_test.py", "caffe2/python/operator_test/thresholded_relu_op_test.py", "caffe2/python/operator_test/tile_op_test.py", "caffe2/python/operator_test/top_k_test.py", "caffe2/python/operator_test/torch_integration_test.py", "caffe2/python/operator_test/transpose_op_test.py", "caffe2/python/operator_test/trigonometric_op_test.py", "caffe2/python/operator_test/unique_ops_test.py", "caffe2/python/operator_test/unique_uniform_fill_op_test.py", "caffe2/python/operator_test/upsample_op_test.py", "caffe2/python/operator_test/utility_ops_test.py", "caffe2/python/operator_test/video_input_op_test.py", "caffe2/python/operator_test/weight_scale_test.py", "caffe2/python/operator_test/weighted_multi_sample_test.py", "caffe2/python/operator_test/weighted_sample_test.py", "caffe2/python/operator_test/weighted_sum_test.py", "caffe2/python/operator_test/wngrad_test.py", "caffe2/python/optimizer.py", "caffe2/python/optimizer_context.py", "caffe2/python/optimizer_test.py", "caffe2/python/optimizer_test_util.py", "caffe2/python/parallel_workers.py", "caffe2/python/parallel_workers_test.py", "caffe2/python/parallelize_bmuf_distributed_test.py", "caffe2/python/pipeline.py", "caffe2/python/pipeline_test.py", "caffe2/python/predictor/mobile_exporter.py", "caffe2/python/predictor/mobile_exporter_test.py", "caffe2/python/predictor/predictor_exporter.py", "caffe2/python/predictor/predictor_exporter_test.py", "caffe2/python/predictor/predictor_py_utils.py", "caffe2/python/predictor/predictor_test.py", "caffe2/python/predictor/serde.py", "caffe2/python/predictor_constants.py", "caffe2/python/python_op_test.py", "caffe2/python/queue_util.py", "caffe2/python/record_queue.py", "caffe2/python/recurrent.py", "caffe2/python/regularizer.py", "caffe2/python/regularizer_context.py", "caffe2/python/regularizer_test.py", "caffe2/python/rnn/__init__.py", "caffe2/python/rnn/lstm_comparison.py", "caffe2/python/rnn/rnn_cell_test_util.py", "caffe2/python/rnn_cell.py", "caffe2/python/schema.py", "caffe2/python/schema_test.py", "caffe2/python/scope.py", "caffe2/python/scope_test.py", "caffe2/python/serialized_test/coverage.py", "caffe2/python/serialized_test/serialized_test_util.py", "caffe2/python/session.py", "caffe2/python/session_test.py", "caffe2/python/sparse_to_dense_mask_test.py", "caffe2/python/sparse_to_dense_test.py", "caffe2/python/task.py", "caffe2/python/task_test.py", "caffe2/python/test/blob_deallocation_test.py", "caffe2/python/test/do_op_test.py", "caffe2/python/test/executor_test.py", "caffe2/python/test/executor_test_util.py", "caffe2/python/test/fakefp16_transform_test.py", "caffe2/python/test/gpu_context_test.py", "caffe2/python/test/python_protobuf_test.py", "caffe2/python/test_util.py", "caffe2/python/text_file_reader.py", "caffe2/python/timeout_guard.py", "caffe2/python/transformations.py", "caffe2/python/transformations_test.py", "caffe2/python/trt/test_trt.py", "caffe2/python/trt/transform.py", "caffe2/python/tt_core.py", "caffe2/python/tt_core_test.py", "caffe2/python/utils.py", "caffe2/python/utils_test.py", "caffe2/python/workspace.py", "caffe2/python/workspace_test.py", "caffe2/quantization/server/batch_matmul_dnnlowp_op_test.py", "caffe2/quantization/server/batch_permutation_dnnlowp_op_test.py", "caffe2/quantization/server/channel_shuffle_dnnlowp_op_test.py", "caffe2/quantization/server/concat_dnnlowp_op_test.py", "caffe2/quantization/server/conv_depthwise_dnnlowp_op_test.py", "caffe2/quantization/server/conv_dnnlowp_acc16_op_test.py", "caffe2/quantization/server/conv_dnnlowp_op_test.py", "caffe2/quantization/server/conv_groupwise_dnnlowp_acc16_op_test.py", "caffe2/quantization/server/conv_groupwise_dnnlowp_op_test.py", "caffe2/quantization/server/dequantize_dnnlowp_op_test.py", "caffe2/quantization/server/dnnlowp_test_utils.py", "caffe2/quantization/server/elementwise_add_dnnlowp_op_test.py", "caffe2/quantization/server/elementwise_linear_dnnlowp_op_test.py", "caffe2/quantization/server/elementwise_mul_dnnlowp_op_test.py", "caffe2/quantization/server/elementwise_sum_dnnlowp_op_test.py", "caffe2/quantization/server/fully_connected_dnnlowp_acc16_op_test.py", "caffe2/quantization/server/fully_connected_dnnlowp_op_test.py", "caffe2/quantization/server/fully_connected_fp16_test.py", "caffe2/quantization/server/fully_connected_rowwise_dnnlowp_op_test.py", "caffe2/quantization/server/gather_dnnlowp_op_test.py", "caffe2/quantization/server/group_norm_dnnlowp_op_test.py", "caffe2/quantization/server/int8_gen_quant_params_test.py", "caffe2/quantization/server/int8_quant_scheme_blob_fill_test.py", "caffe2/quantization/server/lstm_unit_dnnlowp_op_test.py", "caffe2/quantization/server/observer_test.py", "caffe2/quantization/server/pool_dnnlowp_op_test.py", "caffe2/quantization/server/quantize_dnnlowp_op_test.py", "caffe2/quantization/server/relu_dnnlowp_op_test.py", "caffe2/quantization/server/resize_nearest_3d_dnnlowp_op_test.py", "caffe2/quantization/server/resize_nearest_dnnlowp_op_test.py", "caffe2/quantization/server/sigmoid_dnnlowp_op_test.py", "caffe2/quantization/server/spatial_batch_norm_dnnlowp_op_test.py", "caffe2/quantization/server/tanh_dnnlowp_op_test.py", "caffe2/quantization/server/utils.py", "scripts/get_python_cmake_flags.py", "setup.py", "tools/amd_build/build_amd.py", "tools/autograd/gen_variable_type.py", "tools/clang_tidy.py", "tools/pyi/gen_pyi.py", "tools/setup_helpers/cmake.py"], "labels": []}, "721cfbf842": {"title": "[PT Model Split] Support 2 operators in PT by C2 conversion (#45231)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45231\n\nThere are two operators:\n`PriorCorrectionCalibrationPrediction` and `GatherRangesToDense` is not supported in PT which makes GLOW cannot work.\n\nTo unblock, we first try to use C2->PT conversion. In the long-term, we need to implement PT custom ops.\n\nThis diff does this conversion to unblock current project.\n\nTest Plan:\nRun unit test. the Test input is from current DPER example.\nAll pass.\n```buck test //caffe2/caffe2/python/operator_test:torch_integration_test -- test_prior_correct_calibration_prediction_op  --print-passing-details\n\n> c2 reference output\n> [0.14285715 0.27272728 0.39130434 0.5 ]\n\n> PT converted output\n> tensor([0.1429, 0.2727, 0.3913, 0.5000])\n\nbuck test //caffe2/caffe2/python/operator_test:torch_integration_test -- test_gather_ranges_to_dense_op  --print-passing-details\n\nc2 reference output\n> [array([[6, 5, 4, 3], [0, 0, 0, 0]], dtype=int64)]\n\n> PT converted output\n> [tensor([[6, 5, 4, 3], [0, 0, 0, 0]])]\n```\n\nReviewed By: allwu, qizzzh\n\nDifferential Revision: D23858329\n\nfbshipit-source-id: ed37118ca7f09e1cd0ad1fdec3d37f66dce60dd9", "pr_number": "45231", "files_changed": ["caffe2/operators/gather_ranges_to_dense_op.cc", "caffe2/operators/gather_ranges_to_dense_op.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": ["fb-exported"]}, "60665ace17": {"title": "[quant] Add optimized approach to calculate qparams for qembedding_bag (#45149)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45149\n\nThe choose_qparams_optimized calculates the the optimized qparams.\nIt uses a greedy approach to nudge the min and max and calculate the l2 norm\n  and tries to minimize the quant error by doing `torch.norm(x-fake_quant(x,s,z))`\n\nTest Plan: Imported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23848060\n\nfbshipit-source-id: c6c57c9bb07664c3f1c87dd7664543e09f634aee", "pr_number": "45149", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/QTensor.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py", "tools/autograd/gen_python_functions.py", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp", "torch/overrides.py"], "labels": []}, "c760bc8fb1": {"title": "Add GlowLoadAOTModel flag (#45189)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45189\n\nPull Request resolved: https://github.com/pytorch/glow/pull/4902\n\nTest Plan: Test locally\n\nReviewed By: yinghai\n\nDifferential Revision: D23810445\n\nfbshipit-source-id: 56e717d80abbfe76b15d0f4249e1e399a9722753", "pr_number": "45189", "files_changed": ["caffe2/opt/onnxifi_op.h", "third_party/foxi"], "labels": ["fb-exported"]}, "2d00ebd29f": {"title": "Failing test demonstrating problems with mixed output shapes (#44455)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44455\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23886119\n\nPulled By: bertmaher\n\nfbshipit-source-id: 41787930f154cf4e8a1766613c4cf33b18246555", "pr_number": "44455", "files_changed": ["test/test_jit_fuser_te.py"], "labels": []}, "956a25d061": {"title": "Revert D23858329: [PT Model Split] Support 2 operators in PT by C2 conversion", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23858329 (https://github.com/pytorch/pytorch/commit/721cfbf8425cf2c1dc5e27d1332e32e1a42ef541)\n\nOriginal commit changeset: ed37118ca7f0\n\nfbshipit-source-id: 30c700f80665be11afc608b00a77766064e60b35", "pr_number": null, "files_changed": ["caffe2/operators/gather_ranges_to_dense_op.cc", "caffe2/operators/gather_ranges_to_dense_op.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": []}, "070fe15e4c": {"title": "Add link to profiling recipe from rpc main docs (#45235)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45235\n\nThis is so that users know that the profiler works as expected with\nRPC and they can learn how to use it to profile RPC-based workloads.\nghstack-source-id: 112773748\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D23777888\n\nfbshipit-source-id: 4805be9b949c8c7929182f291a6524c3c6a725c1", "pr_number": "45235", "files_changed": ["docs/source/rpc.rst"], "labels": []}, "6a2e9eb51c": {"title": "torch.fft: Multi-dimensional transforms (#44550)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44550\n\nPart of the `torch.fft` work (gh-42175).\nThis adds n-dimensional transforms: `fftn`, `ifftn`, `rfftn` and `irfftn`.\n\nThis is aiming for correctness first, with the implementation on top of the existing `_fft_with_size` restrictions. I plan to follow up later with a more efficient rewrite that makes `_fft_with_size` work with arbitrary numbers of dimensions.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23846032\n\nPulled By: mruberry\n\nfbshipit-source-id: e6950aa8be438ec5cb95fb10bd7b8bc9ffb7d824", "pr_number": "44550", "files_changed": ["aten/src/ATen/WrapDimUtils.h", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/fft.rst", "test/test_spectral_ops.py", "torch/csrc/api/include/torch/fft.h", "torch/fft/__init__.py"], "labels": ["open source"]}, "0b6b735863": {"title": "[fix] type promotion atan2 (#43466)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43360\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43466\n\nReviewed By: malfet\n\nDifferential Revision: D23834928\n\nPulled By: mruberry\n\nfbshipit-source-id: 2e7e0b4fcf1a846efc171c275d65a6daffd3c631", "pr_number": "43466", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "test/test_torch.py", "test/test_type_promotion.py"], "labels": ["open source", "triaged"]}, "b470fa4500": {"title": "Add complex number support for binary logical operators (#43174)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43174\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23684425\n\nPulled By: mruberry\n\nfbshipit-source-id: 4857b16e18ec4c65327136badd7f04c74e32d330", "pr_number": "43174", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu", "c10/util/complex.h", "test/test_torch.py"], "labels": ["module: complex", "open source", "triaged"]}, "3dd0e362db": {"title": "[TensorExpr] Fix min and max for integral inputs in CUDA backend (#44984)", "body": "Summary:\nFor integral types, isnan is meaningless. Provide specializations for\nmaximum and minimum which don't call it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44984\n\nTest Plan: python test/test_jit_fuser_te.py -k TestTEFuser.test_minmax_int_ops\n\nReviewed By: ezyang\n\nDifferential Revision: D23885259\n\nPulled By: asuhan\n\nfbshipit-source-id: 2e6da2c43c0ed18f0b648a2383d510894c574437", "pr_number": "44984", "files_changed": ["test/test_jit_fuser_te.py", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["oncall: jit"]}, "b3d7c2f978": {"title": "[ONNX] Update ONNX docs for release (#45086)", "body": "Summary:\nONNX doc updates.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45086\n\nReviewed By: ezyang\n\nDifferential Revision: D23880383\n\nPulled By: bzinodev\n\nfbshipit-source-id: ca29782fd73024967ee7708c217a005233e7b970", "pr_number": "45086", "files_changed": ["docs/source/onnx.rst"], "labels": ["open source"]}, "29dc3c5ec8": {"title": "Sparse softmax support (CUDA) (#42307)", "body": "Summary:\nThis PR implements softmax support for sparse tensors.\n\nResolves gh-23651 for CUDA.\n\n- [x]  sparse softmax\n    - [x]  CUDA C++ implementation\n    - [x]  unittests\n    - [x]  update softmax documentation\n    - [x]  autograd support\n- [x]  sparse log_softmax\n    - [x]  CUDA C++ implementation\n    - [x]  unittests\n    - [x]  update log_softmax documentation\n    - [x]  autograd support\n\nHere are some benchmark (script is [here](https://gist.github.com/aocsa/fbc1827b3e49901512a33ba96092cbc1)) results for `torch.sparse.softmax and torch.softmax`,  using CPU and GPU, values are float64 scalars, timing repeat is 1000:\n\n| size         | density | sparse CUDA | sparse CPU |\n|--------------|---------|-------------|------------|\n|  (32, 10000) |   0.01  |    380.2    |    687.5   |\n| (32, 10000)  | 0.05    | 404.3       | 2357.9     |\n| (32, 10000)  | 0.1     | 405.9       | 3677.2     |\n| (512, 10000) | 0.01    | 438.0       | 5443.4     |\n| (512, 10000) | 0.05    | 888.1       | 24485.0    |\n| (512, 10000) | 0.1     | 1921.3      | 45340.5    |\n\n| size         | density | dense CUDA | dense CPU |\n|--------------|---------|-------------|------------|\n|  (32, 10000) |   0.01  |     23.6    |   1943.2   |\n| (32, 10000)  | 0.05    | 23.6        | 1954.0     |\n| (32, 10000)  | 0.1     | 23.5        | 1950.0     |\n| (512, 10000) | 0.01    | 639.3       | 39797.9    |\n| (512, 10000) | 0.05    | 640.3       | 39374.4    |\n| (512, 10000) | 0.1     | 639.6       | 39192.3    |\n\nTimes are in microseconds (us).\n\nQuick note:  I updated the performance test again.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42307\n\nReviewed By: ngimel\n\nDifferential Revision: D23774427\n\nPulled By: mruberry\n\nfbshipit-source-id: bfabf726075b39dde544c10249f27ae1871f82c7", "pr_number": "42307", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/ParamUtils.cpp", "aten/src/ATen/native/sparse/ParamUtils.h", "aten/src/ATen/native/sparse/SoftMax.cpp", "aten/src/ATen/native/sparse/cuda/SoftMax.cu", "test/test_sparse.py"], "labels": ["module: cuda", "module: sparse", "open source", "triaged"]}, "6d21d5f0b3": {"title": "gtest-ify JIT tests, through the letter c (#45249)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45249\n\nReland of https://github.com/pytorch/pytorch/pull/45055 and\nhttps://github.com/pytorch/pytorch/pull/45020\n\nSee https://github.com/pytorch/pytorch/pull/45018 for context.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23892645\n\nPulled By: suo\n\nfbshipit-source-id: e7fe58d5e1a5a0c44f4e2aec9694145afabde0fd", "pr_number": "45249", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_autodiff.cpp", "test/cpp/jit/test_class_import.cpp", "test/cpp/jit/test_class_parser.cpp", "test/cpp/jit/test_cleanup_passes.cpp", "test/cpp/jit/test_code_template.cpp", "test/cpp/jit/test_constant_pooling.cpp", "test/cpp/jit/test_create_autodiff_subgraphs.cpp", "test/cpp/jit/test_custom_class.cpp", "test/cpp/jit/test_custom_class_registrations.cpp", "test/cpp/jit/test_custom_class_registrations.h", "test/cpp/jit/test_custom_operators.cpp", "test/cpp/jit/test_dce.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h"], "labels": ["oncall: jit"]}, "dc67b47bc9": {"title": "Deprecate old fft functions (#44876)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44876\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23866715\n\nPulled By: mruberry\n\nfbshipit-source-id: 73305eb02f92cbd1ef7d175419529d19358fedda", "pr_number": "44876", "files_changed": ["aten/src/ATen/native/SpectralOps.cpp", "docs/source/fft.rst", "torch/_torch_docs.py"], "labels": ["open source", "triaged"]}, "bea7901e38": {"title": "Enable torch.tensor typechecks (#45077)", "body": "Summary:\nthis fixes https://github.com/pytorch/pytorch/issues/42983.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45077\n\nReviewed By: ezyang\n\nDifferential Revision: D23842493\n\nPulled By: walterddr\n\nfbshipit-source-id: 1c516a5ff351743a187d00cba7ed0be11678edf1", "pr_number": "45077", "files_changed": ["mypy.ini", "tools/pyi/gen_pyi.py", "torch/_C/__init__.pyi.in", "torch/tensor.py", "torch/types.py"], "labels": []}, "71d1b5b0e2": {"title": "Add foreach APIs for binary ops with ScalarList (#44743)", "body": "Summary:\nIn this PR:\n1) Added binary operations with ScalarLists.\n2) Fixed _foreach_div(...) bug in native_functions\n3) Covered all possible cases with scalars and scalar lists in tests\n4) [minor] fixed bug in native_functions by adding \"use_c10_dispatcher: full\" to all _foreach functions\n\ntested via unit tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44743\n\nReviewed By: bwasti, malfet\n\nDifferential Revision: D23753711\n\nPulled By: izdeby\n\nfbshipit-source-id: bf3e8c54bc07867e8f6e82b5d3d35ff8e99b5a0a", "pr_number": "44743", "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu", "aten/src/ATen/native/cuda/ForeachFunctors.cuh", "aten/src/ATen/native/cuda/MultiTensorApply.cuh", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_foreach.py", "test/test_native_functions.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "tools/codegen/model.py", "tools/pyi/gen_pyi.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": []}, "bc591d76a1": {"title": "add skip_if_rocm to all requires_nccl tests (#45158)", "body": "Summary:\nrequires_nccl annotation should skip_if_rocm as well\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45158\n\nReviewed By: seemethere\n\nDifferential Revision: D23879952\n\nPulled By: walterddr\n\nfbshipit-source-id: 818fb31ab75d5f02e77fe3f1367faf748855bee7", "pr_number": "45158", "files_changed": ["test/distributed/algorithms/ddp_comm_hooks/test_ddp_hooks.py", "test/distributed/test_c10d.py", "torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py"], "labels": []}, "f9ae296a85": {"title": "renaming TestDdpCommHook class so it doesn't get picked up as a test by pytest (#44905)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44905\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D23825308\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 17a07b3bd211850d6ecca793fd9ef3f326ca9274", "pr_number": "44905", "files_changed": ["test/distributed/test_c10d.py"], "labels": []}, "5195d727b5": {"title": "adding a test for ddp save()/load() (#44906)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44906\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D23825386\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 2276e6e030ef9cffd78fc78c2ffe34d60a1e160e", "pr_number": "44906", "files_changed": ["test/distributed/test_c10d.py"], "labels": []}, "bfdf4323ac": {"title": "Bump up NCCL to 2.7.8 (#45251)", "body": "Summary:\nUse latest NCCL\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45251\n\nReviewed By: mingzhe09088\n\nDifferential Revision: D23893064\n\nPulled By: mrshenli\n\nfbshipit-source-id: 820dd166039e61a5aa59b4c5bbc615a7b18be8c3", "pr_number": "45251", "files_changed": ["third_party/nccl/nccl"], "labels": []}, "8507ea22b2": {"title": "replace timer test with a mocked variant (#45173)", "body": "Summary:\nI noticed that the recently introduced adaptive_autorange tests occasionally timeout CI, and I've been meaning to improve the Timer tests for a while. This PR allows unit tests to swap the measurement portion of `Timer` with a deterministic mock so we can thoroughly test behavior without having to worry about flaky CI measurements. It also means that the tests can be much more detailed and still finish very quickly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45173\n\nTest Plan: You're lookin' at it.\n\nReviewed By: ezyang\n\nDifferential Revision: D23873548\n\nPulled By: robieta\n\nfbshipit-source-id: 26113e5cea0cbf46909b9bf5e90c878c29e87e88", "pr_number": "45173", "files_changed": ["test/test_utils.py", "torch/utils/_benchmark/utils/timer.py"], "labels": []}, "2b38c09f69": {"title": "Moves prim ops from C10 back to JIT (#45144)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45144\n\nMoves prim ops from C10 back to JIT.\n\nThese were originally moved to C10 from JIT in D19237648 (https://github.com/pytorch/pytorch/commit/f362cd510dcedbf7384d418aad60e0ba963baeb6)\nghstack-source-id: 112775781\n\nTest Plan:\nbuck test //caffe2/test/cpp/jit:jit\n\nhttps://pxl.cl/1l22N\n\nbuck test adsatlas/gavel/lib/ata_processor/tests:ata_processor_test\n\nhttps://pxl.cl/1lBxD\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23697598\n\nfbshipit-source-id: 36d1eb8c346e9b161ba6af537a218440a9bafd27", "pr_number": "45144", "files_changed": ["aten/src/ATen/templates/TypeDefault.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_c10.cpp"], "labels": ["oncall: jit"]}, "e57a08119b": {"title": "Add a warning log when there is high skew of uneven inputs in DDP training (#45238)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45238\n\nAdds a warning when there is much higher than expected amount of\ndiscrepancy of inputs across different processes when running with uneven\ninputs. This is because a skew in the thousands can reduce performance a\nnontrivial amount as shown in benchmarks, and it was proposed to add this\nwarning as a result. Tested by running the tests so the threshold is hit and\nobserving the output.\nghstack-source-id: 112773552\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D23719270\n\nfbshipit-source-id: 306264f62c1de65e733696a912bdb6e9376d5622", "pr_number": "45238", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": []}, "b8eab8cdbd": {"title": "[hotfix] typo in NaiveConvolutionTranspose2d.cu (#45224)", "body": "Summary:\nFixes typo in e2f49c8\nFixes https://github.com/pytorch/pytorch/issues/45172\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45224\n\nReviewed By: ezyang\n\nDifferential Revision: D23879872\n\nPulled By: walterddr\n\nfbshipit-source-id: c3db6d4c6f2ac0e6887862d4217a79c030647cb9", "pr_number": "45224", "files_changed": ["aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "test/test_nn.py"], "labels": []}, "3f5eee666c": {"title": "Adjust TF32 tests (#44240)", "body": "Summary:\n- The thresholds of some tests are bumped up. Depending on the random generator, sometimes these tests fail with things like 0.0059 is not smaller than 0.005. I ran `test_nn.py` and `test_torch.py` for 10+ times to check these are no longer flaky.\n- Add `tf32_on_and_off` to new `matrix_exp` tests.\n- Disable TF32 on test suites other than `test_nn.py` and `test_torch.py`\n\ncc: ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44240\n\nReviewed By: mruberry\n\nDifferential Revision: D23882498\n\nPulled By: ngimel\n\nfbshipit-source-id: 44a9ec08802c93a2efaf4e01d7487222478b6df8", "pr_number": "44240", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CublasHandlePool.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "test/jit/test_tracer.py", "test/test_jit_fuser.py", "test/test_nn.py", "test/test_torch.py", "torch/testing/_internal/common_cuda.py", "torch/testing/_internal/common_nn.py"], "labels": ["oncall: jit", "open source", "triaged"]}, "c79d493096": {"title": "added rocm 3.8 docker image (#45205)", "body": "Summary:\njeffdaily\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45205\n\nReviewed By: malfet\n\nDifferential Revision: D23906606\n\nPulled By: walterddr\n\nfbshipit-source-id: 604a12bf4c97260215a1881cc96e35e7c42b4578", "pr_number": "45205", "files_changed": [".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_base.sh"], "labels": ["open source", "triaged"]}, "26001a2334": {"title": "Revert D23753711: [pytorch][PR] Add foreach APIs for binary ops with ScalarList", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23753711 (https://github.com/pytorch/pytorch/commit/71d1b5b0e227e407e60c0a3dd6a4caabdcd6c89a)\n\nOriginal commit changeset: bf3e8c54bc07\n\nfbshipit-source-id: 192692e0d3fff4cade9983db0a1760fedfc9674c", "pr_number": null, "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu", "aten/src/ATen/native/cuda/ForeachFunctors.cuh", "aten/src/ATen/native/cuda/MultiTensorApply.cuh", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_foreach.py", "test/test_native_functions.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "tools/codegen/model.py", "tools/pyi/gen_pyi.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": []}, "c211a9102f": {"title": "add rocm 3.8 to nightly builds (#45222)", "body": "Summary:\nCorresponding change in builder repo: https://github.com/pytorch/builder/pull/528.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45222\n\nReviewed By: ezyang\n\nDifferential Revision: D23894831\n\nPulled By: walterddr\n\nfbshipit-source-id: c6a256ec325ddcf5836b4d293f546368d58db538", "pr_number": "45222", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/cimodel/data/dimensions.py", ".circleci/config.yml"], "labels": ["module: rocm", "open source"]}, "c3a5aed5f7": {"title": "Run pytorch_core CUDA tests on GPU using TPX", "body": "Summary:\nModify contbuild to disable sanitizers, add option to run \"cuda\" test using TPX RE\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: walterddr, cspanda\n\nDifferential Revision: D23854578\n\nfbshipit-source-id: 327d7cc3655c17034a6a7bc78f69967403290623", "pr_number": null, "files_changed": ["test/test_cuda.py"], "labels": []}, "e2bcdc7b69": {"title": "[Caffe2] Fix LayerNormOp when batch_size == 0. (#45250)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45250\n\n[Caffe2] Fix LayerNormOp when batch_size == 0.\n\nTest Plan: buck test mode/dev-nosan //caffe2/caffe2/python/operator_test:layer_norm_op_test\n\nReviewed By: houseroad\n\nDifferential Revision: D23892091\n\nfbshipit-source-id: 9a34654dd6880c9d14b7111fcf850e4f48ffdf91", "pr_number": "45250", "files_changed": ["caffe2/operators/layer_norm_op.h", "caffe2/python/operator_test/layer_norm_op_test.py"], "labels": ["fb-exported"]}, "022ba5a78b": {"title": "Make ddp_comm_hook_wrapper a private method. (#44643)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44643\n\nThis method is not used anywhere else.\n\nAlso formatted the file.\n\nTest Plan: buck test caffe2/test/distributed/algorithms/ddp_comm_hooks:test_ddp_hooks\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23675945\n\nfbshipit-source-id: 2d04f94589a20913e46b8d71e6a39b70940c1461", "pr_number": "44643", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/__init__.py"], "labels": ["fb-exported"]}, "cbe1eac1f4": {"title": "[caffe2] adds Cancel to SafeDequeueBlobsOp and SafeEnqueueBlobsOp (#45177)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45177\n\n## Motivation\n* To be able to make C2 ops cancellable so we can safely exit.\n* Some C2 operators are now blocking thus being non-cancellable. If an error\noccurs we need to be able to safely stop all net execution so we can throw\nthe exception to the caller.\n\n## Summary\n* When an error occurs in a net or it got cancelled, running ops will have the\n`Cancel` method called.\nThis diff adds `Cancel` method to the `SafeEnqueueBlobsOp`\nand `SafeDequeueBlobsOp` to have the call queue->close() to force all the\nblocking ops to return.\n* Adds unit test that verified the error propagation.\n\nTest Plan:\n## Unit test added to verify that queue ops propagate errors\n\n```\nbuck test caffe2/caffe2/python:hypothesis_test -- test_safe_dequeue_blob__raises_exception_when_hang --stress-runs 1000\n```\n\n```\nSummary\n  Pass: 1000\n  ListingSuccess: 1\n```\n\nReviewed By: d4l3k\n\nDifferential Revision: D23846967\n\nfbshipit-source-id: c7ddd63259e033ed0bed9df8e1b315f87bf59394", "pr_number": "45177", "files_changed": ["caffe2/queue/queue_ops.h"], "labels": ["fb-exported"]}, "71e6ce6616": {"title": "[JIT] Specialize AutogradZero: merge AutogradAnyNonZero and Not(AutogradAnyNonZero) checks into one. (#44987)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44987\n\nThis PR introduces new `prim::AutogradAllZero` and\n`prim::AutogradAllNonZero` ops that are used for a batch check for\nmultiple tensors. The specialize-autogradzero pass now generates one\ncheck for all expected-to-be-undefined tensors, one check for all\nexpected-to-be-defined tensors, and a bunch of checks for size\nparameters passed to `grad_sum_to_size` (this probably could be cleaned\nup somehow as well in future).\n\nAn example of what we generated before this change:\n```\n%1626 : bool = prim::AutogradAnyNonZero(%0)\n%1627 : bool = prim::AutogradAnyNonZero(%2)\n%1628 : bool = aten::__not__(%1627)\n%1629 : bool = prim::AutogradAnyNonZero(%3)\n%1630 : bool = aten::__not__(%1629)\n%1631 : bool = prim::AutogradAnyNonZero(%4)\n%1632 : bool = aten::__not__(%1631)\n%1633 : bool = prim::AutogradAnyNonZero(%5)\n%1634 : bool = aten::__not__(%1633)\n%1635 : bool = prim::AutogradAnyNonZero(%6)\n%1636 : bool = aten::__not__(%1635)\n%1637 : bool = prim::AutogradAnyNonZero(%7)\n%1638 : bool = aten::__not__(%1637)\n%1639 : bool = prim::AutogradAnyNonZero(%8)\n%1640 : bool = aten::__not__(%1639)\n%1641 : bool = prim::AutogradAnyNonZero(%9)\n%1642 : bool = aten::__not__(%1641)\n%1643 : bool = prim::AutogradAnyNonZero(%10)\n%1644 : bool = aten::__not__(%1643)\n%1645 : bool = prim::AutogradAnyNonZero(%11)\n%1646 : bool = aten::__not__(%1645)\n%1647 : bool = prim::AutogradAnyNonZero(%12)\n%1648 : bool = aten::__not__(%1647)\n%1649 : bool = prim::AutogradAnyNonZero(%13)\n%1650 : bool = aten::__not__(%1649)\n%1651 : bool = prim::AutogradAnyNonZero(%14)\n%1652 : bool = aten::__not__(%1651)\n%1653 : bool = prim::AutogradAnyNonZero(%15)\n%1654 : bool = aten::__not__(%1653)\n%1655 : bool = prim::AutogradAnyNonZero(%16)\n%1656 : bool = aten::__not__(%1655)\n%1657 : bool = prim::AutogradAnyNonZero(%17)\n%1658 : bool = prim::AutogradAnyNonZero(%18)\n%1659 : bool = prim::AutogradAnyNonZero(%19)\n%1660 : bool = prim::AutogradAnyNonZero(%20)\n%1661 : bool = aten::__is__(%self_size.16, %1625)\n%1662 : bool = aten::__is__(%other_size.16, %1625)\n%1663 : bool = aten::__is__(%self_size.14, %1625)\n%1664 : bool = aten::__is__(%self_size.12, %1625)\n%1665 : bool = prim::AutogradAnyNonZero(%ingate.7)\n%1666 : bool = prim::AutogradAnyNonZero(%forgetgate.7)\n%1667 : bool = prim::AutogradAnyNonZero(%cellgate.7)\n%1668 : bool = prim::AutogradAnyNonZero(%30)\n%1669 : bool = prim::AutogradAnyNonZero(%31)\n%1670 : bool = aten::__is__(%self_size.10, %1625)\n%1671 : bool = aten::__is__(%other_size.10, %1625)\n%1672 : bool = prim::AutogradAnyNonZero(%34)\n%1673 : bool = prim::AutogradAnyNonZero(%35)\n%1674 : bool = aten::__is__(%self_size.8, %1625)\n%1675 : bool = aten::__is__(%other_size.8, %1625)\n%1676 : bool = aten::__is__(%self_size.6, %1625)\n%1677 : bool = aten::__is__(%other_size.6, %1625)\n%1678 : bool = prim::AutogradAnyNonZero(%outgate.7)\n%1679 : bool = prim::AutogradAnyNonZero(%41)\n%1680 : bool = prim::AutogradAnyNonZero(%42)\n%1681 : bool = prim::AutogradAnyNonZero(%43)\n%1682 : bool = aten::__is__(%self_size.4, %1625)\n%1683 : bool = aten::__is__(%other_size.4, %1625)\n%1684 : bool[] = prim::ListConstruct(%1626, %1628, %1630, %1632, %1634, %1636, %1638, %1640, %1642, %1644, %1646, %1648, %1650, %1652, %1654, %1656, %1657, %1658, %1659, %1660, %1661, %1662, %1663, %1664, %1665, %1666, %1667, %1668, %1669, %1670, %1671, %1672, %1673, %1674, %1675, %1676, %1677, %1678, %1679, %1680, %1681, %1682, %1683)\n%1685 : bool = aten::all(%1684)\n```\n\nSame example after this change:\n```\n%1625 : None = prim::Constant()\n%1626 : bool = aten::__is__(%self_size.16, %1625)\n%1627 : bool = aten::__is__(%other_size.16, %1625)\n%1628 : bool = aten::__is__(%self_size.14, %1625)\n%1629 : bool = aten::__is__(%self_size.12, %1625)\n%1630 : bool = aten::__is__(%self_size.10, %1625)\n%1631 : bool = aten::__is__(%other_size.10, %1625)\n%1632 : bool = aten::__is__(%self_size.8, %1625)\n%1633 : bool = aten::__is__(%other_size.8, %1625)\n%1634 : bool = aten::__is__(%self_size.6, %1625)\n%1635 : bool = aten::__is__(%other_size.6, %1625)\n%1636 : bool = aten::__is__(%self_size.4, %1625)\n%1637 : bool = aten::__is__(%other_size.4, %1625)\n%1638 : bool = prim::AutogradAllNonZero(%0, %17, %18, %19, %20, %ingate.7, %forgetgate.7, %cellgate.7, %30, %31, %34, %35, %outgate.7, %41, %42, %43)\n%1639 : bool = prim::AutogradAllZero(%2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16)\n%1640 : bool[] = prim::ListConstruct(%1626, %1627, %1628, %1629, %1630, %1631, %1632, %1633, %1634, %1635, %1636, %1637, %1638, %1639)\n%1641 : bool = aten::all(%1640)\n```\n\nMy performance measurements showed some changes, but I don't really\ntrust them and think that they are probably just a noise. Below are\ntables with min-aggregation over 10 runs:\n\nFastRNN models:\n\n| name                                             | base time (s) |   diff time (s) |   % change |\n| :---                                             |          ---: |            ---: |       ---: |\n| lstm[aten]:bwd                                   |     30.059927 |       29.834089 |      -0.8% |\n| lstm[aten]:fwd                                   |     25.673708 |       25.700039 |       0.1% |\n| lstm[cudnn]:bwd                                  |     17.866232 |       17.893120 |       0.2% |\n| lstm[cudnn]:fwd                                  |     11.418444 |       11.408514 |      -0.1% |\n| lstm[jit]:bwd                                    |     27.127205 |       27.141029 |       0.1% |\n| lstm[jit]:fwd                                    |     17.018047 |       16.975451 |      -0.3% |\n| lstm[jit_multilayer]:bwd                         |     27.502396 |       27.365149 |      -0.5% |\n| lstm[jit_multilayer]:fwd                         |     16.918591 |       16.917767 |      -0.0% |\n| lstm[jit_premul]:bwd                             |     22.281199 |       22.215082 |      -0.3% |\n| lstm[jit_premul]:fwd                             |     14.848708 |       14.896231 |       0.3% |\n| lstm[jit_premul_bias]:bwd                        |     20.761206 |       21.170969 |       2.0% |\n| lstm[jit_premul_bias]:fwd                        |     15.013515 |       15.037978 |       0.2% |\n| lstm[jit_simple]:bwd                             |     26.715771 |       26.697786 |      -0.1% |\n| lstm[jit_simple]:fwd                             |     16.675898 |       16.545893 |      -0.8% |\n| lstm[py]:bwd                                     |     56.327065 |       54.731030 |      -2.8% |\n| lstm[py]:fwd                                     |     39.876324 |       39.230572 |      -1.6% |\n\nTorch Hub models:\n\n| name                                             | base time (s) |   diff time (s) |   % change |\n| :---                                             |          ---: |            ---: |       ---: |\n| test_eval[BERT_pytorch-cuda-jit]                 |      0.111706 |        0.106604 |      -4.6% |\n| test_eval[LearningToPaint-cuda-jit]              |      0.002841 |        0.002801 |      -1.4% |\n| test_eval[Super_SloMo-cuda-jit]                  |      0.384869 |        0.384737 |      -0.0% |\n| test_eval[attension_is_all_you_nee...-cuda-jit]  |      0.123857 |        0.123923 |       0.1% |\n| test_eval[demucs-cuda-jit]                       |      0.077270 |        0.076878 |      -0.5% |\n| test_eval[fastNLP-cuda-jit]                      |      0.000255 |        0.000249 |      -2.3% |\n| test_eval[moco-cuda-jit]                         |      0.426472 |        0.427380 |       0.2% |\n| test_eval[pytorch_CycleGAN_and_pix...-cuda-jit]  |      0.026483 |        0.026423 |      -0.2% |\n| test_eval[pytorch_mobilenet_v3-cuda-jit]         |      0.036202 |        0.035853 |      -1.0% |\n| test_eval[pytorch_struct-cuda-jit]               |      0.001439 |        0.001495 |       3.9% |\n| test_train[BERT_pytorch-cuda-jit]                |      0.247236 |        0.247188 |      -0.0% |\n| test_train[Background_Matting-cuda-jit]          |      3.536659 |        3.581864 |       1.3% |\n| test_train[LearningToPaint-cuda-jit]             |      0.015341 |        0.015331 |      -0.1% |\n| test_train[Super_SloMo-cuda-jit]                 |      1.018626 |        1.019098 |       0.0% |\n| test_train[attension_is_all_you_nee...-cuda-jit] |      0.446314 |        0.444893 |      -0.3% |\n| test_train[demucs-cuda-jit]                      |      0.169647 |        0.169846 |       0.1% |\n| test_train[fastNLP-cuda-jit]                     |      0.001990 |        0.001978 |      -0.6% |\n| test_train[moco-cuda-jit]                        |      0.855323 |        0.856974 |       0.2% |\n| test_train[pytorch_mobilenet_v3-cuda-jit]        |      0.497723 |        0.485416 |      -2.5% |\n| test_train[pytorch_struct-cuda-jit]              |      0.309692 |        0.308792 |      -0.3% |\n\nDifferential Revision: D23794659\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 859b68868ef839c5c6cbc7021879ee22d3144ea8", "pr_number": "44987", "files_changed": ["aten/src/ATen/core/interned_strings.h", "torch/csrc/jit/passes/specialize_autogradzero.cpp", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/profiling_record.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["oncall: jit"]}, "cd7a682282": {"title": "[caffe2] adds hypothesis test for queue ops cancel (#45178)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45178\n\n## Motivation\n* To be able to make C2 ops cancellable so we can safely exit.\n* Some C2 operators are now blocking thus being non-cancellable. If an error\noccurs we need to be able to safely stop all net execution so we can throw\nthe exception to the caller.\n\n## Summary\n* Adds a hypothesis test for queue ops cancellation.\n\nTest Plan:\n## Unit test added to verify that queue ops propagate errors\n\n```\nbuck test caffe2/caffe2/python:hypothesis_test\nbuck test caffe2/caffe2/python:hypothesis_test -- test_safe_dequeue_blob__raises_exception_when_hang --stress-runs 1000\n```\n\n```\nSummary\n  Pass: 1000\n  ListingSuccess: 1\n```\n\nReviewed By: d4l3k\n\nDifferential Revision: D23847576\n\nfbshipit-source-id: 2fc351e1ee13ea8b32d976216d2d01dfb6fcc1ad", "pr_number": "45178", "files_changed": ["caffe2/python/hypothesis_test.py"], "labels": ["fb-exported"]}, "b84dd771e6": {"title": "Grammatically updated the tech docs (#45192)", "body": "Summary:\nSmall grammatical update to the [https://pytorch.org/docs/stable/tensors.html](url) docs.\n\n**_update1_**\n![update1](https://user-images.githubusercontent.com/62737243/93969792-5c0ea800-fd8a-11ea-8c9f-0033f51a1fdc.png)\n\n**_update2_**\n![update2](https://user-images.githubusercontent.com/62737243/93969801-603ac580-fd8a-11ea-812d-d3026b9fc8a5.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45192\n\nReviewed By: bwasti\n\nDifferential Revision: D23877870\n\nPulled By: ezyang\n\nfbshipit-source-id: 929ba3d479925b5132dbe87fad2da487408db7c7", "pr_number": "45192", "files_changed": ["docs/source/tensors.rst"], "labels": ["open source"]}, "6311c5a483": {"title": "Minor touchups. (#44317)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44317\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23820828\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: b83bdea9aed2fb52bd254ff15914d55a1af58c04", "pr_number": "44317", "files_changed": ["aten/src/ATen/native/vulkan/Vulkan.h", "aten/src/ATen/native/vulkan/api/Allocator.h", "aten/src/ATen/native/vulkan/api/Command.cpp", "aten/src/ATen/native/vulkan/api/Command.h", "aten/src/ATen/native/vulkan/api/Common.h", "aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/Descriptor.cpp", "aten/src/ATen/native/vulkan/api/Descriptor.h", "aten/src/ATen/native/vulkan/api/Pipeline.cpp", "aten/src/ATen/native/vulkan/api/Pipeline.h", "aten/src/ATen/native/vulkan/api/Resource.cpp", "aten/src/ATen/native/vulkan/api/Resource.h", "aten/src/ATen/native/vulkan/api/Shader.cpp", "aten/src/ATen/native/vulkan/api/Shader.h"], "labels": []}, "5a59330647": {"title": "Add architectural support for multi-GPU. (#44059)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44059\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D23820825\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 0719b00581487a77ebadff867d1e4ac89354bf90", "pr_number": "44059", "files_changed": ["aten/src/ATen/native/vulkan/api/Adapter.h", "aten/src/ATen/native/vulkan/api/Command.cpp", "aten/src/ATen/native/vulkan/api/Command.h", "aten/src/ATen/native/vulkan/api/Common.h", "aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/api/Descriptor.cpp", "aten/src/ATen/native/vulkan/api/Descriptor.h", "aten/src/ATen/native/vulkan/api/Pipeline.cpp", "aten/src/ATen/native/vulkan/api/Pipeline.h", "aten/src/ATen/native/vulkan/api/Resource.cpp", "aten/src/ATen/native/vulkan/api/Resource.h", "aten/src/ATen/native/vulkan/api/Runtime.cpp", "aten/src/ATen/native/vulkan/api/Runtime.h", "aten/src/ATen/native/vulkan/api/Shader.cpp", "aten/src/ATen/native/vulkan/api/Shader.h", "aten/src/ATen/native/vulkan/api/api.h", "aten/src/ATen/test/vulkan_api_test.cpp"], "labels": []}, "1539d4a664": {"title": "Add operator to compute the equalization scale (#45096)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45096\n\nAdd operator to compute the equalization scale. This will be used in the integration of equalization into dper int8 fixed quant scheme quantization flow.\n\nDesign docs:\nhttps://fb.quip.com/bb7SAGBxPGNC\n\nhttps://fb.quip.com/PDAOAsgoLfRr\n\nTest Plan: buck test caffe2/caffe2/quantization/server:compute_equalization_scale_test\n\nReviewed By: jspark1105\n\nDifferential Revision: D23779870\n\nfbshipit-source-id: 5e6a8c220935a142ecf8e61100a8c71932afa8d7", "pr_number": "45096", "files_changed": ["caffe2/opt/bound_shape_inferencer.cc", "caffe2/quantization/server/compute_equalization_scale.cc", "caffe2/quantization/server/compute_equalization_scale.h", "caffe2/quantization/server/compute_equalization_scale_test.py"], "labels": ["fb-exported"]}, "0137e3641d": {"title": "Refactor subgraph merging (#44238)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44238\n\nRefactor create_autodiff_subgraphs to use the same updating of output aliasing properties logic as tensorexpr fuser, and factor that out to a common function in subgraph utils.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin, robieta\n\nDifferential Revision: D23871565\n\nPulled By: eellison\n\nfbshipit-source-id: 72df253b16baf8e4aabf3d68b103b29e6a54d44c", "pr_number": "44238", "files_changed": ["torch/csrc/jit/passes/create_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.h"], "labels": ["oncall: jit"]}, "5dd288eb06": {"title": "[JIT] Regularize tensorexpr fuser strategy with other fusers (#44972)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44972\n\nPreviously, our fusion strategy would be:\n- start at the end of the block, find a fusable node\n- iteratively try to merge inputs into the fusion group, sorted topologically\n\nThis strategy works pretty well, but has the possibility of missing fusion groups. See my attached test case for an example where we wouldn't find all possible fusion groups. bertmaher found an example of a missed fusion groups in one of our rnn examples (jit_premul) that caused a regression from the legacy fuser.\n\nHere, I'm updating our fusion strategy to be the same as our other fusion passes - create_autodiff_subgraphs, and graph_fuser.cpp.\n\nThe basic strategy is:\n- iterate until you find a fusible node\n- try to merge the nodes inputs, whenever a succesful merge occurs restart at the beginning of the nodes inputs\n- after you've exhausted a node, continue searching the block for fusion opportunities from the node\n- continue doing this on the block until we go through an iteration without an succesful merges\n\nSince we create the fusion groups once, and only re-specialize within the fusion groups, we should be running this very infrequently (only re-triggers when we fail undefinedness specializations). Also bc it's the same algorithm as the existing fuser it is unlikely to cause a regression.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin, robieta\n\nDifferential Revision: D23821581\n\nPulled By: eellison\n\nfbshipit-source-id: e513d1ef719120dadb0bfafc7a14f4254cd806ee", "pr_number": "44972", "files_changed": ["test/jit/test_profiler.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.h"], "labels": ["oncall: jit"]}, "bee1d448e7": {"title": "Fix test_rpc_profiling_remote_record_function (#45162)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45162\n\nThis test was flaky because it was not able to validate that the\noverall record_function's CPU times are greater than the sum of its children.\nIt turns out that this is a general bug in the profiler that can be reproduced\nwithout RPC, see https://github.com/pytorch/pytorch/issues/45160. Hence,\nremoving this from the test and replacing it by just validating the expected\nchildren.\n\nRan the test 1000 times and they all passed.\nghstack-source-id: 112632327\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D23851854\n\nfbshipit-source-id: 5d9023acd17800a6668ba4849659d8cc902b8d6c", "pr_number": "45162", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "92ebb04f92": {"title": "added check for NumberType (#44375)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44107\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44375\n\nReviewed By: mrshenli\n\nDifferential Revision: D23906728\n\nPulled By: eellison\n\nfbshipit-source-id: 3b534e5dd3af1f5e43a7314953e64117cbe8ffe4", "pr_number": "44375", "files_changed": ["torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["oncall: jit", "open source", "triaged"]}, "0b6e5ad4a9": {"title": "Resolve comments in #44354. (#45150)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45150\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D23846796\n\nPulled By: ailzhang\n\nfbshipit-source-id: 7bef89d833848ac3f8993c4c037acf1d4f2ca674", "pr_number": "45150", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.cpp"], "labels": []}, "677a59dcaa": {"title": "[aten] Call fbgemm functions for embedding prepack/unpack (#44845)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44845\n\nfbgemm functions are vectorized and faster\n\n```\nFinished test run: https://our.intern.facebook.com/intern/testinfra/testrun/6473924484856786\nSummary (total time 15.08s):\n  PASS: 7\n  FAIL: 0\n  SKIP: 0\n  FATAL: 0\n  TIMEOUT: 0\n  OMIT: 0\n```\n\nPerformance Before:\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: qembeddingbag_byte_prepack\n# Mode: Eager\n# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 68.727\n\n# Benchmarking PyTorch: qembeddingbag_byte_prepack\n# Mode: Eager\n# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 131.500\n\n# Benchmarking PyTorch: qembeddingbag_byte_prepack\n# Mode: Eager\n# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 248.190\n\n# Benchmarking PyTorch: qembeddingbag_4bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 172.742\n\n# Benchmarking PyTorch: qembeddingbag_4bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 333.008\n\n# Benchmarking PyTorch: qembeddingbag_4bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 652.423\n\n# Benchmarking PyTorch: qembeddingbag_2bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 167.282\n\n# Benchmarking PyTorch: qembeddingbag_2bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 398.901\n\n# Benchmarking PyTorch: qembeddingbag_2bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 785.254\n\n# Benchmarking PyTorch: qembeddingbag_byte_unpack\n# Mode: Eager\n# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 122.653\n\n# Benchmarking PyTorch: qembeddingbag_byte_unpack\n# Mode: Eager\n# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 230.617\n\n# Benchmarking PyTorch: qembeddingbag_byte_unpack\n# Mode: Eager\n# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 408.807\n\n# Benchmarking PyTorch: qembeddingbag_4bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 176.087\n\n# Benchmarking PyTorch: qembeddingbag_4bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 337.514\n\n# Benchmarking PyTorch: qembeddingbag_4bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 659.716\n\n# Benchmarking PyTorch: qembeddingbag_2bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 342.529\n\n# Benchmarking PyTorch: qembeddingbag_2bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 665.197\n\n# Benchmarking PyTorch: qembeddingbag_2bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 1307.923\n```\n\nPerformance After:\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: qembeddingbag_byte_prepack\n# Mode: Eager\n# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 10.782\n\n# Benchmarking PyTorch: qembeddingbag_byte_prepack\n# Mode: Eager\n# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 17.443\n\n# Benchmarking PyTorch: qembeddingbag_byte_prepack\n# Mode: Eager\n# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 25.898\n\n# Benchmarking PyTorch: qembeddingbag_4bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 13.903\n\n# Benchmarking PyTorch: qembeddingbag_4bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 18.575\n\n# Benchmarking PyTorch: qembeddingbag_4bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 30.650\n\n# Benchmarking PyTorch: qembeddingbag_2bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 14.158\n\n# Benchmarking PyTorch: qembeddingbag_2bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 19.818\n\n# Benchmarking PyTorch: qembeddingbag_2bit_prepack\n# Mode: Eager\n# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 30.852\n\n# Benchmarking PyTorch: qembeddingbag_byte_unpack\n# Mode: Eager\n# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 47.596\n\n# Benchmarking PyTorch: qembeddingbag_byte_unpack\n# Mode: Eager\n# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 91.025\n\n# Benchmarking PyTorch: qembeddingbag_byte_unpack\n# Mode: Eager\n# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 131.425\n\n# Benchmarking PyTorch: qembeddingbag_4bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 12.637\n\n# Benchmarking PyTorch: qembeddingbag_4bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 20.856\n\n# Benchmarking PyTorch: qembeddingbag_4bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 33.944\n\n# Benchmarking PyTorch: qembeddingbag_2bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim128\n# Input: num_embeddings: 80, embedding_dim: 128\nForward Execution Time (us) : 21.181\n\n# Benchmarking PyTorch: qembeddingbag_2bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim256\n# Input: num_embeddings: 80, embedding_dim: 256\nForward Execution Time (us) : 34.213\n\n# Benchmarking PyTorch: qembeddingbag_2bit_unpack\n# Mode: Eager\n# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim512\n# Input: num_embeddings: 80, embedding_dim: 512\nForward Execution Time (us) : 59.622\n```\nghstack-source-id: 112836216\n\nTest Plan: buck test //caffe2/test:quantization -- 'test_embedding_bag*'  --print-passing-details\n\nReviewed By: radkris-git\n\nDifferential Revision: D23675777\n\nfbshipit-source-id: 0b1a787864663daecc7449295f9ab6264eac52fc", "pr_number": "44845", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp"], "labels": []}, "03dde4c62a": {"title": "Resend diff D23858329 (#45315)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45315\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45314\n\nin D23858329 (https://github.com/pytorch/pytorch/commit/721cfbf8425cf2c1dc5e27d1332e32e1a42ef541), we put PriorCorrectionCalibrationPrediction unit test in OSS file which causes test failure issue in public trunk.\n\nthis diff moves it to FB only test file.\n\nTest Plan:\n```\n buck test //caffe2/caffe2/python/operator_test:torch_integration_test -- test_gather_ranges_to_dense_op\n\nbuck test //caffe2/caffe2/fb/python/operator_test:torch_integration_test -- test_prior_correct_calibration_prediction_op\n```\nall pass.\n\nReviewed By: houseroad\n\nDifferential Revision: D23899012\n\nfbshipit-source-id: 1ed97d8702e2765991e6caf5695d4c49353dae82", "pr_number": "45315", "files_changed": ["caffe2/operators/gather_ranges_to_dense_op.cc", "caffe2/operators/gather_ranges_to_dense_op.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": ["fb-exported"]}, "0f2c648c97": {"title": "log metadata when model loading failed (#44430)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44430\n\nlog metadata even when model loading is failed\n\nTest Plan: {F331550976}\n\nReviewed By: husthyc\n\nDifferential Revision: D23577711\n\nfbshipit-source-id: 0504e75625f377269f1e5df0f1ebe34b8e564c4b", "pr_number": "44430", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/observer.h"], "labels": ["fb-exported", "oncall: jit"]}, "7e5492e1be": {"title": "[minor] Fix undefined variable (#45246)", "body": "Summary:\nThe commit https://github.com/pytorch/pytorch/commit/2a37f3fd2f74e2d10f3440e6dfef2d5389caab62 https://github.com/pytorch/pytorch/pull/45130 deleted the python variable `capability` which is used in later lines.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45246\n\nReviewed By: walterddr\n\nDifferential Revision: D23923916\n\nPulled By: malfet\n\nfbshipit-source-id: c5d7fef9e4a87ccc621191200e5965710e9d6aaa", "pr_number": "45246", "files_changed": ["torch/cuda/__init__.py"], "labels": ["open source"]}, "630bd85aae": {"title": "[pytorch] refine dispatch keys in native_functions.yaml (2/N) (#45284)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45284\n\nThis is the 2nd batch of the change described in #45010.\n\nIn this batch we relaxed some filters to cover more 'backend specific' ops:\n* ops that not call any 'Tensor::is_xxx()' method OR only call\n  'Tensor::is_cuda()' - we are adding CUDA dispatch key anyway;\n* ops that call other ATen ops but ARE differentiable - differentiability\n  is a fuzzy indicator of not being 'composite';\n\nInherited other filters from the 1st batch:\n* These ops don't already have dispatch section in native_functions.yaml;\n* These ops call one or more DispatchStub (thus \"backend specific\");\n\nDifferential Revision: D23909901\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nPulled By: ljk53\n\nfbshipit-source-id: 3b31e176324b6ac814acee0b0f80d18443bd81a1", "pr_number": "45284", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": []}, "c6500bcf14": {"title": "[reland] Make grad point to bucket buffer in DDP to save memory usage (#44344)", "body": "Summary:\n[test all]\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44344\n\nreland #41954\n\nAdd one argument in DDP API to enable/disable letting grads pointing  to views. When it is disabled, behavior is the same as DDP right now; when it is enabled, Make both variable.grad() and grad in distautograd context point to bucket buffer in DDP to save memory usage.\nIn this case, grad will be view of bucket buffer tensors, in order to make it compatiable with optimizer.zero_grad(), we\nmade changes in #41283.\n\nAlso be noted that we can not make variable.grad() pointing to bucket buffer during construction time, because we want to\nkeep grad undefined for unused parameters.\nghstack-source-id: 112845787\n\nTest Plan:\n1. When grad_is_view=false:\na. roberta_base, peak memory usage 8250MB, p50 per iteration latency 0.923second, https://www.internalfb.com/intern/fblearner/details/218029699/?notif_channel=cli\nb. resnet, peak memory usage 3089MB, p50 per iteration latency 0.120second, https://www.internalfb.com/intern/fblearner/details/218029035/?notif_channel=cli\nc. accuracy benchmark, distributed=false, .accuracy 40.914535522461, .loss: 1.6370717287064; distributed=true, .accuracy: 39.966053009033, .loss: 1.6849111318588\nhttps://www.internalfb.com/intern/fblearner/details/218035688/?notif_channel=cli\nd. classy vision uru production flow, https://www.internalfb.com/intern/fblearner/details/219065811/?notif_channel=cli\ne. pytext flow, https://www.internalfb.com/intern/fblearner/details/219137458/?notif_channel=cli\n\n2. When grad_is_view=true:\na. roberta_base, peak memory usage 7183MB, p50 per iteration latency 0.908second, https://www.internalfb.com/intern/fblearner/details/217882539?tab=operator_details\nb. resnet, peak memory usage 2988 MB, p50 per iteration latency 0.119second, https://www.internalfb.com/intern/fblearner/details/218028479/?notif_channel=cli\nc. accuracy benchmark, distributed=false, .accuracy 41.713260650635, .loss: 1.69939661026; distributed=true, .accuracy: 39.966053009033, .loss: 1.6849111318588, https://www.internalfb.com/intern/fblearner/details/218037058/?notif_channel=cli\nd. classy vision uru production flow, expected, can not work well with apex.amp https://www.internalfb.com/intern/fblearner/details/219205218/?notif_channel=cli\ne. pytext flow, detach_() related error, expected, as pytext zero_grad depends on apex repo where detach_() is called. also seeing the warning in finalize_bucket_dense due to tied weights, which is expected. https://www.internalfb.com/intern/fblearner/details/219150229/?notif_channel=cli\n\nReviewed By: mrshenli\n\nDifferential Revision: D23588186\n\nfbshipit-source-id: f724d325b954ef6f06ede31759bf01dd29a6f5e5", "pr_number": "44344", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/functions/accumulate_grad.h", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": []}, "0122299f9b": {"title": "Enable distributed package on windows, Gloo backend supported only (#42897)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42095\n\nFor test case part will be committed to this PR later\n\nmrshenli, please help to review\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42897\n\nReviewed By: osalpekar\n\nDifferential Revision: D23841786\n\nPulled By: mrshenli\n\nfbshipit-source-id: 334ba1ed73eff2f668857390fc32d1bc7f08e5f3", "pr_number": "42897", "files_changed": [".jenkins/pytorch/win-test-helpers/installation-helpers/install_miniconda3.bat", "CMakeLists.txt", "caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "test/cpp/dist_autograd/CMakeLists.txt", "test/distributed/test_c10d.py", "test/distributed/test_c10d_spawn.py", "test/run_test.py", "tools/build_variables.bzl", "torch/CMakeLists.txt", "torch/csrc/Module.cpp", "torch/csrc/WindowsTorchApiMacro.h", "torch/csrc/distributed/c10d/comm.h", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/unpickler.cpp", "torch/csrc/utils/future.h", "torch/distributed/rendezvous.py", "torch/lib/c10d/CMakeLists.txt", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/GlooDeviceFactory.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/Utils.cpp", "torch/lib/c10d/Utils.hpp", "torch/lib/c10d/test/CMakeLists.txt", "torch/lib/c10d/test/CUDATest.hpp", "torch/lib/c10d/test/FileStoreTest.cpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp", "torch/lib/c10d/test/TestUtils.hpp", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_utils.py", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["module: build", "module: distributions", "open source", "triaged"]}, "31ae8117ba": {"title": "[RFC] Remove per-op-registration related code in caffe2/tools/codegen/gen.py (#45134)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45134\n\nPer-Op-Registration was a mechanism used for mobile selective build v0. Since then, a new dispathing mechanism has been built for PyTorch, and this code path isn't used any more. Remove it to simplify understanding/updating the code-generator's code-flow.\nghstack-source-id: 112723942\n\nTest Plan: `buck build` and sandcastle.\n\nReviewed By: ezyang\n\nDifferential Revision: D23806632\n\nfbshipit-source-id: d93cd324650c541d9bfc8eeff2ddb2833b988ecc", "pr_number": "45134", "files_changed": ["aten/src/ATen/templates/PerOpRegistration.cpp", "tools/codegen/gen.py"], "labels": []}, "bc3151dee0": {"title": "[quant] Remove unused qconfig argument in qat linear module (#45307)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45307\n\nfixes: https://github.com/pytorch/pytorch/issues/35634\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23917339\n\nfbshipit-source-id: 65f8844b98198bbf93547b3d71408c2a54605218", "pr_number": "45307", "files_changed": ["torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/intrinsic/qat/modules/linear_relu.py", "torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py"], "labels": []}, "103fa3894a": {"title": "Revert D23841786: [pytorch][PR] Enable distributed package on windows, Gloo backend supported only", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23841786 (https://github.com/pytorch/pytorch/commit/0122299f9ba729aa0c9bd43764af53225e03672c)\n\nOriginal commit changeset: 334ba1ed73ef\n\nfbshipit-source-id: ec95432f9957df56a5a04e52661f5db920b7f57f", "pr_number": null, "files_changed": [".jenkins/pytorch/win-test-helpers/installation-helpers/install_miniconda3.bat", "CMakeLists.txt", "caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "test/cpp/dist_autograd/CMakeLists.txt", "test/distributed/test_c10d.py", "test/distributed/test_c10d_spawn.py", "test/run_test.py", "tools/build_variables.bzl", "torch/CMakeLists.txt", "torch/csrc/Module.cpp", "torch/csrc/WindowsTorchApiMacro.h", "torch/csrc/distributed/c10d/comm.h", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/unpickler.cpp", "torch/csrc/utils/future.h", "torch/distributed/rendezvous.py", "torch/lib/c10d/CMakeLists.txt", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/GlooDeviceFactory.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/Utils.cpp", "torch/lib/c10d/Utils.hpp", "torch/lib/c10d/test/CMakeLists.txt", "torch/lib/c10d/test/CUDATest.hpp", "torch/lib/c10d/test/FileStoreTest.cpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp", "torch/lib/c10d/test/TestUtils.hpp", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_utils.py", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": []}, "bdf329ef8a": {"title": "SyncBN: preserve qconfig if it exists (#45317)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45317\n\nEager mode quantization depends on the presence of the `config`\nmodel attribute.  Currently converting a model to use `SyncBatchNorm`\nremoves the qconfig - fixing this.  This is important if a BN is not\nfused to anything during quantization convert.\n\nTest Plan:\n```\npython test/test_quantization.py TestDistributed.test_syncbn_preserves_qconfig\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23922072\n\nfbshipit-source-id: cc1bc25c8e5243abb924c6889f78cf65a81be158", "pr_number": "45317", "files_changed": ["test/quantization/test_workflow_module.py", "torch/nn/modules/batchnorm.py"], "labels": []}, "95df8657c9": {"title": "Enables test linalg (#45278)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/45271.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45278\n\nReviewed By: ngimel\n\nDifferential Revision: D23926124\n\nPulled By: mruberry\n\nfbshipit-source-id: 26692597f9a1988e5fa846f97b8430c3689cac27", "pr_number": "45278", "files_changed": ["test/run_test.py", "test/test_linalg.py"], "labels": []}, "99e0a87bbb": {"title": "[nvFuser] Latency improvements for pointwise + reduction fusion (#45218)", "body": "Summary:\nA lot of changes are in this update, some highlights:\n\n- Added Doxygen config file\n- Split the fusion IR (higher level TE like IR) from kernel IR (lower level CUDA like IR)\n- Improved latency with dynamic shape handling for the fusion logic\n- Prevent recompilation for pointwise + reduction fusions when not needed\n- Improvements to inner dimension reduction performance\n- Added input -> kernel + kernel launch parameters cache, added eviction policy\n- Added reduction fusions with multiple outputs (still single reduction stage)\n- Fixed code generation bugs for symbolic tiled GEMM example\n- Added thread predicates to prevent shared memory form being loaded multiple times\n- Improved sync threads placements with shared memory and removed read before write race\n- Fixes to FP16 reduction fusions where output would come back as FP32\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45218\n\nReviewed By: ezyang\n\nDifferential Revision: D23905183\n\nPulled By: soumith\n\nfbshipit-source-id: 12f5ad4cbe03e9a25043bccb89e372f8579e2a79", "pr_number": "45218", "files_changed": ["aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h", "caffe2/CMakeLists.txt", "test/cpp/jit/test_gpu.cpp", "test/cpp/jit/tests.h", "test/test_jit_cuda_fuser.py", "test/test_jit_cuda_fuser_legacy.py", "test/test_jit_cuda_fuser_profiling.py", "tools/build_variables.bzl", "torch/csrc/jit/codegen/cuda/codegen.cpp", "torch/csrc/jit/codegen/cuda/codegen.h", "torch/csrc/jit/codegen/cuda/compute_at.cpp", "torch/csrc/jit/codegen/cuda/compute_at.h", "torch/csrc/jit/codegen/cuda/docs/.gitignore", "torch/csrc/jit/codegen/cuda/docs/documentation.h", "torch/csrc/jit/codegen/cuda/docs/fuser.doxygen", "torch/csrc/jit/codegen/cuda/docs/images/ir_architecture.png", "torch/csrc/jit/codegen/cuda/docs/main_page.md", "torch/csrc/jit/codegen/cuda/executor.cpp", "torch/csrc/jit/codegen/cuda/executor.h", "torch/csrc/jit/codegen/cuda/executor_kernel_arg.cpp", "torch/csrc/jit/codegen/cuda/executor_kernel_arg.h", "torch/csrc/jit/codegen/cuda/executor_launch_params.h", "torch/csrc/jit/codegen/cuda/executor_utils.cpp", "torch/csrc/jit/codegen/cuda/executor_utils.h", "torch/csrc/jit/codegen/cuda/expr_evaluator.cpp", "torch/csrc/jit/codegen/cuda/expr_evaluator.h", "torch/csrc/jit/codegen/cuda/fusion.cpp", "torch/csrc/jit/codegen/cuda/fusion.h", "torch/csrc/jit/codegen/cuda/graph_fuser.cpp", "torch/csrc/jit/codegen/cuda/index_compute.cpp", "torch/csrc/jit/codegen/cuda/instrumentation.cpp", "torch/csrc/jit/codegen/cuda/instrumentation.h", "torch/csrc/jit/codegen/cuda/interface.cpp", "torch/csrc/jit/codegen/cuda/ir_base_nodes.cpp", "torch/csrc/jit/codegen/cuda/ir_base_nodes.h", "torch/csrc/jit/codegen/cuda/ir_cloner.cpp", "torch/csrc/jit/codegen/cuda/ir_cloner.h", "torch/csrc/jit/codegen/cuda/ir_graphviz.cpp", "torch/csrc/jit/codegen/cuda/ir_graphviz.h", "torch/csrc/jit/codegen/cuda/ir_interface_nodes.h", "torch/csrc/jit/codegen/cuda/ir_internal_nodes.h", "torch/csrc/jit/codegen/cuda/ir_iostream.cpp", "torch/csrc/jit/codegen/cuda/ir_iostream.h", "torch/csrc/jit/codegen/cuda/ir_nodes.cpp", "torch/csrc/jit/codegen/cuda/ir_printer.h", "torch/csrc/jit/codegen/cuda/iter_visitor.cpp", "torch/csrc/jit/codegen/cuda/iter_visitor.h", "torch/csrc/jit/codegen/cuda/kernel.cpp", "torch/csrc/jit/codegen/cuda/kernel.h", "torch/csrc/jit/codegen/cuda/kernel_cache.cpp", "torch/csrc/jit/codegen/cuda/kernel_cache.h", "torch/csrc/jit/codegen/cuda/kernel_ir.cpp", "torch/csrc/jit/codegen/cuda/kernel_ir.h", "torch/csrc/jit/codegen/cuda/kernel_ir_builder.cpp", "torch/csrc/jit/codegen/cuda/kernel_ir_builder.h", "torch/csrc/jit/codegen/cuda/kernel_resource_strings.h", "torch/csrc/jit/codegen/cuda/lower2device.cpp", "torch/csrc/jit/codegen/cuda/lower2device.h", "torch/csrc/jit/codegen/cuda/lower_index.cpp", "torch/csrc/jit/codegen/cuda/lower_index.h", "torch/csrc/jit/codegen/cuda/lower_insert_syncs.cpp", "torch/csrc/jit/codegen/cuda/lower_insert_syncs.h", "torch/csrc/jit/codegen/cuda/lower_loops.cpp", "torch/csrc/jit/codegen/cuda/lower_loops.h", "torch/csrc/jit/codegen/cuda/lower_thread_predicate.cpp", "torch/csrc/jit/codegen/cuda/lower_thread_predicate.h", "torch/csrc/jit/codegen/cuda/lower_unroll.cpp", "torch/csrc/jit/codegen/cuda/lower_unroll.h", "torch/csrc/jit/codegen/cuda/lower_utils.cpp", "torch/csrc/jit/codegen/cuda/lower_validation.cpp", "torch/csrc/jit/codegen/cuda/manager.cpp", "torch/csrc/jit/codegen/cuda/parser.cpp", "torch/csrc/jit/codegen/cuda/partition.cpp", "torch/csrc/jit/codegen/cuda/predicate_compute.cpp", "torch/csrc/jit/codegen/cuda/predicate_compute.h", "torch/csrc/jit/codegen/cuda/scheduler.cpp", "torch/csrc/jit/codegen/cuda/scheduler.h", "torch/csrc/jit/codegen/cuda/shape_inference.cpp", "torch/csrc/jit/codegen/cuda/tensor_view.cpp", "torch/csrc/jit/codegen/cuda/transform_iter.h", "torch/csrc/jit/codegen/cuda/transform_replay.cpp", "torch/csrc/jit/codegen/cuda/transform_rfactor.cpp", "torch/csrc/jit/codegen/cuda/type.h", "torch/csrc/jit/codegen/cuda/utils.h"], "labels": ["oncall: jit", "open source"]}, "241afc9188": {"title": "Migrate `addr` from the TH to Aten (CPU) (#44364)", "body": "Summary:\nRelated https://github.com/pytorch/pytorch/issues/24507\nFixes https://github.com/pytorch/pytorch/issues/24666\n\nThis PR is to modernize the CPU implementation of the vector `outer product`.\nThe existing TH implementation for `torch.attr` is migrated to `aten`, as the `torch.ger` manipulates the `addr` functions to calculate outer product,\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44364\n\nReviewed By: ezyang\n\nDifferential Revision: D23866733\n\nPulled By: mruberry\n\nfbshipit-source-id: 5159ea22f0e3c991123fe7c19cc9beb6ad00301e", "pr_number": "44364", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THBlas.cpp", "aten/src/TH/generic/THBlas.h", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "test/backward_compatibility/check_backward_compatibility.py", "test/test_linalg.py", "test/test_op_aliases.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/csrc/jit/passes/normalize_ops.cpp"], "labels": ["open source", "triaged"]}, "76ee58e2ec": {"title": "[TensorExpr] Move inner loops vectorization logic to its own method (#45287)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45287\n\nTest Plan: CI, build\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D23913432\n\nPulled By: asuhan\n\nfbshipit-source-id: 3bf8fe09753f349e3c857863a43d2b1fca5101c1", "pr_number": "45287", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.h"], "labels": ["oncall: jit"]}, "00e704e757": {"title": "[fix] torch.repeat : dim-0 backward (#45212)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/45201\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45212\n\nReviewed By: mrshenli\n\nDifferential Revision: D23905545\n\nPulled By: albanD\n\nfbshipit-source-id: c5bf9cf481c8cf3ccc1fdbfb364006b29f67dc9f", "pr_number": "45212", "files_changed": ["tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source"]}, "2739a7c599": {"title": "Byte-for-byte compatibility fixes in codegen (#44879)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44879\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23825163\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 4d8028274f82c401b393c4fe1b9e32de3f4909c6", "pr_number": "44879", "files_changed": ["aten/src/ATen/native/VariableMethodStubs.cpp", "tools/codegen/api/cpp.py", "tools/codegen/gen.py", "tools/codegen/local.py", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": []}, "bf8cd21f2a": {"title": "Py transformer coder test (#43976)", "body": "Summary:\nFixes #{[37756](https://github.com/pytorch/pytorch/issues/37756)}\n\nAdded the missing Transformer coder python test scripts from C++ API test scripts\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43976\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23873250\n\nPulled By: glaringlee\n\nfbshipit-source-id: cdeae53231e02208463e7629ba2c1f00990150ea", "pr_number": "43976", "files_changed": ["test/test_nn.py"], "labels": ["open source", "triaged"]}, "043bd51b48": {"title": "Remove hacky_wrapper from VariableType and TraceType (#44005)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44005\n\nPreviously, VariableType and TraceType kernels were still written in the legacy way, i.e. they took one TensorOptions argument instead of scattered dtype, layout, device, pin_memory,  and they used hacky_wrapper to be callable.\n\nNow with this PR, variable and tracing kernels are written in the new way and no hacky_wrapper is needed for them.\nghstack-source-id: 112825791\n\nTest Plan:\nwaitforsandcastle\n\nhttps://www.internalfb.com/intern/fblearner/details/215954270/\n\nReviewed By: ezyang\n\nDifferential Revision: D23466042\n\nfbshipit-source-id: bde730a9e3bb4cb80ad484417be1ebecbdc2d377", "pr_number": "44005", "files_changed": ["tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/VariableType.cpp"], "labels": []}, "2ac7de7d53": {"title": "Remove hacky_wrapper from BackendSelect kernels (#44062)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44062\n\nPreviously, BackendSelect kernels were still written in the legacy way, i.e. they took one TensorOptions argument instead of scattered dtype, layout, device, pin_memory,  and they used hacky_wrapper to be callable. This caused a re-wrapping step. Calling into a BackencSelect kernel required taking the individual scattered arguments, packing them into a TensorOptions, and the kernel itself then gathered them again for redispatch.\n\nNow with this PR, BackendSelect kernels are written in the new way and no hacky_wrapper or rewrapping is needed for them.\nghstack-source-id: 112825789\n\nTest Plan:\nvs master: https://www.internalfb.com/intern/fblearner/details/216117032/\n\nvs previous diff: https://www.internalfb.com/intern/fblearner/details/216170194/\n\nReviewed By: ezyang\n\nDifferential Revision: D23484192\n\nfbshipit-source-id: e8fb49c4692404b6b775d18548b990c4cdddbada", "pr_number": "44062", "files_changed": ["aten/src/ATen/native/MetaTensor.cpp", "aten/src/ATen/templates/BackendSelectRegister.cpp", "c10/core/DefaultDtype.cpp", "c10/core/DefaultDtype.h", "c10/core/TensorOptions.h", "caffe2/core/tensor.h", "tools/codegen/api/dispatcher.py", "tools/codegen/gen.py", "torch/csrc/DynamicTypes.cpp"], "labels": []}, "78fcde9c50": {"title": "Trace scattered tensor options arguments (#44071)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44071\n\nPreviously, tracing re-gathered ScalarType, Layout, Device, bool into a TensorOptions object and called `tracer::addInput()` on the gathered TensorOptions argument. `tracer::addInput()` then scattered them again and added the individual scattered arguments to the traced graph. This PR avoids the extraneous gathering and re-scattering step and calls `tracer::addInput()` on the individual arguments directly. This avoid the perf hit for an unnecessary gathering step.\n\nThis applies to both c10-full and non-c10-full ops. In the case of c10-full ops, the tracing kernels takes scattered arguments and we can directly pass them to `tracer::addInput()`. In the case of non-c10-full ops, the kernel takes a `TensorOptions` argument but we still call `tracer::addInput()` on the scattered arguments.\nghstack-source-id: 112825793\n\nTest Plan:\nwaitforsandcastle\n\nvs master: https://www.internalfb.com/intern/fblearner/details/216129483/\n\nvs previous diff: https://www.internalfb.com/intern/fblearner/details/216170069/\n\nReviewed By: ezyang\n\nDifferential Revision: D23486638\n\nfbshipit-source-id: e0b53e6673cef8d7f94158e718301eee261e5d22", "pr_number": "44071", "files_changed": ["c10/core/TensorOptions.h", "tools/autograd/gen_variable_type.py", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tracer.h"], "labels": ["oncall: jit"]}, "b70fac75ac": {"title": "CMake: Fix python dependencies in codegen (#45275)", "body": "Summary:\nI noticed while working on https://github.com/pytorch/pytorch/issues/45163 that edits to python files in the  `tools/codegen/api/` directory wouldn't trigger rebuilds. This tells CMake about all of the dependencies, so rebuilds are triggered automatically.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45275\n\nReviewed By: zou3519\n\nDifferential Revision: D23922805\n\nPulled By: ezyang\n\nfbshipit-source-id: 0fbf2b6a9b2346c31b9b0384e5ad5e0eb0f70e9b", "pr_number": "45275", "files_changed": ["cmake/Codegen.cmake"], "labels": ["open source"]}, "8b00c4c794": {"title": "[ONNX] Correct a minor typo in warning (#45187)", "body": "Summary:\nThe warning for batch_norm was mentioning dropout.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45187\n\nReviewed By: glaringlee\n\nDifferential Revision: D23873215\n\nPulled By: ezyang\n\nfbshipit-source-id: 1dcc82ad16522215f49b4cd0fc0e357b2094e4f2", "pr_number": "45187", "files_changed": ["torch/onnx/symbolic_opset9.py"], "labels": ["open source"]}, "a117d968f6": {"title": "[quant][graph] Remove redundant aten::wait calls in the graph (#45257)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45257\n\nCurrently we inline fork-wait calls when we insert observers for quantization\nIn the case where fork and wait are in different subgraphs, inlining the fork-wait calls\nonly gets rid of the fork. This leaves the aten::wait call in the graph with a torch.Tensor as input,\nwhich is currently not supported.\nTo avoid this we check to make sure input to all wait calls in the graph is of type Future[tensor]\nin the cleanup phase\n\nTest Plan:\npython test/test_quantization.py TestQuantizeJitPasses.test_quantize_fork_wait\n\nImported from OSS\n\nReviewed By: qizzzh\n\nDifferential Revision: D23895412\n\nfbshipit-source-id: 3c58c6be7d7e7904eb6684085832ac21f827a399", "pr_number": "45257", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/freeze_module.cpp"], "labels": ["oncall: jit"]}, "536580e976": {"title": "Vectorize bitwise_not (#45103)", "body": "Summary:\nBenchmark (Debian 10, Release build, gcc 8.3, no turbo, Intel(R) Xeon(R)\nE-2136 CPU @ 3.30GHz):\n\n```python\nimport timeit\nfor dtype in ('torch.int64', 'torch.int32', 'torch.int16', 'torch.int8', 'torch.uint8'):\n    for n, t in [(10_000, 100000),\n                (100_000, 10000)]:\n        print(f'torch.bitwise_not(a), numel() == {n} for {t} times, dtype={dtype}')\n        print(timeit.timeit('torch.bitwise_not(a)', setup=f'import torch; a = torch.arange(-{n//2}, {n//2}, dtype={dtype})', number=t))\n```\n\nBefore:\n\n```\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.int64\n0.5479081739904359\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.int64\n0.3350257440470159\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.int32\n0.39590477803722024\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.int32\n0.25563537096604705\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.int16\n0.31152817397378385\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.int16\n0.20817365101538599\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.int8\n0.8573925020173192\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.int8\n0.4150037349900231\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.uint8\n0.8551108679967001\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.uint8\n0.37137620500288904\n```\n\nAfter:\n\n```\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.int64\n0.5232444299617782\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.int64\n0.33852163201663643\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.int32\n0.3931163849774748\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.int32\n0.24392802000511438\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.int16\n0.3122224889229983\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.int16\n0.1977886479580775\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.int8\n0.26711542706470937\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.int8\n0.18208567495457828\ntorch.bitwise_not(a), numel() == 10000 for 100000 times, dtype=torch.uint8\n0.2615354140289128\ntorch.bitwise_not(a), numel() == 100000 for 10000 times, dtype=torch.uint8\n0.17972210398875177\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45103\n\nReviewed By: ailzhang\n\nDifferential Revision: D23848675\n\nPulled By: ezyang\n\nfbshipit-source-id: 6dde1ab32d9a343a49de66ad9f9b062fa23824d2", "pr_number": "45103", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_int.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp"], "labels": ["module: cpu", "module: operators (deprecated)", "open source"]}, "d1d9017a66": {"title": "[NNC] fix Half conversion of immediates in Cuda backend (#45213)", "body": "Summary:\nThe Cuda HalfChecker casts up all loads and stores of Half to Float, so we do math in Float on the device. It didn't cast up HalfImmediate (ie. constants) so they could insert mixed-size ops. Fix is to do that.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45213\n\nReviewed By: ezyang\n\nDifferential Revision: D23885287\n\nPulled By: nickgg\n\nfbshipit-source-id: 912991d85cc06ebb282625cfa5080d7525c8eba9", "pr_number": "45213", "files_changed": ["test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/tests.h", "test/test_jit_fuser_te.py", "torch/csrc/jit/tensorexpr/cuda_half_support.h"], "labels": ["oncall: jit"]}, "d1a11618f5": {"title": "[static runtime] Add _out variants and reuse memory (#44128)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44128\n\nTest Plan: Imported from OSS\n\nReviewed By: hlu1\n\nDifferential Revision: D23604304\n\nPulled By: bwasti\n\nfbshipit-source-id: 06a23cb75700a0fc733069071843b7b498e7b9e9", "pr_number": "44128", "files_changed": ["benchmarks/static_runtime/CMakeLists.txt", "caffe2/CMakeLists.txt", "test/test_static_runtime.py", "tools/build_variables.bzl", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["oncall: jit"]}, "e5f6e5af13": {"title": "Add Deep and wide to test and flatten/tranpose for good measure (#44129)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44129\n\nTest Plan: Imported from OSS\n\nReviewed By: hlu1\n\nDifferential Revision: D23604302\n\nPulled By: bwasti\n\nfbshipit-source-id: 5787f6f32a80b22b1b712c4116f70370dad98f12", "pr_number": "44129", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["oncall: jit"]}, "dc9e9c118e": {"title": "CUDA BFloat16 neg (#45240)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45240\n\nReviewed By: mruberry\n\nDifferential Revision: D23933392\n\nPulled By: ngimel\n\nfbshipit-source-id: 2472dc550600ff470a1044ddee39054e22598038", "pr_number": "45240", "files_changed": ["aten/src/ATen/native/cuda/UnarySignKernels.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "triaged"]}, "5a0514e3e6": {"title": "[pytorch] Update fmt to 7.0.3 (#45304)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45304\n\nAs title\n\nTest Plan: sandcastle\n\nReviewed By: malfet\n\nDifferential Revision: D23916328\n\nfbshipit-source-id: 47c76886c1f17233304dc59289ff6baa16c50b8d", "pr_number": "45304", "files_changed": ["third_party/fmt"], "labels": ["fb-exported"]}, "22401b850b": {"title": "port all JIT tests to gtest (#45264)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45264\n\nContext for why we are porting to gtest in: https://github.com/pytorch/pytorch/pull/45018.\n\nThis PR completes the process of porting and removes unused files/macros.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23901392\n\nPulled By: suo\n\nfbshipit-source-id: 89526890e1a49462f3f77718f4ee273c5bc578ba", "pr_number": "45264", "files_changed": ["aten/src/ATen/test/thread_init_test.cpp", "test/cpp/jit/CMakeLists.txt", "test/cpp/jit/README.md", "test/cpp/jit/gtest.cpp", "test/cpp/jit/test_base.cpp", "test/cpp/jit/test_base.h", "test/cpp/jit/test_class_parser.cpp", "test/cpp/jit/test_class_type.cpp", "test/cpp/jit/test_gpu.cpp", "test/cpp/jit/test_graph_executor.cpp", "test/cpp/jit/test_inliner.cpp", "test/cpp/jit/test_interface.cpp", "test/cpp/jit/test_interpreter.cpp", "test/cpp/jit/test_ir.cpp", "test/cpp/jit/test_irparser.cpp", "test/cpp/jit/test_jit_type.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/test_lite_trainer.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_mobile_type_parser.cpp", "test/cpp/jit/test_module_api.cpp", "test/cpp/jit/test_peephole_optimize.cpp", "test/cpp/jit/test_qualified_name.cpp", "test/cpp/jit/test_save_load.cpp", "test/cpp/jit/test_schema_matching.cpp", "test/cpp/jit/test_subgraph_matcher.cpp", "test/cpp/jit/test_subgraph_rewriter.cpp", "test/cpp/jit/test_subgraph_utils.cpp", "test/cpp/jit/test_utils.cpp", "test/cpp/jit/test_utils.h", "test/cpp/jit/tests.h"], "labels": ["oncall: jit"]}, "c8166d4b58": {"title": "Add `torch.cuda.comm` to typechecking CI (#45350)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45350\n\nReviewed By: walterddr\n\nDifferential Revision: D23935750\n\nPulled By: malfet\n\nfbshipit-source-id: 5a7d2d4fbc976699d80bb5caf4727c19fa2c5bc8", "pr_number": "45350", "files_changed": ["mypy.ini", "torch/cuda/comm.py"], "labels": ["module: typing", "triaged"]}, "f07ac6a004": {"title": "Fix Windows build failure after DDP PR merged (#45335)", "body": "Summary:\nFixes #{issue number}\nThis is resubmit for PR https://github.com/pytorch/pytorch/issues/42897 . Together with fix for Windows build issue introduced by PR https://github.com/pytorch/pytorch/issues/44344 .\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45335\n\nReviewed By: zou3519\n\nDifferential Revision: D23931471\n\nPulled By: mrshenli\n\nfbshipit-source-id: f49b5a114944c1450b32934b3292170be064f494", "pr_number": "45335", "files_changed": [".jenkins/pytorch/win-test-helpers/installation-helpers/install_miniconda3.bat", "CMakeLists.txt", "caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "test/cpp/dist_autograd/CMakeLists.txt", "test/distributed/test_c10d.py", "test/distributed/test_c10d_spawn.py", "test/run_test.py", "tools/build_variables.bzl", "torch/CMakeLists.txt", "torch/csrc/Module.cpp", "torch/csrc/WindowsTorchApiMacro.h", "torch/csrc/distributed/c10d/comm.h", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/unpickler.cpp", "torch/csrc/utils/future.h", "torch/distributed/rendezvous.py", "torch/lib/c10d/CMakeLists.txt", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/GlooDeviceFactory.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/Utils.cpp", "torch/lib/c10d/Utils.hpp", "torch/lib/c10d/test/CMakeLists.txt", "torch/lib/c10d/test/CUDATest.hpp", "torch/lib/c10d/test/FileStoreTest.cpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp", "torch/lib/c10d/test/TestUtils.hpp", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_utils.py", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["oncall: jit", "open source", "triaged"]}, "d5748d9a1a": {"title": "Enable binary ops with Scalar Lists with for foreach APIs (#45298)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45298\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23931986\n\nPulled By: izdeby\n\nfbshipit-source-id: 281267cd6f90d57a169af89f9f10b0f4fcab47e3", "pr_number": "45298", "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu", "aten/src/ATen/native/cuda/ForeachFunctors.cuh", "aten/src/ATen/native/cuda/MultiTensorApply.cuh", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_foreach.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "tools/codegen/model.py", "tools/pyi/gen_pyi.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": []}, "27ab9bc0f9": {"title": "[RPC profiling] Extend RPC profiling to support async function execution over RPC. (#44664)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44664\n\nCloses https://github.com/pytorch/pytorch/issues/39971. This PR adds support for functions decorated with `rpc.functions.async_execution` to be profiled over RPC as builtins, jit functions, and blocking python UDFs currently can be. The reasoning for this is to provide complete feature support in terms of RPC profiling and the various types of functions users can run.\n\nTo enable this, the PR below this enables calling `disableProfiler()` safely from another thread. We use that functionality to defer disabling the profiler on the server until the future corresponding to the RPC request completes (rather than only the blocking `processRPC` call as was done previously). Since when the future completes we've kicked off the async function and the future corresponding to it has completed, we are able to capture any RPCs the function would have called and the actual work done on the other node.\n\nFor example, if the following async function is ran on a server over RPC:\n\n```\ndef slow_add(x, y):\n    time.sleep(1)\n    return torch.add(x, y)\n\nrpc.functions.async_execution\ndef slow_async_add(to, x, y):\n    return rpc.rpc_async(to, slow_add, args=(x, y))\n```\n\nwe expect to see the original RPC profiled, the nested RPC profiled, and the actual torch.add() work. All of these events should be recorded with the correct node id. Here is an example profiling output:\n\n```\n-------------------------------------------------------------------------------------------------------------------------  ---------------  ---------------  ---------------  --------\n-------  ---------------  ---------------  ---------------\nName                                                                                                                       Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Node ID\n-------------------------------------------------------------------------------------------------------------------------  ---------------  ---------------  ---------------  --------\n-------  ---------------  ---------------  ---------------                                                                                                                            rpc_async#slow_async_add(worker1 -> worker2)                                                                               0.00%            0.000us          0                1.012s\n         1.012s           1                1\naten::empty                                                                                                                7.02%            11.519us         7.02%            11.519us         11.519us         1                1\nrpc_async#slow_async_add(worker1 -> worker2)#remote_op: rpc_async#slow_add(worker2 -> worker3)                             0.00%            0.000us          0                1.006s\n         1.006s           1                2                                                                                                                                          rpc_async#slow_async_add(worker1 -> worker2)#remote_op: aten::empty                                                        7.21%            11.843us         7.21%            11.843us\n         11.843us         1                2\nrpc_async#slow_async_add(worker1 -> worker2)#remote_op: rpc_async#slow_add(worker2 -> worker3)#remote_op: aten::add        71.94%           118.107us        85.77%           140.802us        140.802us        1                3\nrpc_async#slow_async_add(worker1 -> worker2)#remote_op: rpc_async#slow_add(worker2 -> worker3)#remote_op: aten::empty      13.82%           22.695us         13.82%           22.695us\n         22.695us         1                3                                                                                                                                          -------------------------------------------------------------------------------------------------------------------------  ---------------  ---------------  ---------------  --------\n-------  ---------------  ---------------  ---------------\nSelf CPU time total: 164.164us\n```\n\nThis PR also moves a bunch of the profiling logic to `rpc/utils.cpp` to declutter `request_callback` code.\nghstack-source-id: 112868470\n\nTest Plan:\n```\nrvarm1@devbig978:fbcode  (52dd34f6)$ buck test mode/no-gpu mode/dev-nosan //caffe2/test/distributed/rpc:process_group_agent -- test_rpc_profiling_async_function --print-passing-details --stress-runs 1\n```\n\nReviewed By: mrshenli\n\nDifferential Revision: D23638387\n\nfbshipit-source-id: eedb6d48173a4ecd41d70a9c64048920bd4807c4", "pr_number": "44664", "files_changed": ["test/cpp/jit/test_misc.cpp", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/profiler.h", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/process_group_agent_test_fixture.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "7c5436d557": {"title": "[RPC profiling] Add tests to ensure RPC profiling works on single threaded (#44923)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44923\n\nThis ensures that RPC profiling works in single-threaded server\nscenarios and that we won't make the assumption that we'll have multiple\nthreads when working on this code. For example, this assumption resulted in a\nbug in the previous diff (which was fixed)\nghstack-source-id: 112868469\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D23691304\n\nfbshipit-source-id: b17d34ade823794cbe949b70a5ab35723d974203", "pr_number": "44923", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "eee7dad376": {"title": "Add torch.do_assert, which is symbolically traceable (#45188)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45188\n\nThis is a symbolically traceable alternative to Python's `assert`.\nIt should be useful to allow people who want to use FX to also\nbe able to assert things.\n\nA bunch of TODO(before) land are inline - would love thoughts\non where is the best place for this code to live, and what this\nfunction should be called (since `assert` is reserved).\n\nTest Plan:\n```\npython test/test_fx.py TestFX.test_symbolic_trace_assert\n```\n\nImported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23861567\n\nfbshipit-source-id: d9d6b9556140faccc0290eba1fabea401d7850de", "pr_number": "45188", "files_changed": ["docs/source/torch.rst", "test/test_fx.py", "test/test_utils.py", "torch/__init__.py"], "labels": []}, "04be420549": {"title": "[static runtime] Remove ops in static from backwards compatibility checks (#45354)", "body": "Summary:\nThis should get the builds green again\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45354\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23939615\n\nPulled By: bwasti\n\nfbshipit-source-id: e93b11bc9592205e52330bb15928603b0aea21ac", "pr_number": "45354", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": []}, "3b7e4f89b2": {"title": "Add deprecation warning to PG backend and make TP backend stable. (#45356)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45356\n\nIn this PR, I'm adding a warning to the PG backend mentioning it would\nbe deprecated in the future. In addition to this I removed the warning from the\nTP backend that it is a beta feature.\nghstack-source-id: 112940501\n\nTest Plan: waitforbuildbot\n\nReviewed By: mrshenli\n\nDifferential Revision: D23940144\n\nfbshipit-source-id: d44054aa1e4ef61004a40bbe0ec45ff07829aad4", "pr_number": "45356", "files_changed": ["docs/source/rpc.rst"], "labels": []}, "a2b4177c5b": {"title": "Add barrier() at the end of init_process_group and new_group. (#45181)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45181\n\n`init_process_group` and `new_group` update a bunch of global\nvariables after initializing the actual process group. As a result, there is a\nrace that after initializing the process group on say rank 0, if we immediately\ncheck the default process group on rank 1 (say via RPC), we might actually get\nan error since rank 1 hasn't yet updated its _default_pg variable.\n\nTo resolve this issue, I've added barrier() at the end of both of these calls.\nThis ensures that once these calls return we are guaranteed about correct\ninitialization on all ranks.\n\nSince these calls are usually done mostly during initialization, it should be\nfine to add the overhead of a barrier() here.\n\n#Closes: https://github.com/pytorch/pytorch/issues/40434, https://github.com/pytorch/pytorch/issues/40378\nghstack-source-id: 112923112\n\nTest Plan:\nReproduced the failures in\nhttps://github.com/pytorch/pytorch/issues/40434 and\nhttps://github.com/pytorch/pytorch/issues/40378 and verified that this PR fixes\nthe issue.\n\nReviewed By: mrshenli\n\nDifferential Revision: D23858025\n\nfbshipit-source-id: c4d5e46c2157981caf3ba1525dec5310dcbc1830", "pr_number": "45181", "files_changed": ["test/cpp_extensions/cpp_c10d_extension.cpp", "test/distributed/test_c10d.py", "torch/distributed/distributed_c10d.py", "torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": []}, "37513a1118": {"title": "Use explicit templates in CUDALoops kernels (#44286)", "body": "Summary:\nReland attempt of https://github.com/pytorch/pytorch/pull/41059\nUse explicit templates instead of lambdas to reduce binary size without affecting the perf by 100-200Kb per arch per CU, namely:\nBinaryMulDivKernel.cu 3.8Mb -> 3.5Mb\nCompareEQKernel.cu 1.8Mb -> 1.7Mb\nBinaryAddSubKernel.cu 2.0Mb -> 1.8Mb\nBinaryBitwiseOpsKernels.cu 2.6Mb -> 2.3Mb\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44286\n\nReviewed By: ngimel\n\nDifferential Revision: D23859691\n\nPulled By: malfet\n\nfbshipit-source-id: 2c4e86f35e0f94a62294dc5d52a3ba364db23e2d", "pr_number": "44286", "files_changed": ["aten/src/ATen/native/cuda/AbsKernel.cu", "aten/src/ATen/native/cuda/BinaryAddSubKernel.cu", "aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "aten/src/ATen/native/cuda/CompareEQKernel.cu", "aten/src/ATen/native/cuda/CompareGEKernel.cu", "aten/src/ATen/native/cuda/CompareGTKernel.cu", "aten/src/ATen/native/cuda/CompareLEKernel.cu", "aten/src/ATen/native/cuda/CompareLTKernel.cu", "aten/src/ATen/native/cuda/CompareNEKernel.cu", "aten/src/ATen/native/cuda/FillKernel.cu", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu"], "labels": []}, "439930c81b": {"title": "adding a beta parameter to the smooth_l1 loss fn (#44433)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44433\n\nNot entirely sure why, but changing the type of beta from `float` to `double in autocast_mode.cpp and FunctionsManual.h fixes my compiler errors, failing instead at link time\n\nfixing some type errors, updated fn signature in a few more files\n\nremoving my usage of Scalar, making beta a double everywhere instead\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D23636720\n\nPulled By: bdhirsh\n\nfbshipit-source-id: caea2a1f8dd72b3b5fd1d72dd886b2fcd690af6d", "pr_number": "44433", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/PointwiseOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/PointwiseOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/cpp/api/functional.cpp", "tools/autograd/derivatives.yaml", "torch/csrc/api/include/torch/nn/functional/loss.h", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/nn/functional.py", "torch/nn/modules/loss.py", "torch/overrides.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "5211fb97ac": {"title": "Remove device maps from TensorPipe for v1.7 release (#45353)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45353\n\nTemporarily removing this feature, will add this back after branch cut.\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23939865\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7dceaffea6b9a16512b5ba6036da73e7f8f83a8e", "pr_number": "45353", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/backend_registry.py", "torch/distributed/rpc/options.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "8ab2ad306d": {"title": "Enable `torch.cuda.nccl` typechecking (#45344)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/45336\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45344\n\nReviewed By: walterddr\n\nDifferential Revision: D23935306\n\nPulled By: malfet\n\nfbshipit-source-id: dd09d4f8ff7a327131764487158675027a13bf69", "pr_number": "45344", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in", "torch/csrc/cuda/python_nccl.cpp", "torch/cuda/nccl.py"], "labels": ["module: typing", "triaged"]}, "0444c372e1": {"title": "[optimizer] introduce optimizer functional API, refactor Adagrad (#44715)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44715\n\nWe have provided a nice and intuitive API in Python. But in the context of large scale distributed training (e.g. Distributed Model Parallel), users often want to use multithreaded training instead of multiprocess training as it provides better resource utilization and efficiency.\n\nThis PR introduces functional optimizer concept (that is similar to the concept of `nn.functional`), we split optimizer into two parts: 1. optimizer state management 2. optimizer computation. We expose the computation part as a separate functional API that is available to be used by internal and OSS developers, the caller of the functional API will maintain their own states in order to directly calls the functional API. While maintaining the end user API be the same, the functional API is TorchScript friendly, and could be used by the distributed optimizer to speed up the training without GIL.\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D23935258\n\nPulled By: wanchaol\n\nfbshipit-source-id: d2a5228439edb3bc64f7771af2bb9e891847136a", "pr_number": "44715", "files_changed": ["torch/optim/adagrad.py", "torch/optim/functional.py"], "labels": []}, "08caf15502": {"title": "[optimizer] refactor Adam to use functional API (#44791)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44791\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D23935257\n\nPulled By: wanchaol\n\nfbshipit-source-id: 6f6e22a9287f5515d2e4e6abd4dee2fe7e17b945", "pr_number": "44791", "files_changed": ["test/test_optim.py", "torch/optim/adam.py", "torch/optim/functional.py"], "labels": []}, "32c355af5b": {"title": "[dist_optim] introduce distributed functional optimizer (#45221)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45221\n\nThis PR introduces a distributed functional optimizer, so that\ndistributed optimizer can reuse the functional optimizer APIs and\nmaintain their own states. This could enable the torchscript compatible\nfunctional optimizer when using distributed optimizer, helps getting rid\nof GIL and improve overall performance of training, especially distributed\nmodel parallel training\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D23935256\n\nPulled By: wanchaol\n\nfbshipit-source-id: 59b6d77ff4693ab24a6e1cbb6740bcf614cc624a", "pr_number": "45221", "files_changed": ["torch/distributed/optim/functional_adagrad.py", "torch/distributed/optim/optimizer.py", "torch/optim/functional.py", "torch/testing/_internal/distributed/rpc/dist_optimizer_test.py"], "labels": []}, "606b1a9a2e": {"title": "Move xla codegen to aten. (#45241)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45241\n\nTest Plan: Imported from OSS\n\nReviewed By: soumith\n\nDifferential Revision: D23926750\n\nPulled By: ailzhang\n\nfbshipit-source-id: f768e24a9baeca9f9df069a62d6f8b94a853a1ee", "pr_number": "45241", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/RegistrationDeclarations.h", "tools/codegen/api/dispatcher.py", "tools/codegen/gen.py"], "labels": []}, "958c208666": {"title": "[quant] conv_transpose graph patterns (#45078)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45078\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23821580\n\nPulled By: z-a-f\n\nfbshipit-source-id: 813a4ef1bbc429720765d61791fe754b6678a334", "pr_number": "45078", "files_changed": ["test/quantization/test_quantize_jit.py", "torch/csrc/jit/passes/graph_rewrite_helper.cpp", "torch/csrc/jit/passes/quantization/finalize.cpp", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/helper.h", "torch/csrc/jit/passes/quantization/quantization_patterns.h", "torch/testing/_internal/common_quantization.py"], "labels": ["oncall: jit"]}, "d2bd556e7d": {"title": "Quantization: add API summary section (#45093)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45093\n\nThis adds a tl;dr; style summary of the quantization API\nto the documentation. Hopefully this will make this easier\nfor new folks to learn how to use quantization.\n\nThis is not meant to be all-encompassing.  Future PRs\ncan improve the documentation further.\n\nTest Plan:\n1. build the doc as specified in https://github.com/pytorch/pytorch#building-the-documentation\n2. inspect the quantization page in Chrome, format looks good\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23828257\n\nPulled By: vkuzo\n\nfbshipit-source-id: 9311ee3f394cd83af0aeafb6e2fcdc3e0321fa38", "pr_number": "45093", "files_changed": ["docs/source/quantization.rst"], "labels": []}, "278da57255": {"title": "Quantization: combine previous summary with new summary (#45135)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45135\n\nThe previous quantization summary had steps on what to do for\ndynamic, static, QAT.  This PR moves these steps to comments in the\nexample code, so it is more clear how to accomplish the steps.\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23842456\n\nPulled By: vkuzo\n\nfbshipit-source-id: db2399e51e9ae33c8a1ac610e3d7dbdb648742b0", "pr_number": "45135", "files_changed": ["docs/source/quantization.rst"], "labels": []}, "eb39624394": {"title": "quant docs: add reduce_range explanatation to top level doc (#45305)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45305\n\nAdds an explanatation for reduce_range to the main quantization\ndoc page.\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23916669\n\nPulled By: vkuzo\n\nfbshipit-source-id: ef93fb774cb15741cd92889f114f6ab76c39f051", "pr_number": "45305", "files_changed": ["docs/source/quantization.rst"], "labels": []}, "7763e1d7b1": {"title": "quant docs: document how to customize qconfigs in eager mode (#45306)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45306\n\nAdds details to the main quantization doc on how specifically\nusers can skip or customize quantization of layers.\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23917034\n\nPulled By: vkuzo\n\nfbshipit-source-id: ccf71ce4300c1946b2ab63d1f35a07691fd7a2af", "pr_number": "45306", "files_changed": ["docs/source/quantization.rst"], "labels": []}, "92189b34b7": {"title": "Add get_all_users_of function to GraphManipulation (#45216)", "body": "Summary:\nThis PR adds get_all_users_of function. The function returns all the users of a specific node. A test unit is also added.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45216\n\nReviewed By: ezyang\n\nDifferential Revision: D23883572\n\nPulled By: scottxu0730\n\nfbshipit-source-id: 3eb68a411c3c6db39ed2506c9cb7bb7337520ee4", "pr_number": "45216", "files_changed": ["test/test_fx.py", "torch/fx/experimental/GraphManipulation.py", "torch/fx/experimental/__init__.py"], "labels": ["fx"]}, "d9af3d2fcd": {"title": "[quant] ConvTranspose warnings (#45081)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45081\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D23822449\n\nPulled By: z-a-f\n\nfbshipit-source-id: f21a5f3ef4d09f703c96fff0bc413dbadeac8202", "pr_number": "45081", "files_changed": ["aten/src/ATen/native/quantized/cpu/qconv.cpp", "torch/nn/quantized/modules/conv.py"], "labels": []}, "304e1d1e19": {"title": "[Distributed] getNumKeys API to c10d TCPStore (#43962)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43962\n\nTCPStore needs a getNumKeys API for our logging needs.\nghstack-source-id: 112939761\n\nTest Plan: Adding tests to C++ Store Tests\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D22985085\n\nfbshipit-source-id: 8a0d286fbd6fd314dcc997bae3aad0e62b51af83", "pr_number": "43962", "files_changed": ["caffe2/distributed/file_store_handler.cc", "caffe2/distributed/file_store_handler.h", "caffe2/distributed/redis_store_handler.cc", "caffe2/distributed/redis_store_handler.h", "caffe2/distributed/store_handler.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/FileStore.hpp", "torch/lib/c10d/HashStore.cpp", "torch/lib/c10d/HashStore.hpp", "torch/lib/c10d/PrefixStore.cpp", "torch/lib/c10d/PrefixStore.hpp", "torch/lib/c10d/Store.hpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": []}, "addf94f2d6": {"title": "[Distributed] DeleteKey API for c10d TCP Store (#43963)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43963\n\nAdded a DeleteKey API for the TCP Store\nghstack-source-id: 112939762\n\nTest Plan:\nModified the existing get/set test to use delete. verified that the\ncorrect keys were deleted and that the numKeys API returned the right values\n\nReviewed By: jiayisuse\n\nDifferential Revision: D23009117\n\nfbshipit-source-id: 1a0d95b43d79e665a69b2befbaa059b2b50a1f66", "pr_number": "43963", "files_changed": ["caffe2/distributed/file_store_handler.cc", "caffe2/distributed/file_store_handler.h", "caffe2/distributed/redis_store_handler.cc", "caffe2/distributed/redis_store_handler.h", "caffe2/distributed/store_handler.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/FileStore.hpp", "torch/lib/c10d/HashStore.cpp", "torch/lib/c10d/HashStore.hpp", "torch/lib/c10d/PrefixStore.cpp", "torch/lib/c10d/PrefixStore.hpp", "torch/lib/c10d/Store.hpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": []}, "cf808bed73": {"title": "[Distributed] Adding Python tests for the TCPStore getNumKeys and deleteKey (#45223)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45223\n\nPrevious diffs in this stack implemented the getNumKeys and deleteKey\nAPIs in the c10d Store as well as added tests at the C++ layer. This diff adds\ntests at the Python level in test_c10d.py\nghstack-source-id: 112939763\n\nTest Plan: Ensured these new python tests as well as previous C++ tests pass\n\nReviewed By: jiayisuse\n\nDifferential Revision: D23878455\n\nfbshipit-source-id: 0a17ecf66b28d46438a77346e5bf36414e05e25c", "pr_number": "45223", "files_changed": ["test/distributed/test_c10d.py"], "labels": []}, "0fa551f0ab": {"title": "[c2] Fix int types for learning rate", "body": "Summary: Currently GetSingleArgument is overflowing since it's expecting an int instead of an int64 when using a 1cycle (hill policy) annealing schedule\n\nTest Plan:\nunittest\n\nbuck test  caffe2/caffe2/python/operator_test:learning_rate_op_test\n\nDifferential Revision: D23938169\n\nfbshipit-source-id: 20d65df800d7a0f1dd9520705af31f63ae716463", "pr_number": null, "files_changed": ["caffe2/python/operator_test/learning_rate_op_test.py", "caffe2/sgd/learning_rate_op.h"], "labels": []}, "2b21e7767e": {"title": "Added optimizers based on multi tensor apply (#45299)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45299\n\nAdding a new namespace `torch.optim._multi_tensor` with a bunch of updated optimizers. Those optimizers are using _foreach APIs which improve performance significantly.\n\n### Tests\n- updated existing tests to use both optimizers\n- added `test_multi_tensor_optimizers` test to verify correctness.\n\n### Perf results\n\n**Adam**\ntimeit: 42.69 ms --> 10.16 ms\nautorange: 41.96 ms --> 10.28 ms\n\n**AdamW**\ntimeit: 51.38 ms --> 15.63 ms\nautorange: 50.82 ms --> 16.07 ms\n\n**SGD**\ntimeit: 6.28 ms --> 4.40 ms\nautorange: 6.13 ms --> 4.73 ms\n\n**RMSprop**\ntimeit: 28.63 ms --> 5.89 ms\nautorange: 28.27 ms -->  5.76 ms\n\n**Rprop**\ntimeit: 213.30 --> 178.42\nautorange: 212.03 --> 178.03\n\n**ASGD**\ntimeit: 21.67 --> 9.33\nautorange: 21.64 --> 9.27\n\n**Adamax**\ntimeit: 55.60 --> 48.29\nautorange: 55.22 -> 49.13\n\n**Rerf Script used**\n\n```\nimport torch\nimport time\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau, StepLR\nimport torch.nn as nn\nimport time\nimport torchvision\nimport torch.utils._benchmark as benchmark_utils\n\ndevice = \"cuda\"\nmodel = torchvision.models.resnet.resnet101(pretrained=True).to(device)\ntargets = torch.randint(0, 1000, (100, 100), device=device)\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.SGD(model.parameters(), lr=1e-3) # <----------------------- optimizer.\n                                                          # would compare optim.SGD vs optim._multi_tensor.SGD\nrunning_loss = 0.0\ntarget = torch.empty(128, dtype=torch.long, device=device).random_(5)\n\noptimizer.zero_grad()\ninputs = torch.rand(128, 3, 100, 100, device=device , requires_grad=True)\noutputs = model(inputs)\nloss = criterion(outputs, target)\nloss.backward()\noptimizer.step()\nrunning_loss += loss.item()\n\ndef main():\n    timer = benchmark_utils.Timer(\n        stmt=\"optimizer.step()\",\n        globals=globals(),\n        label=\"str(optimizer)\",\n    )\n\n    for i in range(1):\n        print(f\"Run: {i}\\n{'-' * 40}\")\n        print(f\"timeit:\\n{timer.timeit(1000)}\\n\")\n        print(f\"autorange:\\n{timer.blocked_autorange()}\\n\\n\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23931987\n\nPulled By: izdeby\n\nfbshipit-source-id: 582134ef2d402909d27d89a45c5b588fb7130ea1", "pr_number": "45299", "files_changed": ["test/test_optim.py", "torch/__init__.py", "torch/optim/_multi_tensor/__init__.py", "torch/optim/_multi_tensor/__init__.pyi", "torch/optim/_multi_tensor/adadelta.py", "torch/optim/_multi_tensor/adadelta.pyi", "torch/optim/_multi_tensor/adam.py", "torch/optim/_multi_tensor/adam.pyi", "torch/optim/_multi_tensor/adamax.py", "torch/optim/_multi_tensor/adamax.pyi", "torch/optim/_multi_tensor/adamw.py", "torch/optim/_multi_tensor/adamw.pyi", "torch/optim/_multi_tensor/asgd.py", "torch/optim/_multi_tensor/asgd.pyi", "torch/optim/_multi_tensor/rmsprop.py", "torch/optim/_multi_tensor/rmsprop.pyi", "torch/optim/_multi_tensor/rprop.py", "torch/optim/_multi_tensor/rprop.pyi", "torch/optim/_multi_tensor/sgd.py", "torch/optim/_multi_tensor/sgd.pyi"], "labels": []}, "19dda7c68a": {"title": "Fallback to CPU when remote end does not have CUDA for profiling (#44967)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44967\n\nWhen enabling profiler on server, if it is a different machine it may\nnot have CUDA while caller does. In this case, we would crash but now we\nfallback to CPU and log a warning.\nghstack-source-id: 112977906\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23790729\n\nfbshipit-source-id: dc6eba172b7e666842d54553f52a6b9d5f0a5362", "pr_number": "44967", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_impl.h", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/request_callback_no_python.h"], "labels": []}, "23dfca8351": {"title": "Support record_shapes in RPC profiling (#44419)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44419\n\nCloses https://github.com/pytorch/pytorch/issues/39969\n\nThis PR adds support for propagation of input shapes over the wire when the profiler is invoked with `record_shapes=True` over RPC. Previously, we did not respect this argument.\n\nThis is done by saving the shapes as an ivalue list and recovering it as the type expected (`std::vector<std::vector<int>>` on the client). Test is added to ensure that remote ops have the same `input_shapes` as if the op were run locally.\nghstack-source-id: 112977899\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23591274\n\nfbshipit-source-id: 7cf3b2e8df26935ead9d70e534fc2c872ccd6958", "pr_number": "44419", "files_changed": ["torch/csrc/autograd/profiler.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "e52762cbb7": {"title": "Revert D23917034: quant docs: document how to customize qconfigs in eager mode", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23917034 (https://github.com/pytorch/pytorch/commit/7763e1d7b1be5b13394396646cb00a8766b4a626)\n\nOriginal commit changeset: ccf71ce4300c\n\nfbshipit-source-id: 9ce99e880b4a22e824f4413354a0f3703e7c5c2c", "pr_number": null, "files_changed": ["docs/source/quantization.rst"], "labels": []}, "54a253fded": {"title": "Revert D23931987: Added optimizers based on multi tensor apply", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23931987 (https://github.com/pytorch/pytorch/commit/2b21e7767e51d0a57297922598edb0fff9f0e722)\n\nOriginal commit changeset: 582134ef2d40\n\nfbshipit-source-id: ffd500aea55fda34155442fb15e2529cb9c00100", "pr_number": null, "files_changed": ["test/test_optim.py", "torch/__init__.py", "torch/optim/_multi_tensor/__init__.py", "torch/optim/_multi_tensor/__init__.pyi", "torch/optim/_multi_tensor/adadelta.py", "torch/optim/_multi_tensor/adadelta.pyi", "torch/optim/_multi_tensor/adam.py", "torch/optim/_multi_tensor/adam.pyi", "torch/optim/_multi_tensor/adamax.py", "torch/optim/_multi_tensor/adamax.pyi", "torch/optim/_multi_tensor/adamw.py", "torch/optim/_multi_tensor/adamw.pyi", "torch/optim/_multi_tensor/asgd.py", "torch/optim/_multi_tensor/asgd.pyi", "torch/optim/_multi_tensor/rmsprop.py", "torch/optim/_multi_tensor/rmsprop.pyi", "torch/optim/_multi_tensor/rprop.py", "torch/optim/_multi_tensor/rprop.pyi", "torch/optim/_multi_tensor/sgd.py", "torch/optim/_multi_tensor/sgd.pyi"], "labels": []}, "3da1061059": {"title": "Revert D23916669: quant docs: add reduce_range explanatation to top level doc", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23916669 (https://github.com/pytorch/pytorch/commit/eb3962439404cff611d83e932dafc77f3cf32d5f)\n\nOriginal commit changeset: ef93fb774cb1\n\nfbshipit-source-id: 7b56020427e76e13f847494044179c81d508db11", "pr_number": null, "files_changed": ["docs/source/quantization.rst"], "labels": []}, "110aa45387": {"title": "Revert D23842456: Quantization: combine previous summary with new summary", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23842456 (https://github.com/pytorch/pytorch/commit/278da57255f6498010b1886782b6203196bb2471)\n\nOriginal commit changeset: db2399e51e9a\n\nfbshipit-source-id: 7878257330bf83751cb17c0971a5c894bdf256ba", "pr_number": null, "files_changed": ["docs/source/quantization.rst"], "labels": []}, "37a671abc7": {"title": "Revert D23828257: Quantization: add API summary section", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23828257 (https://github.com/pytorch/pytorch/commit/d2bd556e7d311c326b0e056bbba7ff665e2acf8a)\n\nOriginal commit changeset: 9311ee3f394c\n\nfbshipit-source-id: 80b16fc123191e249e6a070ec5360a15fe91cf61", "pr_number": null, "files_changed": ["docs/source/quantization.rst"], "labels": []}, "8cef7326f4": {"title": "Benchmarks: add 'default' options for fuser and executor. (#45347)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45347\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D23935519\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 8323fafe7828683c4d29c12a1e5722adb6f945ff", "pr_number": "45347", "files_changed": ["benchmarks/fastrnns/fuser.py"], "labels": []}, "a07d82982a": {"title": "CI: Add a run of FastRNN benchmarks in default executor/fuser configuration. (#45348)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45348\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D23935520\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: efecaaab68caaaa057b354884f4ae37b6ef36983", "pr_number": "45348", "files_changed": [".jenkins/pytorch/test.sh"], "labels": []}, "bc5710f2f7": {"title": "Benchmarks: tweak PE config settings. (#45349)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45349\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D23935518\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 5a7c508c6fc84eafbc23399f095d732b903510dc", "pr_number": "45349", "files_changed": ["benchmarks/fastrnns/fuser.py"], "labels": []}, "f84b2e865f": {"title": "Revert D23878455: [Distributed] Adding Python tests for the TCPStore getNumKeys and deleteKey", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23878455 (https://github.com/pytorch/pytorch/commit/cf808bed738632dba15531337ceb9fa5f148d7b4)\n\nOriginal commit changeset: 0a17ecf66b28\n\nfbshipit-source-id: 93e60b23f66324e3e5266c45abb0cec295bb3d23", "pr_number": null, "files_changed": ["test/distributed/test_c10d.py"], "labels": []}, "78caa028b6": {"title": "Revert D23009117: [Distributed] DeleteKey API for c10d TCP Store", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23009117 (https://github.com/pytorch/pytorch/commit/addf94f2d6518666ef4b86007b098c1b0f864b44)\n\nOriginal commit changeset: 1a0d95b43d79\n\nfbshipit-source-id: ad3fe5501267e1a0a7bf23410766f1e92b34b24d", "pr_number": null, "files_changed": ["caffe2/distributed/file_store_handler.cc", "caffe2/distributed/file_store_handler.h", "caffe2/distributed/redis_store_handler.cc", "caffe2/distributed/redis_store_handler.h", "caffe2/distributed/store_handler.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/FileStore.hpp", "torch/lib/c10d/HashStore.cpp", "torch/lib/c10d/HashStore.hpp", "torch/lib/c10d/PrefixStore.cpp", "torch/lib/c10d/PrefixStore.hpp", "torch/lib/c10d/Store.hpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": []}, "4005afe94b": {"title": "[ONNX] Update narrow for dynamic inputs (#44039)", "body": "Summary:\nUpdate narrow for dynamic inputs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44039\n\nReviewed By: mruberry\n\nDifferential Revision: D23742215\n\nPulled By: bzinodev\n\nfbshipit-source-id: 0d58d2fe996f91a124af988a9a21ee433e842d07", "pr_number": "44039", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py"], "labels": ["open source", "triaged"]}, "5b839bca78": {"title": "[ONNX] Optimize export_onnx api to reduce string and model proto exchange (#44332)", "body": "Summary:\nOptimize export_onnx api to reduce string and model proto exchange in export.cpp\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44332\n\nReviewed By: bwasti, eellison\n\nDifferential Revision: D23880129\n\nPulled By: bzinodev\n\nfbshipit-source-id: 1d216d8f710f356cbba2334fb21ea15a89dd16fa", "pr_number": "44332", "files_changed": ["torch/csrc/jit/passes/onnx/shape_type_inference.cpp", "torch/csrc/jit/python/python_ir.cpp", "torch/csrc/jit/serialization/export.cpp", "torch/csrc/jit/serialization/export.h"], "labels": ["oncall: jit", "open source", "triaged"]}, "8b143771d0": {"title": "Updates and simplifies nonzero as_tuple behavior", "body": "", "pr_number": null, "files_changed": ["test/test_torch.py", "tools/autograd/templates/python_torch_functions.cpp"], "labels": []}, "8bdbedd4ee": {"title": "Revert \"Updates and simplifies nonzero as_tuple behavior\"", "body": "This reverts commit 8b143771d0f0bcd93d925263adc8b0d6b235b398.", "pr_number": null, "files_changed": ["test/test_torch.py", "tools/autograd/templates/python_torch_functions.cpp"], "labels": []}, "c3bf402cbb": {"title": "handle onnx nll with default ignore index (#44816)", "body": "Summary:\nin ONNX NegativeLogLikelihoodLoss specification, ignore_index is optional without default value.\ntherefore, when convert nll op to ONNX, we need to set ignore_index attribute even if it is not specified (e.g. ignore_index=-100).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44816\n\nReviewed By: ezyang\n\nDifferential Revision: D23880354\n\nPulled By: bzinodev\n\nfbshipit-source-id: d0bdd58d0a4507ed9ce37133e68533fe6d1bdf2b", "pr_number": "44816", "files_changed": ["test/onnx/expect/TestOperators.test_softmaxcrossentropy.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_3d.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_3d_none.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_4d.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_weights.expect", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset12.py"], "labels": ["module: onnx", "open source", "triaged"]}, "13f76f2be4": {"title": "Fix preserve submodule attribute in freezing (#45143)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45143\n\nThis PR prevents freezing cleaning up a submodule when user requests to\npreserve a submodule.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23844969\n\nPulled By: bzinodev\n\nfbshipit-source-id: 80e6db3fc12460d62e634ea0336ae2a3551c2151", "pr_number": "45143", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/freeze_module.cpp"], "labels": ["oncall: jit"]}, "95a97e51b5": {"title": "[ONNX] Improve scripting inplace indexing ops (#44351)", "body": "Summary:\nFix a couple of issues with scripting inplace indexing in prepare_inplace_ops_for_onnx pass.\n1- Tracing index copy (such as cases lik x[1:3] = data) already applies broadcasting on rhs if needed. The broadcasting node (aten::expand) is missing in scripting cases.\n\n2- Inplace indexing with ellipsis (aten::copy_) is replaced with aten::index_put and then handled with slice+select in this pass.\nSupport for negative indices for this op added.\n\nShape inference is also enabled for scripting tests using new JIT API.\nA few more tests are enabled for scripting.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44351\n\nReviewed By: ezyang\n\nDifferential Revision: D23880267\n\nPulled By: bzinodev\n\nfbshipit-source-id: 78b33444633eb7ae0fbabc7415e3b16001f5207f", "pr_number": "44351", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_models.py", "test/onnx/test_models_onnxruntime.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp", "torch/csrc/jit/passes/remove_inplace_ops.cpp", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["oncall: jit", "open source"]}, "7818a214c5": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D23959094\n\nfbshipit-source-id: 6caa046d263114bff38a38d756099aac357e4f04", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": []}, "6417a70465": {"title": "Updates linalg warning + docs (#45415)", "body": "Summary:\nChanges the deprecation of norm to a docs deprecation, since PyTorch components still rely on norm and some behavior, like automatically flattening tensors, may need to be ported to torch.linalg.norm. The documentation is also updated to clarify that torch.norm and torch.linalg.norm are distinct.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45415\n\nReviewed By: ngimel\n\nDifferential Revision: D23958252\n\nPulled By: mruberry\n\nfbshipit-source-id: fd54e807c59a2655453a6bcd9f4073cb2c12e8ac", "pr_number": "45415", "files_changed": ["test/test_linalg.py", "torch/functional.py"], "labels": []}, "e4950a093a": {"title": "Backward support for generalized eigenvalue solver with LOBPCG in forward [only k-rank SYMEIG case] (#43002)", "body": "Summary:\nAs per title. Fixes [#{38948}](https://github.com/pytorch/pytorch/issues/38948). Therein you can find some blueprints for the algorithm being used in this PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43002\n\nReviewed By: zou3519\n\nDifferential Revision: D23931326\n\nPulled By: albanD\n\nfbshipit-source-id: e6994af70d94145f974ef87aa5cea166d6deff1e", "pr_number": "43002", "files_changed": ["test/test_autograd.py", "torch/_lobpcg.py"], "labels": ["module: autograd", "open source", "triaged"]}, "e2ffdf467a": {"title": "docker: Add torchelastic to docker image (#45438)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45438\n\nAdds torchelastic (as well as its dependencies) to the official docker\nimages\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: tierex\n\nDifferential Revision: D23963787\n\nPulled By: seemethere\n\nfbshipit-source-id: 54ebb4b9c50699e543f264975dadf99badf55753", "pr_number": "45438", "files_changed": ["Dockerfile"], "labels": ["module: docker"]}, "48d29c830d": {"title": "[hotfix] disable problematic cuda tests on rocm builds (#45435)", "body": "Summary:\nDisable the recent 3 cuda tests on amd rocm build/tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45435\n\nReviewed By: malfet\n\nDifferential Revision: D23962881\n\nPulled By: walterddr\n\nfbshipit-source-id: ad4ea1f835b4722cdbdce685806cfd64376cc16f", "pr_number": "45435", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": []}, "e5242aaf89": {"title": "Update TensorPipe submodule (#45433)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45433\n\nPrimarily in order to pick up the fix landed in https://github.com/pytorch/tensorpipe/pull/225 which fixes the handling of scopes in link-local IPv6 addresses, which was reported by a user.\n\nTest Plan: The specific upstream change is covered by new unit tests. The submodule update will be validated by the PyTorch CI.\n\nReviewed By: beauby\n\nDifferential Revision: D23962289\n\nfbshipit-source-id: 4ed762fc19c4aeb1398d1337d61b3188c4c228be", "pr_number": "45433", "files_changed": ["third_party/tensorpipe"], "labels": ["fb-exported"]}, "993628c74a": {"title": "Build shape expressions and remove outputs that are only used by `aten::size`s (#45080)", "body": "Summary:\nCurrently, TE materializes all intermediate results even if they are only used for computing their shapes. This diff ports the approach the OF (Old Fuser) took to deal with this issue. Namely, given the structure of a fusion group we infer all the sizes outside a fusion group based on fusion group's inputs.\n\nA simple example would be:\n\n```\n        def test_fuse(a, b):\n            c = a + b\n            d = c + b\n            return d\n```\n\nHere we don't need to cache `c` as computing a gradient for `b` in `d = c + b` doesn't need it. We do need to compute sizes for all arguments here in case broadcasts happen.\n\nWithout this optimization, TE would need to materialize `c` so we can get its size\n\n```\n[DUMP profiling_graph_executor_impl.cpp:499] Optimized Graph:\n[DUMP profiling_graph_executor_impl.cpp:499] graph(%a.1 : Tensor,\n[DUMP profiling_graph_executor_impl.cpp:499]       %b.1 : Tensor):\n[DUMP profiling_graph_executor_impl.cpp:499]   %11 : Tensor = prim::DifferentiableGraph_0(%b.1, %a.1)\n[DUMP profiling_graph_executor_impl.cpp:499]   return (%11)\n[DUMP profiling_graph_executor_impl.cpp:499] with prim::DifferentiableGraph_0 = graph(%11 : Tensor,\n[DUMP profiling_graph_executor_impl.cpp:499]       %13 : Tensor):\n[DUMP profiling_graph_executor_impl.cpp:499]   %59 : int[] = aten::size(%13) # <string>:3:44\n[DUMP profiling_graph_executor_impl.cpp:499]   %62 : int[] = aten::size(%11) # <string>:3:93\n[DUMP profiling_graph_executor_impl.cpp:499]   %83 : Double(1:1, requires_grad=0, device=cuda:0), %84 : Double(1:1, requires_grad=0, device=cuda:0), %85 : bool = prim::TypeCheck(%11, %13)\n[DUMP profiling_graph_executor_impl.cpp:499]   %86 : Tensor, %87 : Tensor = prim::If(%85)\n[DUMP profiling_graph_executor_impl.cpp:499]     block0():\n[DUMP profiling_graph_executor_impl.cpp:499]       %d.4 : Double(1:1, requires_grad=0, device=cuda:0), %c.4 : Double(1:1, requires_grad=0, device=cuda:0) = prim::TensorExprGroup_0(%83, %84)\n[DUMP profiling_graph_executor_impl.cpp:499]       -> (%d.4, %c.4)\n[DUMP profiling_graph_executor_impl.cpp:499]     block1():\n[DUMP profiling_graph_executor_impl.cpp:499]       %94 : Function = prim::Constant[name=\"fallback_function\", fallback=1]()\n[DUMP profiling_graph_executor_impl.cpp:499]       %95 : (Tensor, Tensor) = prim::CallFunction(%94, %11, %13)\n[DUMP profiling_graph_executor_impl.cpp:499]       %96 : Tensor, %97 : Tensor = prim::TupleUnpack(%95)\n[DUMP profiling_graph_executor_impl.cpp:499]       -> (%96, %97)\n[DUMP profiling_graph_executor_impl.cpp:499]   %60 : int[] = aten::size(%87) # <string>:3:55\n[DUMP profiling_graph_executor_impl.cpp:499]   %61 : int[]? = aten::_size_if_not_equal(%59, %60) # <string>:3:19\n[DUMP profiling_graph_executor_impl.cpp:499]   %64 : int[]? = aten::_size_if_not_equal(%62, %60) # <string>:3:68\n[DUMP profiling_graph_executor_impl.cpp:499]   %67 : int[] = aten::size(%86) # <string>:3:55\n[DUMP profiling_graph_executor_impl.cpp:499]   %68 : int[]? = aten::_size_if_not_equal(%60, %67) # <string>:3:19\n[DUMP profiling_graph_executor_impl.cpp:499]   %71 : int[]? = aten::_size_if_not_equal(%62, %67) # <string>:3:68\n[DUMP profiling_graph_executor_impl.cpp:499]   return (%86, %61, %64, %68, %71)\n[DUMP profiling_graph_executor_impl.cpp:499] with prim::TensorExprGroup_0 = graph(%1 : Double(1:1, requires_grad=0, device=cuda:0),\n[DUMP profiling_graph_executor_impl.cpp:499]       %4 : Double(1:1, requires_grad=0, device=cuda:0)):\n[DUMP profiling_graph_executor_impl.cpp:499]   %5 : int = prim::Constant[value=1]()\n[DUMP profiling_graph_executor_impl.cpp:499]   %c.3 : Double(1:1, requires_grad=0, device=cuda:0) = aten::add(%4, %1, %5) # /scratch/villedepommes/pytorches/bench/test/test_jit.py:2872:16\n[DUMP profiling_graph_executor_impl.cpp:499]   %2 : int = prim::Constant[value=1]()\n[DUMP profiling_graph_executor_impl.cpp:499]   %d.3 : Double(1:1, requires_grad=0, device=cuda:0) = aten::add(%c.3, %1, %2) # /scratch/villedepommes/pytorches/bench/test/test_jit.py:2873:16\n[DUMP profiling_graph_executor_impl.cpp:499]   return (%d.3, %c.3)\n```\n\nWith this optimization we use `prim::BroadcastSizes` to compute the size of `c`. No need to materialize it.\n\n```\n[DUMP profiling_graph_executor_impl.cpp:499] Optimized Graph:\n[DUMP profiling_graph_executor_impl.cpp:499] graph(%a.1 : Tensor,\n[DUMP profiling_graph_executor_impl.cpp:499]       %b.1 : Tensor):\n[DUMP profiling_graph_executor_impl.cpp:499]   %11 : Tensor = prim::DifferentiableGraph_0(%b.1, %a.1)\n[DUMP profiling_graph_executor_impl.cpp:499]   return (%11)\n[DUMP profiling_graph_executor_impl.cpp:499] with prim::DifferentiableGraph_0 = graph(%11 : Tensor,\n[DUMP profiling_graph_executor_impl.cpp:499]       %13 : Tensor):\n[DUMP profiling_graph_executor_impl.cpp:499]   %59 : int[] = aten::size(%13) # <string>:3:44\n[DUMP profiling_graph_executor_impl.cpp:499]   %62 : int[] = aten::size(%11) # <string>:3:93\n[DUMP profiling_graph_executor_impl.cpp:499]   %88 : Double(1:1, requires_grad=0, device=cuda:0), %89 : Double(1:1, requires_grad=0, device=cuda:0), %90 : bool = prim::TypeCheck(%11, %13)\n[DUMP profiling_graph_executor_impl.cpp:499]   %91 : Tensor = prim::If(%90)\n[DUMP profiling_graph_executor_impl.cpp:499]     block0():\n[DUMP profiling_graph_executor_impl.cpp:499]       %d.4 : Double(1:1, requires_grad=0, device=cuda:0) = prim::TensorExprGroup_0(%88, %89)\n[DUMP profiling_graph_executor_impl.cpp:499]       -> (%d.4)\n[DUMP profiling_graph_executor_impl.cpp:499]     block1():\n[DUMP profiling_graph_executor_impl.cpp:499]       %97 : Function = prim::Constant[name=\"fallback_function\", fallback=1]()\n[DUMP profiling_graph_executor_impl.cpp:499]       %98 : (Tensor) = prim::CallFunction(%97, %11, %13)\n[DUMP profiling_graph_executor_impl.cpp:499]       %99 : Tensor = prim::TupleUnpack(%98)\n[DUMP profiling_graph_executor_impl.cpp:499]       -> (%99)\n[DUMP profiling_graph_executor_impl.cpp:499]   %85 : int[] = aten::size(%91)\n[DUMP profiling_graph_executor_impl.cpp:499]   %86 : int[] = prim::BroadcastSizes(%59, %62)\n[DUMP profiling_graph_executor_impl.cpp:499]   %61 : int[]? = aten::_size_if_not_equal(%59, %86) # <string>:3:19\n[DUMP profiling_graph_executor_impl.cpp:499]   %64 : int[]? = aten::_size_if_not_equal(%62, %86) # <string>:3:68\n[DUMP profiling_graph_executor_impl.cpp:499]   %68 : int[]? = aten::_size_if_not_equal(%86, %85) # <string>:3:19\n[DUMP profiling_graph_executor_impl.cpp:499]   %71 : int[]? = aten::_size_if_not_equal(%62, %85) # <string>:3:68\n[DUMP profiling_graph_executor_impl.cpp:499]   return (%91, %61, %64, %68, %71)\n[DUMP profiling_graph_executor_impl.cpp:499] with prim::TensorExprGroup_0 = graph(%1 : Double(1:1, requires_grad=0, device=cuda:0),\n[DUMP profiling_graph_executor_impl.cpp:499]       %4 : Double(1:1, requires_grad=0, device=cuda:0)):\n[DUMP profiling_graph_executor_impl.cpp:499]   %5 : int = prim::Constant[value=1]()\n[DUMP profiling_graph_executor_impl.cpp:499]   %c.3 : Double(1:1, requires_grad=0, device=cuda:0) = aten::add(%4, %1, %5) # /scratch/villedepommes/pytorches/bench/test/test_jit.py:2872:16\n[DUMP profiling_graph_executor_impl.cpp:499]   %2 : int = prim::Constant[value=1]()\n[DUMP profiling_graph_executor_impl.cpp:499]   %d.3 : Double(1:1, requires_grad=0, device=cuda:0) = aten::add(%c.3, %1, %2) # /scratch/villedepommes/pytorches/bench/test/test_jit.py:2873:16\n[DUMP profiling_graph_executor_impl.cpp:499]   return (%d.3)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45080\n\nReviewed By: bertmaher\n\nDifferential Revision: D23856410\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 2956286eb03a4894a5baa151c35e6092466322b1", "pr_number": "45080", "files_changed": ["test/test_jit_fuser_te.py", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.h"], "labels": ["oncall: jit"]}, "03342af3a3": {"title": "Add env variable to bypass CUDACachingAllocator for debugging (#45294)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45294\n\nWhile tracking down a recent memory corruption bug we found that\ncuda-memcheck wasn't finding the bad accesses, and ngimel pointed out that\nit's because we use a caching allocator so a lot of \"out of bounds\" accesses\nland in a valid slab.\n\nThis PR adds a runtime knob (`PYTORCH_NO_CUDA_MEMORY_CACHING`) that, when set,\nbypasses the caching allocator's caching logic so that allocations go straight\nto cudaMalloc.  This way, cuda-memcheck will actually work.\n\nTest Plan:\nInsert some memory errors and run a test under cuda-memcheck;\nobserve that cuda-memcheck flags an error where expected.\n\nSpecifically I removed the output-masking logic here:\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/tensorexpr/cuda_codegen.cpp#L819-L826\n\nAnd ran:\n```\nPYTORCH_NO_CUDA_MEMORY_CACHING=1 cuda-memcheck pytest -k test_superslomo test_jit_fuser_te.py\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D23964734\n\nPulled By: bertmaher\n\nfbshipit-source-id: 04efd11e8aff037b9edde80c70585cb820ee6e39", "pr_number": "45294", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp", "docs/source/notes/cuda.rst"], "labels": []}, "36c3fbc9e3": {"title": "CUDA BFloat Conv (non-cuDNN) (#45007)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45007\n\nReviewed By: zou3519\n\nDifferential Revision: D23933174\n\nPulled By: ngimel\n\nfbshipit-source-id: 84eb028f09c9197993fb9981c0efb535014e5f78", "pr_number": "45007", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/THC/THCBlas.cu", "aten/src/THC/THCBlas.h", "aten/src/THCUNN/generic/SpatialConvolutionMM.cu", "aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu", "test/test_nn.py", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "6ab1c0b1ca": {"title": "Disable a few tests in preparation to enabling PE+TE (#44815)", "body": "Summary:\nDisable a few tests in preparation to enabling PE+TE\nNext PR: https://github.com/pytorch/pytorch/pull/45396\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44815\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23948445\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 93e641b7b8a3f13bd3fd3840116076553408f224", "pr_number": "44815", "files_changed": ["test/test_jit.py", "test/test_jit_cuda_fuser.py"], "labels": ["oncall: jit"]}, "87b356d093": {"title": "[static runtime] Split out graph preparation from runtime (#44131)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44131\n\nTest Plan: Imported from OSS\n\nReviewed By: hlu1\n\nDifferential Revision: D23604305\n\nPulled By: bwasti\n\nfbshipit-source-id: 7b47da4961d99074199417ef1407a788c7d80ee6", "pr_number": "44131", "files_changed": ["benchmarks/static_runtime/deep_wide_pt_bench.cc", "benchmarks/static_runtime/test_static_runtime.cc", "test/test_static_runtime.py", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/init.cpp"], "labels": ["oncall: jit"]}, "722faeb2a4": {"title": "[RELAND] Added optimizers based on multi tensor apply (#45408)", "body": "Summary:\nOriginal PR https://github.com/pytorch/pytorch/pull/45299.  The present PR fixes minor bugs that caused revert.\n\nAdding a new namespace `torch.optim._multi_tensor` with a bunch of updated optimizers. Those optimizers are using _foreach APIs which improve performance significantly.\n\n### Tests\n- updated existing tests to use both optimizers\n- added `test_multi_tensor_optimizers` test to verify correctness.\n\n### Perf results\n\n**Adam**\ntimeit: 42.69 ms --> 10.16 ms\nautorange: 41.96 ms --> 10.28 ms\n\n**AdamW**\ntimeit: 51.38 ms --> 15.63 ms\nautorange: 50.82 ms --> 16.07 ms\n\n**SGD**\ntimeit: 6.28 ms --> 4.40 ms\nautorange: 6.13 ms --> 4.73 ms\n\n**RMSprop**\ntimeit: 28.63 ms --> 5.89 ms\nautorange: 28.27 ms -->  5.76 ms\n\n**Rprop**\ntimeit: 213.30 --> 178.42\nautorange: 212.03 --> 178.03\n\n**ASGD**\ntimeit: 21.67 --> 9.33\nautorange: 21.64 --> 9.27\n\n**Adamax**\ntimeit: 55.60 --> 48.29\nautorange: 55.22 -> 49.13\n\n**Rerf Script used**\n\n```\nimport torch\nimport time\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau, StepLR\nimport torch.nn as nn\nimport time\nimport torchvision\nimport torch.utils._benchmark as benchmark_utils\n\ndevice = \"cuda\"\nmodel = torchvision.models.resnet.resnet101(pretrained=True).to(device)\ntargets = torch.randint(0, 1000, (100, 100), device=device)\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.SGD(model.parameters(), lr=1e-3) # <----------------------- optimizer.\n                                                          # would compare optim.SGD vs optim._multi_tensor.SGD\nrunning_loss = 0.0\ntarget = torch.empty(128, dtype=torch.long, device=device).random_(5)\n\noptimizer.zero_grad()\ninputs = torch.rand(128, 3, 100, 100, device=device , requires_grad=True)\noutputs = model(inputs)\nloss = criterion(outputs, target)\nloss.backward()\noptimizer.step()\nrunning_loss += loss.item()\n\ndef main():\n    timer = benchmark_utils.Timer(\n        stmt=\"optimizer.step()\",\n        globals=globals(),\n        label=\"str(optimizer)\",\n    )\n\n    for i in range(1):\n        print(f\"Run: {i}\\n{'-' * 40}\")\n        print(f\"timeit:\\n{timer.timeit(1000)}\\n\")\n        print(f\"autorange:\\n{timer.blocked_autorange()}\\n\\n\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45408\n\nReviewed By: gchanan\n\nDifferential Revision: D23956680\n\nPulled By: izdeby\n\nfbshipit-source-id: c5eab7bf5fce14a287c15cead1cdc26e42cfed94", "pr_number": "45408", "files_changed": ["aten/src/ATen/native/cuda/ForeachPointwiseOp.cu", "test/test_optim.py", "torch/__init__.py", "torch/optim/_multi_tensor/__init__.py", "torch/optim/_multi_tensor/__init__.pyi", "torch/optim/_multi_tensor/adadelta.py", "torch/optim/_multi_tensor/adadelta.pyi", "torch/optim/_multi_tensor/adam.py", "torch/optim/_multi_tensor/adam.pyi", "torch/optim/_multi_tensor/adamax.py", "torch/optim/_multi_tensor/adamax.pyi", "torch/optim/_multi_tensor/adamw.py", "torch/optim/_multi_tensor/adamw.pyi", "torch/optim/_multi_tensor/asgd.py", "torch/optim/_multi_tensor/asgd.pyi", "torch/optim/_multi_tensor/rmsprop.py", "torch/optim/_multi_tensor/rmsprop.pyi", "torch/optim/_multi_tensor/rprop.py", "torch/optim/_multi_tensor/rprop.pyi", "torch/optim/_multi_tensor/sgd.py", "torch/optim/_multi_tensor/sgd.pyi"], "labels": []}, "47debdca42": {"title": "Document change for DDP enabled on Windows platform (#45392)", "body": "Summary:\nDocument change for DDP enabled on Windows platform\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45392\n\nReviewed By: gchanan\n\nDifferential Revision: D23962344\n\nPulled By: mrshenli\n\nfbshipit-source-id: 8924c6ca36d68699871d8add3e0aab6542ea269c", "pr_number": "45392", "files_changed": ["README.md", "docs/source/distributed.rst", "torch/distributed/__init__.py"], "labels": ["open source"]}, "9163e8171e": {"title": "Adding Type Double to Caffe2 Mean Op", "body": "Summary: Adding support for type double to caffe2 MeanOp and MeanGradientOp.\n\nTest Plan:\nAll tests passed.\n\nExample FBL job failed without this diff:\nf221169563\n\nError message:\n```\nc10::Error: [enforce fail at mean_op.h:72] . Mean operator only supports 32-bit float, but input was of type double (Error from operator:\ninput: \"dpsgd_8/Copy_3\" input: \"dpsgd_8/Copy_4\" output: \"dpsgd_8/Mean_2\" name: \"\" type: \"Mean\" device_option { device_type: 0 device_id: 0 })\n```\n\nExample FBL job is running without failure with the canary package built from this diff:\nf221468723\n\nReviewed By: chenshouyuan\n\nDifferential Revision: D23956222\n\nfbshipit-source-id: 6c81bbc390d812ae0ac235e7d025141c8402def1", "pr_number": null, "files_changed": ["caffe2/operators/mean_op.h"], "labels": []}, "57c18127dc": {"title": "[ONNX] Update div export to perform true divide (#44831)", "body": "Summary:\nrelated https://github.com/pytorch/pytorch/issues/43787\n\nNow that PyTorch div is actually performing true divide, update onnx export code to stay consistent.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44831\n\nReviewed By: eellison\n\nDifferential Revision: D23880316\n\nPulled By: bzinodev\n\nfbshipit-source-id: 3bb8db34142ac4fed4039295ad3c4cb79487987f", "pr_number": "44831", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset9.py"], "labels": ["module: onnx", "open source", "triaged"]}, "7a4c417ed3": {"title": "Fix typo (#45379)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45379\n\nRegisteres -> Registers in reducer.h.\nghstack-source-id: 112982279\n\nTest Plan: N/A\n\nReviewed By: mrshenli\n\nDifferential Revision: D23951203\n\nfbshipit-source-id: 96c7dc2e1e12c132339b9ac83ce1da52c812740c", "pr_number": "45379", "files_changed": ["torch/csrc/distributed/c10d/reducer.h"], "labels": []}, "986af53be2": {"title": "type check for torch.testing._internalcodegen:* (#45368)", "body": "Summary:\npart of `torch.testing._internal.*` effort\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45368\n\nReviewed By: malfet\n\nDifferential Revision: D23950512\n\nPulled By: walterddr\n\nfbshipit-source-id: 399f712d12cdd9795b0136328f512c3f86a15f24", "pr_number": "45368", "files_changed": ["mypy.ini", "torch/testing/_internal/codegen/random_topo_test.py"], "labels": []}, "5d1fee23b3": {"title": "Remove convert_target from NN tests. (#45291)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45291\n\nIt's not necessary, you can just check if the dtype is integral.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23911963\n\nPulled By: gchanan\n\nfbshipit-source-id: 230139e1651eb76226f4095e31068dded30e03e8", "pr_number": "45291", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "a77d633db1": {"title": "[ONNX] Fix view for dynamic input shape (#43558)", "body": "Summary:\nExport of view op with dynamic input shape is broken when using tensors with a 0-dim.\nThis fix removes symbolic use of static input size to fix this issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43558\n\nReviewed By: ailzhang\n\nDifferential Revision: D23965090\n\nPulled By: bzinodev\n\nfbshipit-source-id: 628e9d7ee5d53375f25052340ca6feabf7ba7c53", "pr_number": "43558", "files_changed": ["test/onnx/expect/TestOperators.test_view.expect", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset12.py", "torch/onnx/symbolic_opset8.py", "torch/onnx/symbolic_opset9.py"], "labels": ["module: onnx", "open source", "triaged"]}, "c6b7eeb654": {"title": "Gh/taylorrobie/timer cleanup (#45361)", "body": "Summary:\nThis PR cleans up some of the rough edges around `Timer` and `Compare`\n* Moves `Measurement` to be dataclass based\n* Adds a bunch of type annotations. MyPy is now happy.\n* Allows missing entries in `Compare`. This is one of the biggest usability issues with `Compare` right now, both from an API perspective and because the current failure mode is really unpleasant.\n* Greatly expands the testing of `Compare`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45361\n\nTest Plan: Changes to Timer are covered under existing tests, changes to `Compare` are covered by the expanded `test_compare` method.\n\nReviewed By: bwasti\n\nDifferential Revision: D23966816\n\nPulled By: robieta\n\nfbshipit-source-id: 826969f73b42f72fa35f4de3c64d0988b61474cd", "pr_number": "45361", "files_changed": ["test/test_utils.py", "torch/utils/_benchmark/utils/common.py", "torch/utils/_benchmark/utils/compare.py", "torch/utils/_benchmark/utils/timer.py"], "labels": []}, "a4486fe7ba": {"title": "[ROCm] Print name irrespective of seq number assignment for roctx traces (#45229)", "body": "Summary:\nRecent changes to the seq_num correlation behavior in profiler (PR https://github.com/pytorch/pytorch/issues/42565)  has changed the behavior for emit_nvtx(record_shapes=True)  which doesn't print the name of the operator properly.\n\nCreated PR to dump out the name in roctx traces, irrespective of the sequence number assigned only for ROCm.\n\ncc: jeffdaily sunway513\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45229\n\nReviewed By: zou3519\n\nDifferential Revision: D23932902\n\nPulled By: albanD\n\nfbshipit-source-id: c782667ff002b70b51f1cc921afd1b1ac533b39d", "pr_number": "45229", "files_changed": ["torch/csrc/autograd/profiler.cpp"], "labels": ["module: rocm", "open source"]}, "1097fe0088": {"title": "Remove CriterionTest.test_cuda code for dtype None. (#45316)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45316\n\nIt's never used.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23919449\n\nPulled By: gchanan\n\nfbshipit-source-id: f9aaeeabf3940389156bfc01bc3118d348ca4cf6", "pr_number": "45316", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": []}, "190f91e3db": {"title": "Adding Histogram Binning Calibration to DSNN and Adding Type Double to Caffe2 ParallelSumOp/SumReluOp", "body": "Summary: As title.\n\nTest Plan:\nFBL job without this diff failed:\nf221545832\n\nError message:\n```\nNonRetryableException: AssertionError: Label is missing in training stage for HistogramBinningCalibration\n```\n\nFBL job with canary package built in this diff is running without failure:\nf221650379\n\nReviewed By: chenshouyuan\n\nDifferential Revision: D23959508\n\nfbshipit-source-id: c077230de29f7abfd092c84747eaabda0b532bcc", "pr_number": null, "files_changed": ["caffe2/quantization/server/elementwise_sum_relu_op.cc"], "labels": []}, "6b65b3cbd8": {"title": "[Distributed] DeleteKey API for c10d TCP Store (#45401)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45401\n\nAdded a DeleteKey API for the TCP Store\nghstack-source-id: 112997162\n\nTest Plan:\nModified the existing get/set test to use delete. verified that the\ncorrect keys were deleted and that the numKeys API returned the right values\n\nReviewed By: mrshenli\n\nDifferential Revision: D23955730\n\nfbshipit-source-id: 5c9f82be34ff4521c59f56f8d9c1abf775c67f9f", "pr_number": "45401", "files_changed": ["caffe2/distributed/file_store_handler.cc", "caffe2/distributed/file_store_handler.h", "caffe2/distributed/redis_store_handler.cc", "caffe2/distributed/redis_store_handler.h", "caffe2/distributed/store_handler.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/FileStore.hpp", "torch/lib/c10d/HashStore.cpp", "torch/lib/c10d/HashStore.hpp", "torch/lib/c10d/PrefixStore.cpp", "torch/lib/c10d/PrefixStore.hpp", "torch/lib/c10d/Store.hpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": []}, "331ebaf7cb": {"title": "[Distributed] Adding Python tests for the TCPStore getNumKeys and deleteKey (#45402)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45402\n\nPrevious diffs in this stack implemented the getNumKeys and deleteKey\nAPIs in the c10d Store as well as added tests at the C++ layer. This diff adds\ntests at the Python level in test_c10d.py\nghstack-source-id: 112997161\n\nTest Plan: Running these new python tests as well as previous C++ tests\n\nReviewed By: mrshenli\n\nDifferential Revision: D23955729\n\nfbshipit-source-id: c7e0af7c884de2d488320e2a1d94aec801a782e5", "pr_number": "45402", "files_changed": ["test/distributed/test_c10d.py"], "labels": []}, "e54e1fe51e": {"title": "[package] Add dependency viz (#45214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45214\n\nWhen in verbose mode the package exporter will produce an html visualization\nof dependencies of a module to make it easier to trim out unneeded code,\nor debug inclusion of things that cannot be exported.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D23873525\n\nPulled By: zdevito\n\nfbshipit-source-id: 6801991573d8dd5ab8c284e09572b36a35e1e5a4", "pr_number": "45214", "files_changed": ["test/test_package.py", "torch/package/exporter.py"], "labels": []}, "6a206df891": {"title": "20000x faster audio conversion for SummaryWriter (#44201)", "body": "Summary:\nStumbled upon a little gem in the audio conversion for `SummaryWriter.add_audio()`: two Python `for` loops to convert a float array to little-endian int16 samples. On my machine, this took 35 seconds for a 30-second 22.05 kHz excerpt. The same can be done directly in numpy in 1.65 milliseconds. (No offense, I'm glad that the functionality was there!)\n\nWould also be ready to extend this to support stereo waveforms, or should this become a separate PR?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44201\n\nReviewed By: J0Nreynolds\n\nDifferential Revision: D23831002\n\nPulled By: edward-io\n\nfbshipit-source-id: 5c8f1ac7823d1ed41b53c4f97ab9a7bac33ea94b", "pr_number": "44201", "files_changed": ["torch/utils/tensorboard/summary.py"], "labels": ["open source", "triaged"]}, "96f8755034": {"title": "Fixed handling of nan for evenly_distribute_backward (#45280)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45280\n\nPerformance is the same on CPU and on CUDA is only 1-1.05x slower. This change is necessary for the future nan ops including nan(min|max|median)\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D23908796\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: c2b57acbe924cfa59fbd85216811f29f4af05088", "pr_number": "45280", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/FunctionsManual.cpp"], "labels": ["module: autograd"]}, "49b198c454": {"title": "type check for torch.testing._internal.common_utils (#45375)", "body": "Summary:\npart of torch.testing._internal.* effort\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45375\n\nReviewed By: malfet\n\nDifferential Revision: D23964315\n\nPulled By: walterddr\n\nfbshipit-source-id: efdd643297f5c7f75670ffe60ff7e82fc413d18d", "pr_number": "45375", "files_changed": ["mypy.ini", "tools/pyi/gen_pyi.py", "torch/_C/__init__.pyi.in", "torch/testing/_internal/common_utils.py"], "labels": []}, "5855aa8dac": {"title": "Type check quasirandom (#45434)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42978.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45434\n\nReviewed By: walterddr\n\nDifferential Revision: D23967139\n\nPulled By: ajitmaths\n\nfbshipit-source-id: bcee6627f367fd01aa9a5c10a7c24331fc1823ad", "pr_number": "45434", "files_changed": ["mypy.ini", "torch/quasirandom.py"], "labels": []}, "7ac872b934": {"title": "[JIT] Modify to_backend API so that it accepts wrapped modules (#43612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43612\n\n**Summary**\nThis commit modifies the `torch._C._jit_to_backend` function so that it\naccepts `ScriptModules` as inputs. It already returns `ScriptModules`\n(as opposed to C++ modules), so this makes sense and makes the API more\nintuitive.\n\n**Test Plan**\nContinuous integration, which includes unit tests and out-of-tree tests\nfor custom backends.\n\n**Fixes**\nThis commit fixes #41432.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo, jamesr66a\n\nDifferential Revision: D23339854\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 08ecef729c4e1e6bddf3f483276947fc3559ea88", "pr_number": "43612", "files_changed": ["test/custom_backend/backend.py", "test/jit/test_backends.py", "torch/csrc/jit/backends/backend_init.cpp"], "labels": ["oncall: jit"]}, "52cbc9e4ec": {"title": "[TensorExpr] Always inline and DCE in the LLVM backend (#45445)", "body": "Summary:\nInline pytorch into wrapper, which is especially helpful in combination\nwith dead code elimination to reduce IR size and compilation times when\na lot of parameters are unused.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45445\n\nTest Plan: CI\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23969009\n\nPulled By: asuhan\n\nfbshipit-source-id: a21509d07e4c130b6aa6eae5236bb64db2748a3d", "pr_number": "45445", "files_changed": ["torch/csrc/jit/tensorexpr/llvm_codegen.cpp"], "labels": ["oncall: jit"]}, "4af4b71fdc": {"title": "[JIT] Update docs for recently added features (#45232)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45232\n\n**Summary**\nThis commit updates the TorchScript language reference to include\ndocumentation on recently-added TorchScript enums. It also removed\n`torch.no_grad` from the list of known unsupported `torch` modules and\nclasses because it is now supported.\n\n**Test Plan**\nContinuous integration.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23971884\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 5e2c164ed59bc0926b11201106952cff86e9356e", "pr_number": "45232", "files_changed": ["docs/source/jit_language_reference.rst", "docs/source/jit_unsupported.rst"], "labels": []}, "a0f0cb1608": {"title": "[JIT] Add test for ignored class type property (#45233)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45233\n\n**Summary**\nThis commit modifies `TestClassType.test_properties` to check that\nproperties on class types can be ignored with the same syntax as\nignoring properties on `Modules`.\n\n**Test Plan**\n`python test/test_jit.py TestClassType.test_properties`\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D23971885\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: f2228f61fe26dff219024668cc0444a2baa8834c", "pr_number": "45233", "files_changed": ["test/jit/test_class_type.py"], "labels": ["oncall: jit"]}, "0c8a6008ac": {"title": "Fix torch.pow when the scalar base is a complex number (#45259)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43829\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45259\n\nReviewed By: gchanan\n\nDifferential Revision: D23962073\n\nPulled By: anjali411\n\nfbshipit-source-id: 1b16afbb98f33fa7bc53c6ca296c5ddfcbdd2b72", "pr_number": "45259", "files_changed": ["aten/src/ATen/native/Pow.cpp", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "8c66cd120b": {"title": "Disable complex inputs to torch.round (#45330)", "body": "Summary:\n- Related with https://github.com/pytorch/pytorch/issues/44612\n- Disable complex inputs to `torch.round`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45330\n\nReviewed By: gchanan\n\nDifferential Revision: D23970781\n\nPulled By: anjali411\n\nfbshipit-source-id: b8c9ac315ae0fc872701aa132367c3171fd56185", "pr_number": "45330", "files_changed": ["aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "test/test_autograd.py"], "labels": ["module: complex", "open source", "triaged"]}, "208df1aeb8": {"title": "Use python 3.8 in pytorch docker image (#45466)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45466\n\nTest Plan: Imported from OSS\n\nReviewed By: seemethere\n\nDifferential Revision: D23975294\n\nPulled By: tierex\n\nfbshipit-source-id: 964de7928b541121963e9de792630bcef172bb5c", "pr_number": "45466", "files_changed": ["Dockerfile"], "labels": []}, "534f2ae582": {"title": "Disable inplace abs for complex tensors (#45069)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45069\n\n`torch.abs` is a `C -> R` function for complex input. Following the general semantics in torch, the in-place version of abs should be disabled for complex input.\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee, malfet\n\nDifferential Revision: D23818397\n\nPulled By: anjali411\n\nfbshipit-source-id: b23b8d0981c53ba0557018824d42ed37ec13d4e2", "pr_number": "45069", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "test/test_autograd.py", "test/test_torch.py"], "labels": ["complex_autograd", "module: complex"]}, "6967e6295e": {"title": "Fix DDP docs (#45454)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45454\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23973367\n\nPulled By: mrshenli\n\nfbshipit-source-id: 11f20d51d0d0f92f199e4023f02b86623867bae0", "pr_number": "45454", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": []}, "c5ade5f698": {"title": "Fix no_sync docs (#45455)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45455\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23973365\n\nPulled By: mrshenli\n\nfbshipit-source-id: 87c9878cdc7310754670b83efa65ae6f877f86fb", "pr_number": "45455", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": []}, "8e47fcba5f": {"title": "Update docs for RPC async_execution (#45458)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45458\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D23973366\n\nPulled By: mrshenli\n\nfbshipit-source-id: 3697f07fa972db21746aa25eaf461c1b93293f58", "pr_number": "45458", "files_changed": ["docs/source/rpc.rst", "torch/distributed/rpc/functions.py"], "labels": []}, "5be954b502": {"title": "Fix WorkerInfo link format (#45476)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45476\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D23982069\n\nPulled By: mrshenli\n\nfbshipit-source-id: 6d932e77c1941dfd96592b388353f0fc8968dde6", "pr_number": "45476", "files_changed": ["torch/csrc/distributed/rpc/init.cpp"], "labels": []}, "5a6a31168f": {"title": "add circle ci job name dimension to report test stats (#45457)", "body": "Summary:\nTo support abnormal detection for test time spike\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45457\n\nReviewed By: malfet\n\nDifferential Revision: D23975628\n\nPulled By: walterddr\n\nfbshipit-source-id: f28d0f12559070004d637d5bde83289f029b15b8", "pr_number": "45457", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "test/print_test_stats.py"], "labels": []}, "35596d39e9": {"title": "Coalesce TLS accesses in RecordFunction constructor (#44970)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44970\n\nRight now, when RecordFunction is not active (usual case),\nwe do two TLS accesses (check for thread local callbacks, and check for\nthread local boolean).\nExperimenting with reducing number of TLS accesses in RecordFunction\nconstructor.\n\nTest Plan: record_function_benchmark\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23791165\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 6137ce4bface46f540ece325df9864fdde50e0a4", "pr_number": "44970", "files_changed": ["aten/src/ATen/ThreadLocalState.cpp", "aten/src/ATen/ThreadLocalState.h", "aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h", "binaries/record_function_benchmark.cc"], "labels": []}, "50b91103a9": {"title": "add self cuda time to avoid double/quadruple counting (#45209)", "body": "Summary:\nIn profiler, cuda did not report self time, so for composite functions there was no way to determine which function is really taking time. In addition, \"total cuda time\" reported was frequently more than total wallclock time. This PR adds \"self CUDA time\" in profiler, and computes total cuda time based on self cuda time, similar to how it's done for CPU. Also, slight formatting changes to make table more compact. Before:\n```\n--------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                  Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls\n--------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\naten::matmul          0.17%            890.805us        99.05%           523.401ms        5.234ms          49.91%           791.184ms        7.912ms          100\naten::mm              98.09%           518.336ms        98.88%           522.511ms        5.225ms          49.89%           790.885ms        7.909ms          100\naten::t               0.29%            1.530ms          0.49%            2.588ms          25.882us         0.07%            1.058ms          10.576us         100\naten::view            0.46%            2.448ms          0.46%            2.448ms          12.238us         0.06%            918.936us        4.595us          200\naten::transpose       0.13%            707.204us        0.20%            1.058ms          10.581us         0.03%            457.802us        4.578us          100\naten::empty           0.14%            716.056us        0.14%            716.056us        7.161us          0.01%            185.694us        1.857us          100\naten::as_strided      0.07%            350.935us        0.07%            350.935us        3.509us          0.01%            156.380us        1.564us          100\naten::stride          0.65%            3.458ms          0.65%            3.458ms          11.527us         0.03%            441.258us        1.471us          300\n--------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 528.437ms\nCUDA time total: 1.585s\n\nRecorded timeit time:  789.0814 ms\n\n```\nNote recorded timeit time (with proper cuda syncs) is 2 times smaller than \"CUDA time total\" reported by profiler\n\nAfter\n```\n--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls\n--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n        aten::matmul         0.15%     802.716us        99.06%     523.548ms       5.235ms     302.451us         0.04%     791.151ms       7.912ms           100\n            aten::mm        98.20%     519.007ms        98.91%     522.745ms       5.227ms     790.225ms        99.63%     790.848ms       7.908ms           100\n             aten::t         0.27%       1.406ms         0.49%       2.578ms      25.783us     604.964us         0.08%       1.066ms      10.662us           100\n          aten::view         0.45%       2.371ms         0.45%       2.371ms      11.856us     926.281us         0.12%     926.281us       4.631us           200\n     aten::transpose         0.15%     783.462us         0.22%       1.173ms      11.727us     310.016us         0.04%     461.282us       4.613us           100\n         aten::empty         0.11%     591.603us         0.11%     591.603us       5.916us     176.566us         0.02%     176.566us       1.766us           100\n    aten::as_strided         0.07%     389.270us         0.07%     389.270us       3.893us     151.266us         0.02%     151.266us       1.513us           100\n        aten::stride         0.60%       3.147ms         0.60%       3.147ms      10.489us     446.451us         0.06%     446.451us       1.488us           300\n--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 528.498ms\nCUDA time total: 793.143ms\n\nRecorded timeit time:  788.9832 ms\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45209\n\nReviewed By: zou3519\n\nDifferential Revision: D23925491\n\nPulled By: ngimel\n\nfbshipit-source-id: 7f9c49238d116bfd2db9db3e8943355c953a77d0", "pr_number": "45209", "files_changed": ["torch/autograd/profiler.py", "torch/csrc/autograd/profiler.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "dddb685c11": {"title": "This PR flips a switch to enable PE + TE (#45396)", "body": "Summary:\nThis PR flips a switch to enable PE + TE\nnext PR: https://github.com/pytorch/pytorch/pull/45397\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45396\n\nReviewed By: suo\n\nDifferential Revision: D23966878\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 2010a0b07c595992a88b3fe0792d6af315cf421e", "pr_number": "45396", "files_changed": ["torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["oncall: jit"]}, "417e3f85e5": {"title": "Support tuple inputs in NN Module test (#44853)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44853\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D23750441\n\nPulled By: glaringlee\n\nfbshipit-source-id: 1b111a370a726b40521134b711c35f48dda99411", "pr_number": "44853", "files_changed": ["test/cpp_api_parity/parity-tracker.md", "test/test_nn.py", "torch/testing/_internal/common_nn.py"], "labels": []}, "b0bdc82a00": {"title": "[FX][EZ] Fix bug where copying node made non-unique name (#45311)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45311\n\nTest Plan: Imported from OSS\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D23917864\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 10d0a4017ffe160bce4ba0d830e035616bbded74", "pr_number": "45311", "files_changed": ["test/test_fx.py", "torch/fx/graph.py", "torch/quantization/fx/quantize.py"], "labels": ["fx"]}, "6bdb871d47": {"title": "[FX] Lint pass for Graphs (#44973)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44973\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D23792631\n\nPulled By: jamesr66a\n\nfbshipit-source-id: d8faef0c311d8bd611ba0a7e1e2f353e3e5a1068", "pr_number": "44973", "files_changed": ["test/test_fx.py", "torch/fx/graph.py", "torch/fx/node.py"], "labels": ["fx"]}, "8c309fc052": {"title": "Add more tests for mt optimizers (#45475)", "body": "Summary:\nAdd more test cases for mt optimizers and fix Adam/AdamW\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45475\n\nReviewed By: soumith\n\nDifferential Revision: D23982727\n\nPulled By: izdeby\n\nfbshipit-source-id: 4b24d37bd52a2fa3719d3e3a5dcf3b96990b0f5b", "pr_number": "45475", "files_changed": ["test/test_optim.py", "torch/optim/_multi_tensor/adam.py", "torch/optim/_multi_tensor/adamw.py"], "labels": []}, "d2623da52c": {"title": "replaced whitelist with allowlist (#45260)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/41754\n\n**(1)**\nIntially file was named **gen_op_registration_whitelist.py** I changed it to **gen_op_registration_allowlist.py**\n\n**(2)**\nThere were some **whitelist** in comment inside the file, I changed it to **allowlist**\n![update1](https://user-images.githubusercontent.com/62737243/94106752-b296e780-fe59-11ea-8541-632a1dbf90d6.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45260\n\nReviewed By: dhruvbird\n\nDifferential Revision: D23947182\n\nPulled By: ljk53\n\nfbshipit-source-id: 31b486592451dbb0605d7950e07747cbb72ab80f", "pr_number": "45260", "files_changed": ["cmake/Codegen.cmake", "tools/code_analyzer/gen_op_registration_allowlist.py", "tools/code_analyzer/gen_op_registration_whitelist.py"], "labels": ["open source", "triaged"]}, "92306b85d5": {"title": "[TensorExpr] Consolidate {buffer,function,tensor}.{h.cpp} in tensor.{h,cpp}. (#45388)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45388\n\nClasses defined in these files are closely related, so it is reasonable\nto have them all in one file. The change is purely a code move.\n\nDifferential Revision: D23952867\n\nTest Plan: Imported from OSS\n\nReviewed By: nickgg\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 12cfaa968bdfc4dff00509e34310a497c7b59155", "pr_number": "45388", "files_changed": ["test/cpp/tensorexpr/test_boundsinference.cpp", "test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_kernel.cpp", "test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_train.cpp", "test/cpp/tensorexpr/test_train_impl.cpp", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/buffer.cpp", "torch/csrc/jit/tensorexpr/buffer.h", "torch/csrc/jit/tensorexpr/codegen.h", "torch/csrc/jit/tensorexpr/eval.h", "torch/csrc/jit/tensorexpr/function.cpp", "torch/csrc/jit/tensorexpr/function.h", "torch/csrc/jit/tensorexpr/ir.cpp", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/reduction.h", "torch/csrc/jit/tensorexpr/tensor.cpp", "torch/csrc/jit/tensorexpr/tensor.h"], "labels": ["oncall: jit"]}, "3c33695a6d": {"title": "[TensorExpr] Rename `Buffer` to `Placeholder`. (#45389)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45389\n\nDifferential Revision: D23952866\n\nTest Plan: Imported from OSS\n\nReviewed By: nickgg\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 17eedd3ac17897501403482ac1866c569d247c75", "pr_number": "45389", "files_changed": ["test/cpp/tensorexpr/test_aten.cpp", "test/cpp/tensorexpr/test_boundsinference.cpp", "test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_registerizer.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/test_train.cpp", "test/cpp/tensorexpr/test_train.h", "test/cpp/tensorexpr/test_train_impl.cpp", "torch/csrc/jit/tensorexpr/codegen.h", "torch/csrc/jit/tensorexpr/eval.h", "torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/ir.cpp", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/reduction.h", "torch/csrc/jit/tensorexpr/stmt.h", "torch/csrc/jit/tensorexpr/tensor.cpp", "torch/csrc/jit/tensorexpr/tensor.h"], "labels": ["oncall: jit"]}, "b86008ab75": {"title": "[TensorExpr] Remove buf_ field from class Tensor. (#45390)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45390\n\nTensor objects should always refer to their Function's bufs. Currently\nwe never create a Tensor with a buffer different than of its function,\nbut having it in two places seems incorrect and dangerous.\n\nDifferential Revision: D23952865\n\nTest Plan: Imported from OSS\n\nReviewed By: nickgg\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: e63fc26d7078427514649d9ce973b74ea635a94a", "pr_number": "45390", "files_changed": ["test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "torch/csrc/jit/tensorexpr/bounds_inference.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/tensor.cpp", "torch/csrc/jit/tensorexpr/tensor.h"], "labels": ["oncall: jit"]}, "bb478810e0": {"title": "[quant] torch.max_pool1d (#45152)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45152\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23846473\n\nPulled By: z-a-f\n\nfbshipit-source-id: 38fd611e568e4f8b39b7a00adeb42c7b99576360", "pr_number": "45152", "files_changed": ["aten/src/ATen/native/MaxPooling.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/qpool.cpp", "aten/src/ATen/native/quantized/library.cpp", "test/quantization/test_quantized_op.py", "torch/nn/quantized/functional.py", "torch/overrides.py"], "labels": []}, "489af4ddcb": {"title": "[quant] Add quant APIs to save/load observer state_dict (#44846)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44846\n\nThe save function traverses the model state dict to pick out the observer stats\nload function traverse the module hierarchy to load the state dict into module attributes depending on observer type\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFx.test_save_observer_state_dict\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D23746821\n\nfbshipit-source-id: 05c571b62949a2833602d736a81924d77e7ade55", "pr_number": "44846", "files_changed": ["test/quantization/test_quantize_fx.py", "test/quantization/test_workflow_module.py", "torch/quantization/observer.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "1ed1a2f5b0": {"title": "[wip] fast typeMeta/ScalarType conversion approach 2 (#44965)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44965\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D23789657\n\nPulled By: bhosmer\n\nfbshipit-source-id: 5afdd52d24bd097891ff4a7313033f7bd400165e", "pr_number": "44965", "files_changed": ["aten/src/ATen/native/DispatchStub.h", "aten/src/ATen/templates/TensorBody.h", "aten/src/TH/THStorageFunctions.hpp", "c10/core/DefaultDtype.cpp", "c10/core/ScalarType.h", "c10/core/ScalarTypeToTypeMeta.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/TensorOptions.h", "c10/core/UndefinedTensorImpl.h", "c10/util/typeid.cpp", "c10/util/typeid.h", "torch/csrc/DynamicTypes.h", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["oncall: jit"]}, "56af122659": {"title": "Revert D23966878: [pytorch][PR] This PR flips a switch to enable PE + TE", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23966878 (https://github.com/pytorch/pytorch/commit/dddb685c114a94df01a1919c7238f331fa63fc6d)\n\nOriginal commit changeset: 2010a0b07c59\n\nfbshipit-source-id: 132556039730fd3e4babd0d7ca8daf9c8d14f728", "pr_number": null, "files_changed": ["torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": []}, "37f9af7f29": {"title": "Missing tests about torch.xxx(out=...) (#44465)", "body": "Summary:\nPR opened just to run the CI tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44465\n\nReviewed By: ngimel\n\nDifferential Revision: D23907565\n\nPulled By: mruberry\n\nfbshipit-source-id: 620661667877f1e9a2bab17d19988e2dc986fc0f", "pr_number": "44465", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: tests", "open source", "triaged"]}, "87f98a5b54": {"title": "Updates torch.floor_divide documentation to clarify it's actually torch.trunc_divide (or torch.rtz_divide) (#45411)", "body": "Summary:\nAddresses https://github.com/pytorch/pytorch/issues/43874 for 1.7. 1.8 will need to take floor_divide through a proper deprecation process.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45411\n\nReviewed By: ngimel\n\nDifferential Revision: D23974997\n\nPulled By: mruberry\n\nfbshipit-source-id: 16dd07e50a17ac76bfc93bd6b71d4ad72d909bf4", "pr_number": "45411", "files_changed": ["torch/_torch_docs.py"], "labels": []}, "0806c58e9f": {"title": "Optimize view_as_complex and view_as_real (#44908)", "body": "Summary:\nThis avoids unnecessary memory allocations in `view_as_complex` and `view_as_real`. I construct the new tensor directly with the existing storage to avoid creating a new storage object and also use `DimVector`s to avoid allocating for the sizes and strides. Overall, this saves about 2 us of overhead from `torch.fft.fft` which currently has to call `view_as_real` and `view_as_complex` for every call.\n\nI've used this simple benchmark to measure the overhead:\n```python\nIn [1]: import torch\n   ...: a = torch.rand(1, 2)\n   ...: ac = torch.view_as_complex(a)\n   ...: %timeit torch.view_as_real(ac)\n   ...: %timeit torch.view_as_complex(a)\n   ...: %timeit ac.real\n```\n\nResults before:\n```\n2.5 \u00b5s \u00b1 62.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n2.22 \u00b5s \u00b1 36 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n4.17 \u00b5s \u00b1 8.76 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n```\n\nand after:\n```\n1.83 \u00b5s \u00b1 9.26 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n1.57 \u00b5s \u00b1 7.17 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n3.47 \u00b5s \u00b1 34.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44908\n\nReviewed By: agolynski\n\nDifferential Revision: D23793479\n\nPulled By: anjali411\n\nfbshipit-source-id: 64b9cad70e3ec10891310cbfa8c0bdaa1d72885b", "pr_number": "44908", "files_changed": ["aten/src/ATen/native/ComplexHelper.h"], "labels": ["module: complex", "open source", "triaged"]}, "7cde662f08": {"title": "Add check for Complex Type to allow non integral alpha. (#45200)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/45184\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45200\n\nReviewed By: gchanan\n\nDifferential Revision: D23940134\n\nPulled By: anjali411\n\nfbshipit-source-id: cce7b1efc22ec189ba6c83e31ce712bb34997139", "pr_number": "45200", "files_changed": ["aten/src/ATen/native/BinaryOps.h", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "6d37126a10": {"title": "Makes rdiv consistent with div (#45407)", "body": "Summary:\nIn addition to making rdiv consistent with div, this PR significantly expands division testing, accounting for floor_divide actually performing truncation division, too.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45407\n\nReviewed By: ngimel\n\nDifferential Revision: D23974967\n\nPulled By: mruberry\n\nfbshipit-source-id: 82b46b07615603f161ab7cd1d3afaa6d886bfe95", "pr_number": "45407", "files_changed": ["test/onnx/expect/TestOperators.test_view_flatten.expect", "test/test_torch.py", "torch/tensor.py"], "labels": []}, "0a38aed025": {"title": "Auto set libuv_ROOT env var for Gloo submodule on Windows platform (#45484)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45484\n\nReviewed By: lw\n\nDifferential Revision: D23990724\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1987ce7eb7d3f9d3120c07e954cd6581cd3caf59", "pr_number": "45484", "files_changed": ["CMakeLists.txt", "README.md"], "labels": ["open source"]}, "bb19a55429": {"title": "Improves fft doc consistency and makes deprecation warnings more prominent (#45409)", "body": "Summary:\nThis PR makes the deprecation warnings for existing fft functions more prominent and makes the torch.stft deprecation warning consistent with our current deprecation planning.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45409\n\nReviewed By: ngimel\n\nDifferential Revision: D23974975\n\nPulled By: mruberry\n\nfbshipit-source-id: b90d8276095122ac3542ab625cb49b991379c1f8", "pr_number": "45409", "files_changed": ["aten/src/ATen/native/SpectralOps.cpp", "torch/_torch_docs.py", "torch/functional.py"], "labels": []}, "df0de780c3": {"title": "Add cusolver guard for cuda >= 10.1.243 (#45452)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/issues/45403\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45452\n\nReviewed By: mruberry\n\nDifferential Revision: D23977009\n\nPulled By: ngimel\n\nfbshipit-source-id: df66425773d7500fa37e64d5e4bcc98167016be3", "pr_number": "45452", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h"], "labels": ["open source"]}, "b3135c2056": {"title": "Enable torch.cuda.amp typechecking (#45480)", "body": "Summary:\nFix `torch._C._autocast_*_nesting` declarations in __init__.pyi\n\nFix iterable constructor logic: not every iterable can be constructed using `type(val)(val)` trick, for example it would not work for `val=range(10)` although `isinstance(val, Iterable)` is True\nChange optional resolution logic to meet mypy expectations\n\nFixes https://github.com/pytorch/pytorch/issues/45436\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45480\n\nReviewed By: walterddr\n\nDifferential Revision: D23982822\n\nPulled By: malfet\n\nfbshipit-source-id: 6418a28d04ece1b2427dcde4b71effb67856a872", "pr_number": "45480", "files_changed": ["mypy.ini", "torch/_C/__init__.pyi.in", "torch/cuda/amp/autocast_mode.py", "torch/cuda/amp/grad_scaler.py"], "labels": ["module: typing", "triaged"]}, "ab5edf21b0": {"title": "Revert D23789657: [wip] fast typeMeta/ScalarType conversion approach 2", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD23789657 (https://github.com/pytorch/pytorch/commit/1ed1a2f5b004eb03257d35e2ff08030fac6c4f64)\n\nOriginal commit changeset: 5afdd52d24bd\n\nfbshipit-source-id: 6d827be8895bcb39c8e85342eee0f7a3f5056c76", "pr_number": null, "files_changed": ["aten/src/ATen/native/DispatchStub.h", "aten/src/ATen/templates/TensorBody.h", "aten/src/TH/THStorageFunctions.hpp", "c10/core/DefaultDtype.cpp", "c10/core/ScalarType.h", "c10/core/ScalarTypeToTypeMeta.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/TensorOptions.h", "c10/core/UndefinedTensorImpl.h", "c10/util/typeid.cpp", "c10/util/typeid.h", "torch/csrc/DynamicTypes.h", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": []}, "fe9019cbfe": {"title": "Reorganized Sorting.cpp method order (#45083)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45083\n\nThis PR just reorders the methods in Sorting.cpp placing related methods next to each other.\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D23908817\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 1dd7b693b5135fddf5dff12303474e85ce0c2f83", "pr_number": "45083", "files_changed": ["aten/src/ATen/native/Sorting.cpp"], "labels": []}, "5f49d14be2": {"title": "Add mobile_optimized tag to optimized model. (#45479)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45479\n\nAdd a top level boolean attribute to the model called mobile_optimized that is set to true if it is optimized.\n\nTest Plan: buck test //caffe2/test:mobile passes\n\nReviewed By: kimishpatel\n\nDifferential Revision: D23956728\n\nfbshipit-source-id: 79c5931702208b871454319ca2ab8633596b1eb8", "pr_number": "45479", "files_changed": ["test/test_mobile_optimizer.py", "torch/csrc/jit/passes/xnnpack_rewrite.cpp"], "labels": ["fb-exported", "oncall: jit"]}, "09b3e16b40": {"title": "[JIT] Enable @unused syntax for ignoring properties (#45261)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45261\n\n**Summary**\nThis commit enables `unused` syntax for ignoring\nproperties. Inoring properties is more intuitive with this feature enabled.\n`ignore` is not supported because class type properties cannot be\nexecuted in Python (because they exist only as TorchScript types) like\nan `ignored` function and module properties that cannot be scripted\nare not added to the `ScriptModule` wrapper so that they\nmay execute in Python.\n\n**Test Plan**\nThis commit updates the existing unit tests for class type and module\nproperties to test properties ignored using `unused`.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar, Krovatkin, mannatsingh\n\nDifferential Revision: D23971881\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 8d3cc1bbede7753d6b6f416619e4660c56311d33", "pr_number": "45261", "files_changed": ["test/jit/test_class_type.py", "test/test_jit_py3.py", "torch/_jit_internal.py", "torch/fx/graph_module.py", "torch/jit/_script.py", "torch/jit/frontend.py", "torch/nn/modules/rnn.py"], "labels": ["oncall: jit"]}, "ea59251f51": {"title": "Fix model_name not logged properly issue. (#45488)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45488\n\nmodel_name logging was broken, issue is from the recent change of assigning the method name into the module name, this diff is fixing it.\nghstack-source-id: 113103942\n\nTest Plan:\nmade sure that now the model_name is logged from module_->name().\nverified with one model which does not contain the model metadata, and the model_name field is logged as below:\n\n09-28 21:59:30.065 11530 12034 W module.cpp: TESTINGTESTING run() module = __torch__.Model\n09-28 21:59:30.065 11530 12034 W module.cpp: TESTINGTESTING metadata does not have model_name assigning to __torch__.Model\n09-28 21:59:30.066 11530 12034 W MobileModuleQPLObserver.cpp: TESTINGTESTING onEnterRunMethod log  model_name = __torch__.Model\n09-28 21:59:30.066 11530 12034 W MobileModuleQPLObserver.cpp: TESTINGTESTING onEnterRunMethod log  method_name = labels\n09-28 21:59:30.068 11530 12034 W MobileModuleQPLObserver.cpp: TESTINGTESTING onExitRunMethod()\n\nReviewed By: linbinyu\n\nDifferential Revision: D23984165\n\nfbshipit-source-id: 5b00f50ea82106b695c2cee14029cb3b2e02e2c8", "pr_number": "45488", "files_changed": ["torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h"], "labels": ["oncall: jit"]}, "15f85eea18": {"title": "Support bfloat16 and complex dtypes for logical_not (#43537)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43537\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D23751950\n\nPulled By: mruberry\n\nfbshipit-source-id: d07ecd9aae263eb8e00928d4fc981e0d66066fbb", "pr_number": "43537", "files_changed": ["aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnarySignKernels.cu", "test/test_torch.py"], "labels": ["open source", "triaged"]}, "f47fd0eb72": {"title": "Updated `cholesky_backward` for complex inputs (#45267)", "body": "Summary:\nUpdated `cholesky_backward` to work correctly for complex input.\nNote that the current implementation gives the conjugate of what JAX would return. anjali411 is that correct thing to do?\nRef. https://github.com/pytorch/pytorch/issues/44895\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45267\n\nReviewed By: bwasti\n\nDifferential Revision: D23975269\n\nPulled By: anjali411\n\nfbshipit-source-id: 9908b0bb53c411e5ad24027ff570c4f0abd451e6", "pr_number": "45267", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/FunctionsManual.cpp"], "labels": ["module: complex", "open source", "topic: linear algebra", "triaged"]}, "aa2bd7e1ae": {"title": "Conservative-ish persistent RNN heuristics for compute capability 8.0+ (#43165)", "body": "Summary:\nBased on https://github.com/pytorch/pytorch/pull/43165#issuecomment-697033663 and tests by Vasily Volkov ([persistentRNN-speedup.xlsx](https://github.com/pytorch/pytorch/files/5298001/persistentRNN-speedup.xlsx)).  See comments in code.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43165\n\nReviewed By: zhangguanheng66, mruberry\n\nDifferential Revision: D23991756\n\nPulled By: ngimel\n\nfbshipit-source-id: 4c2c14c9002be2fec76fb21ba55b7dab79497510", "pr_number": "43165", "files_changed": ["aten/src/ATen/native/cudnn/RNN.cpp"], "labels": ["open source"]}, "b2925671b6": {"title": "Updates deterministic flag to throw a warning, makes docs consistent (#45410)", "body": "Summary:\nPer feedback in the recent design review. Also tweaks the documentation to clarify what \"deterministic\" means and adds a test for the behavior.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45410\n\nReviewed By: ngimel\n\nDifferential Revision: D23974988\n\nPulled By: mruberry\n\nfbshipit-source-id: e48307da9c90418fc6834fbd67b963ba2fe0ba9d", "pr_number": "45410", "files_changed": ["aten/src/ATen/Context.cpp", "test/test_torch.py", "torch/__init__.py"], "labels": []}, "6e55a26e10": {"title": "Move mobile specific CPUCachingAllocator to c10/mobile folder. (#45364)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45364\n\nPlus add some more comments about the usage, limitations and cons.\n\nTest Plan: Build and run benchmark binary.\n\nReviewed By: gchanan\n\nDifferential Revision: D23944193\n\nfbshipit-source-id: 30d4f4991d2185a0ab768d94c846d73730fc0835", "pr_number": "45364", "files_changed": ["BUILD.bazel", "aten/src/ATen/test/cpu_caching_allocator_test.cpp", "binaries/speed_benchmark_torch.cc", "c10/CMakeLists.txt", "c10/core/CPUAllocator.cpp", "c10/core/CPUCachingAllocator.cpp", "c10/core/CPUCachingAllocator.h", "c10/mobile/CPUCachingAllocator.cpp", "c10/mobile/CPUCachingAllocator.h"], "labels": ["fb-exported"]}, "0df99ad470": {"title": "Remove unnecessary __at_align32__ in int_elementwise_binary_256 (#45470)", "body": "Summary:\nThey were added in 4b3046ed286e92b5910769bf97f2bc6a1ad473d1 based on a\nmisunderstanding of `_mm256_storeu_si256`, but they\nare actually unnecessary. The [document][1] of `_mm256_storeu_si256` says:\n\n> Moves values from a integer vector to an **unaligned** memory location.\n\nIn this case, it's better to remove the `__at_align32__` qualifier to\nleave the compiler and linker more flexibility to optimize.\n\n[1]: https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-intel-advanced-vector-extensions/intrinsics-for-load-and-store-operations-1/mm256-storeu-si256.html\n\nClose https://github.com/pytorch/pytorch/issues/44810\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45470\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23980060\n\nPulled By: glaringlee\n\nfbshipit-source-id: 12b3558b76c6e81d88a72081060fdb8674464768", "pr_number": "45470", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_int.h"], "labels": ["open source"]}, "b66ac1e928": {"title": "Updates nonzero's as_tuple behavior to no longer warn. (#45413)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44284.\n\n[torch.nonzero](https://pytorch.org/docs/master/generated/torch.nonzero.html?highlight=nonzero#torch.nonzero) is distinct from [numpy.nonzero](https://numpy.org/doc/1.18/reference/generated/numpy.nonzero.html?highlight=nonzero#numpy.nonzero). The latter returns a tensor by default, and the former returns a tuple of tensors. The `as_tuple` argument was added as part of an intended deprecation process to make torch.nonzero consistent with numpy.nonzero, but this was a confusing change for users. A better deprecation path would be to offer torch.argwhere consistent with [numpy.argwhere](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html?highlight=argwhere#numpy.argwhere), which is equivalent to the default torch.nonzero behavior. Once this is offered a change to torch.nonzero should be more straightforward with less user disruption, if we decided that's the correct change to pursue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45413\n\nReviewed By: ngimel\n\nDifferential Revision: D23975015\n\nPulled By: mruberry\n\nfbshipit-source-id: b59237d0d8c2df984e952b62d0a7c247b49d84dc", "pr_number": "45413", "files_changed": ["test/test_torch.py", "tools/autograd/templates/python_torch_functions.cpp"], "labels": []}, "147c88ef2d": {"title": "Add docs to a pytorch.github.io/doc/tag directory when repo is tagged (#45204)", "body": "Summary:\nIn coordination with jlin27.\n\nThis PR is meant to build documentation when the repo is tagged. For instance, tagging the repo with 1.7.0rc1 will push that commit's documentation to pytorch/pytorch.github.io/docs/1.7.\n\nSubsequently tagging 1.7.0rc2 will override the 1.7 docs, as will 1.7.0, and 1.7.1. I think this is as it should be: there should be one, latest, version for the 1.7 docs. This can be tweaked differently if desired.\n\nThere is probably work that needs to be done to adjust the [versions.html](https://pytorch.org/docs/versions.html) to add the new tag?\n\nIs there a way to test the tagging side of this without breaking the production documentation?\n\nAs an aside, the documentation is being built via the `pytorch_linux_xenial_py3_6_gcc5_4_build` image. Some projects are starting to move on from python3.6 since [it is in security-only support mode](https://devguide.python.org/#status-of-python-branches), no new binaries are being released.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45204\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23996800\n\nPulled By: seemethere\n\nfbshipit-source-id: a94a080348a47738c1de5832ab37b2b0d57d2d57", "pr_number": "45204", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["open source", "triaged"]}, "18876b5722": {"title": "Update backward formula for torch.dot and add backward definition for torch.vdot (#45074)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45074\n\nTODO: Add R -> C tests in https://github.com/pytorch/pytorch/pull/44744 (blocked on some JIT changes)\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D23975361\n\nPulled By: anjali411\n\nfbshipit-source-id: 3512bd2962b588a198bc317673bd18cc96ac823f", "pr_number": "45074", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["complex_autograd", "module: complex"]}, "ab5cf16b6c": {"title": "fix standard deviation gradient NaN behavior (#45468)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/4320\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45468\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23991064\n\nPulled By: albanD\n\nfbshipit-source-id: d4274895f2dac8b2cdbd73e5276ce3df466fc341", "pr_number": "45468", "files_changed": ["torch/csrc/autograd/FunctionsManual.cpp"], "labels": []}, "d642992877": {"title": "Quantized operators template selective (#45509)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45509\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44479\n\nTest Plan: Imported from OSS\n\nReviewed By: dhruvbird\n\nDifferential Revision: D23626562\n\nPulled By: iseeyuan\n\nfbshipit-source-id: c2fc8bad25f8e5e9a70eb1001b9066a711b8e8e7", "pr_number": "45509", "files_changed": ["aten/src/ATen/native/quantized/cpu/qadd.cpp", "aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/qconcat.cpp", "aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qconv_unpack.cpp", "aten/src/ATen/native/quantized/cpu/qelu.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp", "aten/src/ATen/native/quantized/cpu/qhardswish.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_unpack.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "aten/src/ATen/native/quantized/cpu/qnormalization.cpp", "aten/src/ATen/native/quantized/cpu/qpool.cpp", "aten/src/ATen/native/quantized/cpu/qrelu.cpp", "aten/src/ATen/native/quantized/cpu/qthreshold.cpp", "aten/src/ATen/native/quantized/library.cpp"], "labels": ["fb-exported"]}, "ef41472544": {"title": "Create experimental FX graph manipulation library (#44775)", "body": "Summary:\nThis PR adds a new GraphManipulation library for operating on the GraphModule nodes.\nIt also adds an implementation of replace_target_nodes_with, which replaces all nodes in the GraphModule or a specific op/target with a new specified op/target. An example use of this function would be replacing a generic operator with an optimized operator for specific sizes and shapes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44775\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23874561\n\nPulled By: gcatron\n\nfbshipit-source-id: e1497cd11e0bbbf1fabdf137d65c746248998e0b", "pr_number": "44775", "files_changed": ["test/test_fx.py", "torch/fx/experimental/GraphManipulation.py"], "labels": ["fx"]}, "06a566373a": {"title": "[PyTorch/NCCL] Fix async error handling (#45456)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45456\n\nRemove work while not holding lock, to avoid deadlock with watchdog thread while GPU is 100%\n\nSyncBatchNorm failure trace: P143879560\n\nTest Plan:\n**Desync test:**\nBACKEND=nccl WORLD_SIZE=3 NCCL_ASYNC_ERROR_HANDLING=1 ./buck-out/gen/caffe2/test/distributed/distributed_nccl_spawn#binary.par -r test_DistributedDataParallel_desync\n\n**SyncBatchNorm test:**\nBACKEND=nccl WORLD_SIZE=3 NCCL_ASYNC_ERROR_HANDLING=1 ./buck-out/gen/caffe2/test/distributed/distributed_nccl_fork#binary.par -r test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_gradient\n\nReviewed By: osalpekar\n\nDifferential Revision: D23972071\n\nfbshipit-source-id: f03d9637a6ec998d64dab1a062a81e0f3697275f", "pr_number": "45456", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["fb-exported"]}, "637570405b": {"title": "Disable multi tensor tesnor tests on rocm (#45535)", "body": "Summary:\nDisable multi tensor test on rocm\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45535\n\nReviewed By: ngimel\n\nDifferential Revision: D24002557\n\nPulled By: izdeby\n\nfbshipit-source-id: 608c9389e3d9cd7dac49ea42c9bb0af55662c754", "pr_number": "45535", "files_changed": ["test/test_optim.py"], "labels": []}, "22a34bcf4e": {"title": "ROCm {emoji:2764} TensorExpr (#45506)", "body": "Summary:\nThis might be an alternative to reverting https://github.com/pytorch/pytorch/issues/45396 .\nThe obvious rough edge is that I'm not really seeing the work group limits that TensorExpr produces.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45506\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23991410\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 11d3fc4600e4bffb1d1192c6b8dd2fe22c1e064e", "pr_number": "45506", "files_changed": ["test/run_test.py", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp"], "labels": ["oncall: jit", "open source"]}, "33aba57e4c": {"title": "Patch generate files for system protobuf (#44583)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42939\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44583\n\nReviewed By: albanD\n\nDifferential Revision: D23692639\n\nPulled By: ezyang\n\nfbshipit-source-id: 49781f704dd6ceab7717b63225d0b4076ce33daa", "pr_number": "44583", "files_changed": ["cmake/ProtoBuf.cmake", "cmake/ProtoBufPatch.cmake"], "labels": ["open source", "triaged"]}, "eb39542e67": {"title": "Add typing annotations for torch.utils.data.* modules (#44136)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44135\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44136\n\nReviewed By: gchanan\n\nDifferential Revision: D23963273\n\nPulled By: ezyang\n\nfbshipit-source-id: 939234dddbe89949bd8e5ff05d06f6c8add6935c", "pr_number": "44136", "files_changed": ["mypy.ini", "torch/utils/data/_utils/worker.py", "torch/utils/data/distributed.py"], "labels": ["module: typing", "open source"]}, "375a83e6c1": {"title": "Annotate torch.utils.(tensorboard/show_pickle/hypify) (#44216)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44215\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44216\n\nReviewed By: gchanan\n\nDifferential Revision: D23963216\n\nPulled By: ezyang\n\nfbshipit-source-id: b3fed51b2a1cbd05e3cd0222c89c38d61d8968c1", "pr_number": "44216", "files_changed": ["caffe2/proto/caffe2_pb2.pyi", "torch/utils/hipify/hipify_python.py", "torch/utils/show_pickle.py", "torch/utils/tensorboard/_caffe2_graph.py"], "labels": ["module: typing", "open source"]}, "c1e6592964": {"title": "Enable type-checking of torch.nn.quantized.* modules (#43110)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/43029\n\nI am not changing the following files in this PR:\n* `torch/nn/quantized/dynamic/modules/rnn.py` due to https://github.com/pytorch/pytorch/issues/43072\n* `torch/nn/quantized/modules/conv.py`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43110\n\nReviewed By: gchanan\n\nDifferential Revision: D23963258\n\nPulled By: ezyang\n\nfbshipit-source-id: 0fb0fd13af283f6f7b3434e7bbf62165357d1f98", "pr_number": "43110", "files_changed": ["mypy.ini", "tools/pyi/gen_pyi.py", "torch/_C/__init__.pyi.in", "torch/nn/functional.py", "torch/nn/quantized/functional.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/utils.py"], "labels": ["module: typing", "oncall: quantization", "open source", "triaged"]}, "0a15646e15": {"title": "CUDA RTX30 series support (#45489)", "body": "Summary:\nI also opened a PR on cmake upstream: https://gitlab.kitware.com/cmake/cmake/-/merge_requests/5292\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45489\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23997844\n\nPulled By: ezyang\n\nfbshipit-source-id: 4e7443dde9e70632ee429184f0d51cb9aa5a98b5", "pr_number": "45489", "files_changed": ["caffe2/utils/GpuDefs.cuh", "cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake", "torch/utils/cpp_extension.py"], "labels": ["open source"]}, "c87ff2cb90": {"title": "Enable transposed tensor copy for complex types (#45487)", "body": "Summary:\nThis enables a special copy operator for transposed tensors with more than 360 elements:\nhttps://github.com/pytorch/pytorch/blob/417e3f85e52cdd7439c16c692f7a154d90729dfc/aten/src/ATen/native/Copy.cpp#L19\n\nSteps to repro: python -c \"import torch; print(torch.svd(torch.randn(61, 61, dtype=torch.complex64)))\"\n\nFixes https://github.com/pytorch/pytorch/issues/45269\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45487\n\nReviewed By: anjali411\n\nDifferential Revision: D23984441\n\nPulled By: malfet\n\nfbshipit-source-id: 10ce1d5f4425fb6de78e96adffd119e545b6624f", "pr_number": "45487", "files_changed": ["aten/src/ATen/native/Copy.cpp", "test/test_torch.py"], "labels": []}, "ccad73ab41": {"title": "Fix D23995953 import.", "body": "Summary: https://github.com/pytorch/pytorch/pull/45511 could not be properly imported\n\nTest Plan: See https://github.com/pytorch/pytorch/pull/45511\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D23995953\n\nfbshipit-source-id: a6224a67d54617ddf34c2392e65f2142c4e78ea4", "pr_number": null, "files_changed": ["benchmarks/record_function_benchmark/record_function_bench.py", "test/test_utils.py", "torch/utils/_benchmark/README.md", "torch/utils/_benchmark/__init__.py", "torch/utils/_benchmark/examples/__init__.py", "torch/utils/_benchmark/examples/compare.py", "torch/utils/_benchmark/examples/end_to_end.py", "torch/utils/_benchmark/examples/fuzzer.py", "torch/utils/_benchmark/examples/op_benchmark.py", "torch/utils/_benchmark/examples/prepare_e2e.sh", "torch/utils/_benchmark/examples/simple_timeit.py", "torch/utils/_benchmark/op_fuzzers/__init__.py", "torch/utils/_benchmark/op_fuzzers/binary.py", "torch/utils/_benchmark/op_fuzzers/unary.py", "torch/utils/_benchmark/utils/__init__.py", "torch/utils/_benchmark/utils/common.py", "torch/utils/_benchmark/utils/compare.py", "torch/utils/_benchmark/utils/fuzzer.py", "torch/utils/_benchmark/utils/timer.py", "torch/utils/benchmark/README.md", "torch/utils/benchmark/__init__.py", "torch/utils/benchmark/examples/__init__.py", "torch/utils/benchmark/examples/compare.py", "torch/utils/benchmark/examples/end_to_end.py", "torch/utils/benchmark/examples/fuzzer.py", "torch/utils/benchmark/examples/op_benchmark.py", "torch/utils/benchmark/examples/prepare_e2e.sh", "torch/utils/benchmark/examples/simple_timeit.py", "torch/utils/benchmark/op_fuzzers/__init__.py", "torch/utils/benchmark/op_fuzzers/binary.py", "torch/utils/benchmark/op_fuzzers/unary.py", "torch/utils/benchmark/utils/__init__.py", "torch/utils/benchmark/utils/common.py", "torch/utils/benchmark/utils/compare.py", "torch/utils/benchmark/utils/fuzzer.py", "torch/utils/benchmark/utils/timer.py"], "labels": []}, "772ce9ac2c": {"title": "Fix memory corruption when running torch.svd for complex.doubles (#45486)", "body": "Summary:\nAccording to http://www.netlib.org/lapack/explore-html/d3/da8/group__complex16_g_esing_gaccb06ed106ce18814ad7069dcb43aa27.html\nrwork should be an array of doubles, but it was allocated as array of floats (actually ints)\n\nFixes crash from https://github.com/pytorch/pytorch/issues/45269\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45486\n\nReviewed By: walterddr\n\nDifferential Revision: D23984444\n\nPulled By: malfet\n\nfbshipit-source-id: 6a1b00a27de47046496ccf6a91b6e8ad283e42e6", "pr_number": "45486", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp"], "labels": ["module: complex", "topic: crash", "triaged"]}, "4aca63d38a": {"title": "[TensorExpr] Change API for creating Load and Store expressions. (#45520)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45520\n\nWith this change `Load`s and `Store`s no longer accept `Placeholder`s in\ntheir constructor and `::make` functions and can only be built with\n`Buf`.\n`Placeholder` gets its own `store`, `load`, `storeWithMask`, and\n`loadWithMask` method for more convenient construction.\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D23998789\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 3fe018e00c1529a563553b2b215f403b34aea912", "pr_number": "45520", "files_changed": ["test/cpp/tensorexpr/test_aten.cpp", "test/cpp/tensorexpr/test_boundsinference.cpp", "test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_registerizer.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/test_train_impl.cpp", "torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/ir.cpp", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/stmt.h", "torch/csrc/jit/tensorexpr/tensor.cpp", "torch/csrc/jit/tensorexpr/tensor.h"], "labels": []}, "93650a82c9": {"title": "Move prim::tolist math.log and aten::cpu to lite interpreter for translation model (#45482)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45482\n\nWorking on some models that need these ops on lite interpreter.\n\nTest Plan: locally build and load/run the TS model without problem.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D23906581\n\nfbshipit-source-id: 01b9de2af2046296165892b837bc14a7e5d59b4e", "pr_number": "45482", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["fb-exported", "oncall: jit"]}, "b4ba66ae32": {"title": "Print tensor shapes and convolution parameters when cuDNN exception is thrown (#45023)", "body": "Summary:\nOriginally proposed at https://github.com/pytorch/pytorch/issues/44473#issuecomment-690670989 by colesbury .\n\nThis PR adds the functionality to print relevant tensor shapes and convolution parameters along with the stack trace once a cuDNN exception is thrown.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45023\n\nReviewed By: gchanan\n\nDifferential Revision: D23932661\n\nPulled By: ezyang\n\nfbshipit-source-id: 5f5f570df6583271049dfc916fac36695f415331", "pr_number": "45023", "files_changed": ["aten/src/ATen/cuda/Exceptions.h", "aten/src/ATen/cudnn/Descriptors.cpp", "aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cudnn/Conv.cpp"], "labels": ["open source", "triaged"]}, "c2c7099944": {"title": "Fix docs for kwargs, q-z (#43589)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43589\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D24006259\n\nPulled By: mruberry\n\nfbshipit-source-id: 39abd474744f152648aad201d7311b42d20efc88", "pr_number": "43589", "files_changed": ["torch/_torch_docs.py"], "labels": ["module: docs", "open source", "triaged"]}, "f5c95d5cf1": {"title": "Source code level attribution in profiler (#43898)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43898\n\nAdding with_source parameter to enable tracking source code\n(filename and line) in profiler for eager, torchscript and autograd\nmodes\n\nTest Plan:\npython test/test_profiler.py\n```\nName                                 Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Source Location\n-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  --------------------------------------------\nts_method_1                          10.43%           235.364us        36.46%           822.920us        822.920us        1                test/test_profiler.py(70): test_source\naten::add                            7.52%            169.833us        8.88%            200.439us        200.439us        1                test/test_profiler.py(69): test_source\naten::normal_                        6.26%            141.380us        6.26%            141.380us        141.380us        1                test/test_profiler.py(67): test_source\naten::add                            5.80%            130.830us        8.41%            189.800us        63.267us         3                test/test_profiler.py(72): test_source\naten::sum                            5.02%            113.340us        8.39%            189.475us        189.475us        1                test/test_profiler.py(64): ts_method_1\naten::add                            4.58%            103.346us        6.33%            142.847us        142.847us        1                test/test_profiler.py(62): ts_method_1\naten::mul                            4.05%            91.498us         9.62%            217.113us        217.113us        1                test/test_profiler.py(71): test_source\naten::add                            4.03%            90.880us         5.60%            126.405us        126.405us        1                test/test_profiler.py(58): ts_method_2\naten::empty                          3.49%            78.735us         3.49%            78.735us         19.684us         4                test/test_profiler.py(72): test_source\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D23432664\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 83ad7ebe0c2502494d3b48c4e687802db9c77615", "pr_number": "43898", "files_changed": ["aten/src/ATen/record_function.h", "benchmarks/profiler_benchmark/profiler_bench.py", "caffe2/CMakeLists.txt", "test/cpp/jit/test_misc.cpp", "test/test_profiler.py", "torch/autograd/profiler.py", "torch/csrc/autograd/function.h", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/profiler.h", "torch/csrc/distributed/rpc/profiler/server_process_global_profiler.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tracer.h", "torch/csrc/jit/python/python_tracer.cpp", "torch/csrc/jit/python/python_tracer.h", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/interpreter.h", "torch/distributed/rpc/server_process_global_profiler.py"], "labels": ["oncall: jit"]}, "cf07ba50fe": {"title": "Update target determinator to point to release/1.7", "body": "Signed-off-by: Eli Uriegas <eliuriegas@fb.com>", "pr_number": null, "files_changed": [".jenkins/pytorch/common_utils.sh"], "labels": []}, "43404c4141": {"title": "[1.7] Remove torch.vmap (#45571)", "body": "torch.vmap is a prototype feature and should not be in the stable\r\nbinary. This PR:\r\n- Removes the `torch.vmap` API\r\n- Removes the documentation entry for `torch.vmap`\r\n- Changes the vmap tests to use an internal API instead of `torch.vmap`.\r\n\r\nTest Plan:\r\n- Tested locally (test_torch, test_type_hints, test_vmap), but also wait\r\nfor CI.", "pr_number": null, "files_changed": ["docs/source/torch.rst", "test/test_vmap.py", "torch/__init__.py"], "labels": []}, "e8cea53b85": {"title": "Add allowlist for complex backward (#45602)", "body": "ghstack-source-id: a3aaa9ba4657445433903ff51cc184afc35e7d0a\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45461", "pr_number": "45461", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/autograd.cpp", "torch/csrc/autograd/python_engine.cpp"], "labels": ["complex_autograd", "module: complex"]}, "07e66d7ca5": {"title": "Enable PE + TE (#45546) (#45591)", "body": "Summary:\r\nThis PR enables PE + TE for 1.7\r\n\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45546\r\n\r\nReviewed By: ZolotukhinM\r\n\r\nDifferential Revision: D24006940\r\n\r\nPulled By: Krovatkin\r\n\r\nfbshipit-source-id: a3326077d34a023941acdb06c4907c96e7ba0115", "pr_number": "45546", "files_changed": ["test/test_jit_cuda_fuser.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["oncall: jit"]}}
d21ee2de66,skip,[wip] Upgrade msvc to 14.13 (#40109)
0e074074f3,jit,Disable inlining an opaque tensor into a constant (#40367)
f6b9848c25,improvements_frontend,Use chain.from_iterable in optimizer.py (#40156)
b623bdeabb,skip,Move TensorOptions ops to c10 (#39492)
5ad885b823,caffe2,[Caffe2][Pruning] Make the caffe2 Sum operator support long types (#40379)
09285070a7,complex_frontend,Doc fix for complex views (#40450)
581ad48806,skip,Revert D21581908: Move TensorOptions ops to c10
d8ec19bc03,skip,Revert D22072830: [wip] Upgrade msvc to 14.13
cc9075c5d4,dispatcher,Add some syntax sugar for when backends use the same function. (#40182)
111b399c91,dispatcher,Delete requires_tensor (#40184)
c7d79f35e3,complex_frontend,Header rename complex_type.h -> complex.h (#39885)
2e6da36298,mobile,[android][ci] Fix CI packaging headers to aar (#40442)
a6a2dd14ea,docs_frontend,Fix typo in warning message (#39854)
883e4c44b2,new_features_frontend,Raise exception when trying to build PyTorch on 32-bit Windows system (#40321)
7e32e6048d,bug_fixes_frontend,Fix linspace step computation for large integral types (#40132)
ddb8565b25,performance_frontend,Revert D22162469: [pytorch][PR] Migrate `var` & `std` to ATen
92d3182c11,mobile,Revert D21232894: Unify PyTorch mobile's threadpool usage.
c314e0deb5,quantization,[quant] Quantized adaptive_avg_pool3d (#40271)
a2d4d9eca6,improvements_frontend,Improve Dynamic Library for Windows (#40365)
7b0f867c48,performance_frontend,Perf improvement of Conv2d and Conv3d (#40324)
46b9e519aa,quantization,Remove print (#40475)
f035f73d53,releng,Fix the issue that run clang-tidy on the aten folder (#39713)
0ecea2d64d,distributed,[JIT x RPC] Consolidate Future type class and Future impl class (#40406)
b4eb82cd29,caffe2,"Temporary commit at 6/17/2020, 6:49:44 PM"
72e8690b78,onnx,Fix typo. in error message (#39958)
e439cf738a,docs_frontend,Fix examples Adaptive avg pooling typo (#40217)
a4dec0674c,docs_frontend,[doc] fix typo in formula of MarginRankingLoss (#40285)
ecd9a64712,jit,fix `torch.jit.trace_module` documentation (#40248)
4975be80f8,distributions,"fix typo ""normal"" -> ""Cauchy"" (#40334)"
e490352dc4,complex_frontend,Simplify complex case for tanh backward (#39997)
43ab9c677b,improvements_frontend,Add invariants check to BatchedTensorImpl (#40171)
727463a727,vmap_frontend,Initial vmap frontend API (#40172)
b4ccdef090,new_features_frontend,Allow torch.cuda.amp.GradScaler to support sparse gradients (#36786)
3ed96e465c,improvements_frontend,Report error when ATEN_THEADING is OMP and USE_OPENMP is turned off. (#40146)
597cb04b2f,caffe2,Use Int8QuantParamsBlob to pass the scale and zeropoint params (#40494)
a2e1a948a4,distributed,Increase number of iterations in DDP SPMD tests (#40506)
de7ac60cf4,distributed,Add out= variants for cuda.comm.broadcast/gather/scatter (#39681)
fc4824aa4a,new_features_frontend,enable mkldnn dilation conv (#40483)
fe18dcd692,jit,Use GLOG logging prefixes (#40491)
527ab13436,skip,[NCCL] Explicitly Abort NCCL Communicators on Process Group Destruction (#40241)
3e2d2fc856,distributed,[NCCL Docs] Adding Comments for Work-level Finish in ProcessGroup (#40404)
0c923eea0a,distributed,"Add finishAndThrow function to ProcessGroup::Work, and use with Gloo (#40405)"
72f2c479e3,th_aten_frontend,Migrate equal from the TH to Aten (CPU) (#33286)
c120fdc05b,releng,Unify `torch/csrc/cuda/shared/cudnn.cpp` include path (#40525)
16f276cef9,dispatcher,Add C++-only `int dim` overloads to `std`-related operations (#40451)
3dcc329746,improvements_frontend,Use tree-based sum for floats to avoid numerical instability (#39516)
e12f73ee12,improvements_frontend,Add missing file to BUILD.bazel (#40536)
adcd755e69,releng,Fix backup solution (#40515)
06debf6373,jit,move __range_length and __derive_index to lite interpreter (#40533)
85b87df5ba,skip,Revert D22208758: [pytorch][PR] Report error when ATEN_THEADING is OMP and USE_OPENMP is turned off.
82e9318a16,improvements_frontend,Adjust CUDA memory leak test (#40504)
cf8a9b50ca,improvements_frontend,Allow ReflectionPad to accept 0-dim batch sizes. (#39231)
eae1ed99a3,caffe2,caffe2 | Fix building with `-Wrange-loop-analysis` on
88ea51c061,docs_frontend,doc string fix for torch.cuda.set_rng_state_all (#40544)
7038579c03,improvements_frontend,"Add batching rule for unsqueeze, squeeze, and transpose (#40455)"
43757ea913,improvements_frontend,Add batching rule for Tensor.permute (#40517)
c362138f43,vmap_frontend,Disallow passing functions that don't return Tensors to vmap (#40518)
7369dc8d1f,caffe2,Use CPU Allocator for reading from zip container
461014d54b,releng,"Unify libtorch_python_cuda_core_sources filelists between CMakeList, fbcode and bazel (#40554)"
f41173b975,quantization,[PyPer][quant] Add quantized embedding operators to OSS. (#40076)
4d40ec1480,jit,[PyTorch Error Logging][1/N] Adding Error Logging for Run_Method (#40535)
dfbf0164c9,distributed,Revert D22103662: [NCCL] Explicitly Abort NCCL Communicators on Process Group Destruction
e231405ef6,jit,[jit] Fix type annotations in select assignments (#40528)
c6e0c67449,jit,[PyTorch Error Logging][2/N] Adding Error Logging for Loading Model (#40537)
e180ca652f,typing_frontend,Add __all__ to torch/_C/_VariableFunctions.pyi (#40499)
b05c34259b,distributed,relax size check in flatten_for_scatter_gather (#40573)
c790476384,releng,"Back out ""Revert D22072830: [wip] Upgrade msvc to 14.13"" (#40594)"
ac79c874ce,caffe2,[PyTorch Operator] [2/n] Adding python test
5466231187,caffe2,Fixes lint (#40606)
547ea787ff,skip,[ONNX] Add eliminate_unused_items pass (#38812)
fab412a8f3,releng,Bump nightlies to 1.7.0 (#40519)
3ab60ff696,complex_frontend,Remove cpu vec256 for std::complex (#39830)
f1406c43fc,dispatcher,[papaya][aten] Fix compiler error: loop variable 'tensor' is always a copy because the range of type 'c10::List<at::Tensor>' does not return a reference. (#40599)
dfc7e71d13,mobile,[Selective Build] Apply query-based on instrumentation_tests
44bf822084,cpp,Add C++ standard version check to top level headers (#40510)
a4cabd1a3c,dispatcher,Generalize Python dispatcher testing API; disallow overwriting fallback (#40469)
a0ba7fb43e,dispatcher,Precompute entries in dispatch tables (#40512)
67c79bb045,jit,update schema to reflect aliasing behavior (#39794)
fc8bca094c,distributed,skip_if_rocm test_rnn in test_c10d_spawn.py (#40577)
10822116c5,releng,build docker image for CUDA11 (#40534)
fb5d784fb4,releng,Further reduce windows build/test matrix (#40592)
7676682584,caffe2,Fix illegal opcode bug in caffe2 (#40584)
5036c94a6e,jit,properly skip legacy tests regardless of the default executor (#40381)
b8f4f6868d,jit,[JIT] Remove dead store in exit_transforms.cpp (#40611)
0494e0ad70,dispatcher,"Back out ""Revert D21581908: Move TensorOptions ops to c10"" (#40595)"
edac323378,releng,Add special rules to launch docker image with RocM (#40632)
41ea7f2d86,memory_format_frontend,Add channels-last support to bundled_inputs (#36764)
375cd852fa,memory_format_frontend,Add a utility function for bundling large input tensors (#37055)
6debc28964,releng,Ignore error code from `apt-get purge` (#40631)
24a8614cac,docs_frontend,[Reland][doc] Add overflow notice for cuFFT on half precision (#40551)
47c72be3d7,improvements_frontend,Port /test/cpp_extensions/rng_extension.cpp to new operator registration API (#39459)
897e610c82,caffe2,FP16 rounding-to-nearest for row-wise SparseAdagrad fusion (#40466)
4102fbdf08,caffe2,[1/n] Allow dense NaN value in dper raw input processor output
15864d1703,distributed,Skip allreducing `local_used_maps_dev_` when `find_unused_param=False`
e368b11226,jit,[JIT] Remove dead stores in loopnest.cpp (#40626)
0a19534dd2,jit,[JIT] Remove dead store in quantization_patterns.h (#40623)
0309f6a4bb,jit,[quant][graphmode][fix] cloning schema in insert_observers (#40624)
61a8de77cf,quantization,[quant] aten::repeat work for quantized tensor (#40644)
411bc2b8d5,quantization,[quant][graphmode][fix] remove unsupported ops in the list (#40653)
ac8c8b028d,amd,[ROCm] restore jit tests (#40447)
68042c7466,typing_frontend,Skip mypy on pynightly if numpy-1.20.0-dev0... is used (#40656)
15be823455,caffe2,caffe2 | Revert range loop analysis fix
2456e078d3,visualization,[TB] Support custom run_name in add_hparams (#40660)
4c25428c8c,visualization,[TB] Add support for hparam domain_discrete
21991b63f5,th_aten_frontend,Migrate `dot` from the TH to Aten (CPU) (#40354)
1399655a98,skip,[Fix] torch_common target shared by lite-interpreter and full-jit
5377827b3e,skip,Revert D22275201: [Fix] torch_common target shared by lite-interpreter and full-jit
502ec8f7f7,skip,Revert D22227939: [TB] Add support for hparam domain_discrete
b4db529352,distributed,Fix wrong link in docs/source/notes/ddp.rst (#40484)
b35cdc5200,releng,"[Fix] torch_common target shared by lite-interpreter and full-jit"" and turn on query-based selective build (#40673)"
4121d34036,cpp,Python/C++ API Parity: Add impl and tests for ParameterDict (#40654)
4a174c83ca,jit,Add option to preserve certain methods during optimize_for_mobile. (#40629)
ed83b9a4be,typing_frontend,Change function parameter `self` to `input` in torch.__init__.pyi (#40235)
63e5a53b8c,improvements_frontend,DNNL: fix build error when DNNL using TBB threading pool (#40699)
fd90e4b309,releng,[CircleCI] Add RocM build/test jobs (#39760)
11a74a58c8,complex_frontend,Setter for real and imag tensor attributes (#39860)
9ca4a46bf8,improvements_frontend,Implement parallel scatter reductions for CPU (#36447)
6e1cf000b3,jit,[jit][oacr] Add some operators for Assistant NLU joint lite model (#40126)
0235676f8a,mobile,[pytorch][ci] run mobile code analysis on PR (#40247)
8f5b28674c,quantization,[JIT] Remove dead store in quantization_patterns.h (#40724)
d7cd16858f,docs_frontend,Add documentation about storage sharing is preserved and serialized f… (#40412)
9393ac011a,complex_frontend,[CUDA] addmm for complex (#40431)
53af9df557,dispatcher,Unify boxed function signature between jit and c10 (#37034)
01e2099bb8,visualization,[TB] Add support for hparam domain_discrete (#40720)
fabd60ec1a,dispatcher,Add comment with UNBOXEDONLY explanation to codegen (#40117)
a371652bc8,dispatcher,Allow to get string references to strings inside torch::List (#39763)
b9cca4b186,caffe2,fix range of results for pairwise operations (#40728)
31de10a392,caffe2,Int8FC dequantize fix (#40608)
4104ab8b18,new_features_frontend,Add `torch.count_nonzero` (#39992)
4a235b87be,build_frontend,pop warning message for cuda module when asan is built in (#35088)
2f94b7f95c,vmap_frontend,Initial vmap docstring (#40575)
a6a31bcd47,vmap_frontend,Enable `out_dims` for vmap frontend API (#40576)
3cc18d7139,releng,.circleci: Remove executor from windows uploads (#40742)
c56255499a,skip,Reverts running clang-tidy on ATen (#40764)
5923a802fa,skip,"Back out ""[pytorch][PR] [ONNX] Add eliminate_unused_items pass"""
ef5a314597,typing_frontend,[typing] fix register_buffer/parameter (#40669)
a303fd2ea6,complex_frontend,Let exp support complex types on CUDA and enable device/dtype in complex tests (#39087)
c3237c7a87,releng,Print hostname of RoCM tester (#40755)
f3949794a3,opbench,Prototype benchmarking util (#38338)
9ac0febb1f,releng,Pin torchvision version for doc_push (#40802)
1571dd8692,build_frontend,Refactor duplicated string literals (#40788)
e762ce8ecf,distributed,Avoid initializing `new_group` in test_backward_no_ddp. (#40727)
40e79bb1d3,releng,Update the version of ninja and scipy (#40677)
9d8dc0318b,caffe2,[pruning] add rowwise counter to sparse adagrad
0a75234934,new_features_frontend,Allow np.memmap objects (numpy arrays based on files) to be processed… (#39847)
29aef8f460,bug_fixes_frontend,Skip some error-producing exp tests that cannot be reliably reproduced (#40824)
0ddaaf6a92,caffe2,[codemod][caffe2] Run clang-format - 5/7
179dbd4f25,jit,[jit] preserve keys on dictionary input tracing (#40792)
8e0714a60d,profiler,[rfc] Reduce number of coin flips in RecordFunction (#40758)
0203d70c63,docs_frontend,[nit] fix some typo within documentation (#40692)
f13653db29,distributions,[Update transforms.py]use build-in `atanh` in TanhTransform (#40160)
2cf9fe2d92,bug_fixes_frontend,Remove more error-exposing tests in exp that cannot be reliably reproduced (#40825)
5f9e7240f5,dispatcher,Fix bug where explicitly providing a namespace never worked. (#40830)
fcadca1bda,improvements_frontend,serialization: validate sparse tensors after loading (#34059)
a0569ad8f8,mobile,[android][readme] Aar native linking add fbjni (#40578)
6aebd2c412,quantization,[quant][graphmode] Add FP16 quant support - Insert Noop Observers (#40708)
55b5ab14d3,quantization,[quant][graphmode] FP16 quant support - Insert cast operators (#40709)
26543e6caf,quantization,[quant][graphmode] FP16 quant support - Operator Fusion (#40710)
c73255801f,bug_fixes_frontend,Fix the autograd codegen for repeat function (#40766)
af34f2f63b,typing_frontend,Added missing generator argument in type annotation(pytorch#40803) (#40873)
49e12d888a,distributed,[NCCL - reland] Explicitly abort NCCL Communicators on Process Group Destruction (#40585)
ad30d465d5,releng,Move install_torchvision to common.sh so that it can be sourced. (#40828)
04b6e4273e,distributed,clang format reducer.cpp (#40876)
2f47e953f7,docs_frontend,Fixes #40158 (#40617)
d7c9f96e43,dispatcher,Optimize perf for calling ops with custom classes (#38257)
8f6e50d013,dispatcher,Make some more ops c10-full (#40747)
ea03f954ad,onnx,[ONNX] Add warning in ONNX export when constant folding is on in training-amenable mode (#40546)
59294fbbb9,caffe2,[caffe2] Reimplement RemoveOpsByType with SSA (#40649)
9fa1f27968,jit,[jit] Fix value association with dictionaries in the tracer (#40885)
591fffc524,typing_frontend,Type-annotate serialization.py (#40862)
1a74bb84f2,caffe2,Remove Int8FC diff restriction.
5db5a0f2bb,caffe2,Re-enable Caffe2 test `RoiAlignTest.CheckCPUGPUEqual` (#40901)
6aabd12390,improvements_frontend,fix issue #31759 (allow valid ASCII python identifiers as dimnames) (#40871)
6ae3cd0d9d,rpc,Configure RPC metrics handlers and pass them into Thrift RPC Agent (#40602)
b678666a04,docs_frontend,Add `module.training` to docs (#40923)
db39542509,skip,[2/n][Compute Meta] support analysis for null flag features
9f14e48834,caffe2,Override shape hints with real weight shape extracted from workspace (#40872)
af5bcba217,releng,.circleci: Build docker images as part of CI workflow (#40827)
ce63f70981,skip,[C2] Fixed a bug in normalization operator (#40925)
a7e09b8727,quantization,pytorch | Namespace init_win symbol in qnnpack.
81aebf380e,quantization,pytorch | Fix linking of qnnpack params on windows. (#40920)
dec3f918a0,th_aten_frontend,Migrate 'torch.dot' from TH to Aten (CUDA) (#40646)
b7517a76ba,improvements_frontend,rshift use default >> operator (#40545)
9cc73966b3,caffe2,[TVM] Fix build and sync with caffe2/caffe2/python/dlpack.h (#40888)
a1c234e372,skip,Revert D22330340: [C2] Fixed a bug in normalization operator
3c6b8a6496,skip,Revert D22360735: .circleci: Build docker images as part of CI workflow
f8d4878b3c,mobile,check for unsupported instructions when exporting mobile models (#40791)
3ca5849f0a,caffe2,Add serializer and deserializer for Int8QuantSchemeBlob and Int8QuantParamsBlob (#40661)
6095808d22,bug_fixes_frontend,fix pca_lowrank memory consumption (#40853)
28e1d241cd,releng,[pytorch] factor out binary size upload command (#40188)
ff17b83fd8,mobile,[pytorch][ci] add custom selective build flow for android build (#40199)
824ab19941,quantization,[quant][graphmode] Support quantization for `aten::apend` (#40743)
3890550940,rpc,[RPC tests] Fix @_skip_if_tensorpipe always skipping for all agents (#40860)
d0f2079b5e,distributed,[RPC tests] Remove world_size and init_method from TensorPipe fixture (#40814)
f9a71d3de4,distributed,[RPC tests] Align ddp_under_dist_autograd test with others (#40815)
f083cea227,distributed,[RPC tests] Fix file descriptor leak (#40913)
f3f113f103,quantization,[quant][graphmode][fix] Print the node in error message (#40889)
3b7df2388e,rpc,[RFC] Profile rpc_async call from JIT (#40652)
0790d11a18,typing_frontend,typing for tensor.T/grad_fn torch.Size (#40879)
8ecd4f36aa,jit,"fix __len__, __contains__, getitem inherited from interface class derived from nn container (closes #40603) (#40789)"
1e64bf4c40,releng,[CircleCI] Delete docker image after testing (#40917)
300a3aaaad,jit,[jit] move private implementation out of `jit/__init__.py` (#40807)
0deb2560b8,skip,"add eq.str, ne.str, and add.str ops (#40958)"
54d7a1e3f4,bug_fixes_frontend,Fix module dict key ordering (#40905)
450ba49653,releng,Add the missing `resource_class` key in the update_s3_htmls job (#41000)
063d5b0d3f,improvements_frontend,Remove get_fail_msg in test_dataloader.test_proper_exit (#40745)
0b9717b86a,build_frontend,"When linking libtorch_cpu.so, put AVX sources last in the input list (#40449)"
e1afa9daff,build_frontend,fix cmake bug (#39930)
46f5cf1e31,releng,Improve error reporting of AVX instruction in CI job (#40681)
73c5a78f43,caffe2,Test test_int8_ops_nnpi.py case typo fix. (#41008)
f6f3c0094a,skip,"Revert D22369579: add eq.str, ne.str, and add.str ops"
c935712d58,improvements_frontend,Use unbind for tensor.__iter__ (#40884)
c38a5cba0d,improvements_frontend,Remove duplicate assignment in collate.py (#40655)
e173278348,docs_frontend,Update quantization.rst (#40896)
7f60642bae,improvements_frontend,[pytorch] add manual registration for trace type (#40903)
0fbd42b20f,improvements_frontend,[pytorch] deprecate PYTORCH_DISABLE_TRACING macro (#41004)
d753f1c2e1,docs_frontend,"Fixes formatting of vander, count_nonzero, DistributedSampler documentation (#41025)"
e026d91506,jit,[JIT] Remove dead store in unpickler.cpp (#40625)
b9b4f05abf,jit,"[nvFuser] Working towards reductions, codegen improvements (#40864)"
35bd2b3c8b,docs_frontend,DOC: Clarify that CrossEntropyLoss mean is weighted (#40991)
945ae5bd7b,docs_frontend,Update the documentation of the scatter_ method with support for reduction methods. (#40962)
87f9b55aa5,improvements_frontend,Use explicit templates in `gpu_kernel_with_scalars` (#40992)
cbe52d762c,caffe2,Mish Activation Function (#40856)
c0f9bf9bea,jit,s/torch::jit::class_/torch::class_/ (#40795)
a78024476b,th_aten_frontend,Port `equal` from THC to ATen (CUDA) (#36483)
078669f6c3,caffe2,"Back out ""[2/n][Compute Meta] support analysis for null flag features"""
dac63a13cb,improvements_frontend,run single-threaded gradgradcheck in test_nn (#40999)
5d1d8a58b8,vmap_frontend,Enable `in_dims` for vmap frontend api (#40717)
63ef706979,build_frontend,[ATen] Add `native_cuda_h` list to CMakeLists.txt (#41038)
6b50874cb7,docs_frontend,Fix HTTP links in documentation to HTTPS (#40878)
6e4f501f1a,onnx,Improve error message for Pad operator (#39651)
b2cc8a2617,onnx,[ONNX]Fix export of full_like (#40063)
56396ad024,onnx,ONNX: support view_as operator (#40496)
50df097599,jit,Fix CUDA jit codegen compilation with gcc-5.4 (#41055)
4aa543ed2e,jit,Fix unordered-map-over-enum for GCC 5.4 (#41063)
a6b703cc89,build_frontend,Make `torch_cpu` compileable when `USE_TENSORPIPE` is not set. (#40846)
0e09511af9,typing_frontend,"type annotations for dataloader, dataset, sampler (#39392)"
a04af4dccb,skip,Revert D22396896: [pytorch][PR] run single-threaded gradgradcheck in test_nn
2d98f8170e,opbench,Add option to warn if elements in a Compare table are suspect (#41011)
733b8c23c4,quantization,Fix several quantization documentation typos (#40567)
452d5e191b,docs_frontend,Grammatically updated the tech docs (#41031)
00ee54d2a4,docs_frontend,Fix link to PyTorch organization (from Governance) (#40984)
630e7ed9cc,improvements_frontend,Splitting embedding_bag to embedding_bag_forward_only and embedding_bag (#40557)
0e6b750288,amd,Insert parentheses around kernel name argument to hipLaunchKernelGGL (#41022)
75155df8b4,docs_frontend,Doc warnings (#41068)
8d570bc708,distributed,Decouple DataParallel/DistributedDataParallel from CUDA (#38454)
93778f3b24,dispatcher,Expose certain methods in OpaqueTensorImpl. (#41060)
8f0e254790,improvements_frontend,"In interpolate, use if instead of elif (#37171)"
3c1c74c366,improvements_frontend,"In interpolate, move exceptional cases to the bottom (#37172)"
4dad829ea3,improvements_frontend,"In interpolate, inline the call to _interp_output_size (#37173)"
5e03a1e926,new_features_frontend,Add support for int[]? arguments in native_functions.yaml (#37174)
38b465db27,amd,ROCm 3.5.1 image (#40385)
eea535742f,new_features_frontend,Add bfloat16 support for nccl path (#38515)
a4fd4905c8,releng,bump docker version to more recent tag (#41105)
cc29c192a6,jit,"add ""aten::add.str"" op and remove two duplicated ops"
054e5d8943,releng,.circleci: Fix job-specs-custom docker tag (#41111)
a8bc7545d5,amd,use PYTORCH_ROCM_ARCH to set GLOO_ROCM_ARCH (#40170)
bce75a2536,caffe2,add first implementation of swish (#41085)
445128d0f2,docs_frontend,Add PyTorch Glossary (#40639)
bacca663ff,docs_frontend,Fix Broken Link in CONTRIBUTING.md (#41066)
3615e344a3,caffe2,Unit test case for the Int8FC to cover quantization scale errors. (#41100)
c55d8a6f62,complex_frontend,Remove std::complex from c10::Scalar (#39831)
3e01931e49,jit,[JIT] Separate to_backend API into libtorch and libtorch_python (#40839)
5a4c45f8d1,jit,[JIT] Move TestBackend to test directory (#40840)
6777ea19fe,jit,[JIT] Add support for backend-lowered submodules (#40841)
e2a291b396,skip,[JIT] Add out-of-source-tree to_backend tests (#40842)
6ef94590fa,quantization,match int8 quantization of nnpi (#41094)
e0e8b98c43,caffe2,Export logic op to pytorch
3d3fd13e04,quantization,[quant][graphmode][fix] filter for list append change (#41020)
e4fbcaa2bc,skip,[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
de4fc23381,jit,clean up duplicated op names (#41092)
03eec07956,vmap_frontend,Move error messages in-line in `_vmap_internals.py` (#41077)
dde18041a6,quantization,[quant][graphmode] Refactor quantization patterns (#40894)
6c9b869930,amd,"[ROCm] Skip Conv2d, Conv3d transpose fp16 test for ROCm3.5 (#41088)"
c93e96fbd9,jit,[jit] move script-related implementation out of torch/jit/__init__.py (#40902)
58d7d91f88,improvements_frontend,Return atomic (#41028)
131a0ea277,jit,Add version number to bytecode. (#36439)
2bc9ee97d1,skip,Revert D22418731: [JIT] Add out-of-source-tree to_backend tests
dfd21ec00d,skip,Revert D22418716: [JIT] Add support for backend-lowered submodules
ec58d739c6,releng,.circleci: Remove pynightly jobs
9d1138afec,dispatcher,Remove unnecessary atomic ops in DispatchStub (#40930)
af2680e9ce,skip,Update ShipIt sync
97052c5fa8,caffe2,Extend SparseAdagrad fusion with stochastic rounding FP16 (#41107)
10caf58a52,typing_frontend,[typing] tensor._version is int (#41125)
b8d2ccf009,improvements_frontend,Unify TensorOptions signatures (#39611)
3f32332ee6,jit,[JIT][Easy]move remove mutation to own file (#41137)
6725c034b6,skip,"Migrate addmm, addbmm and THBlas_gemm to ATen (#40927)"
c7768e21b1,skip,[JIT] Add GitHub workflow for importing issues to triage project (#41056)
04004bf10c,jit,"Fix a minor typo ""forget add"" -> ""forget to add"" (#41131)"
f71cccc457,improvements_frontend,test: Add option to continue testing through error (#41136)
302cf6835e,caffe2,[ROCm][Caffe2] Enable MIOpen 3D Pooling (#38260)
33e26656fa,mobile,list workaround for CREATE_OBJECT failure (#41129)
8e2841781e,jit,[easy] Use torch.typename in JIT error messages (#41024)
155fb22e77,improvements_frontend,Run single-threaded gradgradcheck in testnn (#41147)
bf9cc5c776,new_features_frontend,Add callback with TLS state API in futures (#40326)
7ff7c9738c,skip,"Revert D22418756: [pytorch][PR] Migrate addmm, addbmm and THBlas_gemm to ATen"
86f72953dd,skip,[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
690946c49d,jit,Generalize constant_table from tensor only to ivalue (#40718)
c038f8afcc,releng,Do not install nvidia docker for non-NVIDIA configs (#41144)
62cee0001e,jit,Move async + serialization implementation out of 'jit/__init__.py' (#41018)
07fd5f8ff9,skip,Create lazy_dyndeps to avoid caffe2 import costs. (#39488)
a318234eb0,improvements_frontend,Print raising warnings in Python rather than C++ if other error occurs (#41116)
df1f8a48d8,caffe2,add null check for c2 tensor conversion (#41096)
2252188e85,caffe2,[caffe2] Fix spatial_batch_norm_op dividision-by-zero crash (#40806)
7c29a4e66f,build_frontend,Don't add NCCL dependency to gloo if system NCCL is used (#41180)
c1fa74b2d7,quantization,[quant][refactor] test_only_eval_fn (#41078)
e84ef45dd3,jit,[JIT] Fix JIT triage workflow (#41170)
2cf31fb577,performance_frontend,Fix max_pool2d perf regression (#41174)
22f940b7bd,code_coverage,add clang code coverage compile flags (#41103)
1f1351488e,skip,Revert D21870844: Create lazy_dyndeps to avoid caffe2 import costs.
e374280768,skip,Use explicit templates in CUDALoops kernels (#41059)
a88099ba3e,docs_frontend,restore old documentation references (#39086)
bddba1e336,opbench,Add benchmark for add op. (#40059)
d6feb6141f,performance_frontend,[Vec256][neon] Add neon backend for vec256 (#39341)
82c9f79e0e,performance_frontend,Add fused add_relu op. (#39342)
c5dcf056ee,jit,JIT pass for add relu fusion. (#39343)
7c2c752e6d,skip,Revert D22458928: [pytorch][PR] Use explicit templates in CUDALoops kernels
a79b416847,caffe2,make Int8 FC bias quantization use round flush to infinity
df252c059c,caffe2,[ROCm] Skip caffe2 unique op test for rocm3.5 (#41219)
f6eb92a354,jit,Expose private APIs to enable/disable pickling ScriptModules without RPC (#39631)
75a4862f63,new_features_frontend,Added SiLU activation function (#41034)
8a79eec98a,mobile,Add add_relu fusion pass to optimize_for_mobile. (#40252)
08227072e2,opbench,Benchmark RecordFunction overhead on some models (#40952)
62e16934cb,caffe2,[caffe2] Add the dedup implementation of fused RowWiseAdagrad op on GPUs (#40282)
e568b3fa2d,bug_fixes_frontend,test nan and inf in TestTorchMathOps (#41225)
db38487ece,complex_frontend,Autograd Doc for Complex Numbers (#41012)
33f9fbf8ba,distributed,Modularize parsing NCCL_BLOCKING_WAIT in ProcessGroupNCCL (#41076)
4a09501fbe,caffe2,LogitOp LUT based fake FP16 Op. (#41258)
d927aee312,docs_frontend,Small clarification of torch.cuda.amp multi-model example (#41203)
75b6dd3d49,caffe2,Wrap Caffe2's SparseLengthsSum into a PyTorch op (#39596)
a548c6b18f,skip,add check for duplicated op registration in JIT (#41214)
9b0393fcf1,onnx,[ONNX]Fix export of flatten (#40418)
16c8146da9,skip,Self binning histogram (#40875)
cb6c3526c6,th_aten_frontend,"Migrate addmm, addbmm and THBlas_gemm to ATen (#40927)"
0651887eb4,improvements_frontend,Improve repr for torch.iinfo & torch.finfo (#40488)
7c143e5d3e,releng,Reducing size of docker Linux image (#41207)
9daba76ba1,improvements_frontend,Change to.dtype_layout to c10-full (#41169)
dd0c98d82a,onnx,[ONNX]Add tests for ConvTranspose 1D and 3D (#40703)
7bae5780a2,skip,Revert D22329069: Self binning histogram
1f2e91fa4f,bug_fixes_frontend,Impilcit casting resulting internal build failure. (#41272)
ce3ba3b9bc,jit,[JIT] Add support for backend-lowered submodules (#41146)
48d6e2adce,performance_frontend,Disable the mkldnn for conv2d in some special cases (#40610)
abea7cd561,build_frontend,msvc anonymous namespace bug (#41199)
ac3542fa59,build_frontend,Define PSIMD_SOURCE_DIR when including FP16 (#41233)
67f5d68fdf,skip,Revert D22465221: [pytorch][PR] Reducing size of docker Linux image
6cbb92494d,build_frontend,Better THGeneric.h generation rules in bazel (#41285)
877a59967f,improvements_frontend,Ampere has CUDA_MAX_THREADS_PER_SM == 2048 (#41138)
1c098ae339,typing_frontend,Fix arg type annotations in jit.trace and onnx.export (#41093)
d1f06da9b7,caffe2,Solve log2(x:int) ambiguity by using log2(float(x)) (#41295)
095886fa42,caffe2,[caffe2] Fix the issues when using CUB RadixSort (#41299)
a1ed6e1eb3,skip,Revert D22467871: add check for duplicated op registration in JIT
e544bf2924,caffe2,fix the range of the random weights used in the int8fc test (#41303)
28291d3cf8,caffe2,[caffe2] Revert D22220798 (#41302)
c864158475,caffe2,Add fp16 support to SparseLengthSum PyTorch operator (#41058)
edcf2cdf86,quantization,[quant] dequantize support list and tuple of tensors (#41079)
106b0b6a62,caffe2,Op to create quant scheme blob (#40760)
4b4184fc69,quantization,[quant][graphmode] use RemoveMutation to remove append (#41161)
402be850a8,quantization,[quant] Adding zero point type check for per channel quantization (#40811)
5e72ebeda3,quantization,Fake Quantization Per Tensor Kernel Core Implementation (CPU) (#41029)
fa153184c8,quantization,Fake Quantization Per Channel Kernel Core Implementation (CPU) (#41037)
98df9781a7,new_features_frontend,Impl for ParameterList (#41259)
67a4f375cd,caffe2,Pass the number of indices but not embedding size in PyTorch operator (#41315)
dea39b596e,caffe2,reduce logging for layernorm (#41305)
86d803a9da,releng,.cirlceci: Setup nvidia runtime for cu as well (#41268)
d04a2e4dae,caffe2,"Back out ""Revert D22329069: Self binning histogram"" (#41313)"
fb9e44f8dd,dispatcher,Add support for float[]? arguments in native_functions.yaml (#37175)
7183fd20f8,new_features_frontend,Add interpolate-style overloads to aten::upsample* ops (#37176)
c451ddaeda,caffe2,Add shape inference functions for int8 quantization related ops (#41215)
0c77bd7c0b,quantization,Quantization: preserving pre and post forward hooks (#37233)
6e6931e234,bug_fixes_frontend,fix duplicate extern sdot and missing flags (#41195)
4196605776,distributed,helper function to print out all DDP-relevant env vars (#41297)
d601325de4,caffe2,update operators in the mapping to fp16 emulation
b6e1944d35,releng,.circleci: Explicitly remove nvidia apt repos (#41367)
ca1b8ebbcb,jit,move misc implementation out of `jit/__init__.py` (#41154)
5f6c6ed157,skip,Fix FC issue (#41198)
34e11b45c9,complex_frontend,Remove thrust casting from static_cast_with_inter_type (#39905)
80d5b3785b,new_features_frontend,Add torch.logit function (#41062)
c20426f86d,bug_fixes_frontend,Fix torch.cuda.check_error type errors (#41330)
e888c3bca1,docs_frontend,Update torch.set_default_dtype doc (#41263)
13dd53b3d2,skip,[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
befb22790f,improvements_frontend,Fix a number of deprecation warnings (#40179)
535e8814a4,jit,Add operators for LiteLMLSTM to Lite Interpreter (#41270)
87bf04fe12,improvements_frontend,AvgPool: Ensure all cells are valid in ceil mode (#41368)
0e7b9d4ff8,docs_frontend,Fix logit doc (#41384)
4972cf06a2,jit,[JIT] Add out-of-source-tree to_backend tests (#41145)
5f146a4125,caffe2,fix include file path in unary ops
c528faac7d,caffe2,[ROCm] Skip problematic mgpu tests on ROCm3.5 (#41409)
288ece89e1,skip,Enable TF32 support for cuBLAS (#40800)
e2c4c2f102,performance_frontend,addmm: Reduce constant time overhead (#41374)
c17670ac50,skip,"unsafe_split, unsafe_split_with_sizes, unsafe_chunk operations (#39299)"
05207b7371,releng,.circleci: Re-split postnightly into its own thing (#41354)
c68c5ea0e6,cpp,Upgrade cpp docs Sphinx/breathe/exhale to latest version (#41312)
144f04e7ef,opbench,Fix qobserver test
359cdc20e2,skip,"Revert D22432885: [pytorch][PR] unsafe_split, unsafe_split_with_sizes, unsafe_chunk operations"
a0f110190c,distributions,clamp Categorical logit from -inf to min_fifo when calculating entropy (#41002)
4ddf27ba48,opbench,[op-bench] check device attribute in user inputs
0d4a110c28,jit,[JIT] Fix dead stores in JIT (#41202)
fd0329029f,distributed,Fix flaky profiler and test_callback_simple RPC tests (#41287)
0b73ea0ea2,bc_breaking_frontend,Change BCELoss size mismatch warning into an error (#41426)
96f124e623,caffe2,remove template arguments of layernorm
f074994a31,caffe2,vectorize rounding ops (#41439)
fcd6d91045,skip,Temporary fix for determinant bug on CPU (#35136)
20f3051f7d,improvements_frontend,"[adaptive_]max_pool{1,2,3}d: handle edge case when input is filled with -inf (#40665)"
44b9306d0a,jit,Export replaceAllUsesAfterNodeWith for PythonAPI (#41414)
921d2a164f,caffe2,SparseAdagrad/RowWiseSparseAdagrad mean fusion on CPU & GPU and dedup version for RowWiseSparse mean fusion on GPU
9552ec787c,skip,Revert D22516606: [pytorch][PR] Temporary fix for determinant bug on CPU
14f19ab833,th_aten_frontend,Port index_select to ATen (CUDA) (#39946)
86a2bdc35e,skip,Adjust bound_shape_inferencer to take 4 inputs for FCs (#41452)
f153b35b9b,caffe2,Shape inference for SparseToDense in ExpertCombiner
8548a21c00,skip,Revert D22543215: Adjust bound_shape_inferencer to take 4 inputs for FCs
3a63a939d4,skip,Revert D22517785: [pytorch][PR] Enable TF32 support for cuBLAS
3971777ebb,jit,Krovatkin/reenable test tensorexpr (#41445)
dddac948a3,opbench,Add CUDA to pooling benchmark configs (#41438)
6ff306b8b5,skip,Add non-deterministic alert to CUDA operations that use `atomicAdd()` (#40056)
563b60b890,bug_fixes_frontend,Fix flaky test_stream_event_nogil due to missing event sync (#41398)
c86699d425,build_frontend,[cmake] Use PROJECT_SOURCE_DIR instead of CMAKE_* (#41387)
d5ae4a07ef,distributed,DDP Communication Hook Main Structure (#40848)
008ab27b22,quantization,[quant][pyper] Add embedding_bag weight quantize and dequantize ops (#41293)
954c260061,skip,Revert D22480638: [pytorch][PR] Add non-deterministic alert to CUDA operations that use `atomicAdd()`
225289abc6,caffe2,Adding epsilon input argument to the Logit Op
4367a73399,quantization,Cuda Support for Learnable Fake Quantize Per Tensor (GPU) (#41127)
c62550e3f4,quantization,Cuda Support for Learnable Fake Quantize Per Channel (GPU) (#41262)
8940a4e684,build_frontend,Pull upstream select_compute_arch from cmake for Ampere (#41133)
ff6e560301,rpc,Add C++ end to end test for RPC and distributed autograd. (#36893)
b48ee175e6,improvements_frontend,[reland][DNNL]:enable conv3d (#40691)
2b8db35c7e,improvements_frontend,[reland][DNNL]:enable batchnorm3d (#40995)
b7147fe6d7,quantization,Learnable Fake Quantizer Benchmark Test (#41429)
5bd71259ed,misc,remove blacklist reference (#41447)
71c3b397a6,releng,Reduce Image Size (2) (#41301)
488ee3790e,jit,Support @torch.jit.unused on a @torch.no_grad decorated function (#41496)
60f2fa6a84,docs_frontend,Updates serialization note to explain versioned symbols and dynamic versioning (#41395)
404799d43f,caffe2,Disable failed caffe2 tests for BoundShapeInference on Windows (#41472)
d90fb72b5a,docs_frontend,"remove use of the term ""blacklist"" from docs/cpp/source/Doxyfile (#41450)"
1770937c9c,bug_fixes_frontend,Restore the contiguity preprocessing of linspace (#41286)
e44f460079,jit,[jit] Fix jit not round to even if const is folded (#40897)
200c343184,new_features_frontend,"Implement gcd, lcm (#40651)"
23174ca71b,new_features_frontend,[reland] Enable TF32 support for cuBLAS (#41498)
7a33d8b001,mobile,[PyTorch Mobile] Modularize the autograd source files shared by mobile and full-jit (#41430)
1fb2a7e5a2,onnx,onnx export of fake quantize functions (#39738)
f27e395a4a,misc,[Gloo] update gloo submodule for PyTorch (#41462)
702140758f,cpp,Move GLOG_ constants into c10 namespace (#41504)
b2e52186b9,caffe2,Rename capacity to nbytes in ShareExternalPointer to avoid confusion in future (#41461)
04c0f2e3cc,releng,enable TE on windows (#41501)
e6859ec78f,caffe2,resurrect single quantization op test (#41476)
26790fb26d,caffe2,fix quantization mechanism to match nnpi (#41494)
d80e0c62be,caffe2,fix dequantization to match nnpi (#41505)
b9442bb03e,complex_frontend,Doc note for complex (#41252)
5bba973afd,skip,Reland split unsafe version (#41484)
45c5bac870,bc_breaking_frontend,[WIP] Fix cpp grad accessor API (#40887)
2b14f2d368,improvements_frontend,[reland][DNNL]:enable max_pool3d and avg_pool3d (#40996)
58244a9586,skip,Automated submodule update: FBGEMM (#40332)
b2b8af9645,improvements_frontend,Removes assertAlmostEqual (#41514)
fef30220fd,bug_fixes_frontend,Runs CUDA test_istft_of_sine on CUDA (#41523)
9d92fa2679,distributed,[NCCL] Add timeout to ProcessGroup Work Wait (#40944)
edf3dc73f2,distributed,[NCCL] Support Wait Timeout in ProcessGroupNCCL (#40946)
01dcef2e15,distributed,[NCCL] Tests for WorkNCCL::wait with Timeouts (#40947)
b979129cba,distributed,[Gloo] Support work-level timeouts in ProcessGroupGloo (#40948)
81e964904e,distributed,[Gloo] Tests for Gloo Async Work Wait-level Timeouts (#41265)
94e4248d80,releng,Split ASAN and ROCM tests into test1 and test2 (#41520)
b5e32528d0,distributed,Fix flaky test_udf_remote_message_delay_timeout_to_self (#41217)
1ac4692489,rpc,Remove unnecessary test in rpc_test.py (#41218)
6f5f455c54,rpc,[Gloo] alltoall to ProcessGroupGloo (#41424)
09647e1287,skip,RandomSampler generates samples one at a time when replacement=True (#40026)
ba6b235461,releng,[RocM] Switch to rocm-3.5.1 image (#41273)
86590f226e,skip,Revert D22519869: [pytorch][PR] RandomSampler generates samples one at a time when replacement=True
f49d97a848,docs_frontend,"Notes for lcm and gcd, formatting doc fixes (#41526)"
e324ea85ea,improvements_frontend,Add tests to logical operation in BinaryOpsKernel.cpp (#41515)
454cd3ea2e,releng,Fix RocM resource class allocation (#41553)
a0e58996fb,docs_frontend,"Makes the use of the term ""module"" consistent through the serialization note (#41563)"
9ed825746a,improvements_frontend,Use c10::cuda:: primitives rather than make CUDA runtime calls directly (#41405)
415ff0bceb,caffe2,Create lazy_dyndeps to avoid caffe2 import costs. (#41343)
b1d4e33c8b,skip,Revert D22552377: [pytorch][PR] Reland split unsafe version
728fd37d92,jit,[JIT] make fastrnns runnable on cpu (#41483)
5376785a70,releng,Run NO_AVX jobs on CPU (#41565)
eb3bf96f95,caffe2,"During inbatch broadcast, move Tile op after Fused8BitRowwiseQuantizedToFloat if applicable (#41464)"
e3e58e20cd,jit,enable jit profiling tests on macos (#41550)
cb9029df9d,jit,Assert valid inner type for OptionalType creation (#41509)
1e230a5c52,new_features_frontend,rewrite C++ __torch_function__ handling to work with TensorList operands (#41575)
71fdf748e5,new_features_frontend,Add `torch.atleast_{1d/2d/3d}` (#41317)
c7798ddf7b,skip,Initial implementation of quantile operator (#39417)
241bc648c9,rpc,Adding missing setting `state_.ptr()` and `hook_.ptr()` to `nullptr`. (#41537)
5d7046522b,jit,[JIT] Teach IRPrinter and IRParser to handle 'requires_grad' and 'device' as a part of type info. (#41507)
324c18fcad,bug_fixes_frontend,fix division by low precision scalar (#41446)
7eb71b4beb,improvements_frontend,Profiler: Do not record zero duration kernel events (#41540)
346c69a626,onnx,[ONNX] Export embedding_bag (#41234)
319b20b7db,onnx,[ONNX] Update ORT version (#41372)
43b1923d98,caffe2,Enable SLS FP32 accumulation SparseLengthsWeightedSumFused8BitRowwiseFakeFP32NNPI Op. (#41577)
f85a27e100,jit,"[JIT] Replace ""blacklist"" in test_jit.py (#41453)"
3b7c05b11b,jit,"[JIT] Replace uses of ""blacklist"" in gen_unboxing_wrappers.py (#41454)"
c9bdf474d7,jit,"[JIT] Replace use of ""blacklist"" in xnnpack_rewrite (#41455)"
758edcd7df,jit,"[JIT] Replace use of ""blacklist"" in python/init.cpp (#41456)"
bf0d0900a7,jit,"[JIT] Replace uses of ""blacklist"" in jit/_recursive.py (#41457)"
4f4e3a0f15,jit,"[JIT] Replace uses of ""whitelist"" in jit/_script.py (#41458)"
c2c2c1c106,jit,"[JIT] Remove use of ""whitelist"" in quantization/helper.cpp (#41459)"
fbd960801a,jit,"[JIT] Replace use of ""whitelist"" in lower_tuples pass (#41460)"
3c862c80cf,skip,Move list size constants for profiler::Event and profiler::ProfilerConfig into (#40474)
0f78e596ba,amd,ROCm: Fix linking of custom ops in load_inline (#41257)
a874c1e584,bug_fixes_frontend,Adds missing abs to lcm (#41552)
cf811d2fb3,bc_breaking_frontend,retain undefined tensors in backward pass (#41490)
92b95e5243,build_frontend,Fix NCCL version check when nccl.h in non-standard location. (#40982)
349c40507c,releng,"Revert ""[CircleCI] Delete docker image after testing"" (#41601)"
39b4701d31,caffe2,[caffe2][redo] Reimplement RemoveOpsByType with SSA (#41606)
8fdea489af,skip,remediation of S205607
b774ce54f8,skip,remediation of S205607
1734f24276,skip,Revert D22525217: [pytorch][PR] Initial implementation of quantile operator
445e7eb01b,quantization,Add quantized CELU operator by adding additional parameters to quantized ELU (#39199)
96ac12fdf4,jit,[PT] add overload name for int prim ops (#41578)
d72c9f4200,skip,[PT] add check for duplicated op names in JIT (#41549)
581e9526bb,caffe2,[GradualGating] support better k value change (#41557)
c6d0fdd215,new_features_frontend,torch.isreal (#41298)
e7a09b4d17,dispatcher,RecordFunction in Dispatcher (#37587)
c7bcb285f3,docs_frontend,Makes elementwise comparison docs more consistent (#41626)
46eb8d997c,skip,Revert D22533824: [PT] add check for duplicated op names in JIT
4a3aad354a,jit,[1/N] Implement Enum JIT support (#41390)
a69a262810,bug_fixes_frontend,workaround segfault in deviceGuard construction (#41621)
bd42e1a082,docs_frontend,Doc language fixes (#41643)
9600ed9af3,docs_frontend,typo fixes (#41632)
16dde6e3a0,quantization,Augmenting Observers to Support Dynamic Quantization Range (#41113)
6769b850b2,improvements_frontend,Remove needless test duplication (#41583)
ce443def01,docs_frontend,Grammar patch 1 (.md) (#41599)
26bbbeaea4,jit,[DOCS] Fix the docs for the inputs arg of trace_module func (#41586)
cc3c18edbc,caffe2,More LayerNorm Vectorization in calcMeanStd function. (#41618)
cfcee816f1,releng,.circleci: Prefix docker jobs with docker- (#41689)
6161730174,jit,[JIT] move remove mutation to its own test file (#41502)
de400fa5ac,jit,[JIT] handle specially mapped ops (#41503)
897cabc081,jit,Add operators for smart keyboard to lite interpreter (#41539)
f07816003a,caffe2,[2/n][Compute Meta] support analysis for null flag features
30551ea7b2,releng,Update NCCL from 2.4.8 to 2.7.3 (#41608)
1039bbf4eb,mobile,add named parameters to mobile module (#41376)
3a9a64a4da,caffe2,Add non zero offset test cases for Quantize and Dequantize Ops. (#41693)
5c50cb567c,quantization,Generalized Learnable Fake Quantizer Module (#41535)
65bd38127a,rpc,GLOO process group GPU alltoall (#41690)
fe415589a9,bug_fixes_frontend,disable mkl for expm1 (#41654)
c89c294ef9,new_features_frontend,Add Unflatten Module (#41564)
48569cc330,new_features_frontend,Reland split (#41567)
1f11e930d0,amd,[ROCm] skip test_streams on rocm. (#41697)
523f80e894,releng,".circleci: Remove docker_hub_index_job, wasn't used (#41800)"
2da2b5c081,docs_frontend,update CONTRIBUTING.md for ccache (#41619)
72a1146339,build_frontend,Skip warning 4522 with MSVC (#41648)
46808b49a8,quantization,Change whitelist to allow in file test_quantized_op.py (#41771)
341c4045df,misc,replaced blacklist with blocklist in test/test_type_hints.py (#41644)
03186a86d9,docs_frontend,Add test dependencies to CONTRIBUTING.md (#41799)
1ad7160a59,bug_fixes_frontend,fix backward compat (#41810)
62f4f87914,misc,Removed whitelist reference from tools/clang_format_ci.sh (#41636)
dac393fa24,mobile,[PT] enforce duplicate op name check on mobile
7ffdd765c8,jit,[TensorExpr] more convenient outer Rfactor output (#40050)
941069ca09,improvements_frontend,[tensorexpr][trivial] Remove debug printing from test (#41806)
60e2baf5e0,docs_frontend,[doc] Add LSTM non-deterministic workaround (#40893)
9e0c746b15,quantization,Augmenting Concrete Observer Constructors to Support Dynamic Quantization Range; Modifying Utility Functions in _LearnableFakeQuantize Module for Better Logging and Baseline Construction. (#41815)
302e566205,quantization,add max_and_min function and cpu kernel to speed up observers (#41570)
ce8c7185de,improvements_frontend,Add unittests to Comparison Operator Kernels in `BinaryOpsKernel.cpp` (#41809)
a0f2a5625f,quantization,[quant][graphmode][fix] Make it work with CallMethod on non-Module objects (#41576)
5c9918e757,caffe2,Fix row-wise sparse SparseLengthSum and sparse adagrad fused operator (#41818)
825a387ea2,bug_fixes_frontend,Fix bug on the backpropagation of LayerNorm when create_graph=True (#41595)
e17e55831d,mobile,[pytorch] disable per-op profiling for internal mobile build (#41825)
fced54aa67,rpc,[RPC tests] Fix test_init_(rpc|pg)_then_(rpc|pg) not shutting down RPC (#41558)
fd62847eb2,quantization,cross_layer_equalization (#41685)
9fbcfe848b,skip,Automated submodule update: FBGEMM (#41814)
5152633258,amd,[ROCm] update hip library name (#41813)
f03156f9df,misc,replace blacklist in caffe2/python/onnx/frontend.py (#41777)
2da8c8df08,quantization,[quant] Reaname from quantized... to ...quantized_cpu in the native_functions.yaml (#41071)
b87f0e5085,skip,Add NCCL Alltoall to PT NCCL process group (#39984)
ca68dc7fa2,caffe2,replace std::clamp with shim (#41855)
61511aa1d6,complex_frontend,Remove zmath_std.h (#39835)
9c7ca89ae6,skip,Conda build (#38796)
aa91a65b59,jit,[TensorExpr] Fix propagation of loop options when splitting loops (#40035)
ec683299eb,new_features_frontend,Reland Add non-deterministic alert to CUDA operations that use `atomicAdd()` (#41538)
b80ffd44b0,skip,Revert D20781624: Add NCCL Alltoall to PT NCCL process group
6ceb65f98c,docs_frontend,Document default dim for cross being None (#41850)
47c57e8804,jit,rename TestFuser to TestTEFuser (#41542)
2d15b39745,caffe2,[Onnxifi] Support running with quantized int8 inputs (#41820)
ad7133d3c1,bug_fixes_frontend,Patch for #40026 RandomSampler generates samples one at a time when replacement=True (#41682)
af5d0bff00,onnx,[ONNX] Add pass that fuses Conv and BatchNormalization (#40547)
dfa914a90c,caffe2,Modify lazy_dyndep loading to trigger inside workspace. (#41687)
dbc6a2904b,quantization,[quant][graphmode][fix] Remove assert for uses == 1 in remove dequantize pass (#41859)
4e16be9073,caffe2,[MemLeak] Fix memory leak from releasing unique ptr (#41883)
b40ef422d3,releng,.circleci: Separate out docs build from push (#41871)
586b7f991c,amd,Enable skipped tests from test_torch on ROCm (#41611)
ca3ba1095e,releng,Do not chown files inside docker for pytorch-job-tests (#41884)
2a3ab71f28,quantization,[quant][graphmode][fix] Remove useQuantizable check for dynamic quant (#41892)
0ec7ba4088,mobile,[iOS] Bump up the cocoapods version (#41895)
30ce7b3740,caffe2,Fix bug when compiling with caffe2 (#41868)
4b4273a04e,docs_frontend,Update Adam documentation (#41679)
e831299bae,typing_frontend,Fix typing error of torch/optim/lr_scheduler.pyi (#41775)
272fb3635f,onnx,Add regression test for ONNX exports of modules that embed an Embedding layer inside a Sequential (#32598)
c0e3839845,releng,fix #36801 (#41607)
266657182a,new_features_frontend,Add `torch.movedim` (#41480)
fab1795577,opbench,move benchmark utils into torch namespace (#41506)
37e7f0caf6,docs_frontend,Fix docstring in Unflatten (#41835)
17f76f9a78,improvements_frontend,Verbose param for schedulers that don't have it #38726 (#41580)
77db93228b,bug_fixes_frontend,Temporary fix for determinant bug on CPU (#35136)
1978188639,skip,"Remove two ""return""s that return ""void"" (#41811)"
01c406cc22,bc_breaking_frontend,[pytorch] bump up variable version regardless of differentiability (#41269)
3626473105,skip,NCCL Backend support for torch.bool (#41318)
b85df3709a,bug_fixes_frontend,Add __main__ entrypoint to test_futures.py (#41826)
dfe7d27d0e,mobile,implement lite parameter serializer (#41403)
da3ff5e473,jit,[JIT] dont count constants in subgraph size (#41436)
25b6e2e5ee,jit,[JIT] optimize autodiff subgraph slicing (#41437)
b898bdd4d3,jit,[JIT] Don't re run CSE on every block (#41479)
dbe6bfbd7e,skip,Revert D22496604: NCCL Backend support for torch.bool
a4b831a86a,build_frontend,Replace if(NOT ${var}) by if(NOT var) (#41924)
183b43f323,docs_frontend,Clarify Python 3.5 is the minimum supported version in the installation section. (#41937)
c5fdcd85c7,improvements_frontend,check pruned attributes before deleting (#41913)
7646f3c77f,typing_frontend,Fix type annotation for CosineAnnealingLR (#41866)
b6690eb29a,docs_frontend,Might be good for newcomers to read what N means (#41851)
a1cfcd4d22,misc,Change whitelist to another context in binary_smoketest.py (#41822)
401ac2dd39,caffe2,Replaced whitelisted with allowed (#41867)
36fb14b68b,quantization,[quant] Add Graph Mode Passes to quantize EmbeddingBag operators (#41612)
97ab33d47c,bug_fixes_frontend,Fix memory leak in XNNPACK/MaxPool2D. (#41874)
f00a37dd71,bug_fixes_frontend,Make setup.py Python-2 syntactically correct (#41960)
2da69081d7,docs_frontend,Fix one error message format of torch.dot() (#41963)
e42eab4b1c,skip,Update PULL_REQUEST_TEMPLATE.md (#41812)
6a8c9f601f,misc,Removed whitelist references from test/backward_compatibility/check_b… (#41691)
750d9dea49,improvements_frontend,move min/max tests to TestTorchDeviceType (#41908)
c0bfa45f9d,typing_frontend,Enable typechecking for `torch.futures` (#41675)
79cdd84c81,amd,Downloading different sccache binary in case of ROCm build (#41958)
1b55e2b043,new_features_frontend,add prefetch_factor for multiprocessing prefetching process (#41130)
e9e6cc8c83,quantization,Added Prehook option to prepare method (#41863)
2e95b29988,caffe2,restore at::Half support for caffe2 SumOp (#41952)
d904ea5972,skip,[NCCL] DDP communication hook: getFuture() (#41596)
890b52e09f,jit,Reduce instability in runCleanUpPasses by reordering passes. (#41891)
d4736ef95f,new_features_frontend,Add done() API to Future (#42013)
7e84913233,releng,.circleci: Make sure to install expect for docs push (#41964)
becc1b26dd,misc,updated white list/allow list (#41789)
42a0b51f71,docs_frontend,Easier english updated tech docs (#42016)
0c0864c6be,improvements_frontend,update tests to run back-compat check using new binary (#41949)
976e614915,caffe2,caffe2: add PIPELINE tag (#41482)
dede71d6e3,skip,Support aarch32 neon backend for Vec256 (#41267)
cf7e7909d5,build_frontend,NCCL must depend on librt (#41978)
45e6f2d600,skip,Enable ProcessGroupGlooTest in CI (#41985)
6287f9ed65,rpc,Remove AllGatherTestWithTimeout (#41945)
8e03c38a4f,jit,Add prim::EnumName and prim::EnumValue ops (#41965)
38580422bb,mobile,Allow specifying PYTHON executable to build_android (#41927)
366c014a77,distributed,[Resubmit #41318] NCCL backend support for torch bool (#41959)
12cd083fd7,bc_breaking_frontend,"Updates torch.tensor, torch.as_tensor, and sparse ctors to use the device of inputs tensors they're given, by default (#41984)"
c5b4f60fc2,quantization,Move qconfig removal into convert() (#41930)
b00c05c86c,improvements_frontend,update cub submodule (#42042)
47e6d4b3c8,skip,Revert D22741514: [pytorch][PR] Enable ProcessGroupGlooTest in CI
6af659629a,docs_frontend,DOC: fix two build warnings (#41334)
b7bda236d1,docs_frontend,DOC: split quantization.rst into smaller pieces (#41321)
96aaa311c0,docs_frontend,Grammar Changes (#42076)
fbdaa555a2,improvements_frontend,Enable ProcessGroupGlooTest in CI (take 2) (#42086)
11e5174926,skip,Added support for Huber Loss (#37599)
4290d0be60,caffe2,Remove settings for the logit test case. (#42114)
3e121d9688,improvements_frontend,Amend docstring and add test for Flatten module (#42084)
e62bf89273,quantization,Renaming variables from dX to dY in Learnable Fake Quantize kernels for Better Clarity (#42032)
c261a894d1,quantization,Updates to Python Module for Calculation of dX and Addition of Unit Tests (#42033)
5a6d88d503,quantization,Updates to Scale and Zero Point Gradient Calculation (#42034)
d4735ff490,performance_frontend,Avoid refcount bump in IValue::toStringRef() (#42019)
6367a9d2b0,vulkan,[vulkan] Shaders caching (#39384)
4281240cb5,improvements_frontend,Raise error for duplicate params in param group #40967 (#41597)
d6f1346c37,caffe2,Add a new op for converting the dense feature to sparse representation
e59db43313,build_frontend,Find hip properly (#42064)
5246bc4e87,bug_fixes_frontend,register parameters correctly in c++ MultiheadAttention (#42037)
ed822de0fc,misc,change 2 instances of blacklist to blocklist in tools/pyi/gen_pyi.py (#41979)
f7d50f50b9,releng,.circleci: Prefer netrc for docs push (#42136)
330a107199,mobile,Refactor lite serializer dependencies from full jit (#42127)
6ca5421a8f,performance_frontend,Enable non-synchronizing cub scan for cum* operations (#42036)
cb9c2049cd,misc,replace blacklist in aten/src/ATen/native/cudnn/Conv.cpp (#41627)
d198fb3efe,misc,changed white-allowlisted (#41796)
1df35ba61e,skip,"Back out ""Support aarch32 neon backend for Vec256"""
646042e0fb,docs_frontend,Add suggestion to enumerate ModuleDict in error message (#41946)
509c18a096,docs_frontend,Documentation for `torch.optim.swa_utils` (#41228)
d5de616a4a,releng,Enable c10d Store tests in CI (#42128)
bcd75bd683,docs_frontend,[ModelLints] Refine dropout lint message. (#42046)
c76fada4a8,distributed,Let DDP.train() return self to stay consistent with nn.Module (#42131)
f805184165,caffe2,onnxifi: make it work with AsyncIf
c062cdbd90,caffe2,Log the net if blob doesn't exist when setting output record (#41971)
83762844e5,new_features_frontend,Make `run_binary_ops_test` function generic and Add tests to add_kernel function (#42101)
0a0960126c,caffe2,"If we don't collect tracing, always free the trace data (#42118)"
4f723825b4,vulkan,[vulkan] adaptive_avg_pool2d (#41220)
5336ccc1b2,onnx,[BugFix] Fix bug in onnx::SsaRewrite (#42148)
3c6fae6567,skip,[caffe2][tpx] Use logger instead of print
6bd88f581a,skip,Revert D22790238: [caffe2][tpx] Use logger instead of print
2bc7dae2fc,amd,Use new sccache for RocM builds (#42134)
5124436af4,vmap_frontend,Fix const correctness for VmapPhysicalView struct methods (#41940)
1994ab1473,vmap_frontend,Optimize alignBatchDimsAtFront (#41941)
0571cfd875,vmap_frontend,Implement `MultiBatchVmapTransform::logicalToPhysical(TensorList)` (#41942)
e179966248,caffe2,[caffe2][tpx] log to stderr (#42162)
1a8269a566,misc,Replace blacklist with blocklist in test/run_test.py file. (#42011)
b282297559,misc,Replace whitelist with allowlist (#42067)
672ed3c06b,onnx,replace onnx producer_version when updating results (#41910)
86492410bc,bug_fixes_frontend,Don't run tests with custom arguments with pytest (#41397)
14e75fbdb9,improvements_frontend,Remove py2 specific code from test_utils.py (#42105)
3acd6b7359,docs_frontend,Document formatting (#42065)
f0c46878c6,distributed,Fix the issue GPU skip message(#41378) (#41973)
e7ed0b3fae,bug_fixes_frontend,Avoid zero division in _cubic_interpolate (#42093)
2de549518e,bug_fixes_frontend,Make fmod work with zero divisors consistently (#41948)
73ff252913,skip,"Back out ""[NCCL] DDP communication hook: getFuture()"" (#42152)"
2f61aca17b,caffe2,Skip DataIO tests relying on LevelDB if compiled without it (#42169)
5aa2b572ff,misc,replace black list with block (#42091)
b0424a895c,bug_fixes_frontend,Raise RuntimeError for zero stride pooling (#41819)
b3a9e21a29,vulkan,[vulkan] mm op through addmm (#41221)
f666be7bc1,vulkan,[vulkan] support add for dim < 4 (#41222)
48ae5945de,caffe2,Skip TestExtractPredictorNet if compiled without OpenCV (#42168)
5ed7cd0025,distributed,Allow drop_last option in DistributedSampler (#41171)
64965c4572,misc,Replaced blacklist with blocklist (#42097)
030ab2bda5,misc,Replaced whitelist reference with allowlist (#42071)
4d17ecb071,caffe2,Changed Blacklisted to Blocklisted (#42100)
fd9205e14b,caffe2,Enable caffe2 tests for RocM jobs (#41604)
8ddd2c4e1b,mobile,[pytorch] fix code analyzer for LLVM 9 & 10 (#42135)
4c7fb8c2b6,jit,make FusionCallback refer to specified GraphFuser context (#41560)
e2344db886,releng,Use Python3.7 when running OSX builds/tests (#42191)
3c084fd358,caffe2,Dequant => Swish => Quant Test case. (#41976)
deac621ae2,releng,Stop building PyTorch for VS2017 (#42144)
30eacb5fb6,quantization,[quant][graphmode] Support stack (#42187)
8a644f0c13,caffe2,[Shape Inference] Fix InferFC
b2ef7fa359,caffe2,Add a flag to enforce fp32 to fp16 conversion for all inputs of the onnxifi net. (#39931)
e4c3f526c8,distributed,Fixed Skipping Logic in ProcessGroupNCCLErrors tests (#42192)
b6a9f42758,distributed,Add appropriate error messages for ProcessGroupNCCLTest (#42143)
8deb4fe809,distributed,Fix flaky NCCL error handling tests. (#42149)
8fc5adc88e,skip,Remove dead named_tensors_unsupported_error definitions. (#42171)
4b108ca763,jit,refactor save_data as non member function (#42045)
8c653e05ff,docs_frontend,DOC: fail to build if there are warnings (#41335)
6b3f335641,skip,Enables torch.full bool and integer type inference (#41912)
1c5c289b62,new_features_frontend,[pt] Add incude_last_offset option to EmbeddingBag mean and max (#42215)
90074bbfa6,new_features_frontend,"implement numpy-like functionality isposinf, isneginf (#41588)"
460970483d,skip,Revert D22790718: [pytorch][PR] Enables torch.full bool and integer type inference
c8e15842aa,skip,Automated submodule update: FBGEMM (#42205)
b45b82b006,typing_frontend,Fix type annotation for DistributedDataParallel (#42231)
382781221d,quantization,Extending Learnable Fake Quantize module to support gradient scaling and factory (partial) construction (#41969)
48acdfd505,improvements_frontend,add tests to BinaryOpsKernel -- max/min kernel (#42198)
01b794f169,opbench,Operator-level Benchmark Test for Per Tensor and Per Channel Fake Quantization (#41974)
91546a4b0f,jit,Environment variable for controlling type verbosity in debug output (#41906)
60f51542dc,caffe2,[Caffe2] Fix spatial_bn bug for computing running_var on CPU or on CUDA without CuDNN (#42151)
029007c8b6,dispatcher,Improved coverage for unboxed->boxed kernel wrappers (#38999)
4b6e5f42a4,new_features_frontend,Creates spectral ops test suite (#42157)
79cfd85987,improvements_frontend,grad detach_ only when it has grad_fn in zero_grad call (#41283)
7cdf786a07,docs_frontend,fix typo in GradScaler docstring (#42236)
27b03d62de,caffe2,[HT] Clear the device placement tag for the auto gen sum so that we could break the component for FC sharing the same input (#42219)
6c251f74b2,misc,replace black_list/blacklist with blocklist/block_list (#42089)
c18223f9ef,jit,add Dimname support to IValue (#42054)
fe4f19e164,performance_frontend,[CUDA] max_pool2d NCHW performance improvement (#42182)
872237c1f2,distributed,Output to stderr in distributed tests. (#42139)
7459da268e,typing_frontend,Add typing annotations to torch.random (#42234)
9ea7476d9c,cpp,Add test to lerp function (#42266)
0444bac940,cpp,Add test to cross function
f30ac66e79,caffe2,[caffe2] Fix a performance bug in Dedup SparseAdagrad op (#42287)
4f163df41a,caffe2,[caffe2] Special handling of If/AsyncIf op in RemoveOpsByType (#42286)
2335430086,skip,Update TensorPipe submodule (#42225)
269ec767ca,skip,[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
547bbdac86,skip,Add MSFT Owners to the Windows Maintainership (#42280)
f15af2fe4f,skip,"Remove unused variable ""schema"" (#42245)"
86b2faeb53,skip,Automated submodule update: FBGEMM (#42302)
8e3d1908b6,docs_frontend,Fix minor typo in comment (#42184)
7cd92aaa6b,improvements_frontend,Disable validation layers in non-debug builds. (#42122)
ee2150370e,vulkan,Add Vulkan Test to ATen Mobile Tests. (#42123)
d0ed1e303f,skip,Add missing header guards. (#42272)
ce546328a3,skip,"Const-correctness, variable initialization, and error checking. (#42124)"
c35faae10d,mobile,[pytorch][ci] install nightly instead of stable libtorch for mobile CIs (#42220)
26d58503c2,new_features_frontend,Implementing NumPy-like function torch.signbit() (#41589)
c489bbe122,typing_frontend,Add typing support to torch._six (#42232)
344defc973,quantization,Let bfloat16 support promotion with other types (#41698)
5ff54ff4ff,jit,import freeze (#42319)
153673c33b,quantization,fix quantized elu benchmark (#42318)
7d6c4f62ef,caffe2,Remove 4 unused variables in lp_pool_op.cc (#42329)
b5fcd89479,cpp,Add tests to `sigmoid_backward` and `fmod` (#42289)
27c22b9b3c,skip,Modify function to takes dtype as argument
fbb052c2cc,misc,BlackList to BlockList (#42279)
1c8217a7a6,improvements_frontend,Abstract cuda calls made from `torch_python` (#42251)
31d41f987a,improvements_frontend,torch.where : Scalar Support (#40336)
e54f268a7a,new_features_frontend,Enables torch.full bool and integer type inference (#41912)
2f840b1662,new_features_frontend,Warns when TensorIterator would resize its output (#42079)
0adb584376,improvements_frontend,Make resize_ use normal device dispatch (#42240)
4c6878c97d,distributed,[gloo] change ProcessGroupGlooAsyncTest to use gtest (#42313)
832b1659e7,bug_fixes_frontend,Fix missing attribute when loading model from older version (#42242) (#42290)
352e15f1a2,skip,Revert D22812445: Update TensorPipe submodule
a9e7e787f8,jit,[jit] make clone works for interface type (#42121)
bdd9ef1981,caffe2,Support RowWiseSparseAdam on GPU (#35404)
a9eebaf693,quantization,[quant] Add saturate_to_fp16 op for FP16 quant support (#42147)
8c5bf10264,quantization,[quant] Add FP16Observer for fp16 quant support (#42221)
6bd46b583e,quantization,[quant][graph] Add support for FP16 dynamic quant (#42222)
38bf5be24f,quantization,[quant] Use PlaceholderObserver instead of Fp16Observer and NoopObserver (#42348)
a01e91e6b2,mobile,[pytorch] include all overloads for OSS custom build
4fc525e729,caffe2,[Dper3] Implementation of squeezed input to DC++
2285a2fc11,jit,refactor canonical ordering to also be able to do isAfter checks (#42140)
f502290e91,jit,[JIT] Make create autodiff subgraphs do in place updates to aliasDb (#42141)
3a19af2427,dispatcher,Make operators with optional Tensor? arguments c10-full (#41610)
1542c41a67,bc_breaking_frontend,Change C++ frontend to take optional<Tensor> arguments (#41947)
ff91b169c7,caffe2,Changes to match Fused Op: Dequantize->Swish->Quantize (#42255)
655f376460,jit,Implement Enum sugared value and Enum constant support (#42085)
f8c5800bb5,jit,[TensorExpr] Add debug dumps to kernel.cpp. (#42196)
f41bb1f92b,jit,[TensorExpr] Explicitly cast to bool results of comparison ops in kernel.cpp. (#42201)
2decccea2e,jit,[TensorExpr] Implement shape inference for TE. (#41451)
dcc4d11ffa,jit,[TensorExpr] Make tensorOrConstant non-templatized function. (#42202)
f47e00bdc3,jit,[NNC] Bounds Inference: make inferred bounds respect gaps (#42185)
44b018ddeb,distributed,Convert ProcessGroupNCCLTest.cpp to gtest unittest (#42365)
206db5c127,bc_breaking_frontend,"Improve `torch.norm` functionality, errors, and tests (#41956)"
2912390662,improvements_frontend,Limits cpu scalar error message to where it's appropriate (#42360)
115d226498,releng,Pin NumPy version on MacOS testers to 1.18.5 (#42409)
5769b06ab5,caffe2,[Caffe2] Remove explicitly divide by zero in SpatialBN training mode (#42380)
bdcf320bed,jit,Support custom exception message (#41907)
d403983695,jit,Support List[str].index (#39210) (#40348)
4cbf18ccc3,new_features_frontend,Enables integer -> float type promotion in TensorIterator (#42359)
91c80d122a,improvements_frontend,torch.gcd: Do not use std::abs() because it does not have an unsigned integer overload (#42254)
bfa94487b9,mobile,Remove register_mobile_autograd.cpp. (#42397)
ebfff31e19,caffe2,[distributedhogwild] Introducing new tags for distributed hogwild. (#42381)
192487d716,releng,Update MAGMA to 2.5.3 for Windows (#42410)
2f8d5b68fa,vmap_frontend,vmap fallback kernel (#41943)
4cdbe5c495,vmap_frontend,Implement batching rules for some view ops (#42248)
ebde590864,rpc,Remove debug vestige (#42277)
326d777e53,distributed,Convert _wait_all_workers to _all_gather (#42276)
ed44269edc,skip,Add missing space after -> for topk.values (#42321)
fa6e900e8c,improvements_frontend,Let TensorIterator::nullary_op support check_mem_overlap option (#38693)
34025eb826,performance_frontend,Vectorize arange (#38697)
0eb513beef,jit,Set a proper type for a variable (#42453)
1b9cd747cf,skip,"Revert ""Conda build (#38796)"" (#42472)"
c3236b6649,quantization,[quant] Expose register activation post process hook function to user (#42342)
f0fd1cc873,quantization,Calculate inverse of output scale first. (#41342)
dbdd28207c,caffe2,Expose a generic shape info struct for ONNXIFI Python interface (#42421)
d3acfe3ba8,skip,Fix segfault in `THPGenerator_dealloc` (#42490)
934b68f866,releng,"ecr_gc: Iterate through all tags, reduce prints (#42492)"
d707d4bf6d,mobile,Implement a light SGD optimizer (#42137)
7a5708832f,bug_fixes_frontend,fix masked_select for discontiguous outputs (#41841)
fb56299d4a,jit,Fix check highlight in filecheck. (#42417)
c000b890a8,onnx,[ONNX] Export torch.eye to ONNX::EyeLike (#41357)
04e55d69f9,skip,[ONNX] Enable scripting tests and update jit passes (#41413)
1b18adb7e8,onnx,[ONNX] Export static as_strided (#41569)
24199e0768,skip,tuple_map / tuple_concat (#42326)
c8cb5e5bcb,improvements_frontend,Relax cusparse windows guard on cuda 11 (#42412)
dc1f87c254,typing_frontend,Add typing_extensions as a dependency. (#42431)
0cb86afd72,skip,Revert D22908795: [pytorch][PR] Fix segfault in `THPGenerator_dealloc`
49e06e305f,onnx,[ONNX] Updating input node removal in ONNX function_substitution pass. (#42146)
55d2a732cd,visualization,Skip part of test_figure[_list] if Matplotlib-3.3.0 is installed (#42500)
842759591d,onnx,[ONNX] Refactor ONNX fixup for Loop and If (#40943)
ae67f4c8b8,skip,Revert D22845258: [pytorch][PR] [ONNX] Enable scripting tests and update jit passes
8850fd1952,caffe2,Add python inferface to create OfflineTensor (#42516)
d21e345ef0,bug_fixes_frontend,Fix segfault in `THPGenerator_dealloc` (take 2) (#42510)
0c48aa1e07,typing_frontend,Add typing annotations to hub.py and _jit_internal.py (#42252)
01cd613e7e,vmap_frontend,"Batching rules for: T, view, view_as, reshape, reshape_as (#42458)"
f1d7f001b9,vmap_frontend,"Batching rules for: torch.movedim, torch.narrow, Tensor.unfold (#42474)"
f3e8fff0d2,vmap_frontend,"Batching rules for: chunk, split, unbind (#42480)"
b56db305cf,docs_frontend,Improve the documentation of DistributedDataParallel (#42471)
ecb88c5d11,distributed,Add NCCL Alltoall to PT NCCL process group (#42514)
ec898b1ab5,bug_fixes_frontend,fix discontiguous inputs/outputs for cummin/cummax (#42507)
d2a2ac4eea,bug_fixes_frontend,Fix read/write bulk data (#42504)
94e8676a70,jit,Initialize uninitialized variable (#42419)
4b42a5b5a1,dispatcher,Remove redundant kernels calling TypeDefault in VariableType codegen. (#42031)
5939d8a3e0,releng,"Revert ""Revert D22360735: .circleci: Build docker images as part of C… (#40950)"
38a9984451,jit,[TensorExpr] Properly handle all dtypes in evaluation of CompareSelect exprs. (#42493)
c334ebf1aa,jit,[TensorExpr] Properly handle all dtypes in evaluation of Intrinsics exprs. (#42494)
b3ffebda7a,jit,[TensorExpr] Properly handle all dtypes of the condition in evaluation of IfThenElse exprs. (#42495)
2b8e7e2f2d,distributed,Moving ProcessGroupNCCLTest to Gtest (#42208)
3ca361791f,distributed,TearDown function for ProcessGroupNCCLTest Initializer (#42209)
e97e87368e,distributed,Clean up CUDA Sleep and Tensor Initialization in ProcessGroupNCCLTest (#42211)
0d1a689764,vulkan,[vulkan] reshape op (#41223)
91d87292a6,vulkan,[vulkan][asan] Fix Invalid Memory ops (#41224)
a0695b34cd,releng,.circleci: Have python docs always push to site (#42552)
e995c3d21e,foreach_frontend,"Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar) (#41554)"
56fc7d0345,skip,Fix doc build (#42559)
317b9d3bfc,jit,Implement sort for string in aten (#42398)
b9e68e03c4,bug_fixes_frontend,Fix the bug in THCTensor_(baddbmm) and ATen's addmm_cuda for strided views input (#42425)
c3e2ee725f,skip,Automated submodule update: FBGEMM (#42496)
ccc831ae35,skip,test: Disable test_strided_grad_layout on ROCM (#42561)
afa489dea9,onnx,[ONNX] Enable lower_tuple pass for custom layer (#41548)
29700c0092,jit,[JIT] Fix torch.jit.is_tracing() (#42486)
61027a1a59,typing_frontend,Install typing_extensions in PyTorch CI (#42551)
d45e2d3ef9,caffe2,"Reduce the output overhead of OutputColumnMaxHistogramObserver by enabling changing bin_nums, Update the observer_test.py"
78f4cff8fe,dispatcher,handle multiple returns properly in boxing wrappers (#42437)
3c7fccc1c2,bug_fixes_frontend,Reenable cusparse SpMM on cuda 10.2 (#42556)
882ad117cf,skip,[TensorExpr] Fix a way we were createing np arrays in tests. (#42575)
0f358fab6b,build_frontend,Hide cudnn symbols in libtorch_cuda.so when statically linking cudnn (#41986)
b85216887b,vulkan,[vulkan] max_pool2d (#41379)
0cf71eb547,typing_frontend,Unconditinally use typing extensions in jit_internal (#42538)
924a1dbe9b,skip,Revert D22939119: [TensorExpr] Fix a way we were createing np arrays in tests.
eb8a5fed38,skip,Automated submodule update: FBGEMM (#42584)
db52cd7322,amd,.circleci: Hardcode rocm image to previous tag (#42603)
b08347fd7b,releng,Add CUDA 11 builds for Windows CI (#42420)
dae94ed022,dispatcher,Keep manual_kernel_registration only effective in aten codegen. (#42386)
27e8dc78ca,vulkan,[vulkan] VulkanTensor lazy buffer allocation (#42569)
06d978a9ad,improvements_frontend,[c10/cuda] Reorganize device_count() and robustly surface ASAN warnings (#42249)
76905527fe,skip,Fix illegal memory acess issue for CUDA versionn of SplitByLengths operator.
6d1e43c5a6,jit,Release the GIL before invokeOperator (#42341)
5c5d7a9dca,caffe2,Freeze dynamic (re)quantizaiton ops into standard ones (#42591)
7c33225c72,typing_frontend,Add strict mypy type checking and update code_template.py (#42322)
18a32b807b,caffe2,Add API to collect output_col_minmax_histogram
7221a3d1aa,new_features_frontend,enable torch.optim.swa_utils.SWALR (#42574)
df7c059428,new_features_frontend,Throw error if `torch.set_deterministic(True)` is called with nondeterministic CuBLAS config (#41377)
24e2a8a171,skip,Revert D22780307: Fix illegal memory acess issue for CUDA versionn of SplitByLengths operator.
feeb515ad5,quantization,add Quantizer support to IValue (#42438)
b9c49f0e69,jit,[TensorExpr] Support shape inference in TE for aten::cat. (#42387)
ea9053b86d,jit,[TensorExpr] Handle constant nodes in shape inference. (#42566)
ef50694d44,jit,[TensorExpr] Apply GenericIntrinsicExpander recursively. (#42567)
73351ee91d,jit,[TensorExpr] Disallow fallback to JIT interpreter from TensorExprKernel (flip the default). (#42568)
edf6c4bc4d,rpc,[RPC tests] Merge TensorPipe tests into single entry point (#40816)
b93c7c54eb,rpc,[RPC tests] Merge tests for faulty agent into single script (#40817)
935fcc9580,rpc,[RPC tests] Merge process group tests into single entry point (#40818)
a94039fce5,rpc,[RPC tests] Avoid decorators to skip tests (#40819)
2acef69ce3,rpc,[RPC tests] Make generic fixture an abstract base class (#40820)
e7c7eaab82,rpc,[RPC tests] Move some functions to methods of fixture (#40821)
2e7b464c43,rpc,[RPC tests] Remove global TEST_CONFIG (#40822)
d7516ccfac,rpc,[RPC tests] Enroll TensorPipe in missing test suites (#40823)
4da602b004,rpc,[RPC tests] Generate test classes automatically (#42527)
2501e2b12d,rpc,[RPC tests] Run DdpUnderDistAutogradTest and DdpComparisonTest with fork too (#42528)
102abb877c,jit,"Reland D22939119: ""[TensorExpr] Fix a way we were createing np arrays in tests."" (#42608)"
5023995292,caffe2,fix output size adjustment for onnxifi_op
aa4e91a6dc,skip,Fix `TestSparse.test_bmm_windows_error` when CUDA is not available (#42626)
54ffb05eff,caffe2,better error message between C2 and glow (#41603)
50f0d2b97d,quantization,quant: add q_batchnorm_1d op (#42491)
5d7c3f92b9,jit,Issue warning instead of error when parsing Enum while enum support is not enabled (#42623)
9ea9d1b52e,caffe2,[fbs][2/n] Remove .python3 markers
509fb77b70,caffe2,Adjust bound_shape_inferencer to take 4 inputs for FCs (#41934)
9add11ffc1,bug_fixes_frontend,Fix IS_SPMM_AVAILABLE macro definition (#42643)
04d7e1679d,quantization,[quant] Quantized Average Pool Refactoring (#42009)
ddb8849ffc,typing_frontend,Fix method stub used for fixing mypy issue to work with pylint (#42356)
c14fbc36ed,docs_frontend,Update docs about CUDA stream priority (#41364)
92b7347fd7,caffe2,Enforce counter value to double type in rowwise_counter
3d46e02ea1,bc_breaking_frontend,Add __torch_function__ for methods (#37091)
1848b43c4d,jit,[NNC] Add loop unroll transformation (#42465)
eb9ae7c038,new_features_frontend,Implement `gpu_kernel_multiple_outputs` (#37969)
23607441c2,cpp,Create CuBLAS PointerModeGuard (#42639)
644d787cd8,build_frontend,find rccl properly (#42072)
ccfce9d4a9,new_features_frontend,Adds fft namespace (#41911)
a53fdaa23f,skip,Remove ProfiledType (#42570)
bd458b7d02,rpc,Don't reference TensorPipe headers in our headers (#42521)
c30bc6d4d7,skip,Update TensorPipe submodule (#42522)
0642d17efc,rpc,Enable C++ RPC tests (#42636)
576aab5084,skip,Bump up NCCL to 2.7.6 (#42645)
608f99e4ea,releng,Fix cudnn version on build_environment of Windows CI (#42615)
bcab2d6848,typing_frontend,"And type annotations for cpp_extension, utils.data, signal_handling (#42647)"
049c1b97be,releng,pin numpy version to 1.18.5 (#42670)
a5af2434fe,caffe2,NVMified NE Eval
65066d779b,opbench,Add fastrnns benchmark to CI and upload data to scribe (#42030)
a4dbc64800,jit,Add documentation for PYTORCH_JIT_TYPE_VERBOSITY (#42241)
57854e7f08,jit,[JIT] Clone runOptimizations and similar functions for profiling executor. (#42656)
e28a98a904,jit,Turn on non ASCII string literals serialization (#40719)
79de9c028a,bug_fixes_frontend,Remove VS2017 workaround for autocasting (#42352)
5ca08b8891,opbench,Add benchmark for calculate_qparams (#42138)
40b6dacb50,skip,Delete dead is_named_tensor_only (#42672)
85a00c4c92,amd,Skips spectral tests to prevent ROCm build from timing out (#42667)
1f689b6ef9,dispatcher,suppress all Autograd keys in AutoNonVariableTypeMode (#42610)
f22aa601ce,distributed,All Gather and gather APIs for Python Objects (#42189)
b44a10c179,performance_frontend,List[index]::toOptionalStringRef (#42263)
cdd7db1ffc,caffe2,Bound shape inferencer: fix int8fc scale and bias
cc596ac3a8,jit,[JIT] Add debug dumps in between passes in graph executor. (#42688)
6cb0807f88,amd,Fixes ROCm CI (#42701)
eaace3e10e,skip,Skip CUDA benchmarks on nogpu configs (#42704)
33519e19ab,bug_fixes_frontend,Fix 64-bit indexing in GridSampler (#41923)
dab9bbfce7,releng,Move jit_profiling tests into test1 on Windows (#42650)
f9a6c14364,bug_fixes_frontend,Fix sequence numbers in profiler output (#42565)
40ac95dd3c,onnx,[ONNX] Update ONNX export of torch.where to support ByteTensor as input. (#42264)
4959981cff,onnx,[ONNX] Export tensor (#41872)
9152f2f73a,quantization,Optimization of Backward Implementation for Learnable Fake Quantize Per Tensor Kernels (CPU and GPU) (#42384)
a6c8730045,onnx,[ONNX] Add preprocess pass for onnx export (#41832)
952526804c,jit,Print TE CUDA kernel (#42692)
9597af01ca,jit,Support iterating through an Enum class (#42661)
cb1ac94069,caffe2,[blob reorder] Seperate user embeddings and ad embeddings in large model loading script
73642d9425,new_features_frontend,Updates alias pattern (and torch.absolute to use it) (#42586)
fb8aa0046c,caffe2,"Add use_glow_aot, and include ONNX again as a backend for onnxifiGlow (#4787)"
4eb02add51,caffe2,Blacklist to Blocklist in onnxifi_transformer (#42590)
3c66a3795a,vulkan,[vulkan] Ops registration to TORCH_LIBRARY_IMPL (#42194)
31ed468905,build_frontend,Fix cmake warning (#42707)
c9346ad3b8,complex_frontend,[CPU] Added torch.bmm for complex tensors (#42383)
9c8021c0b1,new_features_frontend,Adds torch.linalg namespace (#42664)
dcee8933fb,build_frontend,Fix some linking rules to allow path with whitespaces (#42718)
2971bc23a6,caffe2,Handle fused scale and bias in fake fp16 layernorm
944ac133d0,jit,[NNC] Remove VarBinding and go back to Let stmts (#42634)
586399c03f,skip,Remove duplicate definitions of CppTypeToScalarType (#42640)
04c62d4a06,vulkan,"[vulkan] Fix warnings: static_cast, remove unused (#42195)"
9f88bcb5a2,bug_fixes_frontend,Minor typo fix (#42731)
eba35025e0,jit,[JIT] Exclude staticmethods from TS class compilation (#42611)
98de150381,cpp,C++ API TransformerEncoderLayer (#42633)
7332c21f7a,quantization,Speed up HistogramObserver by vectorizing critical path (#41041)
6ebc0504ca,distributed,"BAND, BOR and BXOR for NCCL (all_)reduce should throw runtime errors (#42669)"
02f58bdbd7,caffe2,[caffe2] add type annotations for caffe2.distributed.python
4eb66b814e,skip,Automated submodule update: FBGEMM (#42713)
faca3c43e6,opbench,fix celu in quantized benchmark (#42756)
95f4f67552,performance_frontend,Restrict conversion to SmallVector (#42694)
55b1706775,complex_frontend,Skips some complex tests on ROCm (#42759)
2b04712205,caffe2,Exposing Percentile Caffe2 Operator in PyTorch
48e978ba18,quantization,Add fake quantize operator that works in backward pass (#40532)
13bc542829,jit,Fix lite trainer unit test submodule registration (#42714)
3fa0581cf2,caffe2,[fbgemm] use new more general depthwise 3d conv interface (#42697)
d4a4c62df3,caffe2,[caffe2] Fix the timeout (stuck) issues of dedup SparseAdagrad C2 kernel
0a804be47d,distributed,[NCCL] DDP communication hook: getFuture() without cudaStreamAddCallback (#42335)
e95fbaaba3,caffe2,Adding Peter's Swish Op ULP analysis. (#42573)
6755e49cad,cpp,Set proper return type (#42454)
5dd230d6a2,vulkan,"[vulkan] inplace add_, relu_ (#41380)"
c889de7e25,dispatcher,update DispatchKey::toString() (#42619)
18ca999e1a,caffe2,integrate int8 swish with net transformer
b7a9bc0802,skip,Revert D22217029: Add fake quantize operator that works in backward pass
5cd0f5e8ec,caffe2,[PyFI] Update hypothesis and switch from tp2 (#41645)
d8801f590c,jit,fix asan failure for module freezing in conv bn folding (#42739)
79b8328aaf,mobile,optimize_for_mobile: bring packed params to root module (#42740)
b6810c1064,dispatcher,Include/ExcludeDispatchKeySetGuard API (#42658)
87970b70a7,new_features_frontend,Adds 'clip' alias for clamp (#42770)
162972e980,opbench,Fix op benchmark (#42757)
55ac240589,onnx,[ONNX] Fix scalar type cast for comparison ops (#37787)
05f00532f5,skip,Fix TensorPipe submodule (#42789)
bc779667d6,releng,generalize circleci docker build.sh and add centos support (#41255)
e5adf45dde,misc,Add python unittest target to `caffe2/test/TARGETS` (#42766)
77305c1e44,skip,Automated submodule update: FBGEMM (#42781)
e7b5a23607,caffe2,include missing settings import
d83cc92948,onnx,[ONNX] Add support for scalar src in torch.scatter ONNX export. (#42765)
d7aaa3327b,releng,.circleci: Only do comparisons when available (#42816)
752f433a24,distributed,DDP communication hook: skip dividing grads by world_size if hook registered. (#42400)
e06b4be5ae,skip,change pt_defs.bzl to python file (#42725)
3cf2551f2f,bug_fixes_frontend,Fix `torch.nn.functional.grid_sample` crashes if `grid` has NaNs (#42703)
8718524571,vulkan,[vulkan] cat op (concatenate) (#41434)
64a7939ee5,cpp,test_cpp_rpc: Build test_e2e_process_group.cpp only if USE_GLOO is true (#42836)
8f67c7a624,vmap_frontend,BatchedTensor fallback: extended to support ops with multiple Tensor returns (#42628)
a2559652ab,vmap_frontend,Rename some BatchedTensorImpl APIs (#42700)
a414bd69de,distributed,Skip test_c10d.ProcessGroupNCCLTest under TSAN (#42750)
c9e825640a,distributed,[c10d] Template computeLengthsAndOffsets() (#42706)
c14a7f6808,bug_fixes_frontend,adaptive_avg_pool[23]d: check output_size.size() (#42831)
103887892c,docs_frontend,"Fix ""non-negative integer"" error messages (#42734)"
a4b763bc2c,skip,add net transforms for fusion (#42763)
dedcc30c84,caffe2,Fix ROCm CI by increasing test timeout (#42827)
59b10f7929,quantization,[quant] Sorting the list of dispathes (#42758)
ddcf3ded3e,skip,Revert D23002043: add net transforms for fusion
ffc3da35f4,new_features_frontend,Don't materialize output grads (#41821)
e8f4b04d9a,vmap_frontend,vmap: temporarily disable support for random functions (#42617)
d396d135db,cpp,Added torch::cuda::manual_seed(_all) to mirror torch.cuda.manual_seed(_all) (#42638)
42b4a7132e,bug_fixes_frontend,Raise error if `at::native::embedding` is given 0-D weight (#42550)
d28639a080,quantization,Optimization with Backward Implementation of Learnable Fake Quantize Per Channel Kernel (CPU and GPU) (#42810)
916235284c,typing_frontend,[JIT] Fix typing.Final for python 3.8 (#39568)
1041bdebb0,skip,Fix a typo in EmbeddingBag.cu (#42742)
42114a0154,docs_frontend,Update the documentation for scatter to include streams parameter. (#42814)
7524699d58,code_coverage,Modify clang code coverage to CMakeList.txt (for MacOS) (#42837)
575e7497f6,fx,Introduce experimental FX library (#42741)
2c8cbd78bd,bug_fixes_frontend,Fix orgqr input size conditions (#42825)
a7bdf575cb,opbench,align qconv benchmark to conv benchmark (#42761)
57b056b5f2,opbench,align qlinear benchmark to linear benchmark (#42767)
aabdef51f9,jit,[NNC] Registerizer for GPU [1/x] (#42606)
4bafca1a69,new_features_frontend,Adds list of operator-related information for testing (#41662)
6471b5dc66,bug_fixes_frontend,Correct the type of some floating point literals in calc_digamma (#42846)
c660d2a9ae,new_features_frontend,Initial quantile operator implementation (#42755)
9c8f5cb61d,caffe2,Ensure IDEEP transpose operator works correctly
4afbf39737,skip,Add nn.functional.adaptive_avg_pool size empty tests (#42857)
71dbfc79b3,caffe2,Export BatchBucketOneHot Caffe2 Operator to PyTorch
43613b4236,jit,Fix incorrect aten::sorted.str return type (#42853)
0ff0fea42b,fx,[FX] fix lint (#42866)
3bf2978497,caffe2,remove deadline enforcement for hypothesis (#42871)
eeb43ffab9,skip,format for readability (#42851)
7a9ae52550,caffe2,[hypothesis] Deadline followup (#42842)
b0b8340065,new_features_frontend,Collect more data in collect_env (#42887)
5edd9aa95a,bug_fixes_frontend,Fix manual seed to unpack unsigned long (#42206)
a846ed5ce7,quantization,[quant] Reduce number of variants of add/mul (#42769)
e845b0ab51,onnx,[Resending] [ONNX] Add eliminate_unused_items pass (#42743)
ac93d45906,quantization,[quant] Attach qconfig to all modules (#42576)
cd756ee3d4,jit,Support boolean key in dictionary (#42833)
bee174dc3f,new_features_frontend,"Adds linalg.det alias, fixes outer alias, updates alias testing (#42802)"
38c7b9a168,dispatcher,avoid redundant isCustomClassRegistered() checks (#42852)
ab0a04dc9c,new_features_frontend,Add `torch.nansum` (#38628)
a346e90c49,caffe2,Update to NNP-I v1.0.0.5 (#4770)
ecb9e790ed,caffe2,Remove excessive logging in plan_executor (#42888)
4665f3fc8d,skip,Fix freeze_module pass for sharedtype (#42457)
77bd4d3426,performance_frontend,MAINT: speed up istft by using col2im (the original python code used … (#42826)
2f1baf6c25,skip,Fix coding style and safety issues in CuBLAS nondeterministic unit test (#42627)
2878efb35d,skip,Use `C10_API_ENUM` to fix invalid attribute warnings (#42464)
75a15d3d01,improvements_frontend,Follow-up for pytorch/pytorch#37091. (#42806)
686705c98b,performance_frontend,Optimize LayerNorm performance on CPU both forward and backward (#35750)
5157afcf59,caffe2,fix int8 FC (#42691)
5c39146c34,bug_fixes_frontend,Fix get_writable_path (#42895)
bda0007620,vmap_frontend,Improve calling backward() and grad() inside vmap error messages (#42876)
3d3752d716,skip,Revert D22898051: [pytorch][PR] Fix freeze_module pass for sharedtype
ea65a56854,build_frontend,"Use `string(APPEND FOO "" bar"")` instead of `set(FOO ""${FOO} bar"") (#42844)"
59f8692350,vulkan,[pytorch] BUCK build for Vulkan backend
ada8404f2d,jit,[jit] Scaffold a static runtime (#42753)
7f3f5020e6,skip,CUDA reduction: allow outputs to have different strides (#42649)
62bd2ddec7,new_features_frontend,Implemented non-named version of unflatten (#42563)
92885ebe16,new_features_frontend,Implement hypot (#42291)
0134deda0f,fx,[FX] Add interface to reject nodes (#42865)
86841f5f61,docs_frontend,Update cuda init docstring to improve clarity (#42923)
f373cda021,skip,Revert D22994446: [pytorch][PR] CUDA reduction: allow outputs to have different strides
1adeed2720,jit,Speed up CUDA kernel launch when block/thread extents are statically known (#42899)
33d209b5f4,jit,Fix TE microbenchmark harness to use appropriate fuser/executor (#42900)
b8ae563ce6,opbench,Add a microbenchmark for LSTM elementwise portion (#42901)
5d2e9b6ed9,typing_frontend,Add missing type annotation for Tensor.ndim (#42909)
20e0e54dbe,dispatcher,Allow Tensor& in the unboxing logic (#42712)
7a7424bf91,dispatcher,Remove impl_unboxedOnlyKernel (#42841)
8cb42fce17,skip,[quant][fix] Remove activation_post_process in qat modules (#42343)
c9dcc833bc,quantization,[quant][pyper] Make offsets an optional paramter in the qembedding_bag op (#42924)
d39cb84f1f,skip,[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
c88d3a5e76,rpc,Remove Python dependency from TensorPipe RPC agent (#42678)
8493b0d5d6,caffe2,Enroll TensorPipe agent in C++-only E2E test (#42680)
607e49cc83,skip,Revert D22856816: [quant][fix] Remove activation_post_process in qat modules
ba9025bc1a,jit,[tensorexpr] Autograd for testing (#42548)
f03f9ad621,docs_frontend,update clone doc (#42931)
6fb5ce5569,jit,[NNC] Fix some bugs in Round+Mod simplification (#42934)
ebc7ebc74e,typing_frontend,Do not ignore `torch/__init__.pyi` (#42958)
0ff51accd8,new_features_frontend,collect_env.py: Print CPU architecture after Linux OS name (#42961)
6f8446840e,quantization,[quant] Create PerRowQuantizer for floating point scale and zero_point (#42612)
816d37b1d8,quantization,[quant] Make PerChannel Observer work with float qparams (#42690)
fd5ed4b6d6,skip,Update ort-nightly version to dev202008122 (#43019)
eb47940c0a,opbench,Add executor and fuser options to the fastrnn test fixture (#42946)
6753157c5a,typing_frontend,Enable torch.utils typechecks (#42960)
8b5642a786,opbench,Fix to Learnable Fake Quantization Op Benchmarking (#43018)
3544f60f76,caffe2,make deadline=None for all numerics tests (#43014)
a6b69fdd33,rpc,Add DDP+RPC tutorial to RPC docs page. (#42828)
21823aa680,new_features_frontend,Nightly checkout tool (#42635)
89b0b3bc8c,rpc,Allow RPC to be initialized again after shutdown. (#42723)
523b2ce9c6,jit,[jit][static runtime] Simplify the graph and add operator whitelist (#43024)
85752b989d,quantization,[quant][doc] Print more info for fake quantize module (#43031)
830423b80b,cpp,Python/C++ API Parity: TransformerDecoderLayer (#42717)
8cf01c5c35,skip,"Back out ""change pt_defs.bzl to python file"""
a55b7e2a6d,quantization,[reland][quant][fix] Remove activation_post_process in qat modules (#42343) (#43015)
b992a927a9,quantization,Clearer Semantics and Naming for Customized Quantization Range Initialization in Observer (#42602)
3dc845319f,docs_frontend,Add more verbose error message about PackedSequence lengths argument (#42891)
02c8ad70f2,jit,Reconstruct scopes (#41615)
48c183af3d,jit,[TensorExpr] Wrap fuser in a class. (#42936)
fc304bec9f,jit,[TensorExpr] Remove redundant checks from canHandle in TE fuser. (#42937)
b9a105bcc0,jit,[TensorExpr] Cleanup logic in the TensorExpr fuser pass. (#42938)
e4373083a2,complex_frontend,torch.complex and torch.polar (#39617)
b8102b1550,new_features_frontend,Implement torch.nextafter (#42580)
e182ec97b3,caffe2,Fix illegal memory acess issue for CUDA versionn of SplitByLengths operator.
ccd9f3244b,jit,"Get, save, and load module information for each operator (#42133)"
ed242cbec5,build_frontend,Guard TensorPipe agent by USE_TENSORPIPE (#42682)
d60d6d0d7b,skip,Automated submodule update: FBGEMM (#42834)
c7d2774d20,bug_fixes_frontend,Fix typo in collect_env.py (#43050)
a2b86d95d1,caffe2,Make Mish support large inputs. (#43037)
31788ae151,skip,Trim trailing whitespace
2f9fd8ad29,rpc,Build test_e2e_tensorpipe only if Gloo is enabled (#43041)
ff6a2b0b7a,improvements_frontend,Add inplace option for torch.nn.Hardsigmoid and torch.nn.Hardswish layers (#42346)
c3fb152274,improvements_frontend,Test the type promotion between every two dtypes thoroughly (#42585)
1c616c5ab7,complex_frontend,Add complex tensor dtypes for the __cuda_array_interface__ spec (#42918)
75dfa5a459,typing_frontend,Remove `itruediv` because it's already defined in torch/tensor.py (#42962)
71bbd5f1d4,typing_frontend,Add back Tensor.nonzero type annotation (#43053)
059aa34b12,distributions,Clip Binomial results for different endpoints in curand_uniform (#42702)
1f6d0985d7,bug_fixes_frontend,fix searchsorted output type (#42933)
64a7684219,typing_frontend,Enable typechecking of `collect_env.py` during CI (#43062)
fcc10d75e1,jit,[JIT] Add property support to TorchScript classes (#42389)
1c6ace87d1,typing_frontend,Embed torch.nn typing annotations (#43044)
c8e789e06e,caffe2,add fake fp16 fusions to net transforms (#42927)
5014cf4a4d,caffe2,Export MergeIdLists Caffe2 Operator to PyTorch
33c5fe3c1d,caffe2,Enable test_logit FakeLowP test. (#43073)
3d8c144400,cpp,Implemented torch::nn::Unflatten in libtorch (#42613)
450315198a,skip,Fix a casting warning (#42451)
7632a9b090,quantization,[quant] Add embeddingbag_prepack function that works on quantized tensor. (#42762)
66b3382c5b,quantization,[quant] Add torchbind support for embedding_bag packed weights (#42881)
a1a6e1bc91,skip,Fix warning: dynamic initialization in unreachable code. (#43065)
4011685a8b,fx,[fx] split Node into Node/Proxy (#42991)
91b090ceaf,new_features_frontend,Add polygamma where n >= 2 (#42499)
0cf4a5bccb,code_coverage,Add GCC codecoverage flags (#43066)
bcf54f9438,releng,Stop treating ASAN as special case (#43048)
c84f78470b,typing_frontend,Fix type annotations for a number of torch.utils submodules (#42711)
06aaf8c20d,rpc,Add set_device_map to TensorPipeOptions to support GPU args (#42637)
19902f6c0e,distributed,Document unavailable reduction ops with NCCL backend (#42822)
91f3114fc1,jit,[JIT] Represent profiled types as a node attribute (#43035)
8864148823,jit,[jit] DeepAndWide benchmark (#43096)
5bcf9b017a,new_features_frontend,"Implement hstack, vstack, dstack (#42799)"
d4c5f561ec,docs_frontend,Updates torch.clone documentation to be consistent with other functions (#43098)
4ae832e106,performance_frontend,Optimize SiLU (Swish) op in PyTorch (#42976)
e2eb0cb1a9,new_features_frontend,Adds arccosh alias for acosh and adds an alias consistency test (#43107)
248b6a30f4,mobile,add training mode to mobile::Module (#42880)
269fdb5bb2,cpp,prepare to split transformer header file (#43069)
472f291375,jit,Fix freeze_module pass for sharedtype (#42457)
aab66602c4,complex_frontend,Add torch.dot for complex tensors (#42745)
034e6727e7,caffe2,Set default ATen threading backend to native if USE_OPENMP is false (#43067)
7cb8d68ae1,dispatcher,Rename XLAPreAutograd to AutogradXLA. (#43047)
9c3f579528,releng,.circleci: Copy LLVM from pre-built image (#43038)
768c2a8c25,vmap_frontend,vmap: fixed to work with functools.partial (#43028)
37252e8f00,vmap_frontend,Implement batching rules for some unary ops (#43059)
6db0b8785d,new_features_frontend,"Adds movedim method, fixes movedim docs, fixes view doc links (#43122)"
864f0cfb2d,typing_frontend,"Fix type annotations for torch.sparse, enable in CI (#43108)"
825ec18eed,jit,[jit] better error message (#43093)
133e9f96e1,distributed,Use c10 threadpool for GPU to CPU distributed autograd continuations. (#42511)
1f6e6a1166,quantization,Remove unused variable vecVecStartIdx (#42257)
5aa61afbfb,quantization,quant bench: update observer configs (#42956)
a5dfba0a6e,quantization,observers: make eps a buffer (#43149)
3264ba065c,quantization,observers: use clamp instead of min/max in calculate_qparams (#43150)
57af1ec145,quantization,observers: use torch.all to check for valid min and max values (#43151)
cd96dfd44b,skip,Delete accidentally committed file errors.txt. (#43164)
3c5e3966f4,onnx,[ONNX] Squeeze operator should give an error when trying to apply to a dimension with shape > 1 (#38476)
aef2890a75,performance_frontend,Improve zero sized input for addmv (#41824)
e8db0425b5,skip,remove dot from TH (#43148)
c44b1de54e,releng,Pin VC++ version to 14.26 (#43184)
b3bda94393,jit,[NVFuser] Enable E2E BCast-PWise-Reduction fusions (#43129)
b92b556a12,caffe2,Add shape inference to SparseLengthsSumSparse ops (#43181)
ee74c2e5be,build_frontend,Compress fatbin to fit into 32bit indexing (#43074)
53bbf5a48b,docs_frontend,Update README.md (#43100)
5d608d45cf,cpp,Added Encoder Layer constructor with default parameters (#43130)
e39b43fd76,docs_frontend,Issue 43057 (#43063)
0a9c35aba3,dispatcher,maybe minor fix to dispatch/backend_fallback_test.cpp? (#42990)
fbf274f5a7,new_features_frontend,Autocast support for cudnn RNNs (#42385)
0744dd6166,docs_frontend,Fix shapes in the MarginRankingLoss docs (#43131)
493b3c2c7c,skip,Replace all AT_ASSERTM under ATen CPU kernels. (#41876)
dfdd797723,skip,Replace all AT_ASSERTM under ATen CUDA kernels. (#42989)
888ae1b3d8,new_features_frontend,Introducing Matrix exponential (#40161)
2e6e295ecc,jit,refactor _save_parameters to _save_data (#43162)
dd194c1612,jit,add _save_parameters to serialize map (#43163)
3951457ca5,fx,[FX] Add in resnet + quantization tests (#43157)
da5df7e2d2,misc,"Remove use of term ""blacklist"" from tools/autograd/gen_python_functions.py (#42047)"
6c99d5611d,jit,[tensorexpr] Fix promotion of booleans (#43097)
d5bc2a8058,complex_frontend,Remove std::complex from c10::Half (#39833)
d06f1818ad,jit,Fix `codegen/cuda` gcc-5.4 compilation issues (#43223)
e41ca2d9fa,bug_fixes_frontend,In copy_weights_to_flat_buf_views() explicitly construct tuple (#43244)
7c923a1025,releng,Optimize linux CI build/test matrix (#43240)
8094228f26,releng,update path in CI script to access ninja (#43236)
27ec91b0c9,releng,remove thunk fix now that ROCm CI images are >= ROCm 3.5 (#43226)
ab366d0f5f,dispatcher,Fix some mistakes in native_functions.yaml (#43156)
fa6b34b54c,quantization,2 Bit Embedding Conversion Operator support. (#43077)
06d43dc69a,caffe2,default ice-ref to c-step (#4812)
bc0e1e8ed2,releng,Add dataclasses to base Docker images. (#43217)
1e248caba8,releng,[CircleCI] Use `canary` images until VC++ 14.27 issue is resolved (#43220)
7d10298067,vmap_frontend,Implement Tensor.to batching rule (#43206)
d467ac8ff0,distributed,[GLOO] handle empty split size (#43256)
eb7fc2e98f,releng,.circleci: Simplify binary upload process (#43159)
018b4d7abb,skip,Automated submodule update: FBGEMM (#43251)
410d5b95b2,jit,[jit] fix str -> Device implicit conversions (#43213)
aad1ff9f18,quantization,[quant][cleanup]test_qlinear_legacy should be under TestDynamicQuantizedLinear. (#40084)
0617156f0e,vulkan,[vulkan] fix invalid memory op and tests (#43312)
97d62bcd19,releng,Modify Circle CI script to upload test report for analysis. (#43180)
6e1127ea3f,distributed,[NCCL] Changed FutureNCCL's then callback logic for better efficiency. (#42869)
3eb31325fc,improvements_frontend,refactor torch/cuda/nccl.h to remove direct dependency on NCCL in libtorch_python (#42687)
4e964f3b97,releng,Make Windows CUDA-11 tests master only (#43234)
60b524f271,docs_frontend,Update torch.Tensor.is_set_to documentation (#43052)
6a09df99e1,quantization,Fix ASAN error in QNNPACK's integration of qlinear_dynamic. (#41967)
a2ae2d3203,new_features_frontend,Nightly Pull (#43294)
f9a766bb39,caffe2,Increase deadline time for load_save tests (#43205)
397325a109,bug_fixes_frontend,Make _compute_linear_combination.out a true out function (#43272)
66a79bf114,releng,.circleci: Don't quote glob for conda upload (#43297)
ca9d4401d4,releng,.circleci: Remove manual docker installation (#43277)
c8bc298d6c,bc_breaking_frontend,streamline stride propagation logic in TensorIterator (#42922)
4fc9e958c4,quantization,[quant] Add benchmakrs for embedding_bag coversion ops (#43291)
0dc41ff465,mobile,[pytorch] add flag for autograd ops to mobile builds (#43154)
c66ca7a48d,vmap_frontend,vmap: Fix bug with x * 0.1 (#43218)
d0a6819b0e,amd,[ROCm] skip test_rpc in .jenkins/pytorch/test.sh (#43305)
5006d24302,rpc,Make TensorPipe the default backend for RPC (#43246)
a12fe1a242,rpc,Minor RPC doc fixes (#43337)
c89d2c6bf2,misc,Replace black_list with block_list (#42088)
dae2973fae,quantization,[quant][graphmode][fx] Add graph mode quantization on fx (#43175)
51bab0877d,bug_fixes_frontend,Fix torch.hub for new zipfile format. (#42333)
97d594b9f7,skip,Make grad point to bucket buffer in DDP to save memory usage (#41954)
2b7108a96f,releng,Update hardcoded pytorch_android_gradle_custom_build_single hash (#43340)
b0ec336477,quantization,[quant][graphmode][fx][test] Add per op test for graph mode quant on fx (#43229)
e10aa47615,complex_frontend,Fix `at::native::view_as_real()` for ComplexHalf Tensors (#43279)
9a1f2b3617,releng,.circleci: Use dynamic docker image for android (#43356)
5cf8592663,skip,Fix backward compatibility test (#43371)
ad8294d35b,vulkan,[vulkan][ci] Vulkan tests running on linux build via swiftshader (added to docker) (#42614)
e32d014f46,cpp,remove empty override pretty_print (#43341)
7c50c2f79e,mobile,Reimplement per-operator selective build (#39401)
9984d33542,quantization,[quant][graphmode][fx] Add support for conv module (#43285)
844d469ae7,mobile,Remove proprietary notices
217ddea93a,quantization,[quant] Make OP_LIST_TO_FUSER_METHOD public (#43286)
e8139624f2,vulkan,Search on system path for Vulkan headers and libraries as a last resort. (#43301)
665da61d2b,jit,Replace Conv1d with Conv2d (#42867)
17f9edda42,quantization,Bias Correction Implementation (#41845)
e31cd46278,new_features_frontend,Add alias torch.fix for torch.trunc to be compatible with NumPy. (#43326)
c64594f5cc,improvements_frontend,"Extends test_unary_ufunc.py with numerics, contiguity, domain tests (#42965)"
da70976e66,onnx,[ONNX] Add support for operator `add` between tensor list (#41888)
da036250cd,opbench,Add benchmark for performance comparison (#43221)
0cb52cb458,improvements_frontend,Autograd better error (#43308)
c2511bdfa4,skip,Implement first draft of autograd benchmark. (#40586)
7b520297dc,skip,Remove erroneous trailing backslashes (#43318)
f4b6ef9c56,skip,"Do not define the macro ""isnan"" (#43242)"
0bd35de30e,jit,Add Enum convert back to Python object support (#43121)
478fb925e6,jit,[jit] PyTorchStreamReader::getAllRecord should omit archive name prefix (#43317)
3aec1185e0,quantization,"Enables bfloat16 x [float16, complex64, complex128] type promotion (#43324)"
e57b89c8dc,new_features_frontend,"Adds arccos, arcsin, arctan aliases (#43319)"
743cff4a1a,caffe2,Fix PackedGemmMatrixFP16 repacking (#43320)
f20a04fa2d,jit,[TensorExpr] Simplify conditional select (#43350)
4db8ca1129,quantization,[quant] Create nn.quantized.dynamic.EmbeddingBag (#43088)
b354b422ee,quantization,[quant] Make offsets an optional argument (#43090)
3293fdfa80,quantization,[quant] Enable from_float for quantized Embedding_Bag (#43176)
650590da0d,fx,[quant][graphmode][fx] Add support for conv module + relu (#43287)
74781ab5b8,skip,Revert D23242101: [pytorch][PR] Implement first draft of autograd benchmark.
100649d6a9,jit,Normalize loops with non-zero start. (#43179)
6e48c88e09,releng,.circleci: Prefer using env-file for docker run (#43293)
452a473729,fx,[quant][graphmode][fx] Add support for add (#43331)
26be4dcfa1,fx,[quant][graphmode][fx] Add support for add relu (#43332)
3d76f7065e,fx,[quant][graphmode][fx] Add support for cat (#43333)
054073c60d,skip,Refactor Vulkan context into its own files. Use RAII. (#42273)
9e87a8ddf4,fx,[quant][graphmode][fx] Add support for batchnorm (#43334)
109ea59afc,fx,[quant][graphmode][fx] Add support for batchnorm relu (#43335)
ff454cc429,fx,[quant][grapphmode][fx][test][refactor] Refactor quantized add test (#43372)
8eb3de76ba,jit,Fix enum constant printing and add FileCheck to all Enum tests (#42874)
6c772515ed,skip,Revert D23252335: Refactor Vulkan context into its own files. Use RAII.
e96871ea46,fx,[quant][graphmode][fx] Add support for mul and mul relu (#43373)
93f1b5c8da,mobile,Mobile backward compatibility (#42413)
5a02c6b158,fx,[quant][graphmode][fx] Add support for hardswish (#43374)
089bb1a8e4,fx,[quant][graphmode][fx] Add support for elu (#43375)
aec917a408,fx,[quant][graphmode][fx] Add support for layer_norm (#43376)
aa53b2d427,caffe2,Workaround bugs in user side embedding meta info and better msgs (#43355)
f269fb83c1,skip,Add Enum TorchScript serialization and deserialization support (#42963)
a5a6a3e633,new_features_frontend,add support for optional int list with scalar fill (#43262)
490d41aaa6,fx,[quant][graphmode][fx] Add support for instance_norm (#43377)
abe878ce96,jit,Allow Freezing of Module containing interface attribute (#41860)
2a08566b8f,performance_frontend,Simple caching allocator for CPU. (#42006)
04aa42a073,quantization,Refactor qconv to reduce allocations. (#42007)
fb12992b5d,quantization,Call qnnpack's conv setup only if input pointer has changed. (#42008)
5e04bb2c1c,caffe2,caffe2: expose CPUContext RandSeed for backwards compatibility with external RNG (#43239)
98307a2821,quantization,Fix bfloat16 erfinv get incorrect value problem for cpu path (#43399)
40c77f926c,jit,Add prim::TypeCheck operation (#43026)
192c4b0050,fx,[quant][graphmode][fx] Add support for clamp (#43437)
88b564ce39,fx,[quant][graphmode][fx] Add support for general shape ops (#43438)
915fd1c8fc,dispatcher,centralize autograd dispatch key set (#43387)
d94b10a832,skip,Revert D23223281: Add Enum TorchScript serialization and deserialization support
47e1b7a8f1,bug_fixes_frontend,Set CONSTEXPR_EXCEPT_WIN_CUDA as const while it is not constexpr (#43380)
ec9e6e07bc,fx,[quant][graphmode][fx] Add support for general value ops (#43439)
8efa898349,onnx,[ONNX] Export split_to_sequence as slice when output number is static (#42744)
b1d31428e7,jit,Reduce number of `prim::profile` (#43147)
b52e6d00f9,quantization,Change quantizer to account for input tensor's memory format. (#42178)
b003f2cc28,new_features_frontend,Enable input pointer caching in XNNPACK integration. (#42840)
e4af45f3aa,bug_fixes_frontend,Fix bugs in vec256_float_neon.h (#43321)
35351ff409,skip,Fix ToC Link (#43427)
4dc8f3be8c,new_features_frontend,Creates test_tensor_creation_ops.py test suite (#43104)
d70b263e3a,caffe2,[DPER3] Separate user embeddings and ad embeddings in blob reorder
a97ca93c0e,jit,remove prim::profile and special-casing (#43160)
b349f58c21,fx,[fx] enabling typechecking of fx files (#43082)
1f0cfbaaad,fx,[fx] add type annotations (#43083)
c4e841654d,new_features_frontend,Add alias torch.negative to torch.neg. (#43400)
2f9c9796f1,skip,[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
db78c07ced,typing_frontend,Enable torch.cuda.nvtx typechecking (#43443)
c972e6232a,vmap_frontend,Implement batching rules for basic arithmetic ops (#43362)
7cc1efec13,mobile,Add lite SequentialSampler to torch mobile (#43299)
7024ce8a2c,quantization,[quant] Add benchmarks for quantized embeddingbag module (#43296)
0fa99d50bc,typing_frontend,Enable torch.cuda.memory typechecking (#43444)
35a36c1280,jit,Implement JIT Enum type serialization and deserialization (#43460)
4cfac34075,amd,[ROCm] allow .jenkins/pytorch/test.sh to run on centos (#42197)
e08e93f946,new_features_frontend,Reland of benchmark code (#43428)
ed8b08a3ba,quantization,Update quantize_jit to handle new upsample overloads (#43407)
e37f871e87,skip,[2/3][lite interpreter] add metadata when saving and loading models for mobile
284ff04792,quantization,[quant] Support set API for EmbeddingBag quantization (#43433)
87905b5856,mobile,[pytorch] add option to include autograd for code analyzer (#43155)
7b243a4d46,fx,[quant][graphmode[fx][test][refactor] Refactor tests for graph mode quantization on fx (#43445)
f80b695a75,caffe2,Properly format db.h and db.cc (#43027)
675f3f0482,releng,"Fix ""save binary size"" steps (#43529)"
d1d32003bb,caffe2,force pytorch tensors to contiguous before calling c2 ops
cbdaa20c88,caffe2,[serialize] Expose zip file alignment calculation functions (#43531)
f02753fabb,new_features_frontend,Support AMP in nn.parallel (#43102)
b430347a60,typing_frontend,Address JIT/Mypy issue with torch._VF (#43454)
6a2d7a05c4,skip,fix typo in test_dataloader test_multiprocessing_contexts (#43343)
8ecfa9d9a2,build_frontend,[cmake] End support for python3.5 for pytorch (#43105)
1089ff404c,quantization,Refactored the duplicate code into a function in _ConvNd (#43525)
76894062dc,build_frontend,move wholearchive to link option (#43485)
f8e9e7ad4a,performance_frontend,Allocating warp to an input index in compute_cuda_kernel (#43354)
f32ca57c5e,docs_frontend,Fix typo in LSTMCell document (#43395)
3dcfe84861,jit,Grammatical corrections (#43473)
ebc0fc4dfc,docs_frontend,Polish the nightly.py docs in CONTRIBUTING a little (#43494)
9420c773d0,skip,Revert D23299452: [pytorch][PR] fix typo in test_dataloader test_multiprocessing_contexts
62dcd253e3,skip,[quant][graphmode][fx] Testing torchvision (#43526)
3df398a3a8,docs_frontend,Update the QR documentation to include a warning about when the QR.backward is well-defined. (#43547)
9b05fbd92e,docs_frontend,Correct the windows docs (#43479)
5ca6cbbd93,distributed,Remove unnecessary copies in ProcessGroupGloo for multiple inputs allreduce (#43543)
05f27b18fb,skip,"Back out D23047144 ""[2/3][lite interpreter] add metadata when saving and loading models for mobile"""
be637fd5f6,skip,Revert D23306683: [quant][graphmode][fx] Testing torchvision
348e78b086,bc_breaking_frontend,Evenly distribute output grad into all matching inputs for min/max/median (#43519)
c9f125bf70,onnx,Black to Block for various files (#42913)
b3f8834033,vmap_frontend,"Batching rule for torch.pow, torch.result_type (#43515)"
58666982fb,caffe2,check in intel nnpi 1007 into fbcode/tp2
f35e069622,skip,"Back out ""Make grad point to bucket buffer in DDP to save memory usage"" (#43557)"
f6b7c6da19,jit,[TensorExpr] Fuser: move canHandle and some other auxiliary functions into TensorExprFuser class. (#43170)
8dc4b415eb,jit,[TensorExpr] Fuser: only require input shapes to be known (output shapes can be inferred). (#43171)
d18566c617,jit,[TensorExpr] Fuser: disallow aten::slice nodes. (#43365)
b763666f9f,jit,[JIT] Subgraph utils: add an optional vmap argument to the API to allow retrieving value mappings. (#43235)
3ec24f02af,jit,[TensorExpr] Start using typecheck in the fuser. (#43173)
c1553ff94b,opbench,Benchmarks: temporarily disable profiling-te configuration. (#43603)
2b70f82737,docs_frontend,fix typo in test_dataloader test_multiprocessing_contexts (take 2) (#43588)
573940f8d7,typing_frontend,Fix type annotation errors in torch.functional (#43446)
7beeef2c69,releng,.jenkins: Remove openssh installs (#43597)
42f6c3b1f4,bug_fixes_frontend,Raise error on device mismatch in addmm (#43505)
51861cc9b1,releng,.circleci: Add CUDA 11 to nightly binary builds (#43366)
25dcc28cd6,jit,[jit][static] Replace deepcopy with copy (#43182)
2a4d312027,distributed,Allow GPU skip decorators to report the right number of GPUs required in (#43468)
a91e1cedc5,caffe2,Reduce number of hypothesis tests in CI (#43591)
6459f0a077,releng,added rocm 3.7 docker image (#43576)
db1fbc5729,jit,[OACR][NLU] Add aten::str operator (#43573)
306eb3def7,distributed,Additional error checking for `torch.cuda.nccl` APIs. (#43247)
0521c71241,jit,[D23047144 Duplicate][2/3][lite interpreter] add metadata when saving and loading models for mobile (#43584)
769b9381fc,distributed,DDP Communication hook: Fix the way we pass future result to buckets. (#43307)
5a15f56668,caffe2,match batchmatmul on 1.0.0.6 (#43559)
6c28df7ceb,fx,[fx] add test for args/kwargs handling (#43640)
cf26050e29,skip,[pytorch] Move TensorIteratorConfig method implementation to cpp file (#43554)
88e35fb8bd,bug_fixes_frontend,Skip SVD tests when no lapack (#43566)
1bda5e480c,code_coverage,Add Python code coverage (#43600)
9ca338a9d4,onnx,[ONNX] Modified slice node in inplace ops pass (#43275)
033b7ae3ef,new_features_frontend,"implement NumPy-like functionality maximum, minimum (#42579)"
0bf27d64f4,jit,Fix NaN propagation in fuser's min/max implementation (#43590)
f73e32cd04,caffe2,Reduce amount of work done within a global lock within ParallelLoadOp (#43508)
00c1501bc0,jit,[JIT] Cast return values of functions returning Any (#42259)
c4e5ab6ff2,skip,[TensorExpr] Disable a flaky test. (#43678)
28be3ef2f2,amd,Fix hipify script for pytorch extensions (#43528)
01b5c06254,bug_fixes_frontend,[fix] handle empty args in chain_matmul (#43553)
79e6aaeb4c,dispatcher,pull empty() out of use_c10_dispatcher: full (#43572)
a070c619b9,fx,[FX] Native callables in FX lowering (#43426)
f63d06a57b,docs_frontend,"Fix docs for kwargs, a-e (#43583)"
48e08f884e,cpp,C++ APIs TransformerEncoder (#43187)
de84db2a9d,jit,[TensorExpr] Add aten::sum lowering to the kernel (#43585)
c25d0015f0,improvements_frontend,Autograd code clean up (#43167)
288a2effa0,jit,Operator generator based on templated selective build. (#43456)
73dcfc5e78,dispatcher,Update RNN op registration format (#43599)
5a1aa0e21e,fx,[reland][quant][graphmode][fx] Add e2e test on torchvision (#43587)
3830998ac3,fx,"[fx] When generating names, avoid shadowing builtins (#43653)"
bff741a849,mobile,Improve save_for_mobile cxx binary (#43721)
04ccd3ed77,build_frontend,Fix bazel dependencies (#43688)
cdc3e232e9,jit,Add `__str__` and `__repr__` bindings to SourceRange (#43601)
5da97a38d1,quantization,Check if input is ChannelsLast or ChannelsLast3d for quantized AdaptivePool3d. (#42780)
d1c4d75c14,jit,Add API for unexecuted op (#43629)
e189ef5577,jit,Refactor pass to class (#43630)
a4cf4c2437,jit,refactor tests (#43631)
a19fd3a388,jit,Add undefined specializations in backward (#43632)
01f974eb1e,jit,Specialize optionals for grad_sum_to_size (#43633)
9a2d4d550e,mobile,update build flags for benchmark binaries
3afd24d62c,mobile,[pytorch] check in default generated op dependency graph (#43570)
3a0e35c9f2,dispatcher,[pytorch] deprecate static dispatch (#43564)
8032dbc117,new_features_frontend,Add Rowwise Prune PyTorch op (#42708)
87d7c362b1,jit,[JIT] Add JIT support for torch.no_grad (#41371)
1a21c92364,onnx,[ONNX] Update in scatter ONNX export when scalar src has different type (#43440)
654ab209c6,skip,[JIT] Disable broken tests (#43750)
f06d3904f2,skip,Land `code_coverage_tool` to `caffe2/tools` folder
26161e8ab6,skip,[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
c7787f7fbf,bc_breaking_frontend,[numpy compatibility]Fix `argmin/argmax` when multiple max/min values (#42004)
b29375840a,skip,Revert D23379383: Land `code_coverage_tool` to `caffe2/tools` folder
8e507ad00e,improvements_frontend,Update the div formula for numerical stability (#43627)
0ab83f7f9f,vmap_frontend,Fixed undefined behavior in BatchedFallback (#43705)
bdee8e02c0,improvements_frontend,TensorIterator: Check memory overlap in all `unary_op`s (#43418)
065ebdb92f,improvements_frontend,TensorIterator: Check for memory overlap in all `binary_op`s (#43419)
dc0722e9b7,improvements_frontend,TensorIterator: Check for memory overlap in all `compare_op`s (#43420)
c177d25edf,improvements_frontend,TensorIterator: Check for memory overlap in all `nullary_op`s (#43421)
9063bcee04,bug_fixes_frontend,Don't proceed into setup.py too far if Python version is unsupported (#42870)
58a7e73a95,jit,[TensorExpr] Block Codegen (#40054)
b630c1870d,new_features_frontend,Add stateful XNNPack deconvolution2d operator to torch. (#43233)
a76184fe1e,docs_frontend,grammatical error fix (#43697)
1f7434d1ea,quantization,Fix 'module' to 'model' in quantize_dynamic doc (#43693)
b72da0cf28,bug_fixes_frontend,OneDNN: report error for dilation max_pooling and replace AT_ERROR with TORCH_CHECK in oneDNN codes (#43538)
be3ec6ab3e,bug_fixes_frontend,[caffe2][torch] correctly re-raise Manifold StorageException
dc5d365514,bug_fixes_frontend,Fix bug in caching allocator. (#43719)
f4695203c2,cpp,Fixes fft function calls for C++ API (#43749)
bcec8cc3f9,skip,Add amax/amin (#43092)
776c2d495f,jit,[JIT] IRParser: store list attributes as generic ivalue lists. (#43785)
b23e9cdd64,releng,.circleci: Add slash to end of s3 cp (#43792)
89e2a3591e,code_coverage,Add 1% threshold to codecov (#43783)
0564d7a652,code_coverage,Land code coverage tool for OSS (#43778)
20abfc21e4,new_features_frontend,"Adds arctanh, arcsinh aliases, simplifies arc* alias dispatch (#43762)"
4cb8d306e6,foreach_frontend,"Add _foreach_add_(TensorList tensors, Scalar scalar) API (#42531)"
af4ecb3c11,quantization,"quantized conv: add support for graph mode BC testing, and increase coverage (#43524)"
3f5ea2367e,quantization,Adding a version serialization type to ConvPackedParam (#43086)
7d517cf96f,distributed,[NCCL] Dedicated stream to run all FutureNCCL callbacks. (#43447)
3f0120edb4,skip,Revert D23360705: [pytorch][PR] Add amax/amin
633d239409,fx,[torch.fx] Pass placeholders through delegate too (#43432)
a1eae6d158,skip,Implementing NumPy-like function torch.heaviside() (#42523)
cd0bab8d8d,onnx,[ONNX] Where op (#41544)
68b9daa9bf,new_features_frontend,Add `torch.linalg.norm` (#42749)
1a79d7bb28,distributed,DDP communication hook examples (#43310)
47e489b135,jit,Make ExtraFilesMap return bytes instead of str (#43241)
64906497cd,skip,Revert D23391941: [pytorch][PR] Implementing NumPy-like function torch.heaviside()
13c7c6227e,cpp,Python/C++ API Parity: TransformerDecoder (#42886)
eae92b7187,docs_frontend,Updated README.md by correcting grammatical errors (#43779)
8997a4b56b,typing_frontend,[typing] Enable typing in torch.quantization.fuse_modules typechecks … (#43786)
58148c85f4,jit,Use template OperatorGenerator for prim and special operator registration (#43481)
6aaae3b08b,onnx,[ONNX] Addition of diagnostic tool API (#43020)
8538a79bfe,jit,[jit][static] Basic executor (#43647)
000739c31a,dispatcher,Function calls for fallback paths (#43274)
931b8b4ac8,skip,Use ivalue::Future in autograd engine and DistEngine. (#43676)
d10056652b,new_features_frontend,Enable `torch.half` for `lt` and `masked_select` (#43704)
8a41fa4718,mobile,[Selective Build] Move register_prim_ops and register_special_ops to app level (#43539)
60ad7e9c04,jit,[TensorExpr] Make sum available from Python (#43730)
550fb2fd52,bug_fixes_frontend,Expand the coverage of test_blas_empty (#43822)
f31b111a35,skip,Add the sls tensor train op (#33525)
45ba836876,vulkan,"Revert ""Revert D23252335: Refactor Vulkan context into its own files. Use RAII."" (#43628)"
cc52386096,skip,Revert D19987020: [pytorch][PR] Add the sls tensor train op
7f967c08b8,docs_frontend,Document the beta=0 behavior of BLAS functions (#43823)
4e39c310eb,skip,Move torch/csrc/utils/hash.h to c10/util/hash.h. (#42503)
6373063a98,vulkan,Generic Vulkan object cache. (#42394)
287fb273cd,vulkan,Vulkan (source and binary) shader and shader layout cache. (#42325)
387dc24c92,vulkan,Vulkan memory allocator. (#42786)
15aaeb8867,vulkan,Vulkan pipeline and pipeline layout cache. (#42395)
87e8f50aae,vulkan,Vulkan descriptor and descriptor layout cache. (#42642)
d1df098956,vulkan,Vulkan resource cache. (#42709)
628db9699f,vulkan,Vulkan command buffer and pool. (#42930)
ab3ea95e90,bug_fixes_frontend,#include <string> in loopnest.h (#43835)
1830e4f08c,skip,Remove unnamed namespace in headers (#43689)
5021ec826b,docs_frontend,"Fix docs for kwargs, f-p (#43586)"
7b835eb887,releng,Update CUDA11 docker container (#42200)
3dc9645430,releng,Disable RocM CircleCI jobs (#42630)
3aeb70db0b,docs_frontend,"Documents sub properly, adds subtract alias (#43850)"
08126c9153,onnx,[ONNX] Utilize ONNX shape inference for ONNX exporter (#40628)
8fb7c50250,amd,Enable complex blas for ROCm. (#43744)
a860be898e,new_features_frontend,[resubmit] Add amax/amin (#43819)
1dcc4fb6b7,improvements_frontend,Kill unused _pointwise_loss function. (#43523)
1cdb9d2ab5,vmap_frontend,Test runner for batched gradient computation with vmap (#43664)
576880febf,improvements_frontend,Print all traceback for nested backwards in detect_anomaly (#43626)
6ea89166bd,new_features_frontend,Rewrite of ATen code generator (#42629)
b8d34547ee,fx,[quant][graphmode][fx][fix] enable per channel quantization for functional ops (#43534)
42c895de4d,bc_breaking_frontend,"Properly check that reduction strings are valid for l1_loss, smoothl1_loss, and mse_loss. (#43527)"
eb4199b0a7,skip,[quant][graphmode][fx] Add top level APIs (#43581)
ef08f92076,skip,[quant][graphmode][fx] Add support for weight prepack folding (#43728)
63dbef3038,caffe2,Better msg (#43848)
1390cad2d8,jit,[NNC] Hook up registerizer to Cuda codegen [2/x] (#42878)
89452a67de,fx,[fx] GraphModule.src -> GraphModule.code (#43655)
c5d0f091b2,complex_frontend,addmm/addmv should accept complex alpha and beta (#43827)
4ef12be900,complex_frontend,Add __complex__ (#43844)
69dd0bab90,rpc,[RPC profiling] Add test to ensure using record_function works for RPC (#43657)
2bede78a05,new_features_frontend,add qr_backward functionality for wide case (#42216)
1c0faa759e,jit,Update requires grad property (#43634)
a7e7981c0b,jit,Use prim::TensorExprGroup interned symbol (#43635)
259e5b7d71,jit,Add passes to profiling executor pipeline (#43636)
5da8a7bf2d,jit,use types in the IR instead of vmap (#43742)
3c8b1d73c9,jit,Update aliasing in tensorexpr fuser (#43743)
0394c5a283,bug_fixes_frontend,[fix] torch.multinomial : fix for 0 size dim (#43775)
68304c527a,skip,Revert D23385090: [quant][graphmode][fx] Add support for weight prepack folding
f7bae5b6b1,skip,Revert D23385091: [quant][graphmode][fx] Add top level APIs
2f52748515,docs_frontend,Publish all_gather_object and gather_object docs (#43772)
4e4626a23d,distributed,Join-based API to support DDP uneven inputs (#42577)
ffca81e38b,skip,[pytorch][bot] update mobile op deps (#43871)
3278beff44,code_coverage,Skip target determination for codecov test (#43899)
7680d87a76,complex_frontend,Let linspace support bfloat16 and complex dtypes (#43578)
3682df77db,new_features_frontend,Implementing NumPy-like function torch.heaviside() (#42523)
da0e93a8c3,code_coverage,Move `fbcode` related coverage code to `fb/` folder and add `TARGETS` (#43800)
e941a462a3,code_coverage,Enable gcc coverage in OSS (#43883)
4c19a1e350,typing_frontend,Move torch/autograd/grad_mode.pyi stubs inline (#43415)
7137327646,skip,log message at per-test level for`perfpipe_pytorch_test_times` (#43752)
9b820fe904,caffe2,Fix ImportError in the OSS land. (#43912)
f150f924d3,jit,[JIT] Specialize autograd zero: fix the guarding condition. (#43846)
d69d603061,jit,[JIT] Specialize autograd zero: actually remove the original graph after we created its versioned copy. (#43900)
98b846cd1d,jit,[JIT] Remove loop peeling from the profiling executor pipeline. (#43847)
f73ba88946,quantization,Avoid resizing in MinMaxObserver (#43789)
ee53a335c0,onnx,[ONNX] Floordiv (#43022)
deb5fde51c,jit,[TensorExpr] Make KernelSumMultipleAxes much faster (#43905)
7db7da7151,fx,[reland][quant][graphmode][fx] Add top level APIs (#43581) (#43901)
602209751e,skip,[quant][graphmode][fix] Fix insert quant dequant for observers without qparams (#43606)
da32bf4cc6,typing_frontend,Move type annotations for remaining torch.utils stub files inline (#43406)
f17d7a5556,jit,Fix exception chaining in `torch/` (#43836)
3c2f6d2ecf,caffe2,[caffe2] Extend dedup SparseAdagrad fusion with stochastic rounding FP16 (#43124)
69fbc705d8,skip,Remained changes of #43578 (#43921)
d7ee84c9b5,docs_frontend,Update determinism documentation (#41692)
69080e9e7e,improvements_frontend,simplify profile text output by displaying only top-level ops statistics (#42262)
f229d2c07b,skip,Revert D23335106: [quant][graphmode][fix] Fix insert quant dequant for observers without qparams
85d91a3230,jit,[TensorExpr] Check statements in test_kernel.cpp (#43911)
6da26cf0d9,docs_frontend,Update torch.range warning message regarding the removal version number (#43569)
825c109eb7,fx,[reland][quant][graphmode][fx] Add support for weight prepack folding (#43728) (#43902)
f1624b82b5,bug_fixes_frontend,Preserve python backtrace in autograd engine errors. (#43684)
820c4b05a9,onnx,[ONNX] Update slice symbolic function (#42935)
c14a3613a8,jit,Fix NaN propagation in TE fuser's min/max implementation (#43609)
fab012aa28,skip,"Revert ""Added support for Huber Loss (#37599)"" (#43351)"
a67246b2d4,improvements_frontend,Add reduction string test for ctc_loss. (#43884)
0b2694cd11,skip,"Support work.result() to get result tensors for allreduce for Gloo, NCCL backends (#43386)"
2789a4023b,vmap_frontend,TestVmapOperators: add structured tests that batching rules get invoked (#43731)
fa12e225d3,vmap_frontend,Batching rule for torch.mv (#43780)
dbc4218f11,vmap_frontend,"Batching rules for: torch.bmm, torch.dot (#43781)"
9b98bcecfa,vmap_frontend,torch.cat and torch.stack batching rules (#43798)
b1f19c20d6,skip,Run function check and out check in TestTensorDeviceOps (#43830)
a044c039c0,docs_frontend,updated documentation to streamline setup (#42850)
13a48ac1f3,performance_frontend,MaxPool1d without indices optimization (#43745)
224232032c,dispatcher,Move Autograd to an alias dispatch key (#43070)
e3cb582e05,jit,Error printing extension support for multiline errors (#43807)
76ca365661,skip,[pytorch][bot] update mobile op deps (#43937)
8ca3913f47,new_features_frontend,Introduce BUILD_CAFFE2 flag (#43673)
63a0bb0ab9,typing_frontend,Add typing annotations for torch.nn.quantized.dynamic.modules.rnn (#43186)
7035cd0f84,skip,"Revert D23216393: Support work.result() to get result tensors for allreduce for Gloo, NCCL backends"
bc64efae48,caffe2,"Back out ""Revert D19987020: [pytorch][PR] Add the sls tensor train op"" (#43938)"
db6bd9d60b,improvements_frontend,rename input argunment `interested-folder` to `interest-only` -- be consistent with other arguments (#43889)
5472426b9f,new_features_frontend,Reset `DataLoader` workers instead of creating new ones (#35795)
06c277f38e,caffe2,[TVM] Support slice op (#43969)
73f7d63bc9,fx,[FX] Support tensor-valued constants (#43666)
a1a23669f2,fx,[FX] Pickle serialization of GraphModule via forward source (#43674)
bacee6aa2e,skip,Selective meta programming preparation for prim ops (#43540)
24eea364f7,bug_fixes_frontend,Check SparseAdam params are dense on init (#41966) (#43668)
93fbbaab2a,docs_frontend,Update `README.md` in oss (#43893)
5e97f251a8,new_features_frontend,Enable TF32 support for cuDNN (#40737)
e49dd9fa05,improvements_frontend,Delete `raise_from` from `torch._six` (#43981)
ec7f14943c,docs_frontend,[OSS] Update README.md -- Explain more complex arguments and functionalities
8d53df30ea,fx,[FX] Better error when unpacking Proxy (#43740)
d15b9d980c,fx,[quant][graphmode][fx][refactor] Move patterns to separate files (#43891)
0ffe3d84d5,skip,[quant][graphmode][fx] Support dynamic quantization without calibration (#43892)
4134b7abfa,improvements_frontend,Pass CC env variable as ccbin argument to nvcc (#43931)
fbea2ee917,distributed,broadcast_object API for c10d (#43887)
8fd9fe93be,fx,[quant][graphmode][fx] Support dynamic quantization without calibration (#43952)
9db90fe1f3,skip,[TensorExpr] Remove unused functions in kernel.cpp (#43966)
263412e536,complex_frontend,Rename is_complex_t -> is_complex (#39906)
137a4fcc3b,skip,"Back out ""Selective meta programming preparation for prim ops"""
c259146477,new_features_frontend,"add missing NEON {vld1,vst1}_*_x2 intrinsics (#43683)"
1dd658f28f,caffe2,[Codemod][GleanFbcode] Remove dead includes in caffe2/test (#43953)
7000c2efb5,mobile,[2/2][PyTorch][Mobile] Added mobile module metadata logging (#43853)
4bb5d33076,new_features_frontend,is_numpy_scalar should also consider bool and complex types (#43644)
95f912ab13,cpp,Use NewCriterionTest in test_cpp_api_parity.py. (#43954)
c61a16b237,skip,Kill dead code in common_nn as part of merging Criterion and NewCriterionTests. (#43956)
6512032699,opbench,[Static Runtime] Add OSS build for static runtime benchmarks (#43881)
8722952dbd,opbench,Add benchmark for channel_shuffle operator (#43509)
cd58114c6c,jit,Adjust level of verbosity of debug dumps in graph executor T74227880 (#43682)
5807bb92d3,new_features_frontend,TensorIteratorConfig: Check memory overlap by default (#43422)
c88ac25679,new_features_frontend,Check for internal memory overlap in some indexing-type functions (#43423)
14ebb2c67c,new_features_frontend,Allow no-bias MKLDNN Linear call (#43703)
b6b5ebc345,complex_frontend,Add `torch.vdot` (#43004)
6f5282adc8,fx,add quantization debug util to pretty print FX graphs (#43910)
77ef77e5fa,fx,fx quant: rename matches -> is_match (#43914)
df8da5cb5a,fx,fx quant: make load_arg function more clear (#43923)
a6789074fc,new_features_frontend,Implement ChannelShuffle op with XNNPACK (#43602)
73f009a2aa,skip,refactor manual function definitions (#43711)
276158fd05,releng,.circleci: Remove un-needed steps from binary builds (#43974)
544a56ef69,jit,[JIT] Always map node output in vmap (#43988)
b167402e2e,bug_fixes_frontend,[redo] Fix SyncBatchNorm forward pass for non-default process group (#43861)
4716284904,skip,Update persons_of_interest.rst (#44031)
f6f9d22228,onnx,[ONNX] Export KLDivLoss (#41858)
297c938729,foreach_frontend,"Add _foreach_add(TensorList tl1, TensorList tl2) and _foreach_add_(TensorList tl1, TensorList tl2) APIs (#42533)"
402e9953df,skip,[pytorch][bot] update mobile op deps (#44018)
7a77d1c5c2,fx,[FX] Only copy over forward() from exec (#44006)
f15e27265f,fx,[torch.fx] Add support for custom op (#43248)
f9efcb646b,fx,fx quant: clarify state in Quantizer object (#43927)
129f406062,bc_breaking_frontend,Make torch.conj() a composite function and return self for real tensors (#43270)
2f044d4ee5,skip,Fix CI build (#44068)
041573c8cd,caffe2,Add Cost Inference for AdaGrad and RowWiseSparseAdagrad
33d51a9b32,jit,"Respect canFuseOn{CPU,GPU} in TE fuser (#43967)"
78994d165f,new_features_frontend,min_max kernel: add CUDA (#42868)
834279f4ab,new_features_frontend,_min_max_val.dim: CPU implementation (#42894)
486a9fdab2,new_features_frontend,_min_max.dim: CUDA implementation (#42943)
6a6552576d,new_features_frontend,rename _min_max to _aminmax (#44001)
b2aaf212aa,jit,[TensorExpr] Add option to enforce TensorExprKernel fallbacks. (#43972)
b2a9c3baa9,caffe2,[TVM] Support fp16 weights in c2_frontend (#44070)
37658b144b,improvements_frontend,"Remove useless py2 compatibility import __future__, part 1 (#43808)"
a76a56d761,improvements_frontend,"Add ""torch/testing/_internal/data/*.pt"" to .gitignore (#43941)"
f5ba489f93,releng,Move dependent configs to CUDA-10.2 (#44057)
bc45c47aa3,new_features_frontend,Expand the coverage of test_addmm and test_addmm_sizes (#43831)
32e0cedc53,onnx,[ONNX] Move tests to test_pytorch_onnx_onnxruntime (#42684)
ae7699829c,skip,"Remove THC max and min, which are longer used (#43903)"
3da82aee03,jit,[JIT] Remove profile nodes before BatchMM. (#43961)
40fec4e739,jit,[TensorExpr] Fuser: do not fuse ops with 0-dim tensors. (#44073)
ab7606702c,docs_frontend,Rectified a few grammatical errors in documentation (#43695)
665feda15b,new_features_frontend,Adds opinfo-based autograd tests and (un)supported dtype tests (#43451)
5f89aa36cf,bug_fixes_frontend,Actually run backward criterion tests. (#44030)
68a1fbe308,skip,Allow criterion backwards test on modules requiring extra args (i.e. CTCLoss). (#44050)
b6d5973e13,skip,Delete THCStream.cpp (#43733)
24ca6aab02,typing_frontend,Improves type-checking guards. (#43339)
15643de941,misc,"With fixes, Back out ""Back out ""Selective meta programming preparation for prim ops"""""
cae52b4036,skip,Merge CriterionTest into NewCriterionTest. (#44055)
b6e2b1eac7,vmap_frontend,BatchedFallback: stop emitting the entire schema in the fallback warning (#44051)
768c2b0fb2,bug_fixes_frontend,Fix THPVariable_float_scalar (#43842)
98320061ad,distributed,DDP Communication hook: (Patch) Fix the way we pass future result to buckets. (#43734)
c10f30647f,jit,Fix CUDA debug nightly build failure (#44085)
7d95eb8633,skip,[fbgemm] manual submodule update (#44082)
5973b44d9e,skip,Rename NewCriterionTest to CriterionTest. (#44056)
c59e11bfbb,jit,Add soft error reporting to capture all the inference runtime failure. (#44078)
f96b91332f,caffe2,[caffe2.proto] Add AOTConfig (#44020)
b10c527a1f,skip,[pytorch][bot] update mobile op deps (#44100)
d11603de38,opbench,[TensorExpr] Benchmarks: set number of profiling runs to 2 for PE. (#44112)
de672e874d,jit,[JIT] Improve error message for unsupported Optional types (#44054)
91b0d1866a,caffe2,add tanh + quantize unit test (#44076)
42f9897983,bug_fixes_frontend,Mark bucketize as not subject to autograd (#44102)
49215d7f26,bug_fixes_frontend,"For CriterionTests, have check_gradgrad actually only affect gradgrad checks. (#44060)"
bfa1fa5249,releng,Update rocm-3.5.1 build job to rocm-3.7 (#44123)
55ff9aa185,jit,Test TE fuser unary ops and fix sigmoid(half) (#44094)
ba65cce2a2,jit,Fix transposed conv2d rewrite pattern to account for convolution api (#44035)
a153f69417,jit,Fix replaceAtenConvolution for BC. (#44036)
442684cb25,typing_frontend,Enable typechecks for torch.nn.modules.[activation|upsampling] (#44093)
3806c939bd,docs_frontend,Polish DDP join API docstrings (#43973)
d0421ff1cc,opbench,Benchmarks: add scripts for FastRNNs results comparison. (#44134)
f3da9e3b50,jit,Enable Enum pickling/unpickling. (#43188)
352a32e7f3,caffe2,[caffe2] fix clang build
e05fa2f553,quantization,[quant] Prep for conv_transpose packing (#39714)
f91bdbeabd,jit,Enable function calls in TEFuser and SpecializeAutogradZero (#43866)
9b3c72d46e,mobile,[pytorch] Make mobile find_method return an optional (#43965)
6868bf95c6,jit,[JIT] Fuser match on schemas not node kind (#44083)
e64879e180,jit,[tensorexpr] Alias analysis tests (#44110)
74f18476a2,jit,[jit] fix segfault in attribute lookup on loaded ScriptModules (#43284)
9dd8670d7d,jit,[jit] Better match behavior of loaded ScriptModules vs. freshly created ones (#43298)
7816d53798,jit,[JIT] Add mypy type annotations for JIT (#43862)
71510c60ad,fx,fx qat: respect device affinity (#44115)
3105d8a9b2,jit,[TensorExpr] Fuser: rely on input types when checking whether a device is supported. (#44139)
bc4a00c197,caffe2,[TVM] Support Fused8BitRowwiseQuantizedToFloat op (#44098)
70aecd2a7f,skip,[NNC] make inlining immediate (take 2) and fix bugs (#43885)
c40e3f9f98,mobile,[android][jni] Support Tensor MemoryFormat in java wrappers (#40785)
69e38828f5,quantization,[quant] conv_transpose2d_prepack/conv_transpose2d_unpack (#40351)
538d3bd364,releng,Enable CUDA 11 jobs for Windows nightly builds (#44086)
b60ffcdfdd,typing_frontend,Enable typechecks for torch.nn.quantized.modules.linear (#44154)
addfd7a9b9,dispatcher,Add tests against autograd precedence and multiple dispatch. (#44037)
d221256888,jit,[Message] Add what to do for missing operators.
2f8a43341d,caffe2,Add API for onnxifi with AOT Glow ONNX (#44021)
a37c199b8b,caffe2,[c2][cuda] small improvement to dedup adagrad by avoiding recompute of x_ij (#44173)
98ad5ff41f,jit,[te] Disable reductions by default (#44122)
ef28ee50b0,skip,[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
f8f35fddd4,skip,"Optimize code path for adaptive_avg_pool2d when output size is (1, 1) (#43986)"
28b1360d24,skip,[Codemod][FBSourceGoogleJavaFormatLinter] Daily `arc lint --take GOOGLEJAVAFORMAT`
0c01f136f3,improvements_frontend,[BE] Use f-string in various Python functions (#44161)
9a5a732866,skip,Register some backwards functions as operators (#44052)
6cecf7ec68,skip,Enable test_cublas_config_deterministic_error for windows (#42796)
6aba58cfd3,skip,Limit MAX_JOBS to 18 for linux binary builds (#44168)
4d431881d1,build_frontend,Control NCCL build parallelism via MAX_JOBS environment var (#44167)
2a1fc56694,quantization,replace the white list from default mappings (#41802)
f38e7aee71,releng,Updates to SCCACHE for ROCm case (#44155)
0e3cf6b8d2,improvements_frontend,[pytorch] remove code analyzer build folder between builds (#44148)
af13faf18b,fx,[FX] __str__ for GraphModule and Graph (#44166)
539d029d8c,onnx,[ONNX] Fix split export using slice (#43670)
6474057c76,skip,Revert D23503636: [pytorch][PR] [NNC] make inlining immediate (take 2) and fix bugs
0c2bc4fe20,skip,"Revert D23468286: [pytorch][PR] Optimize code path for adaptive_avg_pool2d when output size is (1, 1)"
2ad5a82c43,fx,[fx] get rid of graph_module.root (#44092)
4562b212db,caffe2,Fix potential divide by zero for CostInferenceForRowWiseSparseAdagrad
70bbd08402,fx,[FX] Fix forward merge conflict breakage (#44221)
3d7c22a2ce,jit,[ONNX] Enable new scripting passes for functionalization and remove_mutation (#43791)
8f37ad8290,performance_frontend,[BUILD] Guard '#pragma unroll' with COMPILING_FOR_MIN_SIZE
8b17fd2516,distributed,Add remote_parameters() into RemoteModule class. (#43906)
3699274ce2,caffe2,[DPER3] AOT integration
f3bf6a41ca,onnx,[ONNX] Update repeat op (#43430)
15e99b6ff6,skip,Compile less legacy code when BUILD_CAFFE2 is set to False (#44079)
398409f072,skip,"[PyTorch][Mobile] Insert the module name as `name()` to metadata dict if metadata doesn't contain ""model_name"" (#44227)"
8c64bb4f47,skip,[dper3] replace LengthsGather lowlevel module's PT implemetnatio to use caffe2 op
a940f5ea5d,quantization,torchscript graph mode quant: remove benchmark filter (#44165)
618b4dd763,fx,fx quant prepare: clarify naming (#44125)
d07a36e0c1,skip,Revert D23490149: [pytorch][PR] Compile less legacy code when BUILD_CAFFE2 is set to False
5a0d65b06b,improvements_frontend,"Further expand coverage of addmm/addmv, fix 0 stride (#43980)"
df67f0beab,jit,[TensorExpr fuser] Guard nodes that have tensor output properties determined by non-tensor inputs (#44137)
5bd2902796,jit,[JIT] Remove references to no longer generated _tanh_backward and _sigmoid_backward (#44138)
15a7368115,distributed,Add const to getTensors method of GradBucket. (#44126)
a0ae416d60,quantization,[quant] Support aten::embedding_bag quantization in graph mode (#43989)
164b96c34c,quantization,[quant][pyper] make embedding_bag quantization static (#44008)
199c73be0f,quantization,[quant][pyper] Support quantization of ops in fork-wait subgraph (#44048)
396469f18c,distributed,Explicitly forbidden the other inherited methods of RemoteModule. (#43895)
4fc29e9c43,skip,Revert D23519521: [dper3] replace LengthsGather lowlevel module's PT implemetnatio to use caffe2 op
719d29dab5,new_features_frontend,Implement torch.i0 and torch.kaiser_window (#43132)
68297eeb1a,new_features_frontend,Add support for integer dim arg in `torch.linalg.norm` (#43907)
70c8daf439,mobile,Apply selective build on RNN operators (#44132)
e358d516c8,skip,"Revert D23549149: [PyTorch][Mobile] Insert the module name as `name()` to metadata dict if metadata doesn't contain ""model_name"""
671160a963,caffe2,Revert D23557576: Revert D23519521: [dper3] replace LengthsGather lowlevel module's PT implemetnatio to use caffe2 op
83a6e7d342,new_features_frontend,Adds inequality testing aliases for better NumPy compatibility (#43870)
bb861e1d69,th_aten_frontend,"Ports CUDA var and std reduce all (with no out argument) to ATen, fixes var docs (#43858)"
ac1f471fe2,skip,Benchmarks: re-enable profiling-te configuration. (#44212)
514f20ea51,caffe2,Histogram Binning Calibration
1b2da9ed82,dispatcher,Expose alias key info in dumpState and update test_dispatch. (#44081)
626e410e1d,skip,Revert D23544563: Benchmarks: re-enable profiling-te configuration.
10dd25dcd1,foreach_frontend,Add binary ops for _foreach APIs (#42536)
589a2024c8,skip,Benchmarks: re-enable profiling-te configuration (try 2). (#44270)
5d748e6d22,jit,[TensorExpr] Re-enable tests. (#44218)
0e64b02912,improvements_frontend,FindCUDA error handling (#44236)
7c61f57bec,skip,test_ops: skipTest only takes a single argument (#44181)
6134ac17ba,skip,Revert D23561500: Benchmarks: re-enable profiling-te configuration (try 2).
cce5982c4c,foreach_frontend,Add unary ops: exp and sqrt (#42537)
5de805d8a7,caffe2,[dper3] Export Caffe2 operator LearningRate to PyTorch
1130de790c,skip,Automated submodule update: FBGEMM (#44177)
8d212d3f7a,skip,add 'run_duration' stats for binary builds to scuba (#44251)
de980f937b,skip,skip test_tanhquantize for now (#44312)
fd8e2064e0,quantization,quant: switch observers to use min_max (#42957)
4e0ac120e9,fx,[FX] Only copy over training attr if it\'s there (#44314)
caf23d110f,jit,[JIT] Unshare types for modules that define() in __init__ (#44233)
477f489137,dispatcher,Don't register a fallback for private use to let extensions do it themselves (#44149)
de89261abe,releng,Reduce `sccache` log levels for RocM to a default state (#44310)
63d62d3e44,foreach_frontend,Skips test_addcmul_cuda if using ROCm (#44304)
54931ebb7b,jit,Release saved variable from DifferentiableGraphBackward (#42994)
47ac9bb105,skip,Enable temp disabled tests in test_jit_fuser_te.py (#44222)
1fcccd6a18,fx,[FX] Minor fixups in Graph printout (#44214)
49e979bfde,improvements_frontend,Set default compiler differently according to platform (#43890)
43e38d60d6,fx,[quant][graphmode][fx] Support quantize per channel in all cases (#44042)
6dd53fb58d,bug_fixes_frontend,[fix] output of `embedding_bag` with non-contiguous weight (#44032)
00b5bd536f,fx,fx quant: add docblocks to _find_matches and _find_quants (#43928)
40d138f7c1,foreach_frontend,Added alpha overloads for add/sub ops with lists (#43413)
0351d31722,releng,add rocm nightly build (#44250)
9f54bcc522,fx,[quant][graphmode][fx] Support inplace option (#43983)
be94dba429,jit,[NNC] fix support for FP16 in CudaCodgen (#44209)
6269b6e0f0,fx,[quant][graphmode][fx][api] Call fuse in prepare (#43984)
f9146b4598,skip,fix lint (#44346)
f27be2f781,caffe2,[caffe2] fix wrong comment (#42735)
6013a29fc0,quantization,[quant] Support quantization of embedding lookup operators (#44207)
57b87aaf59,quantization,[quant] Add quantized Embedding module (#44208)
646ffd4886,quantization,[quant] Move EmbeddingBag eager quantization to static (#44217)
bd8e38cd88,jit,[TensorExpr] Fuser: check node inputs' device before merging the node into a fusion group. (#44241)
ecc6358dbe,th_aten_frontend,Port nonzero cuda from THC to ATen (#44259)
4aacfab221,dispatcher,Resolve Autograd key for disable_variable_dispatch flag. (#44268)
1d01fcdc24,quantization,[quant] fill_ path for quantized tensors (#43303)
b22abbe381,distributed,Enable test_distributed to work with spawn mode (#41769)
106459acac,distributed,Rename test_distributed to test_distributed_fork (#42932)
135ebbde6d,caffe2,[Caffe2] Add RMSNormOp (#44338)
7a64b0c27a,jit,Export Node::isBefore/isAfter for PythonAPI (#44162)
8acce55015,jit,Dump optimized graph when logging in already-optimized PE (#44315)
d23f3170ef,build_frontend,Remove pybind11 from required submodules (#44278)
cfd3620b76,caffe2,Don't use VCOMP if Intel OMP is used (#44280)
f044b17ae2,skip,Disable a test (#44348)
a953a825cc,skip,Moves some of TestTorchMathOps to OpInfos (#44277)
2a87742ffa,improvements_frontend,Autocast wrappers for RNN cell apis (#44296)
7c464eed16,distributed,Skipping CUDA tests in ProcessGroupGloo and logs (#42488)
960c088a58,jit,"[te] Fix casting of unsigned char, and abs(int) (#44157)"
350130a69d,jit,Prevent the TE fuser from getting datatypes it can't handle (#44160)
6ec8fabc29,jit,Fix frac in CUDA fuser (#44152)
683380fc91,build_frontend,Use compile time cudnn version if linking with it statically (#44402)
758c2b96f5,bug_fixes_frontend,"BUG: make cholesky_solve_out do broadcast, error checking (#43137)"
364d03a67c,caffe2,Misc. FakeLowP OSS cleanup (#44331)
37093f4d99,opbench,Benchmarks: make fuser and executor configurable from command line. (#44291)
ef4475f902,performance_frontend,"[Reland] Optimize code path for adaptive_avg_pool2d when output size is (1, 1) (#44211)"
15cbd1cf4b,releng,Preserve .ninja_log in build artifacts (#44390)
1df24fd457,distributed,[NCCL] Timeout Loop Thread for Async Error Handling (#41050)
4e5c55ef69,distributed,[NCCL] Use cudaEventQuery to Poll for GPU operation errors (#41051)
f8f7b7840d,distributed,[NCCL] Abort Errored and Timed Out NCCL Communicators from Watchdog Thread (#41052)
afbf2f140b,distributed,[NCCL] WorkNCCL Helper Functions (#41053)
211ece7267,distributed,[NCCL] ProcessGroupNCCL Destructor Blocks on WorkNCCL Completion (#41054)
48c47db8fe,distributed,[NCCL] Add Environment Variable to guard Async Error Handling feature (#44163)
24efd29d19,dispatcher,Check commutativity for computed dispatch table and add a test to check entries. (#44088)
a00d36b0e7,mobile,"[PyTorch][Mobile] Insert the module name as `name()` to metadata dict if metadata doesn't contain ""model_name"" (#44400)"
032480d365,skip,fix typo in embedding_bag_non_contiguous_weight test (#44382)
3674264947,quantization,[quant] quantized path for ConstantPadNd (#43304)
b0bcdbb1ab,jit,[JIT] Support partially specified sizes/strides in IRParser (#44113)
fc51047af5,build_frontend,Small fixes in Dependency.cmake and run_test.py (#44414)
e0c65abd38,skip,Revert D23568330: [pytorch][PR] Moves some of TestTorchMathOps to OpInfos
ba6ddaf04c,caffe2,[pyper] export caffe2 bucketize GPU operator to pytorch
c010ef7f0c,improvements_frontend,use non-overflowing divide in cuda kernel util GET_BLOCKS (#44391)
b69c28d02c,jit,Improving ModuleList indexing error msg (#43361)
a7fba7de22,skip,Convert StoreTestUtils to Gtest (#43382)
69a3ff005d,skip,Modularize FileStoreTest and move to Gtest (#43383)
e028ad0762,skip,Fix HashStoreTests and move to Gtest (#43384)
5ee31308e6,caffe2,[caffe2] exposes Net cancellation through pybind state (#44043)
058d7228ec,caffe2,Expose the interface of nesterov of SGD Optimizer from caffe2 to dper
d3b6d5caf1,jit,[JIT] Add support for del to TS classes (#44352)
89ac30afb8,jit,[JIT] Propagate type sharing setting to submodule compilation (#44226)
6324ef4ced,caffe2,[caffe2] Speed up compilation of aten-op.cc (#44440)
c515881137,new_features_frontend,Add reset_grad() function (#44423)
28bd4929bd,distributed,[NNC] Make it able to normalize loop with variable start (#44133)
65d4a6b7c0,amd,[ROCm] fix cub hipify mappings (#44431)
7b547f086f,bug_fixes_frontend,To fix extra memory allocation when using circular padding (#39273)
28a23fce4c,deprecations_frontend,Deprecate torch.norm and torch.functional.norm (#44321)
6c98d904c0,caffe2,handle the case of -0.0 on tanh quantization (#44406)
356aa54694,skip,[Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
208ad45b4b,mobile,fix scripts (#44464)
af9cad761a,improvements_frontend,Stop ignoring NotImplementedErrors in cuda CriterionTests. (#44381)
fa158c4ca6,jit,Combine criterion and new criterion tests in test_jit. (#43958)
c8914afdfa,skip,Merge criterion_tests and new_criterion_tests. (#44398)
f9a0d0c21e,new_features_frontend,Allow Tensor-likes in torch.autograd.gradcheck (#43877)
cb90fef770,bug_fixes_frontend,Fix return value of PyErr_WarnEx ignored (SystemError) (#44371)
38c10b4f30,distributed,[NCCL] Fix the initialization of futureNCCLCallbackStreams (#44347)
d232fec1f1,caffe2,Partly fix cuda builds of dper broken by caffe2 c++
b3f0297a94,quantization,ConvPackedParams: remove legacy format (#43651)
cc5a1cf616,jit,[JIT] Erase shapes before fallback graph (#44434)
4bead6438a,typing_frontend,Enable torch.autograd typechecks (#44451)
a2a81e1335,distributed,Add a CONTRIBUTING.md for the distributed package. (#44224)
1dd3fae3d2,mobile,[pytorch] Add logging to mobile Method run (#44234)
91b16bff1e,releng,Disable PyTorch iOS ARM64 builds until cert problem is fixed (#44499)
2e744b1820,distributed,"Support work.result() to get result tensors for allreduce for Gloo, NCCL backends (#43970)"
c48f511c7e,improvements_frontend,Moves some of TestTorchMathOps to OpInfos (#44277)
129d52aef2,bug_fixes_frontend,Fix uniqueness check in movedim (#44307)
41f62b17e7,distributed,Fix DDP join() API in the case of model.no_sync() (#44427)
08b431f54c,skip,"Add trace_backward, masked_select_backward, and take_backward as ops (#44408)"
7ff7e6cfc8,skip,"Register cummaxmin_backward, cumprod_backward as operators (#44410)"
69f6d94caa,skip,"Register diag_backward, diagonal_backward, infinitetely...gelu_backward as operators (#44422)"
6ee41974e3,releng,Speedup Linux nightly builds (#44532)
f7278473d3,distributed,[NCCL] Fix NCCL_BLOCKING_WAIT functionality with Async Error Handling (#44411)
0c58a017bd,quantization,[quant][eagermode][refactor] Add set/get method for quantization and fusion mappings (#43990)
b5d75dddd9,new_features_frontend,Enable lerp on half type; fix output memory format (#43541)
51ed31269e,distributed,Replace FutureMessage with c10::ivalue::Future in DistEngine. (#44239)
c6febc6480,jit,[JIT] Add a python hook for a function to interpret JIT graphs. (#44493)
8b8986662f,jit,[JIT] Remove profiling nodes in autodiff forward graph (#44420)
c967e7724e,quantization,[quant] conv_transpose1d_prepack / conv_transpose1d_unpack (#40360)
30fccc53a9,jit,[NNC] Don't attempt to refactor conditional scalars (#44223)
09892de815,docs_frontend,Clarify track_running_stats docs; Make SyncBatchNorm track_running_stats behavior consistent (#44445)
77cc7d1ecd,cpp,C++ APIs Transformer NN Module Top Layer (#44333)
f44de7cdc3,distributed,Add missing rpc.shutdown() (#44417)
a9754fb860,distributed,Use TP Tensor.metadata to carry device info (#44396)
b6b1c01adf,complex_frontend,torch.view_as_complex fails with segfault for a zero dimensional tensor (#44175)
9a3b83cbf2,skip,Update submodule gloo to have latest commits to enable it can work on Windows (#44529)
d07d25a8c5,improvements_frontend,Fix MSELoss when target.requires_grad is True. (#44437)
567c51cce9,distributed,"In common_distributed, fix TEST_SKIPS multiprocessing manager (#44525)"
b73b44f976,mobile,[PyTorch Mobile] Move some string ops to register_prim_ops.cpp and make them selective (#44500)
ea55820606,caffe2,[dper3] Export PackSegments and UnpackSegments to Pytorch
3de2c0b42f,improvements_frontend,Fix L1Loss when target.requires_grad is True. (#44471)
df6ea62526,bug_fixes_frontend,Add nondeterministic check to new upsample overloads
7d78a6fcdd,improvements_frontend,Update interpolate to use new upsample overloads (#43025)
cdf5e2ae86,typing_frontend,add typing annotations for a few torch.utils.* modules (#43806)
a61318a535,mobile,[pytorch] Replace mobile run_method with get_method and operator() (#44202)
442957d8b6,mobile,[pytorch] Remove mobile nonvariadic run_method (#44235)
11fb51d093,fx,[quant][graphmode][fx][fix] Support dictionary output (#44508)
39bb455e36,dispatcher,Update fallback kernel for Autograd keys. (#44349)
b7ef4eec46,jit,[NNC] Add loop slicing transforms (#43854)
5579b53a7f,improvements_frontend,Fix SmoothL1Loss when target.requires_grad is True. (#44486)
2b8f0b2023,caffe2,[caffe2] adds Cancel to OperatorBase and NetBase (#44145)
8a574c7104,build_frontend,[Cmake] Drop quotation marks around `$ENV{MAX_JOBS}` (#44557)
192c4111a3,skip,Simplify target handling in nn gradcheck. (#44507)
70dfeb44bd,quantization,MinMax based observers: respect device affinity for state_dict (#44537)
8bec7cfa91,rpc,[rpc] rename some functions (#43042)
3e5df5f216,rpc,[rpc][jit] support rpc_sync in TorchScript (#43043)
ab6126b50e,rpc,[rpc][jit] support remote call in TorchScript (#43046)
7632484000,vmap_frontend,Add some batched gradient tests (#44494)
e2bb34e860,vmap_frontend,"Batched grad support for: slice, select, diagonal (#44505)"
dd4bbe1a79,dispatcher,Add iterator like functionality for DispatchKeySet (#44066)
1fb5883072,jit,removing conv filters from conv pattern matching (#44512)
a82ea6a91f,fx,[quant][graphmode][fx][fix] Support None qconfig in convert (#44524)
c2b40b056a,code_coverage,Filter default tests for `clang` coverage in oss
f3a79b881f,improvements_frontend,add `lcov` to oss for beautiful html report (#44568)
42f9f2f38f,bug_fixes_frontend,[fix] ReduceOps throw error if dim is repeated (#44281)
b6f0ea0c71,fx,[quant][graphmode][fx][fix] Remove qconfig in convert (#44526)
db24c5c582,code_coverage,Change code coverage option name (#43999)
566b8d0650,new_features_frontend,handle missing NEON vst1_*_x2 intrinsics (#44198) (#44199)
2ae74c0632,build_frontend,Compile less legacy code when BUILD_CAFFE2 is set to False (take 2) (#44453)
64b4307d47,jit,[NNC] Cuda Codegen - mask loops bound to block/thread dimensions (#44325)
d729e2965e,jit,[TensorExpr] Do not inline autodiff graphs if they contain prim::TypeCheck nodes. (#44564)
1f0dcf39fc,jit,[JIT] dont optimize device dtype on inline (#43363)
ab5fee2784,skip,Move the inline implementations of GradBucket class to the header. (#44339)
82b4477948,skip,Pass the input tensor vector by const reference. (#44340)
8641b55158,cpp,fix dangling ptr in embedding_bag (#44571)
d191caa3e7,amd,Cleanup workarounds for compiler bug of ROCm (#44579)
05c1f1d974,amd,[ROCm] remove thrust workaround in ScanKernels (#44553)
6f2c3c39d2,skip,Add SNPE deps for caffe2 benchmark android binary
0743d013a6,caffe2,fuse layernorm + quantize (#44232)
a309355be3,caffe2,[dper3] Create dper LearningRate low-level module
e703c17967,caffe2,Revert D23584071: [dper3] Create dper LearningRate low-level module
7e91728f68,bc_breaking_frontend,Deprecates calling linspace and logspace without setting steps explicitly (#43860)
6d4a605ce9,jit,Fix bug simplifying if-then-else when it can be removed (#44462)
82da6b3702,jit,[JIT] Fix jit-log verbosity selection logic. (#44587)
bcf97b8986,jit,[JIT] Cleanup some places where we log graphs in executors. (#44588)
7862827269,mobile,[pytorch] Add variadic run_method for lite intepreter (#44337)
fe26102a0e,jit,Enable TE in test_jit.py (#44200)
8daaa3bc7e,docs_frontend,Fix latex error in heaviside docs (#44481)
68a5c361ae,opbench,Adding Adapative Autorange to benchmark utils. (#44607)
870f647040,skip,Automated submodule update: FBGEMM (#44581)
c68a99bd61,new_features_frontend,[numpy] Add `torch.exp2` (#44184)
7040a070e3,jit,[torch] Minor: Avoid ostreamstring in Operator's canonicalSchemaString() (#44442)
bd257a17a1,amd,Add HIP/ROCm version to collect_env.py (#44106)
105132b891,releng,Move ONNX circle ci build to torch and remove all caffe2 CI job/workflows (#44595)
95a69a7d09,new_features_frontend,adds list_gpu_processes function (#44616)
21a09ba94d,bug_fixes_frontend,Fix lerp.cu bug when given discontiguous out tensor (#44559)
ace81b6794,skip,Remove an extra empty line in the warning comments. (#44622)
e261e0953e,build_frontend,Fix centos8 gcc (#44644)
856510c96d,jit,[JIT] Dont optimize shape info in batch_mm (#44565)
a475613d1d,jit,[static runtime] Swap to out-variant compatible nodes (#44127)
199435af90,docs_frontend,Update median doc to note return value of even-sized input (#44562)
ad7a2eb1c9,jit,Simplify nested Min and Max patterns. (#44142)
ecac8294a6,typing_frontend,enable type checking for torch._classes (#44576)
9d4943daaf,quantization,[quant] conv_transpose1d / conv_transpose2d (#40370)
cfba33bde3,docs_frontend,Fix the ELU formula in the docs (#43764)
b5dd6e3e61,typing_frontend,split torch.testing._internal.* and add type checking for torch.testing._internal.common_cuda (#44575)
a188dbdf3f,jit,Check for index-rank consistency in FunctionInliner (#44561)
84949672bf,jit,Fix exception chaining in `test/` (#44193)
742654d1b6,quantization,[quant] ConvTranspose1d / ConvTranspose2d (#40371)
d0a56cab07,quantization,[quant] Fixing the output shape for the linear (#44513)
62ebad4ff9,onnx,[ONNX] Export new_empty and new_zeros (#43506)
da11d932bc,onnx,[ONNX] Update arange op to support out argument (#43777)
f7cfbac89b,onnx,[ONNX] Update len symbolic (#43824)
8f327cd6c5,vulkan,"[vulkan][op] add.Scalar, mul.Scalar (#42674)"
89aed1a933,vulkan,[vulkan][op] avg_pool2d (#42675)
43406e218a,onnx,[ONNX] Update ONNX shape inference (#43929)
e594c30bc2,fx,[quant][graphmode][fx] Support fp16 dynamic quantization for linear (#44582)
686e281bcf,new_features_frontend,Updates div to perform true division (#42907)
a91c2be2a9,skip,Automated submodule update: FBGEMM (#44647)
2254e5d976,new_features_frontend,Add note comments to enforce nondeterministic alert documentation (#44140)
551494b01d,jit,[JIT] Fix torch.tensor for empty multidimensional-typed lists (#44652)
e107ef5ca2,typing_frontend,Add type annotations for torch.nn.utils.* (#43080)
71673b31f9,skip,DPP Async Tracing (#44252)
63105fd5b1,skip,Refactor CallbackManager as a nested class of RecordFunction. (#44645)
e7d782e724,jit,[JIT] Add property support for ScriptModules (#42390)
2c4b4aa81b,skip,Revert D23494065: Refactor CallbackManager as a nested class of RecordFunction.
ed862d3682,build_frontend,Split CUDA_NVCC_FLAGS by space (#44603)
c71ce10cfc,bc_breaking_frontend,add dilation to transposeconv's _output_padding method (#43793)
aedce773ed,releng,Deleted docker images for rocm 3.3 and rocm 3.5 (#44672)
2fd142a2ef,docs_frontend,Small clarification to amp gradient penalty example (#44667)
2435d941b1,performance_frontend,Fix FP16 fastAtomicAdd for one case where tensor start address is not 32 bit aligned (#44642)
7036e91abd,skip,Revert D23323486: DPP Async Tracing
72b5665c4f,skip,Upgrade oneDNN (mkl-dnn) to v1.6 (#44706)
5f692a67db,quantization,qat conv_fused.py: one more patch for forward compatibility (#44671)
f5d231d593,distributed,move rebuild buckets from end of first iteration to beginning of second iteration (#44326)
9c364da9b9,docs_frontend,Fix doc builds for bool kwargs (#44686)
6bc77f4d35,improvements_frontend,Use amax/maximum instead of max in optimizers (#43797)
1d733d660d,docs_frontend,[docs] torch.min/max: remove incorrect warning from docs (#44615)
d62994a94d,releng,ci: Add anaconda pruning to CI pipeline (#44651)
07cba8b1fc,vmap_frontend,Run vmap tests in CI (#44656)
4ce6af35c4,caffe2,Enable fp16 for CUDA SparseLengthsSum/Mean (#44089)
8df0400a50,jit,Fix fallback graph in specialize autogradzero (#44654)
69839ea3f6,jit,[NNC] make inlining immediate (take 3) (#44231)
285ba0d068,caffe2,Enable fp16 for UniformFill (#44540)
2f4c31ce3a,jit,[jit] Speed up saving in case of many classes (#44589)
26a91a9f04,opbench,[WIP][JIT] Add benchmarking support of NV Fuser with FP16 dtype support (#44101)
fb085d90e3,skip,Revert D23583017: move rebuild buckets from end of first iteration to beginning of second iteration
993b4651fd,bug_fixes_frontend,Convert num_kernels to int64 before calling into CUDA GET_BLOCKS (#44688)
2c1b215b48,fx,"[fx] remove delegate, replace with tracer (#44566)"
2efc618f19,skip,lr_schedule.py redundant code (#44613)
d66520ba08,jit,[TensorExpr] Fuser: try merging adjacent fusion groups. (#43671)
b85568a54a,opbench,[CI] Add profiling-te benchmarks. (#44756)
3f512b0de2,quantization,[quant][qat] Ensure observers and fq modules are scriptable (#44749)
63469da3bb,rpc,Add a test to ensure DDP join works with RPC (#44439)
06036f76b6,new_features_frontend,CUDA BFloat16 pow (#44760)
dbf17a1d4c,docs_frontend,Fixing a few links in distributed CONTRIBUTING.md (#44753)
b63b684394,docs_frontend,Consolidate CODEOWNERS file for distributed package. (#44763)
a5cc151b8c,build_frontend,Build EigenBlas as static library (#44747)
5e717f0d5e,docs_frontend,delete the space for the docs rendering (#44740)
ced8727d88,skip,Fix a broken link in CONTRIBUTING.md (#44701)
eb75cfb9c0,distributed,"Back out ""Revert D23323486: DPP Async Tracing"" plus windows build fix. (#44702)"
ee493e1a91,quantization,CUDA bfloat compare ops (#44748)
a011b86115,skip,change self.generator to generator (#44461)
3e6bb5233f,docs_frontend,Reference amp tutorial (recipe) from core amp docs (#44725)
07d07e3c6c,jit,Remove EXPERIMENTAL_ENUM_SUPPORT feature guard (#44243)
570102ce85,skip,Remove many unused THC pointwise math operators (#44230)
e6101f5507,bug_fixes_frontend,"fixes lda condition for blas functions, fixes bug with beta=0 in addmv slow path (#44681)"
6954ae1278,new_features_frontend,Vec256 Test cases (#42685)
924717bf51,rpc,Add _get_type() API to RRef (#44663)
257c6d0fde,rpc,Make async_execution compatible with RRef helpers (#44666)
cce7680a23,rpc,Add bound method tests for async_execution with RRef helper (#44716)
1cd5ba49c6,vmap_frontend,"Add batching rule for ""is_complex"", ""conj"" (#44649)"
c44e4878ae,typing_frontend,Enable torch.backends.quantized typechecks (#44794)
1718b16d15,caffe2,[Caffe2] gcs_cuda_only is trivial if CUDA not available (#44578)
e9c6449b46,fx,[FX][EZ] Allow constructing GraphModule with dict for root (#44679)
6debe825be,vulkan,[vulkan] glsl shaders relaxed precision mode to cmake option (#43076)
20ac736200,new_features_frontend,Remove py2 compatible future imports (#44735)
f3bd984e44,distributed,Move the description comment of compute_bucket_assignment_by_size from cpp to the header file. (#44703)
07d9cc80a4,bug_fixes_frontend,Fix error code checks for triangular_solve (CPU) (#44720)
a3835179a1,caffe2,[FakeLowP] Addressing FakeLowP OSS issues. (#44819)
82ab167cce,jit,[NNC] Fix masking for all block and thread dimensions in CudaCodeGen (#44733)
5027c161a9,skip,Add TORCH_SELECTIVE_NAME to AMP definitions (#44711)
cb3b8a33f1,jit,[JIT] Disallow plain Dict type annotation without arg (#44334)
78b806ab4a,jit,[JIT] Disallow plain List type annotation without arg (#44584)
ac0d13cc88,performance_frontend,Vectorize complex copy. (#44722)
7b3432caff,jit,[TensorExpr] Support boolean in simplifier (#44659)
3f5bb2bade,quantization,[quant] Support clone for per channel affine quantized tensor (#44573)
09a84071a3,typing_frontend,enable mypy check for jit_metaprogramming_utils (#44752)
ffe127e4f1,jit,[JIT] Disallow plain Tuple type annotation without arg (#44585)
161490d441,new_features_frontend,Move `torch/version.py` generation to cmake (#44577)
574f9af160,distributed,[NCCL] Add option to run NCCL on high priority cuda stream (#43796)
43fe034514,jit,[JIT] Disallow plain Optional type annotation without arg (#44586)
6befc09465,jit,Fix misuse of PyObject_IsSubclass (#44769)
3fa7f515a5,skip,[pytorch][bot] update mobile op deps (#44700)
0ccc38b773,skip,[caffe2] adds Cancel to SafeDequeueBlobsOp and SafeEnqueueBlobsOp (#44495)
8ec6bc7292,vulkan,[pytorch][vulkan][jni] LiteModuleLoader load argument to use vulkan device
204f985fc3,jit,[NNC] Add simplification of Loop + Condition patterns. (#44764)
29664e6aa3,fx,[FX] Further sanitize generated names (#44808)
79108fc16c,jit,[JIT] Improve Future subtype checking (#44570)
e48201c5cf,docs_frontend,Mention TF32 on related docs (#44690)
ba6534ae2b,distributed,enable type check common_distributed (#44821)
34331b0e0f,improvements_frontend,CUDA BFloat16 and other improvements on abs (#44804)
b61d3d8be8,new_features_frontend,Implement torch.kaiser_window (#44271)
fdeee74590,vulkan,"[pytorch][vulkan] Fix downcast warnings-errors, aten_vulkan buck target"
e18a2219dd,new_features_frontend,"Implement scatter reductions (CUDA), remove divide/subtract (#41977)"
b6f4bb0a70,skip,Revert D23236088: [pytorch][PR] [caffe2] adds Cancel to SafeDequeueBlobsOp and SafeEnqueueBlobsOp
99093277c0,jit,Support Python Slice class in TorchScript (#44335)
28085cbd39,new_features_frontend,Fixed quantile nan propagation and implemented nanquantile (#44393)
503c74888f,skip,Always use NewModuleTest instead of ModuleTest. (#44745)
a40ef25e30,jit,[te] Disable flaky test CudaSharedMemReduce_1 (#44862)
4affbbd9f8,skip,minor style edits to torch/testing/_internal/common_quantized.py (#44807)
f605d7581e,improvements_frontend,Implement better caching allocator for segmentation usecase. (#44618)
6006e45028,skip,.circleci: Switch to dynamic MAX_JOBS (#44729)
d2b4534d4d,distributed,refactor intialize bucket views (#44330)
74c3dcd1d2,skip,Revert D23725053: [pytorch][PR] change self.generator to generator
361b38da19,fx,[quant][fx] Add node name as prefix to observer module name (#44765)
1fde54d531,quantization,[quant][qat] Ensure fake_quant and observer can be disabled on scriptmodule (#44773)
24df3b7373,bug_fixes_frontend,torch.empty_like and torch.zeros_like raise error if any memory format is provided with sparse input (#43699) (#44058)
c1fa42497b,bug_fixes_frontend,fix legacy GET_BLOCKS code from THCUNN/common.h (#44789)
c189328e5d,new_features_frontend,CUDA BFloat16 unary ops part 2 (#44824)
2558e5769d,jit,Implement sort for list of tuples (#43448)
bee97d5be0,distributed,Document the default behavior for dist.new_group() when ranks=None (#44000)
f5440a448a,new_features_frontend,CUDA BFloat16 i0 support (#44750)
5d57025206,jit,[TensorExpr] Add log1p support to the LLVM backend (#44839)
4066022146,bug_fixes_frontend,Do not use `PRId64` in torch/csrc (#44767)
086a2e7a4e,caffe2,[caffe2] add cost inference for FusedFakeQuantFC and FusedFakeQuantFCGradient (#44840)
40e44c5f0a,improvements_frontend,Make nuclear and frobenius norm non-out depend on out variants (#44095)
a153eafab7,new_features_frontend,Let logspace support bfloat16 on both CPU and CUDA (#44675)
f5b92332c1,jit,[TensorExpr] Fix order comparisons for unsigned types (#44857)
1c996b7170,typing_frontend,Enable typechecking for torch.testing._internal.common_quantized.* (#44805)
e535fb3f7d,onnx,[ONNX] Enable true_divide scripting export with ONNX shape inference (#43991)
18b77d7d17,jit,[TensorExpr] Add Mod support to the LLVM backend (#44823)
2043fbdfb6,typing_frontend,Enable torch.backends.cuda typechecking in CI (#44916)
e14b2080be,distributed,[reland] move rebuild buckets from end of first iteration to beginning of second iteration (#44798)
60ae6c9c18,fx,[FX] Fix GraphModule copy methods not regenerating forward (#44806)
9a007ba4cb,jit,[jit] stop parsing the block after seeing exit statements (#44870)
4a9c80e82e,skip,[pytorch][bot] update mobile op deps (#44854)
5dbcbea265,jit,TorchScript with record_function (#44345)
1694fde7eb,bug_fixes_frontend,Fix a GroupNorm cuda bug when input does not require_grad (#44863)
f2b3480795,new_features_frontend,CUDA BFloat softmax (#44837)
e400150c3b,caffe2,Fixed for caffe2/opt/tvm_transformer.cc (#44249)
caea1adc35,complex_frontend,Complex support for stft and istft (#43886)
df39c40054,jit,Cleanup tracer handling of optional arguments (#43009)
6d178f6b8e,bug_fixes_frontend,Stop ignoring errors in cuda nn module tests. (#44783)
07b7e44ed1,skip,Stop using check_criterion_jacobian. (#44786)
cff0e57c31,improvements_frontend,Remove Incorrect Comment in tools/build_libtorch and remove Python2 support in the module import (#44888)
c68cc78299,distributed,Add a device parameter to RemoteModule (#44254)
0063512a4b,onnx,[ONNX] Updates to diagnostic tool to find missing ops (#44124)
174cbff00a,jit,Improve sugared value's error message (#42889)
09f2c6a94c,dispatcher,"Back out ""Revert D23494065: Refactor CallbackManager as a friend class of RecordFunction."" (#44699)"
f175830558,jit,[NNC] Fuse identical conditions in simplifier (#44886)
7bd8a6913d,new_features_frontend,"CUDA BFloat div, addcdiv, addcmul, mean, var (#44758)"
c2cf6efd96,typing_frontend,Enable type check for torch.testing._internal.dist_utils.* (#44832)
6d312132e1,vmap_frontend,Beef up vmap docs and expose to master documentation (#44825)
a47e3697ab,dispatcher,Use iterator of DispatchKeySet. (#44682)
d22dd80128,typing_frontend,Enable type check for torch.testing._internal.common_device_type. (#44911)
af3fc9725d,rpc,"Extract rpc/tensorpipe_utils.{cpp,h} from rpc/utils.{cpp,h} (#44803)"
374e9373b5,jit,[jit] Pull (most) tests out of libtorch_python (#44795)
2d884f2263,caffe2,Optimize Scale function (#44913)
fd4e21c91e,new_features_frontend,Add optional string support to native_functions schema (#43010)
76a109c930,bug_fixes_frontend,[caffe2/aten] Fix clang build (#44934)
06389406bb,new_features_frontend,CUDA BFloat activations 1 (#44834)
e255a4e1fd,new_features_frontend,Enable bfloat16 random kernels on Windows (#44918)
40c09cfe14,releng,[CircleCI] Fix CUDA test setup (#44982)
043466f978,fx,[FX] Pass module's qualname to is_leaf_module (#44966)
572f7e069c,typing_frontend,Enable type check for torch.testing._internal.te_utils.* (#44927)
e9941a5dd4,vulkan,[vulkan][py] torch.utils.optimize_for_vulkan (#44903)
1c15452703,releng,Update Windows builders to latest VS2019 (#44746)
21a1b9c7cf,skip,skip more nccl tests that causes flaky timeouts on rocm build (#44996)
620c999979,skip,update gloo submodule (#45008)
d75c402755,new_features_frontend,"Add cusolver to build, rewrite MAGMA inverse with cusolver (#42403)"
9e5045e978,skip,[pytorch] clean up normalized_dynamic_type() hack (#44889)
0714c003ee,skip,[pytorch][tensorexpr] Make gtest-style macros in tests match actual gtest signatures (#44861)
2163d31016,quantization,histogram observer: ensure buffer shape consistency (#44956)
7ecfaef7ec,new_features_frontend,CUDA BFloat16 layernorm (#45002)
faef89c89f,new_features_frontend,CUDA BFloat Pooling (#44836)
60709ad1bf,new_features_frontend,Adds multiply and divide aliases (#44463)
49db7b59e0,skip,"For logical tests, use the dtypes decorator (#42483)"
da7863f46b,new_features_frontend,Add one dimensional FFTs to torch.fft namespace (#43011)
9f67176b82,complex_frontend,Complex gradcheck logic (#43208)
4810365576,typing_frontend,Enabled torch.testing._internal.jit_utils.* typechecking. (#44985)
a6895d43b6,bug_fixes_frontend,Turn on gradgrad check for BCELoss Criterion Tests. (#44894)
e2f49c8437,performance_frontend,skip im2col & vol2col in cpu/cuda convolution methods (#44600)
4bbb6adff5,jit,[NNC] fix SyncThreads insertion and reenable CudaSharedMem test (#44909)
ac8c7c4e9f,rpc,Make Channel API accept buffer structs rather than raw pointers. (#45014)
a4aba1d465,skip,fix compile error (#45052)
acc2a1e5fa,skip,Update submodule gloo (#45025)
92f8f75c59,dispatcher,Add alias dispatch key Math. (#44354)
42af2c7923,jit,[jit] gtest-ify test_alias_analysis.cpp (#45018)
7de512ced8,improvements_frontend,nightly robustness fixes for linking across devices (#43771)
1a580c1021,quantization,Adding test to quantized copy for 'from float' (#43681)
7118d53711,skip,add .cache to gitignore (#45017)
9dc2bcdc07,th_aten_frontend,Introducing (Const)StridedRandomAccessor + CompositeRandomAccessor + migrate `sort` to ATen (CPU) (#39744)
c941dd3492,skip,[FX] s/get_param/get_attr/ (#45000)
1cab27d485,new_features_frontend,Add a torch.hub.load_local() function that can load models from any local directory with a hubconf.py (#44204)
581a364437,new_features_frontend,CUDA BFloat16 unary ops part 1 (#44813)
dfb8f2d51f,new_features_frontend,"CUDA BFloat16 addmm, addmv (#44986)"
9a31eee107,skip,[vulkan] Remove duplication of op registration and clean unused vars (#44932)
81bb19c9f0,jit,[JIT] Prohibit subscripted assignments for tuple types (#44929)
20f52cdd76,performance_frontend,[hpc]optimize the torch.cat cuda kernel (#44833)
f77ba0e48c,docs_frontend,Change typo 'momemtum' to 'momentum' (#45045)
4b3046ed28,performance_frontend,Vectorize int8_t on CPU (#44759)
8968030f19,releng,[WIP] Add vec256 test to linux CI (#44912)
5621ba87a2,skip,[vulkan] reshape op to use infer_size to expand -1 (#45104)
dfc88d4fd0,vulkan,[vulkan] support dimensions negative indexing (#45068)
09e7f62ce2,rpc,Fix RPC and ProcessGroup GIL deadlock (#45088)
0dda65ac77,onnx,[ONNX] add jit pass for lists (#43820)
32c1a8c79f,caffe2,adjust shape inference in sls tests (#44936)
2111ec3bf3,new_features_frontend,CUDA BFloat16 losses (#45011)
5aed75b21b,quantization,[quant][graphmode][jit] Try to support append (#44641)
d126a0d4fd,skip,[iOS] Disable the iOS nightly build until the cert issue has resolved (#45094)
c947ab0bb9,new_features_frontend,"Added sparse support for asin and neg functions, updated log1p (#44028)"
339961187a,skip,[pytorch] refine dispatch keys in native_functions.yaml (1/N) (#45010)
71aeb84ab4,skip,Revert D23803951: [pytorch] refine dispatch keys in native_functions.yaml (1/N)
1b059f2c6d,distributed,Directly use work.result() to retrieve tensor rather than passing as a separate argument (#44914)
58b6ab69e5,complex_frontend,torch.sgn for complex tensors (#39955)
36ec8f8fb8,caffe2,[dper3] Create dper LearningRate low-level module (#44639)
4a0aa69a66,bug_fixes_frontend,Fix undefined variable 'namedshape' in tensor.py (#45085)
e155fbe915,improvements_frontend,add warning when ParameterList/Dict is used with DataParallel (#44405)
63fd257879,jit,Add `Ellipsis` constant to the list of recognized tokens (#44959)
9fc7a942f0,new_features_frontend,Change from self to self.class() in _DecoratorManager to ensure a new object is every time a function is called recursively (#44633)
ae286d81e0,jit,[JIT] improve alias analysis for list constructs (#39111)
4b42f0b613,new_features_frontend,Support Math keyword in native_functions.yaml. (#44556)
8501b89a87,skip,[ONNX] Update ort release (#45095)
1fd48a9d1f,skip,Revert D23798016: [FX] s/get_param/get_attr/
10f287539f,dispatcher,Align casing in test_dispatch with dispatch keys. (#44933)
ef885c10d8,new_features_frontend,[pytorch] Add triplet margin loss with custom distance (#43680)
e2b40ce793,new_features_frontend,Support BFloat16 for binary logical operators on CUDA (#42485)
09aee06e82,caffe2,[caffe2] Replace embedding conversion ops with fbgemm functions (#44843)
2b1f25885e,quantization,[quant] Fix ConvTranspose mapping (#44844)
c253b10154,jit,Fix incorrect EnumValue serialization issue (#44891)
a4ce3f4194,typing_frontend,Fix type hint warnings for common_methods_invocations.py (#44971)
def433bbb6,releng,.circleci: Upgrade all xcode 9 workers to xcode 11 (#45153)
79fe794f87,fx,[FX] Make Graphs immutable and make GraphModule recompile after assigning graph (#44830)
d1c68a7069,docs_frontend,Clarify that 5-D 'bilinear' grid_sample is actually trilinear (#45090)
cddcfde81d,jit,[JIT] Fix WithTest.test_with_exceptions (#45106)
35cdb01327,typing_frontend,[PyTorch] Enable type check for autocast_test_lists (#45107)
7f4a27be3a,fx,[resubmit][FX] s/get_param/get_attr/ (#45147)
ccfbfe5eb5,fx,[quant][graphmode][fx] Custom module support (#44766)
2a37f3fd2f,new_features_frontend,Relax CUDA architecture check (#45130)
b98ac20849,amd,install ATen/native/cuda and hip headers (#45097)
c0267c6845,caffe2,[caffe2] Support data types in shape hints (#45110)
ebde5a80bb,jit,[tensorexpr] Add flag to fuse with unknown shapes (#44401)
e045119956,jit,[JIT] Add default arguments for class types (#45098)
f575df201f,quantization,[quant][graphmode][jit][api] Expose preserved_attrs from finalize to convert_jit (#44490)
666223df46,jit,[jit] gtestify test_argument_spec.cpp (#45019)
67a19fecef,new_features_frontend,CUDA BFloat16 pooling (#45151)
1bd6533d60,improvements_frontend,Remove thread_local RecordFunctionGuard from profiler. (#44646)
70d2e4d1f6,rpc,[RPC profiling] Allow disableProfiler() to be called from another thread. (#44653)
d4a634c209,rpc,[RPC profiling] Don't wrap toHere() calls with profiling (#44655)
cb75addee4,package,torch.package - a way to package models and code (#45015)
25ed739ac9,package,[packaging] rstrip fix (#45166)
0a9ac98bed,dispatcher,[reland][pytorch] refine dispatch keys in native_functions.yaml (1/N) (#45137)
989d877c95,jit,[JIT] Do not allow creating generics with None types (#44958)
144dacd8d9,new_features_frontend,CUDA BFloat16 batched gemm (#45167)
7fba30c2be,fx,[quant][fx][bug] Fix error in convert step for QAT (#45050)
215679573e,jit,[TensorExpr] Fix operator order in combineMultilane (#45157)
76dc50e9c8,rpc,[RPC] Infer backend type if only options are given (#45065)
e5bade7b2c,jit,[PyTorch Mobile] Move string op registrations to prim and make them selective (#44960)
94c3cdd994,rpc,Let rpc._all_gather use default RPC timeout (#44983)
5b20bf4fd9,complex_frontend,Added support for complex input for Cholesky decomposition (#44895)
9e30a76697,bug_fixes_frontend,Filter `strtod_l` is undeclared errors from sccache log (#45183)
9db3871288,improvements_frontend,Update true_divide_out to use at::. (#45079)
a5a4924c27,new_features_frontend,Warn if `import torch` is called from the source root. (#39995)
da4033d32a,new_features_frontend,Make cudaHostRegister actually useful on cudart. (#45159)
4d80c8c648,jit,Fix inlining interface call in fork subgraph (#43790)
99242eca1d,new_features_frontend,Dockerfile: Support CUDA 11 (#45071)
21fabae47a,performance_frontend,Remove expensive call to PyObject_GetAttrString in PyTorch_LookupSpecial (#44684)
adb2b380ba,fx,[quant][graphmode][fx] qconfig_dict support more types of configurations (#44856)
9e206ee9f1,jit,[NNC] Fix a bug in SplitWithMask when splitting multiple times (#45141)
3f89b779c4,jit,[jit] allow submodule methods inference rule be different (#43872)
d2b045030e,skip,"gtest-ify JIT tests, through the letter c (#45020)"
246bd9422a,skip,gtestify dce and fuser tests (#45055)
2a1a51facb,skip,Fix typos. (#45195)
8e0fc711f4,jit,[TensorExpr] Remove unused EvalConstExpr function (#45180)
0495998862,jit,[TensorExpr] Disallow arithmetic binary operations on Bool (#44677)
f93ead6d37,quantization,[quant][eagermode] Custom module support (#44835)
76c185dcca,jit,"[TensorExpr] When lanes differ, insert Broadcast instead of Cast (#45179)"
89c570ed0a,skip,Revert D23811085: gtestify dce and fuser tests
e9aa6898ab,skip,"Revert D23802296: gtest-ify JIT tests, through the letter c"
27c7158166,caffe2,Remove __future__ imports for legacy Python2 supports (#45033)
721cfbf842,skip,[PT Model Split] Support 2 operators in PT by C2 conversion (#45231)
60665ace17,quantization,[quant] Add optimized approach to calculate qparams for qembedding_bag (#45149)
c760bc8fb1,caffe2,Add GlowLoadAOTModel flag (#45189)
2d00ebd29f,jit,Failing test demonstrating problems with mixed output shapes (#44455)
956a25d061,skip,Revert D23858329: [PT Model Split] Support 2 operators in PT by C2 conversion
070fe15e4c,rpc,Add link to profiling recipe from rpc main docs (#45235)
6a2e9eb51c,new_features_frontend,torch.fft: Multi-dimensional transforms (#44550)
0b6b735863,bug_fixes_frontend,[fix] type promotion atan2 (#43466)
b470fa4500,complex_frontend,Add complex number support for binary logical operators (#43174)
3dd0e362db,jit,[TensorExpr] Fix min and max for integral inputs in CUDA backend (#44984)
b3d7c2f978,onnx,[ONNX] Update ONNX docs for release (#45086)
29dc3c5ec8,new_features_frontend,Sparse softmax support (CUDA) (#42307)
6d21d5f0b3,jit,"gtest-ify JIT tests, through the letter c (#45249)"
dc67b47bc9,deprecations_frontend,Deprecate old fft functions (#44876)
bea7901e38,typing_frontend,Enable torch.tensor typechecks (#45077)
71d1b5b0e2,skip,Add foreach APIs for binary ops with ScalarList (#44743)
bc591d76a1,amd,add skip_if_rocm to all requires_nccl tests (#45158)
f9ae296a85,distributed,renaming TestDdpCommHook class so it doesn't get picked up as a test by pytest (#44905)
5195d727b5,distributed,adding a test for ddp save()/load() (#44906)
bfdf4323ac,skip,Bump up NCCL to 2.7.8 (#45251)
8507ea22b2,opbench,replace timer test with a mocked variant (#45173)
2b38c09f69,jit,Moves prim ops from C10 back to JIT (#45144)
e57a08119b,distributed,Add a warning log when there is high skew of uneven inputs in DDP training (#45238)
b8eab8cdbd,skip,[hotfix] typo in NaiveConvolutionTranspose2d.cu (#45224)
3f5eee666c,improvements_frontend,Adjust TF32 tests (#44240)
c79d493096,releng,added rocm 3.8 docker image (#45205)
26001a2334,skip,Revert D23753711: [pytorch][PR] Add foreach APIs for binary ops with ScalarList
c211a9102f,releng,add rocm 3.8 to nightly builds (#45222)
c3a5aed5f7,new_features_frontend,Run pytorch_core CUDA tests on GPU using TPX
e2bcdc7b69,caffe2,[Caffe2] Fix LayerNormOp when batch_size == 0. (#45250)
022ba5a78b,distributed,Make ddp_comm_hook_wrapper a private method. (#44643)
cbe1eac1f4,caffe2,[caffe2] adds Cancel to SafeDequeueBlobsOp and SafeEnqueueBlobsOp (#45177)
71e6ce6616,jit,[JIT] Specialize AutogradZero: merge AutogradAnyNonZero and Not(AutogradAnyNonZero) checks into one. (#44987)
cd7a682282,caffe2,[caffe2] adds hypothesis test for queue ops cancel (#45178)
b84dd771e6,docs_frontend,Grammatically updated the tech docs (#45192)
6311c5a483,vulkan,Minor touchups. (#44317)
5a59330647,vulkan,Add architectural support for multi-GPU. (#44059)
1539d4a664,caffe2,Add operator to compute the equalization scale (#45096)
0137e3641d,jit,Refactor subgraph merging (#44238)
5dd288eb06,jit,[JIT] Regularize tensorexpr fuser strategy with other fusers (#44972)
bee1d448e7,rpc,Fix test_rpc_profiling_remote_record_function (#45162)
92ebb04f92,jit,added check for NumberType (#44375)
0b6e5ad4a9,dispatcher,Resolve comments in #44354. (#45150)
677a59dcaa,quantization,[aten] Call fbgemm functions for embedding prepack/unpack (#44845)
03dde4c62a,caffe2,Resend diff D23858329 (#45315)
0f2c648c97,jit,log metadata when model loading failed (#44430)
7e5492e1be,bug_fixes_frontend,[minor] Fix undefined variable (#45246)
630bd85aae,dispatcher,[pytorch] refine dispatch keys in native_functions.yaml (2/N) (#45284)
c6500bcf14,distributed,[reland] Make grad point to bucket buffer in DDP to save memory usage (#44344)
0122299f9b,skip,"Enable distributed package on windows, Gloo backend supported only (#42897)"
31ae8117ba,improvements_frontend,[RFC] Remove per-op-registration related code in caffe2/tools/codegen/gen.py (#45134)
bc3151dee0,quantization,[quant] Remove unused qconfig argument in qat linear module (#45307)
103fa3894a,skip,"Revert D23841786: [pytorch][PR] Enable distributed package on windows, Gloo backend supported only"
bdf329ef8a,quantization,SyncBN: preserve qconfig if it exists (#45317)
95df8657c9,new_features_frontend,Enables test linalg (#45278)
99e0a87bbb,jit,[nvFuser] Latency improvements for pointwise + reduction fusion (#45218)
241afc9188,th_aten_frontend,Migrate `addr` from the TH to Aten (CPU) (#44364)
76ee58e2ec,jit,[TensorExpr] Move inner loops vectorization logic to its own method (#45287)
00e704e757,bug_fixes_frontend,[fix] torch.repeat : dim-0 backward (#45212)
2739a7c599,skip,Byte-for-byte compatibility fixes in codegen (#44879)
bf8cd21f2a,improvements_frontend,Py transformer coder test (#43976)
043bd51b48,skip,Remove hacky_wrapper from VariableType and TraceType (#44005)
2ac7de7d53,skip,Remove hacky_wrapper from BackendSelect kernels (#44062)
78fcde9c50,dispatcher,Trace scattered tensor options arguments (#44071)
b70fac75ac,bug_fixes_frontend,CMake: Fix python dependencies in codegen (#45275)
8b00c4c794,skip,[ONNX] Correct a minor typo in warning (#45187)
a117d968f6,quantization,[quant][graph] Remove redundant aten::wait calls in the graph (#45257)
536580e976,performance_frontend,Vectorize bitwise_not (#45103)
d1d9017a66,jit,[NNC] fix Half conversion of immediates in Cuda backend (#45213)
d1a11618f5,jit,[static runtime] Add _out variants and reuse memory (#44128)
e5f6e5af13,jit,Add Deep and wide to test and flatten/tranpose for good measure (#44129)
dc9e9c118e,new_features_frontend,CUDA BFloat16 neg (#45240)
5a0514e3e6,skip,[pytorch] Update fmt to 7.0.3 (#45304)
22401b850b,jit,port all JIT tests to gtest (#45264)
c8166d4b58,typing_frontend,Add `torch.cuda.comm` to typechecking CI (#45350)
f07ac6a004,distributed,Fix Windows build failure after DDP PR merged (#45335)
d5748d9a1a,foreach_frontend,Enable binary ops with Scalar Lists with for foreach APIs (#45298)
27ab9bc0f9,rpc,[RPC profiling] Extend RPC profiling to support async function execution over RPC. (#44664)
7c5436d557,rpc,[RPC profiling] Add tests to ensure RPC profiling works on single threaded (#44923)
eee7dad376,fx,"Add torch.do_assert, which is symbolically traceable (#45188)"
04be420549,mobile,[static runtime] Remove ops in static from backwards compatibility checks (#45354)
3b7e4f89b2,rpc,Add deprecation warning to PG backend and make TP backend stable. (#45356)
a2b4177c5b,distributed,Add barrier() at the end of init_process_group and new_group. (#45181)
37513a1118,improvements_frontend,Use explicit templates in CUDALoops kernels (#44286)
439930c81b,improvements_frontend,adding a beta parameter to the smooth_l1 loss fn (#44433)
5211fb97ac,distributed,Remove device maps from TensorPipe for v1.7 release (#45353)
8ab2ad306d,typing_frontend,Enable `torch.cuda.nccl` typechecking (#45344)
0444c372e1,new_features_frontend,"[optimizer] introduce optimizer functional API, refactor Adagrad (#44715)"
08caf15502,new_features_frontend,[optimizer] refactor Adam to use functional API (#44791)
32c355af5b,distributed,[dist_optim] introduce distributed functional optimizer (#45221)
606b1a9a2e,skip,Move xla codegen to aten. (#45241)
958c208666,quantization,[quant] conv_transpose graph patterns (#45078)
d2bd556e7d,skip,Quantization: add API summary section (#45093)
278da57255,skip,Quantization: combine previous summary with new summary (#45135)
eb39624394,skip,quant docs: add reduce_range explanatation to top level doc (#45305)
7763e1d7b1,skip,quant docs: document how to customize qconfigs in eager mode (#45306)
92189b34b7,fx,Add get_all_users_of function to GraphManipulation (#45216)
d9af3d2fcd,quantization,[quant] ConvTranspose warnings (#45081)
304e1d1e19,distributed,[Distributed] getNumKeys API to c10d TCPStore (#43962)
addf94f2d6,skip,[Distributed] DeleteKey API for c10d TCP Store (#43963)
cf808bed73,skip,[Distributed] Adding Python tests for the TCPStore getNumKeys and deleteKey (#45223)
0fa551f0ab,caffe2,[c2] Fix int types for learning rate
2b21e7767e,skip,Added optimizers based on multi tensor apply (#45299)
19dda7c68a,distributed,Fallback to CPU when remote end does not have CUDA for profiling (#44967)
23dfca8351,rpc,Support record_shapes in RPC profiling (#44419)
e52762cbb7,skip,Revert D23917034: quant docs: document how to customize qconfigs in eager mode
54a253fded,skip,Revert D23931987: Added optimizers based on multi tensor apply
3da1061059,skip,Revert D23916669: quant docs: add reduce_range explanatation to top level doc
110aa45387,skip,Revert D23842456: Quantization: combine previous summary with new summary
37a671abc7,skip,Revert D23828257: Quantization: add API summary section
8cef7326f4,opbench,Benchmarks: add 'default' options for fuser and executor. (#45347)
a07d82982a,skip,CI: Add a run of FastRNN benchmarks in default executor/fuser configuration. (#45348)
bc5710f2f7,opbench,Benchmarks: tweak PE config settings. (#45349)
f84b2e865f,skip,Revert D23878455: [Distributed] Adding Python tests for the TCPStore getNumKeys and deleteKey
78caa028b6,skip,Revert D23009117: [Distributed] DeleteKey API for c10d TCP Store
4005afe94b,onnx,[ONNX] Update narrow for dynamic inputs (#44039)
5b839bca78,onnx,[ONNX] Optimize export_onnx api to reduce string and model proto exchange (#44332)
8b143771d0,skip,Updates and simplifies nonzero as_tuple behavior
8bdbedd4ee,skip,"Revert ""Updates and simplifies nonzero as_tuple behavior"""
c3bf402cbb,onnx,handle onnx nll with default ignore index (#44816)
13f76f2be4,jit,Fix preserve submodule attribute in freezing (#45143)
95a97e51b5,onnx,[ONNX] Improve scripting inplace indexing ops (#44351)
7818a214c5,skip,[skip][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
6417a70465,docs_frontend,Updates linalg warning + docs (#45415)
e4950a093a,new_features_frontend,Backward support for generalized eigenvalue solver with LOBPCG in forward [only k-rank SYMEIG case] (#43002)
e2ffdf467a,releng,docker: Add torchelastic to docker image (#45438)
48d29c830d,skip,[hotfix] disable problematic cuda tests on rocm builds (#45435)
e5242aaf89,skip,Update TensorPipe submodule (#45433)
993628c74a,jit,Build shape expressions and remove outputs that are only used by `aten::size`s (#45080)
03342af3a3,new_features_frontend,Add env variable to bypass CUDACachingAllocator for debugging (#45294)
36c3fbc9e3,new_features_frontend,CUDA BFloat Conv (non-cuDNN) (#45007)
6ab1c0b1ca,jit,Disable a few tests in preparation to enabling PE+TE (#44815)
87b356d093,jit,[static runtime] Split out graph preparation from runtime (#44131)
722faeb2a4,new_features_frontend,[RELAND] Added optimizers based on multi tensor apply (#45408)
47debdca42,distributed,Document change for DDP enabled on Windows platform (#45392)
9163e8171e,caffe2,Adding Type Double to Caffe2 Mean Op
57c18127dc,onnx,[ONNX] Update div export to perform true divide (#44831)
7a4c417ed3,skip,Fix typo (#45379)
986af53be2,typing_frontend,type check for torch.testing._internalcodegen:* (#45368)
5d1fee23b3,skip,Remove convert_target from NN tests. (#45291)
a77d633db1,onnx,[ONNX] Fix view for dynamic input shape (#43558)
c6b7eeb654,new_features_frontend,Gh/taylorrobie/timer cleanup (#45361)
a4486fe7ba,amd,[ROCm] Print name irrespective of seq number assignment for roctx traces (#45229)
1097fe0088,skip,Remove CriterionTest.test_cuda code for dtype None. (#45316)
190f91e3db,caffe2,Adding Histogram Binning Calibration to DSNN and Adding Type Double to Caffe2 ParallelSumOp/SumReluOp
6b65b3cbd8,distributed,[Distributed] DeleteKey API for c10d TCP Store (#45401)
331ebaf7cb,distributed,[Distributed] Adding Python tests for the TCPStore getNumKeys and deleteKey (#45402)
e54e1fe51e,package,[package] Add dependency viz (#45214)
6a206df891,visualization,20000x faster audio conversion for SummaryWriter (#44201)
96f8755034,bug_fixes_frontend,Fixed handling of nan for evenly_distribute_backward (#45280)
49b198c454,typing_frontend,type check for torch.testing._internal.common_utils (#45375)
5855aa8dac,typing_frontend,Type check quasirandom (#45434)
7ac872b934,jit,[JIT] Modify to_backend API so that it accepts wrapped modules (#43612)
52cbc9e4ec,jit,[TensorExpr] Always inline and DCE in the LLVM backend (#45445)
4af4b71fdc,jit,[JIT] Update docs for recently added features (#45232)
a0f0cb1608,jit,[JIT] Add test for ignored class type property (#45233)
0c8a6008ac,complex_frontend,Fix torch.pow when the scalar base is a complex number (#45259)
8c66cd120b,complex_frontend,Disable complex inputs to torch.round (#45330)
208df1aeb8,releng,Use python 3.8 in pytorch docker image (#45466)
534f2ae582,complex_frontend,Disable inplace abs for complex tensors (#45069)
6967e6295e,distributed,Fix DDP docs (#45454)
c5ade5f698,distributed,Fix no_sync docs (#45455)
8e47fcba5f,rpc,Update docs for RPC async_execution (#45458)
5be954b502,rpc,Fix WorkerInfo link format (#45476)
5a6a31168f,releng,add circle ci job name dimension to report test stats (#45457)
35596d39e9,performance_frontend,Coalesce TLS accesses in RecordFunction constructor (#44970)
50b91103a9,improvements_frontend,add self cuda time to avoid double/quadruple counting (#45209)
dddb685c11,skip,This PR flips a switch to enable PE + TE (#45396)
417e3f85e5,skip,Support tuple inputs in NN Module test (#44853)
b0bdc82a00,fx,[FX][EZ] Fix bug where copying node made non-unique name (#45311)
6bdb871d47,fx,[FX] Lint pass for Graphs (#44973)
8c309fc052,foreach_frontend,Add more tests for mt optimizers (#45475)
d2623da52c,misc,replaced whitelist with allowlist (#45260)
92306b85d5,skip,"[TensorExpr] Consolidate {buffer,function,tensor}.{h.cpp} in tensor.{h,cpp}. (#45388)"
3c33695a6d,skip,[TensorExpr] Rename `Buffer` to `Placeholder`. (#45389)
b86008ab75,jit,[TensorExpr] Remove buf_ field from class Tensor. (#45390)
bb478810e0,quantization,[quant] torch.max_pool1d (#45152)
489af4ddcb,quantization,[quant] Add quant APIs to save/load observer state_dict (#44846)
1ed1a2f5b0,skip,[wip] fast typeMeta/ScalarType conversion approach 2 (#44965)
56af122659,skip,Revert D23966878: [pytorch][PR] This PR flips a switch to enable PE + TE
37f9af7f29,bug_fixes_frontend,Missing tests about torch.xxx(out=...) (#44465)
87f98a5b54,docs_frontend,Updates torch.floor_divide documentation to clarify it's actually torch.trunc_divide (or torch.rtz_divide) (#45411)
0806c58e9f,complex_frontend,Optimize view_as_complex and view_as_real (#44908)
7cde662f08,complex_frontend,Add check for Complex Type to allow non integral alpha. (#45200)
6d37126a10,bug_fixes_frontend,Makes rdiv consistent with div (#45407)
0a38aed025,skip,Auto set libuv_ROOT env var for Gloo submodule on Windows platform (#45484)
bb19a55429,docs_frontend,Improves fft doc consistency and makes deprecation warnings more prominent (#45409)
df0de780c3,bug_fixes_frontend,Add cusolver guard for cuda >= 10.1.243 (#45452)
b3135c2056,typing_frontend,Enable torch.cuda.amp typechecking (#45480)
ab5edf21b0,skip,Revert D23789657: [wip] fast typeMeta/ScalarType conversion approach 2
fe9019cbfe,skip,Reorganized Sorting.cpp method order (#45083)
2c300fd74c,vulkan,[android][vulkan] Module load argument to specify device cpu/vulkan (#44896)
17be7c6e5c,vulkan,[vulkan][android][test_app] Add test_app variant that runs module on Vulkan (#44897)
5f49d14be2,mobile,Add mobile_optimized tag to optimized model. (#45479)
09b3e16b40,jit,[JIT] Enable @unused syntax for ignoring properties (#45261)
ea59251f51,jit,Fix model_name not logged properly issue. (#45488)
15f85eea18,complex_frontend,Support bfloat16 and complex dtypes for logical_not (#43537)
f47fd0eb72,complex_frontend,Updated `cholesky_backward` for complex inputs (#45267)
aa2bd7e1ae,performance_frontend,Conservative-ish persistent RNN heuristics for compute capability 8.0+ (#43165)
b2925671b6,bug_fixes_frontend,"Updates deterministic flag to throw a warning, makes docs consistent (#45410)"
6e55a26e10,mobile,Move mobile specific CPUCachingAllocator to c10/mobile folder. (#45364)
0df99ad470,performance_frontend,Remove unnecessary __at_align32__ in int_elementwise_binary_256 (#45470)
b66ac1e928,deprecations_frontend,Updates nonzero's as_tuple behavior to no longer warn. (#45413)
147c88ef2d,new_features_frontend,Add docs to a pytorch.github.io/doc/tag directory when repo is tagged (#45204)
18876b5722,complex_frontend,Update backward formula for torch.dot and add backward definition for torch.vdot (#45074)
ab5cf16b6c,bug_fixes_frontend,fix standard deviation gradient NaN behavior (#45468)
d642992877,quantization,Quantized operators template selective (#45509)
ef41472544,fx,Create experimental FX graph manipulation library (#44775)
06a566373a,distributed,[PyTorch/NCCL] Fix async error handling (#45456)
637570405b,foreach_frontend,Disable multi tensor tesnor tests on rocm (#45535)
22a34bcf4e,amd,ROCm {emoji:2764} TensorExpr (#45506)
33aba57e4c,skip,Patch generate files for system protobuf (#44583)
eb39542e67,typing_frontend,Add typing annotations for torch.utils.data.* modules (#44136)
375a83e6c1,typing_frontend,Annotate torch.utils.(tensorboard/show_pickle/hypify) (#44216)
c1e6592964,typing_frontend,Enable type-checking of torch.nn.quantized.* modules (#43110)
0a15646e15,new_features_frontend,CUDA RTX30 series support (#45489)
c87ff2cb90,complex_frontend,Enable transposed tensor copy for complex types (#45487)
ccad73ab41,opbench,Fix D23995953 import.
772ce9ac2c,complex_frontend,Fix memory corruption when running torch.svd for complex.doubles (#45486)
4aca63d38a,jit,[TensorExpr] Change API for creating Load and Store expressions. (#45520)
93650a82c9,jit,Move prim::tolist math.log and aten::cpu to lite interpreter for translation model (#45482)
b4ba66ae32,new_features_frontend,Print tensor shapes and convolution parameters when cuDNN exception is thrown (#45023)
c2c7099944,docs_frontend,"Fix docs for kwargs, q-z (#43589)"
f5c95d5cf1,new_features_frontend,Source code level attribution in profiler (#43898)
cf07ba50fe,skip,Update target determinator to point to release/1.7
43404c4141,vmap_frontend,[1.7] Remove torch.vmap (#45571)
e8cea53b85,complex_frontend,Add allowlist for complex backward (#45602)
07e66d7ca5,jit,Enable PE + TE (#45546) (#45591)
fc8f987c1a,Uncategorized,Make torch.package private and add a big warning (#45628)
543d09736d,Uncategorized,[1.7] Hide FX (#45631)
1ffcdd000b,Uncategorized,patch https://github.com/pytorch/pytorch/pull/45586 (#45601)
d728e234cb,Uncategorized,[1.7] .jenkins: switch to compare against stable and update allowlist (#45859)
8107dba211,Uncategorized,Upgrade README for Windows (#45553) (#45738)
30d41faf3b,Uncategorized,SET USE_DISTRIBUTED OFF when libuv is not installed (#45554) (#45739)
7d0c7b38b5,Uncategorized,[iOS] 1.7 hotfix (#45891)
ebe8b21b08,Uncategorized,quant docs: add API summary section (#45848)
bf638d5ebf,Uncategorized,[Docs] Adding Store API Docs (#45543) (#45758)
9927825829,Uncategorized,Fix cuDNN error message when it's Conv2d (#45729) (#45770)
8f8da6097b,Uncategorized,[ONNX] Update embedding_bag export (#44693) (#45756)
173a719d36,Uncategorized,[ONNX] Add dim_param support in export with onnx shape inference (#44920) (#45755)
65a1827c19,Uncategorized,Disable angle backwards and handle r to c backward for add (#45839)
